## 大模型算法

### 一、PyTorch、Transformers

#### 1. 模型评价指标

**混淆矩阵**

用来看每个类别之间的准确率，常用来做目标分类

![image-20251021090747400](assets/image-20251021090747400.png)

代码如下

```python
from sklearn.metrics import confusion_matrix
def compute_confusion_matrix(labels,pred_labels_list,gt_labels_list):
    pred_labels_list = np.asarray(pred_labels_list)
    gt_labels_list = np.assarray(gt_labels_list)
    matrix = confusion_matrix(test_label_list,
                              pred_label_list,
                              labels=labels)
    return matrix
```

**Overall Accuracy**

OA 代表了所有预测正确的样本占总样本的比例，公式为 $OA = \frac{TP+TN}{TP+TN+FP+FN}$ 

**Average accuracy**

$AA = \frac{(\frac{TP}{TP+FN} + \frac{TN}{TN+FP})}{2}$

**Recall**

$Recall = \frac{TP}{TP+FN}$，正样本被正确识别的比例

**Precision**

$Precsion = \frac{TP}{TP+FP}$，预测正确的正样本所占的比例

**F1**

$F1 = 2 \times \frac{P \times R}{P + R}$ ，召回率和精确率的加权平均，值越高表明鲁棒性越好

**PR曲线**

横轴是召回率，纵轴代表了P（精确率)，P-R曲线上的一个点代表着，在某一阈值下，模型将大于该阈值的结果判定为正样本，小于该阈值的结果判定为负样本，此时返回结果对应的召回率和精确率。整条P-R曲线是通过将阈值从高到低移动而生成的。原点附近代表当阈值最大时模型的精确率和召回率，在PR曲线我们可以从图中直观的看到某一个曲线被另外一条曲线完全包裹，所包围的面积大于另一条曲线包围的面积，举例图中可以说明A模型的性能优于B和C。

![image-20251021092018100](assets/image-20251021092018100.png)

**置信度**

在目标检测中，我们通常需要将边界框内物体划分为正样本和负样本。我们使用置信度这个指标来进行划分，当小于置信度设置的阈值判定为负样本（背景），大于置信度设置的阈值判定为正样本.

**IOU**

$IOU = \frac{A \bigcap B}{A \bigcup B}$ 分子部分是模型预测框与真实标注框之间的重叠区域，分母部分是两者的并集，预测框和实际框所占有的总区域，在实际模型识别时会根据我们自己设定合适的阈值来判定正负样本。

![image-20251021092234082](assets/image-20251021092234082.png)

**AP、mAP**

PR 曲线包围的面积，mAP是所有类的 AP 值的平均

**交叉熵损失和 KL 散度**

1. **信息熵（Entropy）**

信息熵用于表示随机变量不确定性的度量。它反映了信息源的混乱程度，也就是如果要描述 1 bit 的信息需要多少bit：

- **公式**：
  $$
  H(P) = - \sum_i P(x_i) \log P(x_i)
  $$

- **高熵**：当数据分布较为分散时，信息熵较高，表示不确定性大。

- **低熵**：当数据集中在某些特定值上时，信息熵较低，表示不确定性较小。

2. **交叉熵（Cross-Entropy）**

交叉熵衡量的是预测分布与真实分布之间的信息差异，常用于分类问题中

- **公式**：
  $$
  H(P, Q) = - \sum_i P(x_i) \log Q(x_i)
  $$

- **用途**：它用来衡量实际分布 $P$ 和预测分布 $Q$ 之间的差异。

- 在大模型中，通常  $P(x)=1$ ，即 token 应该输出是确定的

3. **KL 散度（Kullback-Leibler Divergence）**

KL 散度衡量的是从分布 $Q$ 到分布 $P$ 的信息损失：

- **公式**：
  $$
  D_{KL}(P \parallel Q) = \sum_i P(x_i) \log \frac{P(x_i)}{Q(x_i)}
  $$

- **KL 散度与交叉熵的关系**：
  $$
  D_{KL}(P \parallel Q) = H(P, Q) - H(Q)
  $$
  这表明，KL 散度等于交叉熵与真实分布的熵之间的差异。 KL 散度是表示用 $Q$ 去衡量 $P$ 的时候需要多用的比特数

  在大模型训练中，通常使用交叉熵作为损失函数，因为训练数据提供的真实分布 $P$ 是一个 one-hot 向量即模型明确告诉我们期望的 token 是哪一个，因此它的信息熵为 0

4. **KL 散度的物理意义**

- KL 散度表示从分布 $Q$ 编码真实分布 $P$ 所需的额外信息量。通常它是大于零的，且只有当 $P = Q$ 时，KL 散度等于零。

- **为什么 KL 不是一个真正的距离度量？**

  1. KL 散度不满足对称性，即 $KL(P \parallel Q) \neq KL(Q \parallel P)$。
  2. KL 散度不满足三角不等式。

  因此，KL 散度不是一个距离度量。

5. **KL 散度在损失函数中的应用**

KL 散度可以作为损失函数使用，尤其在训练模型时，KL 散度帮助优化模型的预测分布，使其尽可能接近真实分布。具体来说，当真实分布 $Q$ 固定时，KL 散度越小，模型预测的分布与真实分布越接近。

#### 2. 张量

创建

```python
import torch
# 随机创建一个四行三列的向量
x = torch.randn(4, 3)
x = torch.zeros(4, 3, dtype = torch.long)
x = torch.tensor([5.5, 3]) # 创建一个包含两个元素的向量
x = torch.randn_like(x, dtype = torch.float)
y = x.int() # 转换成整数类型
```

获取维度信息

```python
print(x.size())
print(x.shape)
```

```python
single_distance = torch.tensor(25) 
print(single_distance.shape)
# torch.tensor([]) 这是一个标量，最低维度是[1, 1]
with_batch = single_distance.unsqueeze(0)
# torch.tenosr([1])
ready_for_model = with_distance.unsqueeze(1)
# torch.tenosr([1, 1])
value = ready_for_model[0].item() 
```

常见 Tensor 的构建方法

| 函数                | 功能                                              |
| :------------------ | :------------------------------------------------ |
| Tensor(sizes)       | 基础构造函数                                      |
| tensor(data)        | 类似于np.array                                    |
| ones(sizes)         | 全1                                               |
| zeros(sizes)        | 全0                                               |
| eye(sizes)          | 对角为1，其余为0                                  |
| arange(s,e,step)    | 从s到e，步长为step                                |
| linspace(s,e,steps) | 从s到e，均匀分成step份                            |
| rand/randn(sizes)   | rand是[0,1)均匀分布；randn是服从N(0，1)的正态分布 |
| normal(mean,std)    | 正态分布(均值为mean，标准差是std)                 |
| randperm(m)         | 随机排列                                          |

操作

```python
import torch
y = torch.rand(4, 3)
print(x + y)
print(torch.add(x, y))
```

```python
x = torch.rand(4, 3)
print(x[:, 1])
# 输出
tensor([-0.0720,  0.0666,  1.0336, -0.6965])
# 索引出来的结果与原数据共享内存，修改一个另一个也会更改，如果不想更改使用copy()等方法
y = x[0, :]
```

```python
x = torch.randn(4, 4)
# torch.view 也是共享内存的
y = x.view(16) # 或者torch.reshape()
z = x.view(-1, 8)
print(x.size(), y.size(), z.size())
torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])
```

广播

当两个向量最后一维对齐，如果这一维度是相同的或者其中之一是1的时候，视为相同，可以将维度为1的那个张量广播

```python
x = torch.arange(1, 3).view(1, 2)
print(x)
y = torch.arange(1, 4).view(3, 1)
print(y)
print(x + y)
```

```test
tensor([[1, 2]])
tensor([[1],
        [2],
        [3]])
tensor([[2, 3],
        [3, 4],
        [4, 5]])
```

自动求导

默认 `requires_grad` 为 `False`

如果设置它的属性` .requires_grad` 为 `True`，那么它将会追踪对于该张量的所有操作。当完成计算后可以通过调用` .backward()`，来自动计算所有的梯度。这个张量的所有梯度将会自动累加到`.grad`属性。

在 y.backward() 时，如果 y 是标量，则不需要为 backward() 传入任何参数；否则，需要传入一个与 y 同形的Tensor。

代码块包装在 `with torch.no_grad(): `中，在评估模型时特别有用，因为模型可能具有 `requires_grad = True` 的可训练的参数，但是我们不需要在此过程中对他们进行梯度计算。

```python
x = torch.rann(4, 3, requires_grad = True)
```

每个张量都有一个`.grad_fn`属性，该属性引用了创建 `Tensor `自身的`Function`(除非这个张量是用户手动创建的，即这个张量的`grad_fn`是 `None` )。上面给出的例子中，张量由用户手动创建，因此grad_fn返回结果是None。

```python
y = x**2
print(y)

tensor([[1., 1.],
        [1., 1.]], grad_fn=<PowBackward0>)
```

反向传播是累加的

```python
# 再来反向传播⼀一次，注意grad是累加的
out2 = x.sum()
out2.backward()
print(x.grad)

out3 = x.sum()
x.grad.data.zero_()
out3.backward()
print(x.grad)
```

#### 3. 基本流程

自己定制 Dataset

```python
import os
import pandas as pd
from torchvision.io import read_image

class MyDataset(Dataset):
    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):
        """
        Args:
            annotations_file (string): Path to the csv file with annotations.
            img_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
            target_transform (callable, optional): Optional transform to be applied
                on the target.
        """
        self.img_labels = pd.read_csv(annotations_file)
        self.img_dir = img_dir
        self.transform = transform
        self.target_transform = target_transform

    def __len__(self):
        return len(self.img_labels)

    def __getitem__(self, idx):
        """
        Args:
            idx (int): Index
        """
        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])
        image = read_image(img_path)
        label = self.img_labels.iloc[idx, 1]
        if self.transform:
            image = self.transform(image)
        if self.target_transform:
            label = self.target_transform(label)
        return image, label
```

设计 dataloader

```python
transform = transforms.Compose([
  transforms.ToTensor(),
  transforms.Normalize((mean,),(std,))
])

dataset = SomeDataset('./data', train = True, download = True, transform = transform)

dataset_loader = DataLoader(train_dataset, batch_size = 32, shuffle = True)

for batch_index, (data, labels) in enumerate(train_loader):
  output = model(data)
```

神经网络的构造

`Module` 类是 `torch.nn` 模块里提供的一个模型构造类，是所有神经网络模块的基类，我们可以继承它来定义我们想要的模型。下面继承 Module 类构造多层感知机。这里定义的 MLP 类重载了 `Module` 类的 `__init__` 函数和 `forward` 函数。它们分别用于创建模型参数和定义前向计算（正向传播）。下面的 MLP 类定义了一个具有两个隐藏层的多层感知机。

```python
import torch
from torch import nn

class MLP(nn.Module):
  # 声明带有模型参数的层，这里声明了两个全连接层
  def __init__(self, **kwargs):
    # 调用MLP父类Block的构造函数来进行必要的初始化。这样在构造实例时还可以指定其他函数
    super(MLP, self).__init__(**kwargs)
    self.hidden = nn.Linear(784, 256)
    self.act = nn.ReLU()
    self.output = nn.Linear(256,10)
    
   # 定义模型的前向计算，即如何根据输入x计算返回所需要的模型输出
  def forward(self, x):
    o = self.act(self.hidden(x))
    return self.output(o)   
```

网络实例化

```python
X = torch.rand(2,784) # 设置一个随机的输入张量
net = MLP() # 实例化模型
print(net) # 打印模型
net(X) # 前向计算
```

```
MLP(
  (hidden): Linear(in_features=784, out_features=256, bias=True)
  (act): ReLU()
  (output): Linear(in_features=256, out_features=10, bias=True)
)
tensor([[ 0.0149, -0.2641, -0.0040,  0.0945, -0.1277, -0.0092,  0.0343,  0.0627,
         -0.1742,  0.1866],
        [ 0.0738, -0.1409,  0.0790,  0.0597, -0.1572,  0.0479, -0.0519,  0.0211,
         -0.1435,  0.1958]], grad_fn=<AddmmBackward>)
```

我们只需要定义 `forward` 函数，`backward`函数会在使用`autograd`时自动定义，`backward`函数用来计算导数。我们可以在 `forward` 函数中使用任何针对张量的操作和计算。

一个模型的可学习参数可以通过`net.parameters()`返回

```python
params = list(net.parameters())
print(len(params))
print(params[0].size())  # conv1的权重
```

`torch.nn`只支持小批量处理 (mini-batches）。整个 `torch.nn` 包只支持小批量样本的输入，不支持单个样本的输入。比如，`nn.Conv2d` 接受一个4维的张量，即`nSamples x nChannels x Height x Width `如果是一个单独的样本，只需要使用`input.unsqueeze(0)` 来添加一个“假的”批大小维度。

- `torch.Tensor` - 一个多维数组，支持诸如`backward()`等的自动求导操作，同时也保存了张量的梯度。
- `nn.Module `- 神经网络模块。是一种方便封装参数的方式，具有将参数移动到GPU、导出、加载等功能。
- `nn.Parameter `- 张量的一种，当它作为一个属性分配给一个`Module`时，它会被自动注册为一个参数。
- `autograd.Function` - 实现了自动求导前向和反向传播的定义，每个`Tensor`至少创建一个`Function`节点，该节点连接到创建`Tensor`的函数并对其历史进行编码。

**模型初始化**

先定义模型初始化参数

```python
def initialize_weights(model):
	for m in model.modules():
		# 判断是否属于Conv2d
		if isinstance(m, nn.Conv2d):
			torch.nn.init.zeros_(m.weight.data)
			# 判断是否有偏置
			if m.bias is not None:
				torch.nn.init.constant_(m.bias.data,0.3)
		elif isinstance(m, nn.Linear):
			torch.nn.init.normal_(m.weight.data, 0.1)
			if m.bias is not None:
				torch.nn.init.zeros_(m.bias.data)
		elif isinstance(m, nn.BatchNorm2d):
			m.weight.data.fill_(1) 		 
			m.bias.data.zeros_()	
```

然后定义模型

```python
# 模型的定义
class MLP(nn.Module):
  # 声明带有模型参数的层，这里声明了两个全连接层
  def __init__(self, **kwargs):
    # 调用MLP父类Block的构造函数来进行必要的初始化。这样在构造实例时还可以指定其他函数
    super(MLP, self).__init__(**kwargs)
    self.hidden = nn.Conv2d(1,1,3)
    self.act = nn.ReLU()
    self.output = nn.Linear(10,1)
    
   # 定义模型的前向计算，即如何根据输入x计算返回所需要的模型输出
  def forward(self, x):
    o = self.act(self.hidden(x))
    return self.output(o)

mlp = MLP()
print(mlp.hidden.weight.data)
print("-------初始化-------")

mlp.apply(initialize_weights)
# 或者initialize_weights(mlp)
print(mlp.hidden.weight.data)
```

首先应该设置模型的状态：如果是训练状态，那么模型的参数应该支持反向传播的修改；如果是验证/测试状态，则不应该修改模型参数。在PyTorch中，模型的状态设置非常简便，如下的两个操作二选一即可：

```
model.train()   # 训练状态
model.eval()   # 验证/测试状态
```

我们前面在DataLoader构建完成后介绍了如何从中读取数据，在训练过程中使用类似的操作即可，区别在于此时要用for循环读取DataLoader中的全部数据。

```
for data, label in train_loader:
```

之后将数据放到GPU上用于后续计算，此处以.cuda()为例

```
data, label = data.cuda(), label.cuda()
```

开始用当前批次数据做训练时，应当先将优化器的梯度置零：

```
optimizer.zero_grad()
```

之后将data送入模型中训练：

```
output = model(data)
```

根据预先定义的criterion计算损失函数：

```
loss = criterion(output, label)
```

将loss反向传播回网络：

```
loss.backward()
```

使用优化器更新模型参数：

```
optimizer.step()
```

这样一个训练过程就完成了，后续还可以计算模型准确率等指标，这部分会在下一节的图像分类实战中加以介绍。

验证/测试的流程基本与训练过程一致，不同点在于：

- 需要预先设置torch.no_grad，以及将model调至eval模式
- 不需要将优化器的梯度置零
- 不需要将loss反向回传到网络
- 不需要更新optimizer

完整的训练和验证流程

```python
def train(epoch):
    model.train()
    train_loss = 0
    for data, label in train_loader:
        data, label = data.cuda(), label.cuda()
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, label)
        loss.backward()
        optimizer.step()
        train_loss += loss.item()*data.size(0)
    train_loss = train_loss/len(train_loader.dataset)
		print('Epoch: {} \tTraining Loss: {:.6f}'.format(epoch, train_loss))
    

def val(epoch):       
    model.eval()
    val_loss = 0
    with torch.no_grad():
        for data, label in val_loader:
            data, label = data.cuda(), label.cuda()
            output = model(data)
            preds = torch.argmax(output, 1)
            loss = criterion(output, label)
            val_loss += loss.item()*data.size(0)
            running_accu += torch.sum(preds == label.data)
    val_loss = val_loss/len(val_loader.dataset)
    print('Epoch: {} \tTraining Loss: {:.6f}'.format(epoch, val_loss))
```

模型保存

```python
from torchvision import models
model = models.resnet152(pretrained=True)
save_dir = './resnet152.pth'

# 保存整个模型
torch.save(model, save_dir)
# 保存模型权重
torch.save(model.state_dict, save_dir)
```

#### 4. 神经网络常见的层

* **无参数层**

```python
import torch
from torch import nn

class MyLayer(nn.Module):
    def __init__(self, **kwargs):
        super(MyLayer, self).__init__(**kwargs)
    def forward(self, x):
        return x - x.mean()  
```

* **含模型参数的层**

`Parameter` 类其实是 `Tensor` 的子类，如果一个 `Tensor` 是 `Parameter` ，那么它会⾃动被添加到模型的参数列表里。所以在⾃定义含模型参数的层时，我们应该将参数定义成 `Parameter` ，除了直接定义成 `Parameter` 类外，还可以使⽤ `ParameterList` 和 `ParameterDict` 分别定义参数的列表和字典。

> torch.mm 是矩阵的乘积，不支持广播只支持二维的，torch.matmul是支持广播和多维的

```python
class MyListDense(nn.Module):
    def __init__(self):
        super(MyListDense, self).__init__()
        self.params = nn.ParameterList([nn.Parameter(torch.randn(4, 4)) for i in range(3)])
        self.params.append(nn.Parameter(torch.randn(4, 1)))

    def forward(self, x):
        for i in range(len(self.params)):
            x = torch.mm(x, self.params[i])
        return x
net = MyListDense()
print(net)
```

```python
class MyDictDense(nn.Module):
    def __init__(self):
        super(MyDictDense, self).__init__()
        self.params = nn.ParameterDict({
                'linear1': nn.Parameter(torch.randn(4, 4)),
                'linear2': nn.Parameter(torch.randn(4, 1))
        })
        self.params.update({'linear3': nn.Parameter(torch.randn(4, 2))}) # 新增

    def forward(self, x, choice='linear1'):
        return torch.mm(x, self.params[choice])

net = MyDictDense()
print(net)
```

* **二维卷积层**

```python
import torch
from torch import nn

# 卷积运算（二维互相关）
def corr2d(X, K): 
    h, w = K.shape
    X, K = X.float(), K.float()
    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            Y[i, j] = (X[i: i + h, j: j + w] * K).sum()
    return Y

# 二维卷积层
class Conv2D(nn.Module):
    def __init__(self, kernel_size):
        super(Conv2D, self).__init__()
        self.weight = nn.Parameter(torch.randn(kernel_size))
        self.bias = nn.Parameter(torch.randn(1))

    def forward(self, x):
        return corr2d(x, self.weight) + self.bias
```

#### 5. pipeline 

Transformers 库最基础的对象就是 `pipeline()` 函数，它封装了预训练模型和对应的前处理和后处理环节。我们只需输入文本，就能得到预期的答案。目前常用的 [pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines) 有：

- `feature-extraction` （获得文本的向量化表示）
- `fill-mask` （填充被遮盖的词、片段）
- `ner`（命名实体识别）
- `question-answering` （自动问答）
- `sentiment-analysis` （情感分析）
- `summarization` （自动摘要）
- `text-generation` （文本生成）
- `translation` （机器翻译）
- `zero-shot-classification` （零训练样本分类）

```python
from transformers import pipeline

classifier = pipeline("sentiment-analysis")
result = classifier("I've been waiting for a HuggingFace course my whole life.")
print(result)
results = classifier(
  ["I've been waiting for a HuggingFace course my whole life.", "I hate this so much!"]
)
print(results)
```

```
No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)

[{'label': 'POSITIVE', 'score': 0.9598048329353333}]
[{'label': 'POSITIVE', 'score': 0.9598048329353333}, {'label': 'NEGATIVE', 'score': 0.9994558691978455}]

```

pipeline会自动加载模型，实际他背后经过了三个步骤

![image-20251123201947558](assets/image-20251123201947558.png)

因为神经网络模型无法直接处理文本，因此首先需要通过**预处理**环节将文本转换为模型可以理解的数字。具体地，我们会使用每个模型对应的分词器 (tokenizer) 来进行：

1. 将输入切分为词语、子词或者符号（例如标点符号），统称为 **tokens**；
2. 根据模型的词表将每个 token 映射到对应的 token 编号（就是一个数字）；
3. 根据模型的需要，添加一些额外的输入。

我们对输入文本的预处理需要与模型自身预训练时的操作完全一致，只有这样模型才可以正常地工作。注意，每个模型都有特定的预处理操作。这里我们使用 `AutoTokenizer` 类和它的 `from_pretrained()` 函数，它可以自动根据模型 checkpoint 名称来获取对应的分词器。

```python
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.",
    "I hate this so much!",
]
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt")
print(inputs)

# output
{
    'input_ids': tensor([
        [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172, 2607,  2026,  2878,  2166,  1012,   102],
        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,
             0,     0,     0,     0,     0,     0]
    ]), 
    'attention_mask': tensor([
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]
    ])
}
```

```python
from transformers import AutoModel

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModel.from_pretrained(checkpoint)
outputs = model(**inputs)
print(outputs.last_hidden_state.shape)

# 输出是torch.Size([2, 16, 768])，分别是batch_size,Sequence length，Hidden size 
```

然后 `output.logits` 输出的是这种 (下面是两句话，做情感分析的，所以一个tensor对应两个维度)

```
tensor([[-1.5607,  1.6123],
        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward0>)
```

还需要经过一个softmax处理，得到最终的概率输出

#### 6. 模型

除了像之前使用 `AutoModel` 根据 checkpoint 自动加载模型以外，我们也可以直接使用模型对应的 `Model` 类，例如 BERT 对应的就是 `BertModel`：

```python
from transformers import BertModel

model = BertModel.from_pretrained("bert-base-cased")
```

注意，**在大部分情况下，我们都应该使用 `AutoModel` 来加载模型。**这样如果我们想要使用另一个模型（比如把 BERT 换成 RoBERTa），只需修改 checkpoint，其他代码可以保持不变。

保存模型通过调用 `Model.save_pretrained()` 函数实现，例如保存加载的 BERT 模型：

```python
from transformers import AutoModel

model = AutoModel.from_pretrained("bert-base-cased")
model.save_pretrained("./models/bert-base-cased/")
```

这会在保存路径下创建两个文件：

- *config.json*：模型配置文件，存储模型结构参数，例如 Transformer 层数、特征空间维度等；
- *pytorch_model.bin*：又称为 state dictionary，存储模型的权重。

简单来说，配置文件记录模型的**结构**，模型权重记录模型的**参数**，这两个文件缺一不可。我们自己保存的模型同样通过 `Model.from_pretrained()` 加载，只需要传递保存目录的路径。

#### 7. 分词器

由于神经网络模型不能直接处理文本，因此我们需要先将文本转换为数字，这个过程被称为**编码 (Encoding)**，其包含两个步骤：

1. 使用分词器 (tokenizer) 将文本按词、子词、字符切分为 tokens；
2. 将所有的 token 映射到对应的 token ID。

分词器的加载与保存与模型相似，使用 `Tokenizer.from_pretrained()` 和 `Tokenizer.save_pretrained()` 函数。例如加载并保存 BERT 模型的分词器：

```python
from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained("bert-base-cased")
tokenizer.save_pretrained("./models/bert-base-cased/")
```

同样地，在大部分情况下我们都应该使用 `AutoTokenizer` 来加载分词器：

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
tokenizer.save_pretrained("./models/bert-base-cased/")
```

调用 `Tokenizer.save_pretrained()` 函数会在保存路径下创建三个文件：

- *special_tokens_map.json*：映射文件，里面包含 unknown token 等特殊字符的映射关系；
- *tokenizer_config.json*：分词器配置文件，存储构建分词器需要的参数；
- *vocab.txt*：词表，一行一个 token，行号就是对应的 token ID（从 0 开始）。

编码与解码文本

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

sequence = "Using a Transformer network is simple"
tokens = tokenizer.tokenize(sequence)

print(tokens)

# ['using', 'a', 'transform', '##er', 'network', 'is', 'simple']
```

可以看到，BERT 分词器采用的是子词切分策略，它会不断切分词语直到获得词表中的 token，例如 “transformer” 会被切分为 “transform” 和 “##er”。

然后，我们通过 `convert_tokens_to_ids()` 将切分出的 tokens 转换为对应的 token IDs：

```python
ids = tokenizer.convert_tokens_to_ids(tokens)

print(ids)
[7993, 170, 13809, 23763, 2443, 1110, 3014]
```

还可以通过 `encode()` 函数将这两个步骤合并，并且 `encode()` 会自动添加模型需要的特殊 token，例如 BERT 分词器会分别在序列的首尾添加 和 ：

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

sequence = "Using a Transformer network is simple"
sequence_ids = tokenizer.encode(sequence)

print(sequence_ids)
[101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102]
```

其中 101 和 102 分别是 [cls] 和 [sep] 对应的 token IDs。即开始和结束的字符

注意，上面这些只是为了演示。**在实际编码文本时，最常见的是直接使用分词器进行处理**，这样不仅会返回分词后的 token IDs，还包含模型需要的其他输入。例如 BERT 分词器还会自动在输入中添加 `token_type_ids` 和 `attention_mask`：

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
tokenized_text = tokenizer("Using a Transformer network is simple")
print(tokenized_text)
{'input_ids': [101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102], 
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

### 二、强化学习

#### 1. 马尔可夫决策过程

强化学习的两个实体：**智能体（Agent）**与**环境（Environment）**

强化学习中两个实体的交互，下面就是马尔可夫决策过程（MDP）的五元组：

- **状态空间S**：S即为State，指环境中所有可能状态的集合
- **动作空间A**：A即为Action，指智能体所有可能动作的集合
- **奖励R：**R即为Reward，指智能体在环境的某一状态下所获得的奖励。
- **策略P：** 即Policy，决定在给定状态下采取哪个动作的策略
- **衰减 $\gamma$ ：**衰减系数

![image-20251002100645336](assets/image-20251002100645336.png)

给定一个随机策略 $\pi(a_t \mid s_t)$，智能体与环境交互会产生一个状态、动作、奖励序列，也称为一次转移（transition），其形式为
 $(s_t, a_t, r_t, s_{t+1})$。

- $a_t$ 由策略 $\pi$ 根据当前状态 $s_t$ 采样得到。
- $s_{t+1}$ 由状态转移概率 $p_S$ 根据当前状态 $s_t$ 和动作 $a_t$ 采样得到。
- $r_t$ 由奖励概率 $p_R$ 根据当前状态 $s_t$、动作 $a_t$ 和下一状态 $s_{t+1}$ 采样得到。

在策略 $\pi$ 下，生成一条长度为 $T$ 的轨迹
$$
\tau = (s_0, a_0, r_0, s_1, a_1, r_1, s_2, \ldots, s_T)
$$
的概率为：
$$
P(\tau \mid \pi) 
= p(s_0) \prod_{t=0}^{T-1} \pi(a_t \mid s_t)\, p_S(s_{t+1} \mid s_t, a_t)\, p_R(r_t \mid s_t, a_t, s_{t+1}).
$$
**POMDP** 是指部分可观测马尔可夫决策过程，即智能体只能根据动作执行后的观测来判断当前的状态，而无法对全局进行整体感知

状态转移函数为 $p_s(s_{t + 1} |s_t, a_t) = E_{{\epsilon}_{t+1}^s}[T(s_{t+1}= W(s_t, a_t, {\epsilon}_{t + 1}^s))]$ ，即在 $s_t$ 状态执行动作 $a_t$ 后状态为 $s_{t + 1}$ 的概率，用期望是因为加入了随机噪声，有一些确定性场景，比如大模型生成中，$a_t$ 为生成下一个token，那么下一个 $s_{t+1}$ 就是确定的了

智能体目标本质就是实现下面的均衡

- **探索（Exploration）**：尝试不熟悉的动作（action）、状态（state）或策略（policy）。通过探索，智能体可以获取更多环境信息，为未来决策打下基础。
- **利用（Exploitation）**：在已有经验或当前认知下，选择看上去收益最高的动作，进一步提高目标回报（reward）。

强化学习最终目标是使得 $J(\theta) = E_{\tau \sim \pi(\theta)}[G(\tau)]$ 最大，即在满足策略分布的情况下的轨迹所获得的回报的均值最大化

**nlp中的强化学习**

- 我们先喂给模型一个 prompt，期望它能产出符合人类喜好的 response
- 在 $t$ 时刻，模型根据上文，产出一个 token，这个 token 即对应着强化学习中的动作，我们记为 $A_t$。因此不难理解，在 NLP 语境下，强化学习任务的动作空间就对应着词表。
- 在 $t$ 时刻，模型产出 token $A_t$ 对应的即时收益为 $R_t$，总收益为 $V_t$（复习一下，$V_t$ 蕴含着“即时收益”与“未来收益”两个内容）。这个收益即可以理解为“对人类喜好的衡量”。此刻，模型的状态从 $S_t$ 变为 $S_{t+1}$，也就是从“上文”变成“上文 + 新产出的 token”
- 在 NLP 语境下，智能体是语言模型本身，环境则对应着它产出的语料

![image-20251002100710478](assets/image-20251002100710478.png)

#### 2. 贝尔曼方程

这一部分定义了强化学习中如何进行评估以及后面损失函数的设计，奖励是固定的，一次行动环境就会给一个固定的奖励信号；回报是所有奖励的加权和；价值是从当前状态或者动作对未来的预估

##### 2.1 奖励

上面的奖励它表示环境进入状态 下的**即时奖励**。奖励由奖励模型提供，获取奖励的方式有

##### 2.2 回报

但如果只考虑即时奖励，目光似乎太短浅了：当下的状态和动作会影响到未来的状态和动作，进而影响到未来的整体收益。所以，一种更好的设计方式是：**t时刻状态s的总收益 = 身处状态s能带来的即时收益 + 从状态s出发后能带来的未来收益。**写成表达式就是：$V_t = R_t + \gamma V_{t+1}$ 

**回报 $G_t$：** 从时刻 $t$ 起的累计折扣奖励：$G_t = \sum_{k = 0}^{\infty}{{\gamma}^k R_{t+k}}$  

##### 2.3 价值

- **状态价值函数** $V_\theta(s)$: 在策略 $\pi$ 下，从状态 $s$ 开始的期望回报：

$$
V_\theta(s) = \mathbb{E}_\pi [G_t | s_t = s]
$$

从 $V_\theta(s)$ 的定义出发：
$$
V_\theta(s) = \mathbb{E}_\pi [G_t | s_t = s]
$$

将 $G_t$ 展开为奖励和未来回报：

$$
G_t = R_t + \gamma G_{t+1}
$$

代入后得到：

$$
V_\theta(s) = \mathbb{E}_\pi [R_t + \gamma G_{t+1} | S_t = s]
$$

根据期望的线性性质：

$$
V_\theta(s) = \mathbb{E}_\pi [R_t | S_t = s] + \gamma \mathbb{E}_\pi [G_{t+1} | S_t = s]
$$
即时奖励期望：
$$
\mathbb{E}_\pi [R_t | S_t = s] = \sum_a \pi(a | s) \sum_{s'} P(s' | s, a) R(s, a, s')
$$

直观理解就是所有可能状态的概率 $P(s' | s, a)$ 与对应状态的奖励 $R(s, a, s')$ 的期望与策略（Policy）模型在该状态 $s$ 下做出动作 $a$ 的概率之和。

未来回报期望：
$$
\mathbb{E}_\pi [G_{t+1} | S_t = s] = \sum_a \pi(a | s) \sum_{s'} P(s' | s, a) V_\theta(s')
$$

直观理解就是所有可能状态的概率 $P(s' | s, a)$ 与转移状态对应的状态价值函数 $V_\theta(s')$ 的期望与策略（Policy）模型在该状态 $s$ 下做出动作 $a$ 的概率之和。

合并后得到贝尔曼方程：
$$
V_\theta(s) = \sum_a \pi(a | s) \sum_{s'} P(s' | s, a) [R(s, a, s') + \gamma V_\theta(s')]
$$

- **动作价值函数** $Q_\theta(s,a)$: 在策略 $\pi$ 下，从状态 $s$ 执行动作 $a$ 后的期望回报：

$$
Q_\theta(s, a) = \mathbb{E}_\pi [G_t | s_t = s, a_t = a]
$$

类比地，从 $Q_\theta(s, a)$ 出发：

$$
Q_\theta(s, a) = \mathbb{E}_\pi [R_t + \gamma G_{t+1} | S_t = s, A_t = a]
$$

由于动作 $a$ 已确定，直接对下一个状态 $s'$ 求期望：

$$
Q_\theta(s, a) = \sum_{s'} P(s' | s, a) \left[ R(s, a, s') + \gamma \mathbb{E}_\pi [ G_{t+1} | S_{t+1} = s' ] \right]
$$

注意到：

$$
\mathbb{E}_\pi [ G_{t+1} | S_{t+1} = s' ] = V_\theta(s')
$$

状态价值函数的定义为：

$$
V_\theta(s') = \sum_{a'} \pi(a' | s') Q_\theta(s', a')
$$

直观理解就是在状态 $s'$ 下，动作价值函数的所有动作对应的期望是状态价值函数，因为：

$$
Q_\theta(s, a) = \sum_{s'} P(s' | s, a) \left[ R(s, a, s') + \gamma \sum_{a'} \pi(a' | s') Q_\theta(s', a') \right]
$$

##### 2.5 优势

因为状态价值函数是对未来的一个预测，都是不确定的，我们就引入优势，假设当前执行了动作 $a_t$ 后得到的收益可能比在当前状态下的收益多多少，公式可以写为 $Adv_t = Q_{\theta}(s, a) - V_{\theta}(s)$ ，这里 $V_\pi(s)$ 充当了一个「基线」的作用，因为它可以看作是在给定策略、给定状态下，所有动作对应 $Q_\pi(s, a)$ 的期望值：
$$
V_\pi(s) = \mathbb{E}_{\pi(a \mid s)} \big[ Q_\pi(s, a) \big].
$$
通常来说，优势函数 $A_\pi(s, a)$ 表示的是：在状态 $s$ 执行动作 $a$ 比按照策略 $\pi$ 行动要更好还是更差。

我们也可以通过时序差分分解，时序差分就是正常情况我们要计算回报需要计算完整路径，但是计算量太大了，我们就用预估的下一步价值函数和当前动作的回报作为 Q
$$
Adv_t = R_t + \gamma * V_{t+1} - V_t
$$

**广义优势估计**

刚才我们使用 TD 误差来近似优势函数，它是一种**单步TD**方法。为了更好地估计优势函数，我们可以使用 **n 步回报**：
$$
R_t^{(n)} = r_t + \gamma r_{t+1} + ... + \gamma^{n-1}r_{t + n - 1} + \gamma ^nV(s_{t+n})
$$
为了更灵活地进行偏差-方差权衡，GAE可以使用「加权平均」的技巧
$$
A_t^{GAE(\lambda)} = \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l}
$$

数学形式：
$$
A_t^{\text{GAE}} = \delta_t + (\gamma \lambda) \delta_{t+1} 
 + (\gamma \lambda)^2 \delta_{t+2} + \cdots
$$

- $\gamma$（gamma）：折扣因子
- $\lambda$（lambda）：控制偏差/方差的关键参数
  - λ = 1 → 类似 MC 回报，高方差低偏差
  - λ = 0 → TD(0)，低方差高偏差
  - λ ∈ (0,1) → 在中间平衡

PPO 默认 λ ≈ 0.95。

实际代码中一般是：
$$
A_t = \delta_t + \gamma \lambda A_{t+1}
$$
你可以把 GAE 理解成：

> “用很多不同 horizon 的估计来平均优势，每个 horizon 按 λ 进行加权。”

比如：

- 1-step TD
- 2-step TD
- 3-step TD
- ⋯
- 全回报（MC）

然后按权重组合：
$$
\text{GAE} = (1-\lambda)\sum_{k=1}^{\infty} \lambda^{k-1} A^{(k)}.
$$
这是 “加权多步 TD”。

##### 2.6 贝尔曼最优方程

对于 MDP 问题，肯定会存在一个确定性的最优策略，就是总会是 $\sum_{s'} P(s' | s, a) [R(s, a, s') + \gamma V_\theta(s')]$ 这个最大的那个动作，这个时候最优策略 ${\pi}_*(a|s)$  就是一个 one-hot 分布，可以用 max 算子来改写上面的方程 
$$
V_{{\pi}_*}(s) = max_a \sum_{s'} P(s' | s, a) [R(s, a, s') + \gamma V_\theta(s')]
$$


#### 3. 致命三元组

在强化学习中，还有一个著名的「致命三元组」（deadly triad）问题，指的是当以下三个因素同时存在时，学习过程可能会变得不稳定甚至发散：

- **自举 (bootstrapping)** ：用估计值来更新估计值，如时序差分 (Temporal Difference, TD) 学习。
- **函数近似 (function approximation)** ：使用神经网络等函数近似器来表示价值函数。
- **Off-policy 学习** ：使用行为策略产生的数据来更新目标策略，如Q学习。

#### 4. 基于价值的强化学习

##### 4.1 动态规划算法

将 bellman 方程更新为
$$
V_{k+1}(s) = \sum_{a} \pi(a | s) \sum_{s'} P(s' | s, a) [R(s, a, s') + \gamma V_k(s')]
$$
其中 $V_k(s)$ 是第 k 次迭代时状态 s 的价值函数估计

**策略迭代算法** （PI）

策略评估：在给定一个策略（初始可能就是一个随机策略）的情况下，计算该策略下各个状态的价值函数。就是上面那个DP过程。

策略控制：利用上一步求解出的价值函数，进行对策略的「贪婪化」，得到一个新的策略 $\pi'$ ，${\pi}^{'}(s) = arg max_a \sum_{s'} P(s' | s, a) [R(s, a, s') + \gamma V_{\pi}(s')]$ ，要保证每次改进都不得劣于之前的策略，策略的改进一定会收敛

**价值迭代算法** （VI）

最大化操作以及隐含了策略改进，不需要显示地维护状态
$$
V_{k+1}(s) = max_a \sum_{s'} P(s' | s, a) [R(s, a, s') + \gamma V_k(s')]
$$
VI每次更新都在「当前价值函数」的基础上进行「极大化」，这在前期能迅速地把价值函数拉向最优值附近。但到达后期，需要许多额外迭代次数来「细调」价值函数，可能出现「振荡」或「缓慢逼近」的情况。如果状态空间较大，价值迭代在后期趋近最优时往往会变得「较慢」。

而PI在策略改进阶段，一旦整体策略改变，就能跳到一个较高的收益水平。从某种意义上讲，策略评估阶段给出一个相对更加「精确」的价值函数，再进行一次「彻底的」策略改进 ，往往收敛所需的「轮数」（大循环数）较少。

也就是说，PI是少次的「大步跳跃」，而VI是多次的「小步快跑」。

策略迭代方法，

```python
def policy_iteration(env, gamma=0.9, theta=1e-5, max_iter=1000):
    """
    使用策略迭代求解迷宫。
    gamma: 折扣因子(小于1，避免负循环)
    theta: 收敛阈值
    max_iter: 策略迭代的最大迭代次数，防止死循环
    """

    # 1) 收集所有非墙壁状态
    states = []
    for r in range(env.maze_size[0]):
        for c in range(env.maze_size[1]):
            if env.maze[r, c] != -1: 
                states.append((r, c))

    actions = [0, 1, 2, 3]  # 上、下、左、右

    def step_in_model(state, action):
        # 终点不需要再动
        if state == env.goal_pos:
            return state, 0.0, True

        directions = {
            0: (-1, 0),
            1: (1, 0),
            2: (0, -1),
            3: (0, 1)
        }
        move = directions[action]
        new_state = (state[0] + move[0], state[1] + move[1])

        if not (0 <= new_state[0] < env.maze_size[0] and 0 <= new_state[1] < env.maze_size[1]):
            # 越界
            return state, -5, False
        if env.maze[new_state] == -1:
            # 撞墙
            return state, -5, False

        # 正常移动
        reward = -1
        done = False
        if new_state == env.goal_pos:
            reward = 10
            done = True
        return new_state, reward, done

    # 2) 初始化策略、价值函数
    pi = {}
    V = {}
    for s in states:
        if s == env.goal_pos:
            pi[s] = None
            V[s] = 0.0
        else:
            pi[s] = np.random.choice(actions)
            V[s] = 0.0

    # 3) 策略迭代
    iter_count = 0
    while True:
        iter_count += 1
        if iter_count > max_iter:
            print("超过最大迭代次数，提前退出，可能未完全收敛。")
            break

        # ========== (A) 策略评估 ==========
        while True:
            delta = 0
            for s in states:
                if s == env.goal_pos:
                    continue
                v_old = V[s]
                a = pi[s]
                s_next, r, done = step_in_model(s, a)
                if done:
                    V[s] = r
                else:
                    V[s] = r + gamma * V[s_next]
                delta = max(delta, abs(V[s] - v_old))
               
            # 判断是不是所有的值都已经收敛了，即是否价值函数已经逼近真正的价值函数
            if delta < theta:
                break

        # ========== (B) 策略改进 ==========
        policy_stable = True
        for s in states:
            if s == env.goal_pos:
                continue
            old_a = pi[s]

            best_a = None
            best_q = float('-inf')
            for a in actions:
                s_next, r, done = step_in_model(s, a)
                q_sa = r if done else (r + gamma * V[s_next])
                if q_sa > best_q:
                    best_q = q_sa
                    best_a = a

            pi[s] = best_a
            if best_a != old_a:
                policy_stable = False

        if policy_stable:
            print(f"策略在迭代 {iter_count} 次后稳定。")
            break

    return pi, V
```

##### 4.2 蒙特卡罗算法

我们可以发现每次迭代需要遍历动作空间和状态空间，这个会有问题，比如环境是未知的，或者计算量很大，所以这种方法通常不可行

蒙特卡洛（Monte Carlo，MC）方法就是一种**通过多次采样来估计期望值**的方法。在强化学习中，它通过与环境的实际交互，获取状态、动作和奖励的序列（称为**经验**，也就是样本），并基于这些经验来**估计价值函数**。与DP相比，蒙特卡洛方法不需要环境的完整模型，因此更适用于实际场景。

更具体地说，对于一个状态 $s$，我们进行多次试验（或者说，多个回合）：

1. 在每一次试验中，智能体从状态 $s$ 出发，按照策略 $\pi$ 与环境交互，直到到达终止状态（或者折扣因子 $\gamma ^ t$ 已经小到可以忽略不计），这一次交互过程也称作rollout（中文称 展开策略），而得到的「状态、动作、奖励」序列称作「轨迹」（trajectory）。
2. 记录下此次试验中，从状态 $s$ 开始到终止状态的所有奖励，计算该轨迹的折扣回报 $G_t$。
3. 重复多次试验，计算所有回报的平均值，就可以得到对价值函数的一个近似值：$V_\pi(s) = \frac{1}{n}\sum_{t=1}^nG_t$ 

使用增量式计算，在 MC 方法中，通常会进行大量的实验，每次都重新计算平均值效率比较地，我们可以将上述过程写成更新迭代的公式 $V(s_t) \leftarrow V(s_t) + \eta[G_t - V(s_t)]$，在实际操作中通常采用动作价值函数的增量计算来进行更新迭代

在上面介绍的最基本的MC方法中，我们其实隐含了**在策略（On-Policy）** 的假设：所谓「在策略」，或者「同策略」，指的是，与环境交互（生成样本）的**行为策略（behavior policy）** 和 评估价值（更新Q函数）的**目标策略（target policy）** 是同一个策略。

下面具体算法中，epsilon_greedy_policy，有一个贪心策略采样，是因为智能体可能会陷入一个较优环境，导致环境探索不足，因此采用随机贪心选择一些点

```python
def mc_control_on_policy(env, num_episodes=5000, gamma=1.0, epsilon=0.1):
    """
    基于第一访问蒙特卡洛的 on-policy 控制（ε-贪心）。
    :param env: 自定义迷宫环境
    :param num_episodes: 训练的回合数
    :param gamma: 折扣因子
    :param epsilon: 探索率
    :return: Q, 最优的状态-动作价值函数
    """
    # Q 表示状态-动作价值函数，大小为 [行, 列, 动作数]
    Q = np.zeros((env.maze_size[0], env.maze_size[1], env.action_space.n))

    # 这里使用一个字典来存储每个状态-动作对的回报（列表），方便后续取平均做更新
    returns = dict()
    for r in range(env.maze_size[0]):
        for c in range(env.maze_size[1]):
            for a in range(env.action_space.n):
                returns[((r, c), a)] = []

    def epsilon_greedy_policy(state):
        """
        给定当前的 Q 和 explored state, 采用 ε-贪心策略选择动作
        """
        r, c = state
        if random.random() < epsilon:
            # 随机探索
            return np.random.choice(env.action_space.n)
        else:
            # 贪心选择
            return np.argmax(Q[r, c])

    for episode in range(num_episodes):
        # 生成一条回合（episode）
        state = env.reset()
        episode_trace = []  # 存储 (state, action, reward) 元组

        done = False
        while not done:
            action = epsilon_greedy_policy(tuple(state))
            next_state, reward, done, _ = env.step(action)
            episode_trace.append((tuple(state), action, reward))
            state = next_state

        # 回溯回合，更新 Q
        visited_state_actions = set()
        G = 0  # 从后往前计算折扣回报
        # 在这里从后向前计算更简洁（若想从前向后可先沿 episode_trace 再次扫一遍计算回报）
        for t in reversed(range(len(episode_trace))):
            s_t, a_t, r_t = episode_trace[t]
            G = gamma * G + r_t
            # 检查是否是该回合中首次出现的 (s_t, a_t)
            if (s_t, a_t) not in visited_state_actions:
                visited_state_actions.add((s_t, a_t))
                returns[(s_t, a_t)].append(G)
                # 增量方式更新 Q(s, a)
                Q[s_t[0], s_t[1], a_t] = np.mean(returns[(s_t, a_t)])
    return Q

if __name__ == "__main__":
    # 创建环境
    env = MazeEnv()

    # 使用蒙特卡洛方法进行训练
    Q = mc_control_on_policy(env, num_episodes=3000, gamma=1.0, epsilon=0.1)

    # 打印最终学到的 Q
    print("训练结束后学到的状态-动作价值函数 Q：")
    for r in range(env.maze_size[0]):
        for c in range(env.maze_size[1]):
            print(f"State=({r},{c}) -> Q={Q[r, c]}")
        print()

    # 根据学到的 Q 构造出一个贪心策略并测试
    def greedy_policy(state):
        return np.argmax(Q[state[0], state[1]])

    # 测试智能体在环境中的表现
    state = env.reset()
    env.render()
    done = False
    step_count = 0
    while not done and step_count < 50:  # 做一个简单的步数限制，防止卡死
        action = greedy_policy(tuple(state))
        next_state, reward, done, _ = env.step(action)
        state = next_state
        env.render()
        step_count += 1

    if tuple(state) == env.goal_pos:
        print("智能体成功到达目标！")
    else:
        print("智能体未能到达目标。")
```

##### 4.3 重要性采样

**1. 强化学习中的重要性采样**

在强化学习中，我们通常希望评估一个目标策略 $\pi$ 的表现，或者根据目标策略优化智能体的行为。但是，在很多情况下，直接按照目标策略采样数据可能不现实或不高效。此时，我们可以使用与目标策略不同的行为策略 $b$ 来生成数据。

重要性采样的核心思想是利用行为策略产生的数据，通过加权（重要性权重），将这些数据转换为目标策略下的有效数据。

设 p(x) 为目标策略，q(x) 为行为策略，q(x) 比较容易采样 

根据期望的定义，我们可以将 p(x) 下的期望值写成（连续变量版，如果是离散的，把积分换成求和）：

\[
\mathbb{E}_{x\sim p(x)}[f(x)] = \int f(x)p(x)\, dx
\]

如果从 p(x) 中采样比较困难，我们可以引入另一个分布 q(x)，并对上述式进行如下变换：

\[
\mathbb{E}_{x\sim p(x)}[f(x)] = \int f(x)\frac{p(x)}{q(x)}q(x)\, dx
\]

上述可以看做是函数 \( f(x)\frac{p(x)}{q(x)} \) 在分布 q(x) 下的期望，所以可以将其写为：

\[
\mathbb{E}_{x\sim p(x)}[f(x)] = \mathbb{E}_{x\sim q(x)}\!\left[f(x)\frac{p(x)}{q(x)}\right]
\]

其中，\(\frac{p(x)}{q(x)}\) 被称为**重要性权重**。

**2. 重要性采样公式**

设定以下符号：

- **目标策略** $\pi$：我们希望评估或优化的策略。
- **行为策略** $b$：我们用来生成数据的策略。
- **轨迹** $\tau = (s_0, a_0, r_1, s_1, a_1, r_2, \dots)$：一系列状态、动作和奖励。
- **重要性采样权重**：每个轨迹的权重为 $\frac{\pi(a_t | s_t)}{b(a_t | s_t)}$，即目标策略和行为策略选择相同动作的概率之比。

重要性采样的基本估计公式为：
$$
J(\pi) = \mathbb{E}_{b} \left[ \frac{\pi(a_t|s_t)}{b(a_t|s_t)} r_t \right]
$$
其中，$r_t$ 是从状态 $s_t$ 到下一个状态的奖励。其中 $\frac{\pi(a_t|s_t)}{b(a_t|s_t)}$ 被称为重要性权重，即目标策略和行为策略的比值，该值较大时，会导致估计值的方差很大，造成算法的不稳定，可以通过截断重要性采样来解决（即 clip）

> 重要性采样的方差为：
> $$
> \mathrm{Var}(\hat{\mu}) = \frac{1}{N}
> \left( 
> \mathbb{E}_{q}\big[(f(x) w(x))^2\big] 
> - (\mathbb{E}_p[f(x)])^2
> \right)
> $$
> 我们可以发现当 f(x)w(x) 很大时，会导致方程很大，w(x) 就是重要性采样权重
> $$
> (f(x)w(x))^2 = f(x)^2\, w(x)^2
> $$

在离策略学习中，重要性采样帮助我们调整行为策略和目标策略之间的差异，从而更准确地估计目标策略的表现。

##### 4.4 时序差分（Q-learning）

因为 MC 方法是通过采样来进行策略评估，所以需要等一个完整的回合结束才能知道具体的回报是多少，学习效率比较低，因此我们采用时序差分思路，将dp和mc思路结合
$$
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \eta[r_t + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)]
$$
![img](assets/v2-76418e27e9511afaabb247d762e1679c_r.jpg)

**SARSA 算法**

利用上面的公式进行更新，是on-policy算法，所以只能探索到被访问的点，可以利用 $\epsilon - greedy$ 算法折中。下面公式中 $\delta_{t}$ 是 TD 误差，也可以理解为优势
$$
\delta_t =  r_t + \gamma \ Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)
$$
**Q-learning算法**

就是用另一个行为策略先探索，找到一些 $max Q(s_{t+1}, a')$ 用来后面的训练更新

概述：**Q-learning** 是一种基于价值的 **强化学习** 算法，用于解决 **离策略（off-policy）** 的问题。它的目标是通过智能体与环境的交互来学习最优策略，即找到一个策略，使得在任何状态下，智能体采取某个动作时能最大化长期回报。

基本思想：Q-learning 通过不断更新每个 **状态-动作对 (s, a)** 的 **Q值**，来估计智能体在当前状态下采取某个动作后，能获得的最大长期回报。Q-learning 使用的是 **Bellman 方程** 来更新 Q 值，更新公式如下：
$$
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left( r_t + \gamma \max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t) \right)
$$

- $Q(s_t, a_t)$ 是状态 $s_t$ 下，采取动作 $a_t$ 的 Q 值。
- $\alpha$ 是学习率，控制新信息对 Q 值的影响。
- $r_t$ 是在状态 $s_t$ 采取动作 $a_t$ 后得到的即时奖励。
- $\gamma$ 是折扣因子，控制未来奖励的价值。
- $\max_{a'} Q(s_{t+1}, a')$ 是从新状态 $s_{t+1}$ 出发，选择能够最大化长期回报的动作的 Q 值。

Q-learning 的关键思想是 **最大化长期回报**，并通过不断迭代更新 Q 值，使得 Q 值趋于最优。

**步骤**

1. **初始化 Q 表**：
    初始化 Q 表，其中 Q 值通常被初始化为任意值（例如 0），或者一个较小的随机数。
2. **选择动作**：
    在每个时间步 $t$，智能体根据当前的状态 $s_t$ 选择一个动作 $a_t$。选择动作时，可以使用 **ε-贪心策略**（ε-greedy strategy）：
   - 以概率 $1 - \epsilon$ 选择具有最大 Q 值的动作（即贪心选择）。
   - 以概率 $\epsilon$ 随机选择一个动作（即探索）。
3. **执行动作并更新 Q 值**：
    执行选择的动作 $a_t$，然后根据获得的奖励 $r_t$ 和下一状态 $s_{t+1}$，使用 Q-learning 更新公式更新 Q 值。
4. **重复直到收敛**：
    重复步骤 2 和 3，直到达到预定的停止条件（例如，达到最大回合数，或者 Q 值的变化小于某个阈值）。

```python
# 初始化 Q 表为零，或小的随机值
Q = np.zeros((num_states, num_actions))

# 循环直到收敛
for episode in range(num_episodes):
    # 重置环境，获取初始状态
    state = env.reset()
    
    # 每个回合的时间步骤
    done = False
    while not done:
        # 以 ε-贪心策略选择动作
        if np.random.uniform(0, 1) < epsilon:
            action = np.random.choice(num_actions)  # 探索
        else:
            action = np.argmax(Q[state])  # 利用当前 Q 值选择最大动作

        # 执行动作并获得奖励和下一个状态
        next_state, reward, done, _ = env.step(action)
        
        # 更新 Q 值
        Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])
        
        # 更新当前状态
        state = next_state
```

##### 4.5 DQN

我们发现上面利用表格维护 $[s_t, a_t] \rightarrow q_t$ ，因为在二维地图中状态和动作的维度都比较小，所以可以，但是当维度很大的时候用表格就不合适了，所以考虑利用神经网络来拟合，即对于状态-动作对 $(s, a)$ 作为输入，神经网络的输出为 $Q_w(s, a)$ ，我们使用 MSE 作为损失函数训练，其中我们把 $r + \gamma max_{a'}Q_w(s', a')$ 作为目标值
$$
L(w) = E_{(s, a, r, s')} [(r + \gamma max_{a'}Q_w(s', a') - Q_w(s, a))^2]
$$
我们如果给上面那个损失函数求导，会遇到一个问题，「估计值」和「目标值」都是神经网络计算出来的，这个和「监督学习」是有差别的：在监督学习中，目标值是固定的标签

这会带来训练的不稳定的偏差。为了解决这个问题，我们可以「假装」目标值是固定的，也就是说，在给损失函数求梯度时，我们认为 $r + max_{a'}Q_w(s', a')$ 是个常数，而只给 $Q_w(s, a)$  求导，然后像标准梯度下降一样对当前估计值计算梯度，然后更新参数。

因为我们只计算了函数的一部分梯度，而不是整个函数的梯度，所以这种方法称为半梯度法（semi-gradient）。当然这种妥协不是没有代价的，由于忽略了目标值对参数的依赖性，这种更新方式可能导致收敛性问题。

**数据相关性**

第一个问题就是「数据相关性」。在传统的监督学习中，我们通常假设数据之间是「独立同分布」的，样本之间没有依赖性。但是，在Q-Learning这种TD学习中，数据是智能体与环境进行交互，产生一系列的「经验样本」，这些经验样本之间存在很强的「相关性」。例如，智能体连续向左移动几次，则这些经验样本在状态、动作和奖励上都非常相似。

高度相关的样本会使得模型在短时间内接触到相似的输入模式，导致模型参数更新的方向单一且不稳定。如果连续的样本都指向同一个方向的梯度，模型很容易陷入局部最优解。另外，这可能还会让算法的泛化性降低。

**经验回放**

为了解决数据相关性问题，实际中使用的DQN（Deep Q-Network）通常会使用**经验回放机制**（Experience Replay）：将智能体与环境交互的经验 $(s, a', r, s')$ 存储到一个缓冲区中，然后从缓冲区中随机采样一批经验来更新 Q 网络。

这样有什么好处呢？随机采样打破了样本之间的时间相关性，使得模型在训练时接触到的样本不再是连续的序列，而是来自不同时间点的样本，从而降低了样本之间的**时间相关性**。同时，回放缓存中存储了过去多个时间步的经验，这使得每次训练使用的样本具有更高的**多样性**，有助于模型学习到更稳健的特征。

但是 DQN 存在致命三元组问题，因此我们训练两个网络

1. **主网络（Online Network）** ：用于估计当前状态动作的Q值，并根据梯度下降进行参数更新。
2. **目标网络（Target Network）** ：用于计算TD目标，它的参数会滞后于主网络，从而提供更稳定的目标值。

```python
def train_dqn(model_name):
    env = MazeEnv()
    # 定义超参数
    num_episodes = 500
    batch_size = 32
    gamma = 0.99
    lr = 1e-3

    # epsilon 贪心相关参数
    epsilon_start = 1.0
    epsilon_end = 0.01
    epsilon_decay = 300  # 调整衰减速度

    target_update_interval = 50  # 每隔多少个 episode 同步一次目标网络
    replay_buffer_capacity = 10000

    # 创建网络
    policy_net = DQN()
    target_net = DQN()
    target_net.load_state_dict(policy_net.state_dict())
    target_net.eval()

    # 优化器
    optimizer = optim.Adam(policy_net.parameters(), lr=lr)

    # 经验回放缓冲区
    replay_buffer = ReplayBuffer(replay_buffer_capacity)

    # 记录奖励信息
    all_rewards = []

    # 训练过程
    for episode in range(num_episodes):
        state = env.reset()
        state = torch.FloatTensor(state).unsqueeze(0)
        episode_reward = 0
        done = False

        # 计算当前 epsilon
        epsilon = epsilon_end + (epsilon_start - epsilon_end) * \
            np.exp(-1. * episode / epsilon_decay)

        while not done:
            # 根据 epsilon 贪心选择动作
            if random.random() < epsilon:
                action = env.action_space.sample()
            else:
                with torch.no_grad():
                    q_values = policy_net(state)
                    action = q_values.argmax(dim=1).item()

            # 与环境进行一步交互
            next_state, reward, done, _ = env.step(action)
            next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)

            # 将 transition 存到经验回放中
            replay_buffer.push(
                state.squeeze(0).numpy(),
                action,
                reward,
                next_state_tensor.squeeze(0).numpy(),
                done
            )

            episode_reward += reward
            state = next_state_tensor

            # 每步都尝试训练（如果缓冲区够大）
            if len(replay_buffer) >= batch_size:
                # 从回放缓冲区采样
                states_b, actions_b, rewards_b, next_states_b, dones_b = replay_buffer.sample(batch_size)
                states_b = torch.FloatTensor(states_b)
                actions_b = torch.LongTensor(actions_b)
                rewards_b = torch.FloatTensor(rewards_b)
                next_states_b = torch.FloatTensor(next_states_b)
                dones_b = torch.FloatTensor(dones_b)

                # 计算 Q(s, a)
                q_values = policy_net(states_b)
                # 选出与动作对应的 Q-value
                q_values = q_values.gather(1, actions_b.unsqueeze(1)).squeeze(1)

                # 计算 Q'(s', a') 来 更新目标
                with torch.no_grad():
                    # 使用target_net来计算 max Q'(s', a')
                    next_q_values = target_net(next_states_b)
                    max_next_q_values = next_q_values.max(dim=1)[0]
                    # 如果结束，那么目标是 reward；否则是 reward + gamma * max Q'(s', a')
                    target_q_values = rewards_b + gamma * (1 - dones_b) * max_next_q_values

                # 计算损失
                loss = nn.MSELoss()(q_values, target_q_values)

                # 反向传播和更新
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

        all_rewards.append(episode_reward)

        # 每隔一段时间更新目标网络
        if (episode + 1) % target_update_interval == 0:
            target_net.load_state_dict(policy_net.state_dict())

        # 打印训练信息
        print(f"Episode {episode+1}, Epsilon: {epsilon:.3f}, Reward: {episode_reward}")

    # 保存训练好的网络
    torch.save(policy_net.state_dict(), model_name)
    print(f"model saved: {model_name}")

    return all_rewards
```

##### 4.6 $TD(\lambda)$ 

TD 方法不必等待整个回合结束，只进行单步采样，就可以更新价值函数，但是只考虑一步会导致无法准确预测未来的收益，因此我们采用 n 步回报
$$
G_{t:t+n} = r_t + \gamma r_{t+1}+...+\gamma^{n-1} r_{t+n - 1} + \gamma ^{n}V(s_{t+n})
$$
但是，n 是一个超参数，需要人为指定。不同的任务可能需要不同的 n 值，如何选择合适的 n 值是一个挑战。而且，如果选择**固定的n**，意味着所有状态的更新都使用相同步长的回报，这不够灵活。有些状态可能需要更长远的回报，而另一些状态可能只需要较短的回报。

为了解决上述问题，我们在「n步回报」的思想之上，再引入一个「λ 回报」的定义，将所有 n 步回报进行加权平均的回报：
$$
G_t^\lambda = (1-\lambda) \sum_{n=1}^{\infty} \lambda ^{n-1} G_{t:t+n}
$$
$\lambda$ 是一个介于 0 和 1 之间的参数，它控制了不同 n 步回报的权重。

- 当 λ 接近 0 时，短步回报的权重较高，学习方式更接近 TD 方法
- 当 λ 接近 1 时，长步回报的权重较高，学习方式更接近 MC 方法

#### 5. 基于策略的强化学习

基于价值（Value-based）的RL方法，他们是先学习状态价值函数V或状态-动作价值函数Q，然后根据价值函数**间接**指导策略的改进。而基于策略的RL，是对策略进行参数化，**直接**进行优化，没有V或Q做「中间商」。

基于策略的RL可以更好的处理「连续动作空间」的问题。什么是连续动作空间？以我们之前用的走迷宫游戏为例，在一个状态下，智能体都只有上下左右四种动作可以选择，这就是一个典型的「离散」的动作空间；与之相对，**连续动作空间**指的是智能体在每个时间步可以选择的动作是连续的，而不是离散的。例如，在控制问题中，动作可能是施加在机器人关节上的力或扭矩，这些值可以在某个范围内连续变化。

因为基于策略的RL没有价值函数V或者Q这个「中间商」，直接用一个可优化的函数去计算策略 ${\pi}_{\theta}(a|s)$，其中 $\theta$ 是策略的参数。例如，我们可以使用神经网络来建模策略，其中网络的输入是状态 $s$，输出是每个动作的概率。通过调整神经网络的参数 $\theta$，我们可以改变策略的行为。

##### 5.1 策略梯度定理

最大化目标函数，需要计算关于策略梯度 $\theta$ 的梯度 $\nabla_\theta J(\theta) = \nabla_\theta E_{\tau \sim \pi_\theta} [ G(\tau) ]$ ，我们可以将策略梯度改写为下面的形式：$\nabla_\theta J(\theta) = \nabla_\theta E_{\tau \sim \pi_\theta} [\sum_{t=0}^T G(\tau) \nabla_{\theta} log {\pi}_{\theta}(a_t|s_t) ]$，期望的梯度难以计算，我们可以用MC方法，即采样来近似梯度

其中，$\tau$ 表示智能体在策略下与环境交互产生的一个轨迹 (trajectory)，轨迹由一系列的状态和动作组成，表示为 $\tau = (s_0, a_0, s_1, a_1, ...,s_T, a_T)$ ，$G(\tau)$ 就是这个轨迹总的回报

##### 5.2 REINFORCE 算法

原始PG算法的一个主要问题是，它使用了不相关的奖励信息。在上述代码的计算梯度时，我们使用了总回报 $G(\tau)$ 作为权重，它包含了整个轨迹的回报，也就是说，站在时刻 t 的视角去看，这个总回报既包含了时刻t之前的奖励，也包含了时刻t之后的奖励。

这不太符合我们的直觉：我们一个朴素的直觉是，当前时刻的动作选择，**只应该考虑当前以及未来的奖励，而不应该考虑过去的奖励**。过去的就让它过去，不要让它影响我们当下的决策，为了解决这个问题， REINFORCE 使用了 $G_t$ 来代替 $G_\tau$ 作为权重
$$
G_t = \sum_{k =t}^T\gamma ^k r_k
$$
因此REINFORCE算法的梯度计算公式如下：
$$
\nabla_\theta J(\theta) = \nabla_\theta E_{\tau \sim \pi_\theta} [\sum_{t=0}^T G(t) \nabla_{\theta} log {\pi}_{\theta}(a_t|s_t) ]
$$
但是通过采样轨迹来估算策略梯度会有较高的方差，因此我们采用一个基线的概念，即用 $G_t - b(s_t)$ 代替 $G_t$ ，而这个基线可以用 $V(s)$ 来表示，这里就是优势的概念，损失函数定义为
$$
L(\theta) = - \mathbb{E}_{\tau \sim \pi_\theta} \left[ G(t) \log \pi_\theta(a_t | s_t) \right]
$$

```python
class PolicyNetwork(nn.Module):
    def __init__(self, state_dim=2, action_dim=4, hidden_dim=64):
        super(PolicyNetwork, self).__init__()
        # 输入: (x, y) 2维状态
        # 输出: 对应 4 个离散动作的概率分布
        self.layers = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim),
            nn.Softmax(dim=-1)  # 输出动作概率
        )

    def forward(self, x):
        return self.layers(x)


# ------------------------------
# 一些工具函数
# ------------------------------
def compute_discounted_returns(rewards, gamma=0.99):
    """计算从每个时间步开始的折扣回报 G_t。"""
    discounted_returns = []
    G = 0
    # 从后往前计算
    for r in reversed(rewards):
        G = r + gamma * G
        discounted_returns.insert(0, G)  # 头部插入
    return discounted_returns


# ------------------------------
# REINFORCE 训练函数
# ------------------------------
def train_reinforce(
    num_episodes=500,
    gamma=0.99,
    lr=1e-3,
    render_interval=0,
    model_save_path=None
):
    """
    使用 REINFORCE 方法训练策略网络。
    参数:
    - num_episodes: 训练的总回合数
    - gamma: 折扣因子
    - lr: 学习率
    - render_interval: 若 > 0，则每隔多少回合渲染一次迷宫
    - model_save_path: 若不为 None, 则在训练结束保存模型到该路径
    """
    env = MazeEnv()
    policy_net = PolicyNetwork()
    optimizer = optim.Adam(policy_net.parameters(), lr=lr)

    reward_history = []

    for i_episode in range(num_episodes):
        state = env.reset()
        log_probs = []
        rewards = []

        done = False
        step_count = 0

        # ----------------------------
        # 生成一条完整的回合(episode)
        # ----------------------------
        while not done:
            # 是否需要渲染
            if render_interval > 0 and (i_episode + 1) % render_interval == 0:
                env.render()

            # 转换状态为张量
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            # 前向传播, 得到动作概率分布
            action_probs = policy_net(state_tensor)
            dist = Categorical(action_probs)
            # 依据分布采样动作
            action = dist.sample()
            # 记录该动作的对数概率，以用于梯度更新
            log_prob = dist.log_prob(action)
            # 与环境交互
            next_state, reward, done, _ = env.step(action.item())

            log_probs.append(log_prob)
            rewards.append(reward)

            state = next_state
            step_count += 1

        # -----------------------------
        # 计算回合总折扣回报并回传梯度
        # -----------------------------
        discounted_returns = compute_discounted_returns(rewards, gamma)

        # 标准化 returns (可选, 常见做法)
        discounted_returns = torch.FloatTensor(discounted_returns)
        discounted_returns = (discounted_returns - discounted_returns.mean()) / \
                             (discounted_returns.std() + 1e-9)

        # 计算 loss = - Σ (log_pi(a_t|s_t) * Gt)
        loss = 0
        for log_prob, Gt in zip(log_probs, discounted_returns):
            loss += -log_prob * Gt

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        episode_reward = sum(rewards)
        reward_history.append(episode_reward)

        print(f"Episode {i_episode+1}/{num_episodes}, Reward: {episode_reward:.2f}")

    # 如果指定了保存路径，保存训练好的策略网络
    if model_save_path is not None:
        torch.save(policy_net.state_dict(), model_save_path)
        print(f"模型已保存到 {model_save_path}")

    return reward_history
```

##### 5.3 Actor-Critic 方法

Actor-Critic 方法的核心思想是：**利用 Critic 网络来评估当前策略的好坏，然后 Actor 网络根据 Critic 的评估结果来更新策略**。它是一种结合了策略梯度方法和时序差分学习 (Temporal Difference Learning，TD Learning) 的方法。因为 REINFORCE 需要完整的回报来ji'saun

- **Actor (策略)**：Actor 是一个策略网络 $\pi_\theta(a|s)$，它接收当前状态 $s$ 作为输入，输出一个动作 $a$ 的概率分布。Actor 的目标是学习一个好的策略，使得智能体可以获得尽可能高的回报。
- **Critic (值函数)**：Critic 是一个值函数网络 $Q_w(s, a)$ 或 $V_w(s)$，它接收当前状态 $s$（和动作 $a$）作为输入，输出一个对当前状态（或状态-动作对）的评估值。Critic 的目标是准确地评估当前策略的好坏。

**A2C（Advantage Actor-Critic）**

用优势函数作为 Critic，$A(s, a) = Q(s, a) - V(s)$ ，但是如果用 Q 函数来计算优势，还需要一个额外的 Q 函数网络，通常用 TD 误差来近似优势函数，即 $\delta_t = r_t + \gamma V(S_{t+1}) - V(s_t)$  

有一些技巧提高训练

1. **Stop-gradient：** 在更新 Actor 网络时，我们需要使用优势函数来计算策略梯度。为了避免 Critic 网络的更新影响 Actor 网络的更新，我们使用 stop-gradient 操作符来阻止梯度信息流向 Critic 网络。通俗地说，就是**在更新 Actor 网络时，我们把 Critic 网络的输出看作常数，忽略 Critic 网络参数的变化**。
2. **熵正则化项：** 为了**鼓励探索**，我们可以在 Actor 网络的损失函数中添加一个熵正则化项。熵正则化项可以鼓励 Actor 网络输出更加均匀的动作分布，从而避免陷入局部最优。
3. **共享网络：** 为了提高训练效率，我们可以让 Actor 网络和 Critic 网络共享一部分网络参数。比如，可以让它们共享卷积层或全连接层，然后分别使用不同的输出层。
4. **并行交互**：使用多个同步的智能体与环境交互，并统一进行参数更新。

```python
class ActorCritic(nn.Module):
    """
    一个简单的Actor-Critic结构:
      - actor_head: 输出对各动作的概率分布 (logits)
      - critic_head: 输出该状态的价值 V(s)
    """
    def __init__(self, state_dim=2, action_dim=4, hidden_dim=64):
        super(ActorCritic, self).__init__()
        self.shared = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )
        # Actor 部分输出对每个动作的logits (还需经过Softmax)
        self.actor_head = nn.Linear(hidden_dim, action_dim)
        # Critic 部分输出状态价值 V(s)
        self.critic_head = nn.Linear(hidden_dim, 1)

    def forward(self, x):
        """
        前向传播：
        输入： x (batch_size, state_dim)
        输出： actor_logits (batch_size, action_dim),
              critic_value (batch_size, 1)
        """
        shared_out = self.shared(x)
        actor_logits = self.actor_head(shared_out)
        critic_value = self.critic_head(shared_out)
        return actor_logits, critic_value


# --------------------------------------------------------------------------------
# 3. 训练 Actor-Critic
# --------------------------------------------------------------------------------
def train_actor_critic():
    env = MazeEnv()

    # 超参数
    num_episodes = 300
    gamma = 0.99
    lr = 1e-3

    # 创建网络
    model = ActorCritic(state_dim=2, action_dim=4, hidden_dim=64)
    optimizer = optim.Adam(model.parameters(), lr=lr)

    # 用于记录所有回合的总奖励
    all_rewards = []

    for episode in range(num_episodes):
        state = env.reset()
        state = torch.FloatTensor(state).unsqueeze(0)  # shape: (1,2)
        done = False

        # 记录一整个回合的 (state, action, reward, log_probs, values)
        transitions = []
        episode_reward = 0

        while not done:
            # 前向传播，得到logits和价值
            logits, value = model(state)   # logits: shape (1, action_dim)
            # 根据logits采样动作
            dist = torch.distributions.Categorical(logits=logits)
            action = dist.sample()
            log_prob = dist.log_prob(action)

            # 与环境交互
            next_state, reward, done, _ = env.step(action.item())
            next_state_t = torch.FloatTensor(next_state).unsqueeze(0)

            transitions.append({
                'state': state,
                'action': action,
                'reward': reward,
                'log_prob': log_prob,
                'value': value
            })

            episode_reward += reward
            state = next_state_t

        # 回合结束后，计算 returns 并更新网络
        # 1) 计算每个时间步的回报 G_t
        returns = []
        G = 0
        for t in reversed(transitions):
            G = t['reward'] + gamma * G
            returns.insert(0, G)
        returns = torch.FloatTensor(returns).unsqueeze(1)  # shape: (T,1)

        # 2) 计算 Actor 和 Critic 的loss并反向传播
        actor_loss = 0
        critic_loss = 0
        for i, trans in enumerate(transitions):
            advantage = returns[i] - trans['value']
            # Actor损失: -log(pi(a|s)) * advantage (策略梯度)
            actor_loss += -trans['log_prob'] * advantage.detach()
            # Critic损失: MSE( V(s) - G_t )
            critic_loss += advantage.pow(2)

        loss = actor_loss + critic_loss
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        all_rewards.append(episode_reward)
        # 打印训练信息
        print(f"Episode {episode+1}, Reward: {episode_reward}")

    # 训练结束后，保存模型
    torch.save(model.state_dict(), 'actor_critic_model.pth')
    return all_rewards


# --------------------------------------------------------------------------------
# 4. 使用训练好的 Actor-Critic 拟合器进行测试
# --------------------------------------------------------------------------------
def test_actor_critic(model_path='actor_critic_model.pth', num_episodes=1):
    """
    使用训练好的Actor-Critic模型在迷宫环境中测试 num_episodes 次，
    并通过 env.render() 在控制台打印出路径。
    参数:
        model_path: str, 已保存的模型文件路径，例如 'actor_critic_model.pth'
        num_episodes: int, 测试的回合数
    """
    env = MazeEnv()

    # 构建网络并加载参数
    model = ActorCritic(state_dim=2, action_dim=4, hidden_dim=64)
    model.load_state_dict(torch.load(model_path))
    model.eval()

    for episode in range(num_episodes):
        state = env.reset()
        state = torch.FloatTensor(state).unsqueeze(0)
        done = False
        episode_reward = 0
        print(f"===== 测试 Episode {episode + 1} 开始 =====")

        while not done:
            env.render()
            with torch.no_grad():
                logits, _ = model(state)
                dist = torch.distributions.Categorical(logits=logits)
                action = dist.sample()

            next_state, reward, done, _ = env.step(action.item())
            episode_reward += reward
            state = torch.FloatTensor(next_state).unsqueeze(0)

        # 渲染最终状态
        env.render()
        print(f"Episode {episode + 1} 结束，总奖励: {episode_reward}\n")


# --------------------------------------------------------------------------------
# 5. 运行示例
# --------------------------------------------------------------------------------
if __name__ == "__main__":
    # 1) 训练
    #   如果你已经训练过并保存了模型，可以注释掉此行并只执行测试。
    rewards = train_actor_critic()

    # 2) 测试
    #   如果已经存在 "actor_critic_model.pth"，可以直接进行测试
    #   也可以在训练完后直接测试
    test_actor_critic(model_path='actor_critic_model.pth', num_episodes=3)
```

##### 5.5 策略梯度总结

我们将 Actor 的策略梯度写成一个更通用的形式
$$
\nabla J(\theta) = \mathbb{E}\left[\sum_{t=0}^{\infty} \Psi_t \nabla \log \pi_\theta(a_t | s_t)\right]
$$

1. **蒙特卡洛估计（Monte Carlo Estimate）**：
   $$
   \Psi_t = \sum_{i=t}^{\infty} \gamma^i r_i
   $$
   这表示累计的回报，通过回报的累计折扣计算。

2. **带偏的蒙特卡洛估计**：
   $$
   \Psi_t = \sum_{i=t}^{\infty} \gamma^i r_i - V(s_t)
   $$
   这里使用了状态值函数 $V(s_t)$ 来减少估计的方差。

3. **优势函数**：
   $$
   \Psi_t = A_w(s_t, a_t)
   $$
   这表示优势函数，它衡量某个特定动作相对于基准策略的优势。

4. **Q函数**：
   $$
   \Psi_t = Q_w(s_t, a_t)
   $$
   这里是 Q 函数，用来估计在某个状态-动作对下的价值。

5. **TD误差**：
   $$
   \Psi_t = r_t + \gamma V(s_{t+1}) - V(s_t)
   $$
   这表示时间差分（TD）误差，它是通过当前奖励和估计的下一个状态值来调整当前状态的价值。

##### 5.6 最大熵策略（off-policy）

普通 RL 的目标是：
$$
J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}\Big[\sum_{t=0}^\infty \gamma^t r(s_t, a_t)\Big]
$$
SAC 把目标改成「**奖励 + 熵**」：
$$
J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}\Bigg[\sum_{t=0}^\infty 
\gamma^t \Big(r(s_t, a_t) + \alpha H(\pi_\theta(\cdot|s_t))\Big)\Bigg]
$$
其中

- $H(\pi_\theta(\cdot|s_t))$ 是策略在状态 $s_t$ 下的 **熵 entropy**：
   熵越大，说明动作越随机、越分散 → 探索更强
- $\alpha$ 是一个超参数 / 温度参数：
  - α 大：更偏向**高熵**（更随机、更多探索）
  - α 小：更偏向**高回报**（趋向确定性策略）

#### 6. 基于模型的强化学习

##### 6.1 模型

**动态模型**描述了在给定状态和动作的情况下，环境将如何转移到下一个状态，也可以称作 **“状态转移函数”**。它可以表示为一个概率分布：
$$
P(s' \mid s, a)
$$
表示在状态 $s$ 下采取动作 $a$ 后，转移到状态 $s'$ 的概率。

在实际应用中，我们通常使用 **参数化的模型**（如神经网络或高斯过程）来近似这个概率分布。

例如，我们可以用一个神经网络来表示动态模型，输入为当前状态 $s$ 和动作 $a$，输出为下一个状态的预测：
$$
s' = f_\theta(s, a)
$$
其中 $f_\theta$ 表示参数为 $\theta$ 的神经网络。 学习过程的目标是找到合适的 $\theta$，使得模型预测的 $s'$ 尽可能接近真实的下一个状态。

**奖励模型**描述了在给定状态和动作的情况下，智能体将获得多少奖励。 它可以表示为一个函数：
$$
R(s, a)
$$
表示在状态 $s$ 下采取动作 $a$ 后获得的奖励。与动态模型类似，我们也可以使用参数化的模型（如神经网络）来近似奖励函数。

例如，我们可以使用神经网络表示奖励模型，输入为当前状态 $s$ 和动作 $a$，输出为奖励的预测值：
$$
r = r_\phi(s, a)
$$
其中 $r_\phi$ 表示参数为 $\phi$ 的神经网络。 学习过程的目标是找到合适的 $\phi$，使得模型预测的奖励 $r$ 尽可能接近真实奖励。

环境模型的学习通常可以使用 **监督学习** 的方法。通过与环境的交互，我们收集到一系列数据，例如：
$$
(s_t, a_t, r_t, s_{t+1})
$$
然后我们就可以用这些数据训练动态模型和奖励模型。

训练时，将当前状态 $s_t$ 和动作 $a_t$ 作为输入，将下一个状态 $s_{t+1}$ 作为标签，训练模型预测下一状态。

同理，也可以训练奖励模型：将当前状态 $s_t$ 和动作 $a_t$ 作为输入、奖励 $r_t$ 作为标签，训练模型预测奖励。

##### 6.2 MCTS 

每一次“模拟 / rollout”都会经历下面 4 步，在这棵搜索树上走一圈：

1. **Selection 选择**
2. **Expansion 扩展**
3. **Simulation / Rollout 模拟**
4. **Backpropagation 回传**

你可以把它看成在树上不断地“走下去 -> 长出新节点 -> 随机试一试 -> 把结果往上传”。

**Selection（在已有树中向下走）**

从根节点开始，沿着树往下走，每一步选一个“最有前途”的子节点。

怎么判断“有前途”？
 典型做法：**UCB / UCT 公式**（Upper Confidence Bound for Trees）

对于节点 $s$ 的某个动作 $a$，定义：
$$
\text{UCT}(s,a) = Q(s,a) + c \sqrt{\frac{\ln N(s)}{N(s,a) + 1}}
$$

- $Q(s,a)$：当前为止，这个动作的平均回报（ exploitation 利用）
- $N(s)$：节点 $s$ 被访问的次数
- $N(s,a)$：在 $s$ 选 $a$ 的次数
- $c$：探索强度超参数（ exploration 强度）

行为逻辑：

- 前半部分 $Q(s,a)$：选当前看起来“平均成绩高”的动作（利用）
- 后半部分：优先去探索访问次数少的动作（探索）

如此反复，从根往下选到一个“还可以再展开的节点”。

**Expansion（扩展新节点）**

当你走到一个还未完全展开的节点时：

- 从这个状态下选一个“还没出现在树中的动作/后继”
- 把它添加为新的子节点

这一步就是“**让树长出新枝**”。

**Simulation / Rollout（从新节点往后随机构造一条轨迹）**

从新节点开始，你一般不会再用精细的搜索，而是：

- 用一个简单策略 / 随机策略
- 一路往下 roll 到终局（或固定步数）

然后得到一个“模拟结果”（赢/输、得分、reward、是否解出题目等等）。
 这个结果就是一次蒙特卡洛样本。

在大模型推理里，这一步可以变成：

- 让 LLM 继续“自由补全答案”直到结束；
- 或用某种“草率思考”的便宜推理。

**Backpropagation（回传）**

把刚才 simulation 得到的结果，从叶子往根节点**逐层回传**：

对路径上的每个节点、对应的动作：

- 访问次数 $N(\cdot)$ 加 1
- 累积回报更新，用来计算新的 $Q(s,a)$

这样，对“表现一直不错”的分支：

- 平均回报会高，UCT 分数更高 → 下次更常被选择
- 树会在这些区域不断加深、加密

```python
import gym
from gym import spaces
import numpy as np
import copy


class MazeEnv(gym.Env):
    """
    自定义迷宫环境，继承自 gym.Env
    """
    metadata = {'render.modes': ['human']}

    def __init__(self):
        super(MazeEnv, self).__init__()
        # 定义动作空间和状态空间
        # 动作空间：上、下、左、右
        self.action_space = spaces.Discrete(4)
        # 状态空间：智能体在迷宫中的位置（二维坐标）
        self.maze_size = (5, 5)
        self.observation_space = spaces.Box(low=0, high=4, shape=(2,), dtype=np.int32)

        # 定义迷宫（0 表示空地，-1 表示墙壁）
        self.maze = np.zeros(self.maze_size)
        self.maze[0, 3] = -1  # 墙壁位置
        self.maze[1, 1] = -1
        self.maze[1, 3] = -1
        self.maze[2, 1] = -1
        self.maze[3, 3] = -1
        self.maze[4, 1] = -1

        # 起点和终点
        self.start_pos = (0, 0)
        self.goal_pos = (0, 4)

        # 智能体初始位置
        self.agent_pos = self.start_pos

    def step(self, action):
        """
        执行动作
        """
        directions = {
            0: (-1, 0),  # 上
            1: (1, 0),   # 下
            2: (0, -1),  # 左
            3: (0, 1)    # 右
        }
        # 根据动作计算新的位置
        move = directions[action]
        new_pos = (self.agent_pos[0] + move[0], self.agent_pos[1] + move[1])

        # 默认奖励和结束标志
        reward = -1
        done = False

        # 检查新位置是否在迷宫范围内
        if (0 <= new_pos[0] < self.maze_size[0]) and (0 <= new_pos[1] < self.maze_size[1]):
            # 检查新位置是否是墙壁
            if self.maze[new_pos] == -1:
                # 撞到墙壁
                reward = -5
            else:
                # 合法移动，更新位置
                self.agent_pos = new_pos
        else:
            # 越界
            reward = -5

        # 检查是否到达终点
        if self.agent_pos == self.goal_pos:
            reward = 10
            done = True

        obs = np.array(self.agent_pos)
        info = {}
        return obs, reward, done, info

    def reset(self):
        """
        重置环境到初始状态
        """
        self.agent_pos = self.start_pos
        return np.array(self.agent_pos)

    def render(self, mode='human'):
        """
        渲染迷宫环境
        """
        maze_render = np.copy(self.maze)
        maze_render[self.agent_pos] = 2  # 智能体的位置
        maze_render[self.start_pos] = 3  # 起点
        maze_render[self.goal_pos] = 4   # 终点

        symbol_map = {
            -1: 'W',  # 墙壁
            0: ' ',   # 空地
            2: 'A',   # 智能体
            3: 'S',   # 起点
            4: 'G'    # 终点
        }
        print("\n".join(["".join([symbol_map[item] for item in row]) for row in maze_render]))
        print("\n")


class MCTSNode:
    """
    MCTS搜索树上的节点
    """
    def __init__(self, state, parent=None):
        """
        state: 表示智能体在迷宫中的 (row, col) 坐标
        parent: 指向父节点
        """
        self.state = state
        self.parent = parent
        self.children = {}     # action -> MCTSNode
        self.visits = 0        # 该节点被访问次数
        self.value_sum = 0.0   # 该节点从后续模拟中获得的累积价值总和

    def is_leaf(self):
        return len(self.children) == 0

    @property
    def q_value(self):
        """
        节点的平均价值
        """
        if self.visits == 0:
            return 0.0
        return self.value_sum / self.visits


class MCTS:
    """
    基本的蒙特卡洛树搜索，使用UCB作为选择策略。
    """
    def __init__(self, env, ucb_c=1.4, rollout_limit=20):
        """
        env: 迷宫环境
        ucb_c: UCB公式中的探索常数
        rollout_limit: 在模拟(rollout)时的最大步数限制
        """
        self.env = env
        self.ucb_c = ucb_c
        self.rollout_limit = rollout_limit

    def search(self, root_state, n_simulations=100):
        """
        在给定状态下做多次模拟搜索，返回在该状态下最优动作。
        root_state: (row, col)
        n_simulations: MCTS迭代次数
        """
        # 创建根节点
        self.root = MCTSNode(root_state)

        # 重复进行MCTS模拟
        for _ in range(n_simulations):
            node = self._select(self.root)
            reward = self._simulate(node)
            self._backpropagate(node, reward)

        # 在根节点选择访问次数最多的动作
        best_action, best_child = None, None
        max_visits = -1
        for action, child in self.root.children.items():
            if child.visits > max_visits:
                max_visits = child.visits
                best_action = action
                best_child = child
        return best_action

    def _select(self, node):
        """
        选择节点，使用UCB公式在树中向下寻找可以扩展的节点。
        如果节点是叶子，则在此进行扩展（创建子节点）。
        """
        while True:
            if node.is_leaf():
                # 节点尚未扩展就进行扩展
                if node.visits == 0:
                    # 如果该节点从未被访问过，直接返回用来进行模拟
                    return node
                else:
                    # 否则进行扩展
                    self._expand(node)
                    return node
            else:
                # 若不是叶子节点，则基于UCB选择最优子节点
                node = self._ucb_select(node)
                # 如果某个子节点还没被真正模拟过，可直接返回
                if node.visits == 0:
                    return node

    def _expand(self, node):
        """
        对节点node进行扩展。对其可能的所有动作都创建子节点。
        """
        row, col = node.state
        for action in range(self.env.action_space.n):
            # 对每个action都做一次假设，看看能走到哪里
            next_state, reward, done = self._transition(node.state, action)
            # 只要下一个状态有效（不一定非得是非墙，一般撞墙也可以算一种状态，只是reward不同）
            # 这里可以自行判断是否要跳过撞墙/越界的动作
            if True:  
                child_node = MCTSNode(next_state, parent=node)
                node.children[action] = child_node

    def _simulate(self, node):
        """
        从给定节点的状态开始，执行随机策略进行rollout，返回累积奖励。
        为了让示例简单，我们不在树中进行深层次搜索，而是随机走 rollout_limit 步或直到done。
        """
        total_reward = 0.0
        current_state = node.state
        env_copy = copy.deepcopy(self.env)
        env_copy.agent_pos = current_state  # 在复制的环境中把智能体位置设在node对应的状态

        for _ in range(self.rollout_limit):
            # 如果已经到达终点，则停止
            if (env_copy.agent_pos == env_copy.goal_pos):
                total_reward += 10  # 终点奖励
                break

            action = env_copy.action_space.sample()  # 随机动作
            obs, reward, done, _ = env_copy.step(action)
            total_reward += reward
            if done:
                break
        return total_reward

    def _backpropagate(self, node, reward):
        """
        将模拟的返回奖励回传到根节点
        """
        while node is not None:
            node.visits += 1
            node.value_sum += reward
            node = node.parent

    def _ucb_select(self, node):
        """
        在node的所有子节点中使用UCB准则选择一个节点
        UCB = Q + c * sqrt( ln(parent_visits) / (1+child_visits) )
        """
        best_score = -float('inf')
        best_child = None

        for action, child in node.children.items():
            # 如果还没访问过，那么 UCB 公式中 visits=0，需要做一个平滑
            q_value = child.q_value
            ucb_explore = self.ucb_c * np.sqrt(np.log(node.visits + 1) / (child.visits + 1e-8))
            ucb_score = q_value + ucb_explore
            if ucb_score > best_score:
                best_score = ucb_score
                best_child = child
        return best_child

    def _transition(self, state, action):
        """
        对于给定状态和动作，在已知的迷宫模型中计算下一状态和奖励、是否结束。
        因为我们是model-based，所以可以直接复用环境的逻辑。
        """
        row, col = state
        env_copy = copy.deepcopy(self.env)
        env_copy.agent_pos = (row, col)
        obs, reward, done, _ = env_copy.step(action)
        next_state = tuple(obs)
        return next_state, reward, done


def main():
    env = MazeEnv()
    mcts_planner = MCTS(env, ucb_c=1.0, rollout_limit=10)
    
    obs = env.reset()
    done = False
    step_count = 0
    
    while not done and step_count < 50:  # 最多走50步
        env.render()
        current_state = tuple(obs)
        
        # 进行若干次MCTS模拟，得到最优动作
        best_action = mcts_planner.search(root_state=current_state, n_simulations=50)
        
        # 在真实环境中执行该动作
        obs, reward, done, info = env.step(best_action)
        step_count += 1

    env.render()
    if env.agent_pos == env.goal_pos:
        print("成功到达终点，用时步数:", step_count)
    else:
        print("未能在限制步数内到达终点。")


if __name__ == "__main__":
    main()
```

#### 7. RL for LLM 

##### 7.1 RLHF-PPO

如上图，**在RLHF-PPO阶段，一共有四个主要模型**，分别是：

- **Policy Model (策略模型，Actor)**，这就是我们想要训练的目标语言模型，用 $SFT$ 阶段产生的模型对它进行初始化
- **Value Model (价值模型，Critic)**，它的作用是预估总收益，从reward model初始化而来
- **Reward Model：奖励模型**，它的作用是计算即时收益 
- **Reference Model：参考模型**，它的作用是在RLHF阶段给语言模型增加一些“约束”，防止语言模型训歪（朝不受控制的方向更新，效果可能越来越差），也是用SFT阶段模型初始化，在训练过程中参数是冻结的

对Actor模型，我们喂给它一个prompt，它正常输出对应的response。那么response中每一个token肯定有它对应的log_prob结果，我们把这样的结果记为**log_probs**；对Ref模型，我们把Actor生成的"prompt + response"喂给它，那么它同样能给出每个token的log_prob结果，我们记其为**ref_log_probs**；那么这两个模型的输出分布相似度就可以用**`ref_log_probs - log_probs`**来衡量

> 可能会奇怪为什么参考模型输入是 prompt + response ，而不是 prompt 然后自己输出相应的 token 的概率，因为这些参考模型，奖励模型，评价模型考虑的是通过 策略模型 的输出来进行评价，比如我喜欢吃苹果，prompt是我喜欢，然后response是吃苹果，而如果参考模型得到我喜欢，输出第一个是喝，那就已经偏离了，我们无法评价输出效果如何，应该是看一下输出吃的概率，然后利用我喜欢吃继续，而不是用这个模型的最大概率继续

![image-20250722153320724](assets/image-20250722153320724.png)

**损失函数计算：**

（1）直观设计

- Actor 接收到当前上文 $S_t$，产出 token $A_t$ （$P(A_t \mid S_t)$）
- Critic 根据 $S_t, A_t$，产出对总收益的预测 $V_t$
- 那么 Actor loss 可以设计为：

$$
actor\_loss = -\sum_{t \in response\_timestep} V_t \log P(A_t \mid S_t)
$$

求和符号表示我们只考虑 response 部分所有 token 的 loss，为了表达简便，我们先把这个求和符号略去（下文也是同理），也就是说：

$$
actor\_loss = -V_t \log P(A_t \mid S_t)
$$

我们希望 minimize 这个 actor\_loss。直观理解就是当 $V_t$ 大于 0 时，意味着 Critic 对于 Actor 给予了正反馈，那我们就需要在训练中提高该 token 的输出概率

（2）引入优势（Advantage）

对 NLP 任务来说，如果 Critic 对 $A_t$ 的总收益预测为 $V_t$，但实际执行 $A_t$ 后的总收益是  $R_t + \gamma * V_{t+1}$，我们就定义优势为：

$$
Adv_t = R_t + \gamma * V_{t+1} - V_t
$$

我们用 $Adv_t$ 替换掉 $V_t$，则此刻 actor\_loss 变为：

$$
actor\_loss = -Adv_t \log P(A_t \mid S_t)
$$
$R_t$ 应该表示每个Actor产出token 带来的即时收益，其中 T 表示最后一个时刻

![image-20251110123106732](assets/image-20251110123106732.png)

奖励函数我们采用下面的设计
$$
R_t = -kl\_ctl * \left( \log \frac{P(A_t|S_t)}{P_{ref}(A_t|S_t)} \right), \quad t \neq T
$$

$$
R_t = -kl\_ctl * \left( \log \frac{P(A_t|S_t)}{P_{ref}(A_t|S_t)} \right) + R_t, \quad t = T
$$

- **kl_ctl**: 常量，可以理解成一个控制比值的缩放因子，在 deepspeed-chat 中默认值设为 0.1。
- **$$ - \log \frac{P(A_t|S_t)}{P_{ref}(A_t|S_t)} $$**: 这一项你是非常熟悉的，就是 Actor 和 Ref 模型间的 KL 散度，写成更容易理解的形式，就是 **ref_log_probs - log_probs**。在 3.2 中我们说过，为了防止模型过拟合，我们需要把这个 KL 散度加入 loss 计算中，所以这里我们就做这件事。

我们可以看出当 $t \ne T$ 时，更关系 Actor 是否在 Ref 约束下生成 token，而最后一个时刻，我们还要考虑即时收益

这是因为在Reward模型训练阶段，就是用这个位置的 来表示对完整的prompt + response的奖励预测（但不妨碍你理解成是执行完 的即时奖励），然后用这个指标来做模型eval的（但是Reward训练阶段算loss时，还是考虑了response部分所有token输出的reward值）。所以到了RLHF的场景下，其余时刻的即时奖励，我们就用“Actor是否遵循了Ref的约束”来进行评价

我们可以继续加入未来收益作为考量（GAE广义优势估计）
$$
Adv_t = (R_t + \gamma  V_{t+1} - V_t) + \gamma \lambda  Adv_{t+1}
$$
注意到，对于最后一个时刻 ，它的未来收益和未来优势都是0，也就是 ，这是可以直接算出来的。所以可以用动态规划反推 

![image-20250723144537032](assets/image-20250723144537032.png)

- 第一步，我们准备一个batch的prompts
- 第二步，我们将这个batch的prompts喂给Actor模型，让它生成对应的responses
- 第三步，我们把prompt+responses喂给我们的Critic/Reward/Reference模型，让它生成用于计算actor/critic loss的数据，按照强化学习的术语，我们称这些数据为经验（experiences）。critic loss我们将在后文做详细讲解，目前我们只把目光聚焦到actor loss上
- 第四步，我们根据这些经验，实际计算出actor/critic loss，然后更新Actor和Critic模型

我们可能会发现一个 batch 的经验被用来 n 次更新，因为在强化学习中收集一个batch非常耗时

我们假设最开始吃batch，吐出经验的actor叫 $Actor_{old}$ ，而在伪代码中，每次做完ppo_epochs而更新的actor叫 $Actor_{new}$ ，那么我们只要尽量保证每次更新后的 $Actor_{new}$ 能模仿最开始的那个 $Actor_{old}$
$$
actor\_loss = -Adv_tlog\frac{P(A_t|S_t)}{P_{old}(A_t|S_t)}
$$
所以这个公式从直觉上也可以理解成：在Actor想通过模拟交互的方式，使用一个batch的经验值更新自己时，它需要收到真正吃到batch的那个时刻的Actor的约束，这样才能在有效利用batch，提升训练速度的基础上，保持训练的稳定。

但是如果 $Actor_{old}$ 约束能力不够，可以通过裁剪（clip）解决

我们给 $\frac{P(A_t|S_t)}{P_{old}(A_t|S_t)}$ 设置一个范围，例如 $(0.8, 1.2)$

对于 Critic loss，公式为 $Critic\_loss = (R_t + \gamma \times V_{t + 1} - V_{t})^2$

```python
import gym
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import random
from collections import deque

# 设置随机种子，便于复现
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)


# ------------------------------------------------------------------------------
# 1. 改进后的迷宫环境：调整每步的惩罚以减小负奖励
# ------------------------------------------------------------------------------
class MazeEnv(gym.Env):
    """
    自定义迷宫环境，继承自 gym.Env
    """
    metadata = {'render.modes': ['human']}

    def __init__(self):
        super(MazeEnv, self).__init__()
        # 定义动作空间和状态空间
        self.action_space = gym.spaces.Discrete(4)  # 上、下、左、右
        self.maze_size = (5, 5)
        self.observation_space = gym.spaces.Box(
            low=0, high=4, shape=(2,), dtype=np.int32
        )

        # 定义迷宫（0 表示空地，-1 表示墙壁）
        self.maze = np.zeros(self.maze_size)
        self.maze[0, 3] = -1
        self.maze[1, 1] = -1
        self.maze[1, 3] = -1
        self.maze[2, 1] = -1
        self.maze[3, 3] = -1
        self.maze[4, 1] = -1

        # 起点和终点
        self.start_pos = (0, 0)
        self.goal_pos = (0, 4)
        # 智能体初始位置
        self.agent_pos = self.start_pos

    def step(self, action):
        # 定义动作对应的移动
        directions = {
            0: (-1, 0),  # 上
            1: (1, 0),   # 下
            2: (0, -1),  # 左
            3: (0, 1)    # 右
        }
        move = directions[action]
        new_pos = (self.agent_pos[0] + move[0], self.agent_pos[1] + move[1])

        # 调整后的奖励/惩罚
        step_penalty = -0.1    # 每步行动的负奖励
        wall_penalty = -1      # 撞墙或越界惩罚
        goal_reward = 10       # 到达终点

        reward = step_penalty
        done = False

        # 检查新位置是否在迷宫范围内
        if (0 <= new_pos[0] < self.maze_size[0]) and (0 <= new_pos[1] < self.maze_size[1]):
            # 检查新位置是否是墙壁
            if self.maze[new_pos] == -1:
                # 撞到墙壁
                reward += wall_penalty
            else:
                self.agent_pos = new_pos  # 更新位置
        else:
            # 越界
            reward += wall_penalty

        # 是否到达终点
        if self.agent_pos == self.goal_pos:
            reward += goal_reward
            done = True

        obs = np.array(self.agent_pos)
        info = {}
        return obs, reward, done, info

    def reset(self):
        self.agent_pos = self.start_pos
        return np.array(self.agent_pos)

    def render(self, mode='human'):
        maze_render = np.copy(self.maze)
        maze_render[self.agent_pos] = 2  # 智能体
        maze_render[self.start_pos] = 3  # 起点
        maze_render[self.goal_pos] = 4   # 终点

        symbol_map = {
            -1: 'W',  # 墙壁
            0: ' ',   # 空地
            2: 'A',   # 智能体
            3: 'S',   # 起点
            4: 'G'    # 终点
        }
        print("\n".join(["".join([symbol_map[item] for item in row]) for row in maze_render]))
        print("\n")


# ------------------------------------------------------------------------------
# 2. ActorCritic 模型
# ------------------------------------------------------------------------------
class ActorCritic(nn.Module):
    def __init__(self, state_dim=2, action_dim=4, hidden_dim=64):
        super(ActorCritic, self).__init__()

        # 公共特征提取层
        self.base = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )

        # Actor 分支：输出对每个动作的 logits
        self.actor = nn.Linear(hidden_dim, action_dim)

        # Critic 分支：输出状态价值 V(s)
        self.critic = nn.Linear(hidden_dim, 1)

    def forward(self, x):
        features = self.base(x)
        logits = self.actor(features)
        state_value = self.critic(features)
        return logits, state_value


# ------------------------------------------------------------------------------
# 3. Rollout Buffer
# ------------------------------------------------------------------------------
class RolloutBuffer:
    def __init__(self):
        self.states = []
        self.actions = []
        self.log_probs = []
        self.rewards = []
        self.is_done = []
        self.values = []

    def clear(self):
        self.states.clear()
        self.actions.clear()
        self.log_probs.clear()
        self.rewards.clear()
        self.is_done.clear()
        self.values.clear()

    def add(self, state, action, log_prob, reward, done, value):
        self.states.append(state)
        self.actions.append(action)
        self.log_probs.append(log_prob)
        self.rewards.append(reward)
        self.is_done.append(done)
        self.values.append(value)

    def get_size(self):
        return len(self.states)


# ------------------------------------------------------------------------------
# 4. PPO 核心
# ------------------------------------------------------------------------------
class PPOTrainer:
    def __init__(
        self,
        state_dim=2,
        action_dim=4,
        hidden_dim=64,
        gamma=0.99,
        lr=3e-4,
        clip_eps=0.2,
        update_epochs=5,
        lmbda=0.95,
        vf_coef=0.5,
        ent_coef=0.02  # 略微增大，促进探索
    ):
        self.gamma = gamma
        self.clip_eps = clip_eps
        self.update_epochs = update_epochs
        self.lmbda = lmbda
        self.vf_coef = vf_coef
        self.ent_coef = ent_coef

        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.ac = ActorCritic(state_dim, action_dim, hidden_dim).to(self.device)
        self.optimizer = optim.Adam(self.ac.parameters(), lr=lr)

    def select_action(self, state):
        """
        输入单个state (numpy array)，输出一个action，以及log_prob等信息
        """
        state_t = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        logits, value = self.ac(state_t)
        dist = torch.distributions.Categorical(logits=logits)
        action = dist.sample()
        log_prob = dist.log_prob(action)
        return action.item(), log_prob.item(), value.item()

    def get_value(self, state):
        """
        给定一个状态，返回 Critic 估计的价值
        """
        state_t = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        with torch.no_grad():
            _, value = self.ac(state_t)
        return value.item()

    def compute_gae(self, rewards, values, dones, final_value):
        """
        使用 GAE-lambda 计算优势和回报
        如果最后状态没 done，就用 final_value 作为 bootstrap
        """
        advantages = np.zeros_like(rewards, dtype=np.float32)
        returns = np.zeros_like(rewards, dtype=np.float32)
        gae = 0.0

        values = np.append(values, [final_value])

        for t in reversed(range(len(rewards))):
            delta = rewards[t] + self.gamma * values[t + 1] * (1 - dones[t]) - values[t]
            gae = delta + self.gamma * self.lmbda * (1 - dones[t]) * gae
            advantages[t] = gae
            returns[t] = gae + values[t]
        return advantages, returns

    def update(self, buffer: RolloutBuffer, final_value):
        states = torch.FloatTensor(buffer.states).to(self.device)
        actions = torch.LongTensor(buffer.actions).to(self.device)
        old_log_probs = torch.FloatTensor(buffer.log_probs).to(self.device)
        rewards = np.array(buffer.rewards, dtype=np.float32)
        dones = np.array(buffer.is_done, dtype=np.float32)
        values = np.array(buffer.values, dtype=np.float32)

        # 计算 GAE
        advantages, returns = self.compute_gae(rewards, values, dones, final_value)
        advantages = torch.FloatTensor(advantages).to(self.device)
        returns = torch.FloatTensor(returns).to(self.device)

        for _ in range(self.update_epochs):
            logits, value_pred = self.ac(states)
            dist = torch.distributions.Categorical(logits=logits)
            new_log_probs = dist.log_prob(actions)
            entropy = dist.entropy().mean()

            # 计算比率 ratio
            ratio = torch.exp(new_log_probs - old_log_probs)

            # PPO Clip
            surr1 = ratio * advantages
            surr2 = torch.clamp(ratio, 1.0 - self.clip_eps, 1.0 + self.clip_eps) * advantages
            policy_loss = -torch.min(surr1, surr2).mean()

            # Critic loss
            value_pred = value_pred.squeeze(-1)
            value_loss = nn.MSELoss()(value_pred, returns)

            loss = policy_loss + self.vf_coef * value_loss - self.ent_coef * entropy

            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()


def train_ppo(num_episodes=600, max_steps_per_episode=100):
    """
    使用 PPO 训练迷宫环境：
    - 增加训练回合数
    - 每回合限制 100 步
    """
    env = MazeEnv()

    trainer = PPOTrainer(
        state_dim=2,
        action_dim=4,
        hidden_dim=64,
        gamma=0.99,
        lr=3e-4,
        clip_eps=0.2,
        update_epochs=5,
        lmbda=0.95,
        vf_coef=0.5,
        ent_coef=0.02  # 提高熵系数，引导更多探索
    )

    rollout_buffer = RolloutBuffer()
    all_rewards = []

    for episode in range(num_episodes):
        state = env.reset()
        episode_reward = 0
        rollout_buffer.clear()

        for step in range(max_steps_per_episode):
            action, log_prob, value = trainer.select_action(state)
            next_state, reward, done, _ = env.step(action)

            rollout_buffer.add(
                state=state,
                action=action,
                log_prob=log_prob,
                reward=reward,
                done=float(done),
                value=value
            )
            state = next_state
            episode_reward += reward

            if done:
                break

        # 如果回合中途没有 done，就从 Critic 得到 bootstrap 价值
        if done:
            final_value = 0.0
        else:
            final_value = trainer.get_value(state)

        # 用 rollouts 中的数据更新网络
        trainer.update(rollout_buffer, final_value)
        all_rewards.append(episode_reward)

        # 打印训练信息
        if (episode+1) % 10 == 0:
            avg_rew = np.mean(all_rewards[-10:])
            print(f"Episode {episode+1}/{num_episodes} | Reward: {episode_reward:.2f} | Avg10: {avg_rew:.2f}")

    # 训练结束后，保存模型参数
    torch.save(trainer.ac.state_dict(), "ppo_actor_critic_fixed.pth")
    return all_rewards


# ------------------------------------------------------------------------------
# 测试：加载训练好的模型，在迷宫中走若干回合并渲染
# ------------------------------------------------------------------------------
def test_ppo(model_path="ppo_actor_critic_fixed.pth", num_episodes=3):
    env = MazeEnv()

    state_dim = 2
    action_dim = 4
    ac = ActorCritic(state_dim=state_dim, action_dim=action_dim)
    ac.load_state_dict(torch.load(model_path))
    ac.eval()

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    ac.to(device)

    for epi in range(num_episodes):
        state = env.reset()
        done = False
        episode_reward = 0

        print(f"===== 测试 Episode {epi+1} =====")
        while not done:
            env.render()

            state_t = torch.FloatTensor(state).unsqueeze(0).to(device)
            with torch.no_grad():
                logits, critic_value = ac(state_t)
                dist = torch.distributions.Categorical(logits=logits)
                action = dist.sample().item()

            next_state, reward, done, _ = env.step(action)
            state = next_state
            episode_reward += reward

        env.render()
        print(f"Episode {epi+1} 结束，回合总奖励: {episode_reward}\n")


if __name__ == "__main__":
    # 1. 训练
    rewards = train_ppo(num_episodes=600, max_steps_per_episode=100)

    # 2. 测试
    test_ppo("ppo_actor_critic_fixed.pth", num_episodes=3)
```

##### 7.2 DPO

直接将人类的偏好对用于模型的训练，以达到最小的损失满足，属于离线策略，DPO可以通过人类偏好数据，用二元交叉熵对策略进行优化，而不需要多次进行在线数据采样进行优化。其中，$y_w$ 为偏好数据，$y_l$ 为非偏好数据。公式设计的目标就是最大化偏好数据
$$
\mathcal{L}_{DPO}(\pi_{\theta}; \pi_{\text{ref}}) = -\mathbb{E}_{(x, y_w, y_l) \sim D} \left[ \log \sigma \left( \beta \log \frac{\pi_{\theta}(y_w | x)}{\pi_{\text{ref}}(y_w | x)} - \beta \log \frac{\pi_{\theta}(y_l | x)}{\pi_{\text{ref}}(y_l | x)} \right) \right]
$$
![图1：RLHF和DPO方法的比较](assets/img-0.jpeg)

##### 7.3 GRPO

步骤：1.生成补全：对于一个提示词生成G个回复；2.计算优势值：对于每一个G序列，使用奖励模型计算奖励，通常奖励模型是基于同一问题的输出之间的比较数据集进行训练的——优势的计算反映了这些相对 比较，$A_{i,t} = \frac{r_i-mean(r)}{std(r)}$ ；3.估算KL散度：$
D_{\text{KL}}[\pi \theta \parallel \pi_{\text{ref}}] = \frac{\pi_{\text{ref}}(\omega_i | q, O_i, < t)}{\pi_\theta(\omega_i | q, O_i, < t)} - \log \frac{\pi_{\text{ref}}(\omega_i | q, O_i, < t)}{\pi_\theta(\omega_i | q, O_i, < t)} - 1
$ ； 4. 计算损失
$$
\mathcal{L}_{\text{GRPO}}(\theta) = -\frac{1}{G} \sum_{i=1}^G \frac{1}{|\omega_i|} \sum_{t=1}^{|\omega_i|} \left[ \min \left( \frac{\pi_\theta(\omega_{i,t} | q, O_i, < t)}{\pi_{\theta_{\text{old}}}(\omega_{i,t} | q, O_i, < t)}, 1 - \epsilon, 1 + \epsilon \right) \hat{A}_{i,t} - \beta D_{\text{KL}}[\pi_\theta \parallel \pi_{\text{ref}}] \right]
$$
![img](assets/v2-6e7e78165b1af96e810c84cf82fc7a94_720w.webp)

GRPO通过优化PPO算法，解决了计算优势值时需要同时依赖奖励模型（reward model）和价值模型（value model）的问题，成功移除了value model（价值模型），显著降低了推理时的内存占用和时间开销。**Advantage（优势值）**的核心价值在于为模型输出提供更精准的评估，不仅衡量答案的绝对质量，还通过相对比较（与其他回答的对比）来更全面地定位其优劣。

![img](assets/v2-c1ae49558e634c81987d370fc78a5cbb_r.jpg)

#### 8. 模仿学习

如果没有奖励信号，那么可以通过模仿学习，核心思想就是观察专家的行为，学习一个策略，能够像专家一样完成任务

##### 8.1 行为克隆 (Behavioral Cloning, BC)

行为克隆（Behavioral Cloning, BC）是最简单、最直接的模仿学习方法。它的核心思想是将模仿学习视为一个监督学习问题，即：给定一系列专家演示数据，学习一个从状态到动作的映射。

假设我们有一组专家演示数据 \( D = \{(s_i, a_i)\}_{i=1}^N \)，其中 \( s_i \) 表示状态，\( a_i \) 表示专家在状态 \( s_i \) 下采取的动作。行为克隆的目标是学习一个策略 \( \pi(a|s) \)，使得给定状态 \(s\) 时，策略能够预测出与专家相近的动作。

行为克隆的目标是最小化损失函数：

$$
L(\theta) = \mathbb{E}_{(s, a) \sim D} \left[ l(\pi(a | s), a) \right]
$$

其中，\( l(\cdot, \cdot) \) 表示一个合适的损失函数，例如交叉熵损失（Cross-Entropy Loss）或均方误差损失（Mean Squared Error Loss）。

行为克隆的原理非常简单，可以利用现有的监督学习算法，易于理解和实现。但它存在一些问题，例如：

- **没有考虑到环境的动态性**：行为克隆将每个状态-动作对视为独立的样本，忽略了模仿学习中状态转移的关系，可能会导致环境状态的分布发生偏移。
- **泛化能力差**：行为克隆容易出现过拟合问题，难以泛化到新的状态或动作。
- **协变量偏移 (Covariate Shift)**：训练数据来自专家策略，而实际运行时中智能体可能遇到到不同的状态分布，导致性能下降。

解决协变量偏移的方法

- **数据增量 (Data Augmentation)**：通过对专家演示数据进行变换，增加数据的多样性，从而提升模型的泛化能力。例如，可以在天气的变换中加入一些雨天、雾天等状态。
- **Dagger 算法 (Dataset Aggregation)**：Dagger 算法是一种交互式的学习方法，通过不断从环境中收集新的数据，并将其加入到训练集中，从而避免协变量偏移的问题。
- **具有交互式学习算法**：除了 Dagger 算法之外，还有许多其他的交互式学习方法，例如策略迭代等，均能够缓解协变量偏移的问题。

### 三、LLM 基础

#### 1. 模型架构

![image-20251108204949938](assets/image-20251108204949938.png)

1. **编码器 (Encoder)** ：任务是“**理解**”输入的整个句子。它会读取所有输入词元(这个概念会在3.2.2节介绍)，最终为每个词元生成一个富含上下文信息的向量表示。
2. **解码器 (Decoder)** ：任务是“**生成**”目标句子。它会参考自己已经生成的前文，并“咨询”编码器的理解结果，来生成下一个词。

```python
import torch
import torch.nn as nn
import math

# --- 占位符模块，将在后续小节中实现 ---

class PositionalEncoding(nn.Module):
    """
    位置编码模块
    """
    def forward(self, x):
        pass

class MultiHeadAttention(nn.Module):
    """
    多头注意力机制模块
    """
    def forward(self, query, key, value, mask):
        pass

class PositionWiseFeedForward(nn.Module):
    """
    位置前馈网络模块
    """
    def forward(self, x):
        pass

# --- 编码器核心层 ---

class EncoderLayer(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout):
        super(EncoderLayer, self).__init__()
        self.self_attn = MultiHeadAttention() # 待实现
        self.feed_forward = PositionWiseFeedForward() # 待实现
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, mask):
        # 残差连接与层归一化将在 3.1.2.4 节中详细解释
        # 1. 多头自注意力
        attn_output = self.self_attn(x, x, x, mask)
        x = self.norm1(x + self.dropout(attn_output))

        # 2. 前馈网络
        ff_output = self.feed_forward(x)
        x = self.norm2(x + self.dropout(ff_output))

        return x

# --- 解码器核心层 ---

class DecoderLayer(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout):
        super(DecoderLayer, self).__init__()
        self.self_attn = MultiHeadAttention() # 待实现
        self.cross_attn = MultiHeadAttention() # 待实现
        self.feed_forward = PositionWiseFeedForward() # 待实现
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, encoder_output, src_mask, tgt_mask):
        # 1. 掩码多头自注意力 (对自己)
        attn_output = self.self_attn(x, x, x, tgt_mask)
        x = self.norm1(x + self.dropout(attn_output))

        # 2. 交叉注意力 (对编码器输出)
        cross_attn_output = self.cross_attn(x, encoder_output, encoder_output, src_mask)
        x = self.norm2(x + self.dropout(cross_attn_output))

        # 3. 前馈网络
        ff_output = self.feed_forward(x)
        x = self.norm3(x + self.dropout(ff_output))

        return x
```

现在，我们来填充骨架中最关键的模块，注意力机制。

想象一下我们阅读这个句子：“The agent learns because **it** is intelligent.”。当我们读到加粗的 "**it**" 时，为了理解它的指代，我们的大脑会不自觉地将更多的注意力放在前面的 "agent" 这个词上。**自注意力 (Self-Attention)** 机制就是对这种现象的数学建模。它允许模型在处理序列中的每一个词时，都能兼顾句子中的所有其他词，并为这些词分配不同的“注意力权重”。权重越高的词，代表其与当前词的关联性越强，其信息也应该在当前词的表示中占据更大的比重。

为了实现上述过程，自注意力机制为每个输入的词元向量引入了三个可学习的角色：

- **查询 (Query, Q)**：代表当前词元，它正在主动地“查询”其他词元以获取信息。
- **键 (Key, K)**：代表句子中可被查询的词元“标签”或“索引”。
- **值 (Value, V)**：代表词元本身所携带的“内容”或“信息”。

这三个向量都是由原始的词嵌入向量乘以三个不同的、可学习的权重矩阵 ($W^Q,W^K,W^V$) 得到的。整个计算过程可以分为以下几步，我们可以把它想象成一次高效的开卷考试：

- 准备“考题”和“资料”：对于句子中的每个词，都通过权重矩阵生成其 $Q,K,V$ 向量。
- 计算相关性得分：要计算词 $A$ 的新表示，就用词 $A$ 的 $Q$ 向量，去和句子中所有词（包括 $A$ 自己）的 $K$ 向量进行点积运算。这个得分反映了其他词对于理解词 $A$ 的重要性。
- 稳定化与归一化：将得到的所有分数除以一个缩放因子$\sqrt{d_k}$），以防止梯度过小，然后用Softmax函数将分数转换成总和为1的权重，也就是归一化的过程。
- 加权求和：将上一步得到的权重分别乘以每个词对应的V*V*向量，然后将所有结果相加。最终得到的向量，就是词 $A$ 融合了全局上下文信息后的新表示。

这个过程可以用一个简洁的公式来概括：
$$
Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V
$$


如果只进行一次上述的注意力计算（即单头），模型可能会只学会关注一种类型的关联。比如，在处理 "it" 时，可能只学会了关注主语。但语言中的关系是复杂的，我们希望模型能同时关注多种关系（如指代关系、时态关系、从属关系等）。多头注意力机制应运而生。它的思想很简单：把一次做完变成分成几组，分开做，再合并。

它将原始的 Q, K, V 向量在维度上切分成 h 份（h 就是“头”数），每一份都独立地进行一次单头注意力的计算。这就好比让 h 个不同的“专家”从不同的角度去审视句子，每个专家都能捕捉到一种不同的特征关系。最后，将这 h 个专家的“意见”（即输出向量）拼接起来，再通过一个线性变换进行整合，就得到了最终的输出。

```python
class MultiHeadAttention(nn.Module):
    """
    多头注意力机制模块
    """
    def __init__(self, d_model, num_heads):
        super(MultiHeadAttention, self).__init__()
        assert d_model % num_heads == 0, "d_model 必须能被 num_heads 整除"

        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads

        # 定义 Q, K, V 和输出的线性变换层
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)

    def scaled_dot_product_attention(self, Q, K, V, mask=None):
        # 1. 计算注意力得分 (QK^T)
        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)

        # 2. 应用掩码 (如果提供)
        if mask is not None:
            # 将掩码中为 0 的位置设置为一个非常小的负数，这样 softmax 后会接近 0
            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)

        # 3. 计算注意力权重 (Softmax)
        attn_probs = torch.softmax(attn_scores, dim=-1)

        # 4. 加权求和 (权重 * V)
        output = torch.matmul(attn_probs, V)
        return output

    def split_heads(self, x):
        # 将输入 x 的形状从 (batch_size, seq_length, d_model)
        # 变换为 (batch_size, num_heads, seq_length, d_k)
        batch_size, seq_length, d_model = x.size()
        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)

    def combine_heads(self, x):
        # 将输入 x 的形状从 (batch_size, num_heads, seq_length, d_k)
        # 变回 (batch_size, seq_length, d_model)
        batch_size, num_heads, seq_length, d_k = x.size()
        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)

    def forward(self, Q, K, V, mask=None):
        # 1. 对 Q, K, V 进行线性变换
        Q = self.split_heads(self.W_q(Q))
        K = self.split_heads(self.W_k(K))
        V = self.split_heads(self.W_v(V))

        # 2. 计算缩放点积注意力
        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)

        # 3. 合并多头输出并进行最终的线性变换
        output = self.W_o(self.combine_heads(attn_output))
        return output

```

在每个 Encoder 和 Decoder 层中，多头注意力子层之后都跟着一个**逐位置前馈网络(Position-wise Feed-Forward Network, FFN)** 。如果说注意力层的作用是从整个序列中“动态地聚合”相关信息，那么前馈网络的作用从这些聚合后的信息中提取更高阶的特征。

这个名字的关键在于“逐位置”。它意味着这个前馈网络会独立地作用于序列中的每一个词元向量。换句话说，对于一个长度为 `seq_len` 的序列，这个 FFN 实际上会被调用 `seq_len` 次，每次处理一个词元。重要的是，所有位置共享的是同一组网络权重。这种设计既保持了对每个位置进行独立加工的能力，又大大减少了模型的参数量。这个网络的结构非常简单，由两个线性变换和一个 ReLU 激活函数组成：
$$
FFN(x) = max(0, xW_1+b_1)W_2+b_2
$$
其中，$x$ 是注意力子层的输出。$W_1,b_1,W_2,b_2$ 是可学习的参数。通常，第一个线性层的输出维度 `d_ff` 会远大于输入的维度 `d_model`（例如 `d_ff = 4 * d_model`），经过 ReLU 激活后再通过第二个线性层映射回 `d_model` 维度。这种“先扩大再缩小”的模式，也被称为瓶颈结构，被认为有助于模型学习更丰富的特征表示。

```python
class PositionWiseFeedForward(nn.Module):
    """
    位置前馈网络模块
    """
    def __init__(self, d_model, d_ff, dropout=0.1):
        super(PositionWiseFeedForward, self).__init__()
        self.linear1 = nn.Linear(d_model, d_ff)
        self.dropout = nn.Dropout(dropout)
        self.linear2 = nn.Linear(d_ff, d_model)
        self.relu = nn.ReLU()

    def forward(self, x):
        # x 形状: (batch_size, seq_len, d_model)
        x = self.linear1(x)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.linear2(x)
        # 最终输出形状: (batch_size, seq_len, d_model)
        return x

```

在 Transformer 的每个编码器和解码器层中，所有子模块（如多头注意力和前馈网络）都被一个 `Add & Norm` 操作包裹。这个组合是为了保证 Transformer 能够稳定训练。

这个操作由两个部分组成：

- **残差连接 (Add)**：该操作将子模块的输入 `x` 直接加到该子模块的输出 `Sublayer(x)` 上。这一结构解决了深度神经网络中的**梯度消失 (Vanishing Gradients)** 问题。在反向传播时，梯度可以绕过子模块直接向前传播，从而保证了即使网络层数很深，模型也能得到有效的训练。其公式可以表示为：$Output = x + sublayer(x)$
- **层归一化 (Norm)**：该操作对单个样本的所有特征进行归一化，使其均值为0，方差为1。这解决了模型训练过程中的**内部协变量偏移 (Internal Covariate Shift)** 问题，使每一层的输入分布保持稳定，从而加速模型收敛并提高训练的稳定性。

我们已经了解，Transformer 的核心是自注意力机制，它通过计算序列中任意两个词元之间的关系来捕捉依赖。然而，这种计算方式有一个固有的问题：它本身不包含任何关于词元顺序或位置的信息。对于自注意力来说，“agent learns” 和 “learns agent” 这两个序列是完全等价的，因为它只关心词元之间的关系，而忽略了它们的排列。为了解决这个问题，Transformer 引入了**位置编码 (Positional Encoding)** 。

位置编码的核心思想是，为输入序列中的每一个词元嵌入向量，都额外加上一个能代表其绝对位置和相对位置信息的“位置向量”。这个位置向量不是通过学习得到的，而是通过一个固定的数学公式直接计算得出。这样一来，即使两个词元（例如，两个都叫 `agent` 的词元）自身的嵌入是相同的，但由于它们在句子中的位置不同，它们最终输入到 Transformer 模型中的向量就会因为加上了不同的位置编码而变得独一无二。原论文中提出的位置编码使用正弦和余弦函数来生成，其公式如下：
$$
PE_{(pos, 2i)} = sin(\frac{pos}{10000^{\frac{2i}{d_{model}}}})
$$

$$
PE_{(pos, 2i+1)} = cos(\frac{pos}{10000^{\frac{2i}{d_{model}}}})
$$

```Python
class PositionalEncoding(nn.Module):
    """
    为输入序列的词嵌入向量添加位置编码。
    """
    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)

        # 创建一个足够长的位置编码矩阵
        position = torch.arange(max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))

        # pe (positional encoding) 的大小为 (max_len, d_model)
        pe = torch.zeros(max_len, d_model)

        # 偶数维度使用 sin, 奇数维度使用 cos
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        # 将 pe 注册为 buffer，这样它就不会被视为模型参数，但会随模型移动（例如 to(device)）
        self.register_buffer('pe', pe.unsqueeze(0))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x.size(1) 是当前输入的序列长度
        # 将位置编码加到输入向量上
        x = x + self.pe[:, :x.size(1)]
        return self.dropout(x)
```

Decoder-Only 架构的工作模式被称为**自回归 (Autoregressive)** 。这个听起来很专业的术语，其实描述了一个非常简单的过程：

1. 给模型一个起始文本（例如 “Datawhale Agent is”）。
2. 模型预测出下一个最有可能的词（例如 “a”）。
3. 模型将自己刚刚生成的词 “a” 添加到输入文本的末尾，形成新的输入（“Datawhale Agent is a”）。
4. 模型基于这个新输入，再次预测下一个词（例如 “powerful”）。
5. 不断重复这个过程，直到生成完整的句子或达到停止条件。

模型就像一个在玩“文字接龙”的游戏，它不断地“回顾”自己已经写下的内容，然后思考下一个字该写什么。

你可能会问，解码器是如何保证在预测第 `t` 个词时，不去“偷看”第 `t+1` 个词的答案呢？

答案就是**掩码自注意力 (Masked Self-Attention)** 。在 Decoder-Only 架构中，这个机制变得至关重要。它的工作原理非常巧妙：

在自注意力机制计算出注意力分数矩阵（即每个词对其他所有词的关注度得分）之后，但在进行 Softmax 归一化之前，模型会应用一个“掩码”。这个掩码会将所有位于当前位置之后（即目前尚未观测到）的词元对应的分数，替换为一个非常大的负数。当这个带有负无穷分数的矩阵经过 Softmax 函数时，这些位置的概率就会变为 0。这样一来，模型在计算任何一个位置的输出时，都从数学上被阻止了去关注它后面的信息。这种机制保证了模型在预测下一个词时，能且仅能依赖它已经见过的、位于当前位置之前的所有信息，从而确保了预测的公平性和逻辑的连贯性。

**Decoder-Only 架构的优势**

这种看似简单的架构，却带来了巨大的成功，其优势在于：

- **训练目标统一**：模型的唯一任务就是“预测下一个词”，这个简单的目标非常适合在海量的无标注文本数据上进行预训练。
- **结构简单，易于扩展**：更少的组件意味着更容易进行规模化扩展。今天的 GPT-4、Llama 等拥有数千亿甚至万亿参数的巨型模型，都是基于这种简洁的架构。
- **天然适合生成任务**：其自回归的工作模式与所有生成式任务（对话、写作、代码生成等）完美契合，这也是它能成为构建通用智能体基础的核心原因。

总而言之，从 Transformer 的解码器演变而来的 Decoder-Only 架构，通过“预测下一个词”这一简单的范式，开启了我们今天所处的大语言模型时代。

在GPT的训练中，模型的目标是**预测下一个token**。所以：

- **输入**：模型的输入是文本的一部分，通常是**一个token序列**（例如“我 喜欢 吃”）。
- **输出**：模型生成的输出是该序列的下一个token预测（例如“苹果”）。

**训练时使用的是原始文本中的真实token**作为目标，而不是模型自己生成的token。也就是说，GPT在训练时并不会将之前生成的内容作为输入，而是依赖**真实的标注文本**（即训练集中的真实token）来计算损失。

在GPT的训练过程中，通常使用**交叉熵损失（Cross-Entropy Loss）**来衡量预测的概率分布与真实目标分布之间的差异。对于每个时间步，损失函数计算模型的输出概率分布与真实token之间的差异。

对于每个位置 $t$，假设模型预测的token为 $y_t'$（预测的token），而真实的目标token为 $y_t$（真实标签）。交叉熵损失定义为：
$$
L(\theta) = -\sum_{t =1}^{T}{log P(y_t|x_1,x_2,……,x_{t-1};\theta)}
$$
一个 $batch$ 所有 $token$ 预测结束后计算平均交叉熵损失

#### 2. 常见性质

##### 2.1 **外推性**

大模型在训练时和预测时的输入长度不一致，导致模型的泛化能力下降的问题。例如，如果一个模型在训练时只使用了512个 token 的文本，那么在预测时如果输入超过512个 token，模型可能无法正确处理。这就限制了大模型在处理长文本或多轮对话等任务时的效果。

##### 2.2 **涌现能力**

当模型规模（参数量、训练数据量等）增大到某个阈值之后，会**突然**表现出一些在小模型中完全没有、甚至无法预期的复杂能力

##### 2.3 **模型幻觉（Hallucination）**

通常指的是大语言模型生成的内容与客观事实、用户输入或上下文信息相矛盾，或者生成了不存在的事实、实体或事件。幻觉的本质是模型在生成过程中，过度自信地“编造”了信息，而非准确地检索或推理。根据其表现形式，幻觉可以被分为多种类型[11]，例如：

- **事实性幻觉 (Factual Hallucinations)** ： 模型生成与现实世界事实不符的信息。
- **忠实性幻觉 (Faithfulness Hallucinations)** ： 在文本摘要、翻译等任务中，生成的内容未能忠实地反映源文本的含义。
- **内在幻觉 (Intrinsic Hallucinations)** ： 模型生成的内容与输入信息直接矛盾。

幻觉的产生是多方面因素共同作用的结果。首先，训练数据中可能包含错误或矛盾的信息。其次，模型的自回归生成机制决定了它只是在预测下一个最可能的词元，而没有内置的事实核查模块。最后，在面对需要复杂推理的任务时，模型可能会在逻辑链条中出错，从而“编造”出错误的结论。例如：一个旅游规划 Agent，可能会为你推荐一个现实中不存在的景点，或者预订一个航班号错误的机票。

此外，大语言模型还面临着知识时效性不足和训练数据中存在的偏见等挑战。大语言模型的能力来源于其训练数据。这意味着模型所掌握的知识是其训练数据收集时的最新材料。对于在此日期之后发生的事件、新出现的概念或最新的事实，模型将无法感知或正确回答。与此同时训练数据往往包含了人类社会的各种偏见和刻板印象。当模型在这些数据上学习时，它不可避免地会吸收并反映出这些偏见[12]。

为了提高大语言模型的可靠性，研究人员和开发者正在积极探索多种检测和缓解幻觉的方法：

1. **数据层面**： 通过高质量数据清洗、引入事实性知识以及强化学习与人类反馈 (RLHF) 等方式[13]，从源头减少幻觉。
2. **模型层面**： 探索新的模型架构，或让模型能够表达其对生成内容的不确定性。
3. **推理与生成层面**：
   1. **检索增强生成 (Retrieval-Augmented Generation, RAG)** [14]： 这是目前缓解幻觉的有效方法之一。RAG 系统通过在生成之前从外部知识库（如文档数据库、网页）中检索相关信息，然后将检索到的信息作为上下文，引导模型生成基于事实的回答。
   2. **多步推理与验证**： 引导模型进行多步推理，并在每一步进行自我检查或外部验证。
   3. **引入外部工具**： 允许模型调用外部工具（如搜索引擎、计算器、代码解释器）来获取实时信息或进行精确计算。

尽管幻觉问题短期内难以完全消除，但通过上述的策略，可以显著降低其发生频率和影响，提高大语言模型在实际应用中的可靠性和实用性。

##### 2.4 温度

`Temperature`：温度是控制模型输出 “随机性” 与 “确定性” 的关键参数。其原理是引入温度系数 $T> 0$，将 Softmax 改写为 $P_{i}^{(T)} = \frac{e^{z_i}}{\sum_{j=1}^{k}{e^{\frac{z_j}{T}}}}$ 

当T变小时，分布“更加陡峭”，高概率项权重进一步放大，生成更“保守”且重复率更高的文本。当T变大时，分布“更加平坦”，低概率项权重提升，生成更“多样”但可能出现不连贯的内容。

- 低温度（0 ⩽ Temperature < 0.3）时输出更 “精准、确定”。适用场景： 事实性任务：如问答、数据计算、代码生成； 严谨性场景：法律条文解读、技术文档撰写、学术概念解释等场景。
- 中温度（0.3 ⩽ Temperature < 0.7）：输出 “平衡、自然”。适用场景： 日常对话：如客服交互、聊天机器人； 常规创作：如邮件撰写、产品文案、简单故事创作。
- 高温度（0.7 ⩽ Temperature < 1）：输出 “创新、发散”。适用场景： 创意性任务：如诗歌创作、科幻故事构思、广告 slogan brainstorm、艺术灵感启发； 发散性思考。

#### 3. 文本嵌入与分词

N-gram 模型是 NLP 领域中一种基于统计的语言模型，广泛应用于语音识别、手写识别、拼写纠错、机器翻译和搜索引擎等众多任务。N-gram模型的核心思想是基于马尔可夫假设，即一个词的出现概率仅依赖于它前面的N-1个词。这里的N代表连续出现单词的数量，可以是任意正整数。例如，当N=1时，模型称为unigram，仅考虑单个词的概率；当N=2时，称为bigram，考虑前一个词来估计当前词的概率；当N=3时，称为trigram，考虑前两个词来估计第三个词的概率，以此类推N-gram。

N-gram模型通过条件概率链式规则来估计整个句子的概率。具体而言，对于给定的一个句子，模型会计算每个N-gram出现的条件概率，并将这些概率相乘以得到整个句子的概率。例如，对于句子“The quick brown fox”，作为trigram模型，我们会计算 P("brown"∣"The","quick")*P*("*b**ro**w**n*"∣"*T**h**e*","*q**u**i**c**k*")、P("fox"∣"quick","brown")*P*("*f**o**x*"∣"*q**u**i**c**k*","*b**ro**w**n*")等概率，并将它们相乘。

N-gram的优点是实现简单、容易理解，在许多任务中效果不错。但当N较大时，会出现数据稀疏性问题。模型的参数空间会急剧增大，相同的N-gram序列出现的概率变得非常低，导致模型无法有效学习，模型泛化能力下降。此外，N-gram模型忽略了词之间的范围依赖关系，无法捕捉到句子中的复杂结构和语义信息。

尽管存在局限性，N-gram模型由于其简单性和实用性，在许多 NLP 任务中仍然被广泛使用。在某些应用中，结合N-gram模型和其他技术（如深度学习模型）可以获得更好的性能。

##### 3.1 嵌入 embedding

$one-hot$ 独热编码存在的问题：维度灾难（维度和词表大小一样）；两个词相似程度没法表示

![image-20251109090504433](assets/image-20251109090504433.png)

词向量训练方法：Word2Vec, 这种方法它有一个重要假设，就是文本中离得越近的词语，相似度越高，有两种典型的方法，一个是CBOW，另一个是 Skip-gram

**CBOW** 方法，就是用两边词预测中间词，比如要预测"有"，它的独热编码是（000100），那就把 "爱" （001000）和 "温"（000010） 通过 WordEmbedding，WordEmbedding 是 一个 $5 \times 2$ 的矩阵，那么爱假设是 [0.5 0.3]，温是 [0.1 0.4]，然后把它们经过一个 Linear 层变成六维度的，比如 [0.1 0.2 -0.2 0.8 0.1 1.2]，然后计算爱和温的两个输出向量的平均值，经过softmax后找到概率最大的位置，应该是（000100），然后根据交叉熵损失进行训练，因为 $y_i$ 是独热的，只有正确词（比如“有”）的那个位置是 1，所以实际上：
$$
L = -\log(\hat{y}_{\text{“有”}})
$$
**Skip-Gram** 方法，通过中间词去预测上下文，它是将中间词进行编码得到softmax输出，找到最大概率的词看看是不是它旁边的词

Skip-Gram 是反过来，用一个中心词预测它的上下文。

例如中心词是 “注”，上下文是 [“关”, “意”]。

- 输入：中心词 “注” 的独热编码

- WordEmbedding 层：将“注”映射到向量 $\mathbf{v}_{注}$

- 输出层：预测每一个上下文词的概率
  $$
  P(\text{关}|\text{注}) = \text{softmax}(W' \cdot \mathbf{v}_{注})
  $$

  $$
  P(\text{意}|\text{注}) = \text{softmax}(W' \cdot \mathbf{v}_{注})
  $$

对于给定中心词 $c$ 和上下文词 $o$，希望最大化：
$$
P(o|c) = \frac{\exp(\mathbf{v}'_o \cdot \mathbf{v}_c)}{\sum_{w=1}^V \exp(\mathbf{v}'_w \cdot \mathbf{v}_c)}
$$
于是损失是负对数似然：
$$
L = -\log P(o|c)
$$
![image-20251109094618619](assets/image-20251109094618619.png)

但是这种方法都会通过 softmax 计算，当词表很大时，计算时长会增加很多，所以考虑利用负采样的方法

即对于 关 和 注 都进行编码，然后计算他们的点积，之后通过 sigmoid函数，概率大于0.5就输出1，小于0.5就输出0，比如我要训练注，如果和 关 点积那我希望是大于0.5的，而如果和一个不相关的比如 温 点积，我希望是小于0.5的

##### 3.2 分词 Tokenization

将文本序列转换为数字序列的过程，就叫做**分词 (Tokenization)** 。**分词器 (Tokenizer)** 的作用，就是定义一套规则，将原始文本切分成一个个最小的单元，我们称之为**词元 (Token)** 。

早期的自然语言处理任务可能会采用简单的分词策略：

- **按词分词 (Word-based)** ：直接用空格或标点符号将句子切分成单词。这种方法很直观，但会面临“词表爆炸”的问题。一个语言的词汇量是巨大的，如果每个词都作为一个独立的词元，词表会变得难以管理。更糟糕的是，模型将无法处理任何未在词表中出现过的词，例如 “DatawhaleAgent”。
- **按字符分词 (Character-based)** ：将文本切分成单个字符。这种方法词表很小（例如英文字母、数字和标点），不存在 OOV 问题。但它的缺点是，单个字符大多不具备独立的语义，模型需要花费更多的精力去学习如何将字符组合成有意义的词，导致学习效率低下。
- **按字节编词（BBPE）：** BBPE 用 byte （字节）构建最基础词表，将 BPE 从字符级别扩展到字节级别，在字节序列上使用 BPE 算法进行相邻合并。BBPE与BPE构建词表的流程基本一致，只是在字节级别进行合并，需要在BPE的基础上把单个字符转化为单个字节再进行合并。

字节对编码 (Byte-Pair Encoding, BPE) 是最主流的子词分词算法之一，GPT系列模型就采用了这种算法。其核心思想非常简洁，可以理解为一个“贪心”的合并过程：

1. **初始化**：将词表初始化为所有在语料库中出现过的基本字符。
2. **迭代合并**：在语料库上，统计所有相邻词元对的出现频率，找到频率最高的一对，将它们合并成一个新的词元，并加入词表。
3. **重复**：重复第 2 步，直到词表大小达到预设的阈值。

```python
import re, collections

def get_stats(vocab):
    """统计词元对频率"""
    pairs = collections.defaultdict(int)
    for word, freq in vocab.items():
        symbols = word.split()
        for i in range(len(symbols)-1):
            pairs[symbols[i],symbols[i+1]] += freq
    return pairs

def merge_vocab(pair, v_in):
    """合并词元对"""
    v_out = {}
    bigram = re.escape(' '.join(pair))
    p = re.compile(r'(?<!\S)' + bigram + r'(?!\S)')
    for word in v_in:
        w_out = p.sub(''.join(pair), word)
        v_out[w_out] = v_in[word]
    return v_out

# 准备语料库，每个词末尾加上</w>表示结束，并切分好字符
vocab = {'h u g </w>': 1, 'p u g </w>': 1, 'p u n </w>': 1, 'b u n </w>': 1}
num_merges = 4 # 设置合并次数

for i in range(num_merges):
    pairs = get_stats(vocab)
    if not pairs:
        break
    best = max(pairs, key=pairs.get)
    vocab = merge_vocab(best, vocab)
    print(f"第{i+1}次合并: {best} -> {''.join(best)}")
    print(f"新词表（部分）: {list(vocab.keys())}")
    print("-" * 20)

>>>
第1次合并: ('u', 'g') -> ug
新词表（部分）: ['h ug </w>', 'p ug </w>', 'p u n </w>', 'b u n </w>']
--------------------
第2次合并: ('ug', '</w>') -> ug</w>
新词表（部分）: ['h ug</w>', 'p ug</w>', 'p u n </w>', 'b u n </w>']
--------------------
第3次合并: ('u', 'n') -> un
新词表（部分）: ['h ug</w>', 'p ug</w>', 'p un </w>', 'b un </w>']
--------------------
第4次合并: ('un', '</w>') -> un</w>
新词表（部分）: ['h ug</w>', 'p ug</w>', 'p un</w>', 'b un</w>']
--------------------

```

#### 4. 复杂度与优化

1. **自注意力机制（Self-Attention）复杂度**

在Transformer架构中，推理的核心操作是**自注意力机制**（Self-Attention）。其复杂度通常是计算每个token与所有其他tokens之间关系的过程。具体来说，自注意力机制的计算复杂度通常是**O(n²)**，其中 **n** 是输入序列的长度。

- **原因**：每个token都需要与其他所有token进行交互（即计算注意力权重），因此对于每个token，都会有一个 **n x n** 的计算矩阵。

- **公式**：
  $$
  O(\text{Complexity}) = O(n^2 \cdot d)
  $$
  其中，$n$ 是输入序列的长度，$d$ 是每个token的维度（通常是模型的隐藏层维度）。

- **解释**：例如，假设我们有一个包含1000个token的输入序列，每个token的维度是1024。那么，单次自注意力计算的复杂度将是 $O(1000^2 \cdot 1024)$，即每次推理需要进行约1亿次计算。

2. **前馈神经网络（Feed-Forward Networks）复杂度**

在Transformer模型的每一层中，除了自注意力计算，还会有一个**前馈神经网络**（Feed-Forward Network）。该部分的计算复杂度是与每个token的维度和网络层数有关的。

- **公式**：
  $$
  O(\text{Feed-Forward}) = O(n \cdot d^2)
  $$
  其中，$n$ 是输入序列长度，$d$ 是每个token的维度（通常是隐藏层的维度）。

- **解释**：假设我们每个token的维度是1024，输入序列长度是1000。前馈网络的计算复杂度通常是与 $d^2$ 成比例的，因此，对于这个例子，计算复杂度将是 $O(1000 \cdot 1024^2)$。

3. **总的推理复杂度**

在一个Transformer模型中，通常包含多个层（例如，12层、24层或更多）。每层都包含自注意力机制和前馈神经网络。因此，总的推理复杂度通常是**每层复杂度的叠加**。

- **公式**：
  $$
  O(\text{Total Complexity}) = L \cdot O(n^2 \cdot d + n \cdot d^2)
  $$
  其中，$L$ 是模型的层数，$n$ 是序列长度，$d$ 是每个token的维度。

- **解释**：假设有12层Transformer模型，每层的计算复杂度为 $O(n^2 \cdot d + n \cdot d^2)$，并且输入序列长度是1000，token维度是1024。那总的计算复杂度将是每层的复杂度乘以层数12。

4. **空间复杂度**

推理时的**内存消耗**也是非常重要的，因为长序列输入会占用更多的内存，特别是在需要存储中间计算结果时。

- 对于自注意力层，需要存储每一层的注意力矩阵，其内存需求是 $O(n^2 \cdot d)$，因为每个token与所有其他tokens的关系需要在内存中存储。
- 如果模型非常大，特别是在长序列处理时，内存消耗会非常高。

5. KV 缓存优化

**KV Cache** 是Transformer **Decoder** 中用于加速推理的一种机制。它通过缓存**键（Key）**和值（Value）来避免在生成新token时重新计算先前的键值对。这样可以显著提高模型的推理速度，因为在生成每个新token时，模型只需关注当前token，而无需重新计算所有先前token之间的关系。使用KV缓存时，**前面生成的token的K和V可以被重复利用**，从而加速生成过程。

![image-20251108201407552](assets/image-20251108201407552.png)

![image-20251108201413803](assets/image-20251108201413803.png)

![image-20251108201421295](assets/image-20251108201421295.png)

![image-20251108201439123](assets/image-20251108201439123.png)

### 三、LLM架构对比

#### 1. gpt系列对比

GPT1采用了Transformer架构，其中包括多头自注意力机制和前向神经网络。这使得GPT1可以在处理自然语言时捕捉长距离依赖性，并且具有高效的并行性。GPT-1使用了一种称为“生成式预训练”（Generative Pre-Training，GPT）的技术。预训练分为两个阶段：预训练和微调（fine-tuning）。在预训练阶段，GPT-1使用了大量的无标注文本数据集，例如维基百科和网页文本等。通过最大化预训练数据集上的log-likelihood来训练模型参数。在微调阶段，GPT-1将预训练模型的参数用于特定的自然语言处理任务，如文本分类和问答系统等。

![image-20250722141509300](assets/image-20250722141509300.png)

GPT-2主要解决的问题是如何利用大规模未标注的自然语言文本来预训练一个通用的语言模型，从而提高自然语言处理的能力。与GPT-1模型不同之处在于，GPT-2模型使用了更大的模型规模和更多的数据进行预训练，同时增加了许多新的预训练任务。GPT-2具有零样本学习的能力，能够在只看到少量样本的情况下学习和执行新任务。**其主要变化在于训练的数据集规模变大**

GPT-3使用了更深的网络层数和更宽的Transformer网络结构，模型更大，参数更多，表达能力和语言理解能力更强； - GPT-3在预训练阶段使用了更大规模的数据集，并采用了更多样化的预训练任务。 GPT-3的微调阶段采用了zero-shot学习和few-shot的方法，使得GPT-3具备更强的泛化能力和迁移学习能力

InstructGPT：语言模型扩大并不能代表它们会更好地按照用户的意图进行工作，大语言模型很可能会生成一些不真实的、有害的或者是没有帮助的答案。换句话说，这些模型和用户的意图并不一致（not aligned with their users）。**由此OpenAI提出了“align”的概念，即希望模型的输出与人类意图“对齐”，符合人类真实偏好。** 对齐就是让模型回答符合人类的伦理和喜好，而不是乱回答，这里采用了 $RLHF$ 基于人类反馈的强化学习策略。ChatGPT就是在GPT3.5上用指示学习和人类反馈的强化学习来知道模型训练的。

![img](assets/img-1.jpeg)

1. 收集**示范数据**，进行有监督微调`SFT`。 - 标注数据：根据prompts（提示，这里就是写的各种各样的问题），人类会撰写一系列demonstrations（演示）作为模型的期望输出（主要是英文）； - 模型微调：**将prompts和人类标注的答案拼在一起，作为人工标注的数据集**，然后使用这部分数据集对预训练的GPT-3进行监督微调，得到第一个模型`SFT`（supervised fine-tuning，有监督微调) - **因为问题和答案是拼在一起的，所以在 GPT 眼中都是一样的，都是给定一段话然后预测下一个词，所以在微调上跟之前的在别的地方做微调或者是做预训练没有任何区别。** 

2. 收集**比较数据**，训练奖励模型`RM`。 - 生成式标注是很贵的一件事，所以第二步是进行排序式/判别式标注。用上一步得到的`SFT`模型生成各种问题的答案，标注者（labelers）会对这些输出进行比较和排序（由好到坏，比如下图D>C>A=B）。 - 基于这个数据集，用强化学习训练一个`RM`（reward model)。训练好了之后这个RM模型就可以对生成的答案进行打分，且打出的分数能够满足人工排序的关系。

3. 使用强化学习的机制，优化`SFT`模型，得到最终的`RL`模型（InstructGPT)。 将`SFT`模型的输出输入`RM`进行打分，通过强化学习来优化`SFT`模型的参数

#### 2. clip

如下图，在训练阶段，用文本和图像匹配，将文本编码，然后也将图像编码，计算文本向量和图像向量夹角的余弦值，然后用去训练encoder。然后在文本创建 `a photo of a {object}` 和图片去预测相似程度，然后得到分类，这样就可以进行 `zero shot` 学习，即没有样本的情况下也可以学习

![img](assets/img-0-1760844263570-1.jpeg)

#### 3. Deepseek-r1-zero

> SFT 监督微调，大量数据可能会造成模型思维固化，熵降低，少量数据可以使得模型学习领域知识
>
> 训练思路：跳过 SFT 冷启动，直接用 AIME 数据集在 Deepseek-v3 进行 RL 训练，设计提示词见上（强调模型输出思考标签），然后奖励设计是看最后答案是否正确（数学问题），代码运行能否通过测试样例（编程问题），也就是准确性设计，还有就是过程中是否使用了 `<think>` 这样的标签，训练采用 GRPO 训练，奖励模型直接用上述评判标准，参考模型和策略模型都是 Deepseek-v3

DeepSeek-R1-Zero 直接在基础模型上应用强化学习，不使用任何 SFT 数据（不用冷启动）。 为了训练 DeepSeek-R1-Zero，deepseek 采用了一种基于规则的奖励系统，该系统主要由两种奖励组成：

- **准确率奖励**：准确率奖励模型评估响应是否正确。例如，在具有确定性结果的数学问题中，模型需要以指定的格式（box）提供最终答案，从而能够通过基于规则的验证来可靠地确认正确性。同样，对于 LeetCode 问题，可以使用编译器根据预定义的测试用例生成反馈。
- **格式奖励**: 除了准确性奖励模型，还采用了一种格式奖励模型，要求模型将其思考过程放在 `<think>` 和` </think>` 标签之间。

需要强调的是：deepseek 在训练 DeepSeek-R1-Zero 时**没有使用结果奖励（ORM）或者过程奖励（PRM）**。

这里主要通过「结果判定」的方式：对于数学题、编程题等有客观正确答案的任务，可以把最终答案与标准结果对比给出奖励。虽没有逐步的过程标注，但最终答案正确与否足以在 RL 中当作回报（Reward）来引导模型学会更好的推理。

部分中间也会酌情使用**格式奖励**，用来约束模型输出思考过程，这是一种「作弊少、易维护」的思路。

在训练过程中设计的提示词，prompt 会用具体的问题替代

```text
A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. User: prompt. Assistant:
```

随着训练轮次的提升，我们可以看出模型在没有明确指导的情况下，自主学习并改进

> 在大规模强化学习中，模型的「思考过程」会不断与最终的**正确率奖励**相互作用。当模型最初得出的答案并未得到较高奖励时，它会在后续的推理中「**回头反省**」，尝试补充或修正先前的思路，从而获得更高的奖励。随着强化学习的迭代，这种「**主动回溯、推翻先前想法并重新推理**」的行为逐渐巩固，便在输出中表现为所谓的「**aha moment**」。本质上，这是 RL 为模型「留出了」足够的思考和试错空间，当模型自行发现更优思路时，就会出现类似人类「恍然大悟」的瞬间。

![image-20251118084018287](C:\Users\PC\AppData\Roaming\Typora\typora-user-images\image-20251118084018287.png)

#### 4. Deepseek-r1

##### 4.1 阶段1: 冷启动

冷启动阶段使用少量高质量的 CoT 数据对基础模型进行微调，作为 RL 训练的初始起点。侧重点是让模型掌握基本的 CoT 推理能力，并使模型的输出更具可性。

DeepSeek-R1 使用冷启动数据的主要目的是为了解决 DeepSeek-R1-Zero 在训练早期出现的训练不稳定问题。

为了获取这些数据，deepseek 探索了几种策略：利用长思维回答（COT）作为 few-shot 示例，直接提示模型生成包含反思和验证步骤的详细答案，以及收集 DeepSeek-R1-Zero 的输出并通过人工标注者进行细化。最终收集了数千条冷启动数据，用以微调 DeepSeek-V3-Base 作为 RL 训练的起点。DeepSeek-R1 创建的冷启动数据采用了一种可读模式，明确将输出格式定义为：`|special_token|<reasoning_process>|special_token<summary>`。

**数据示例：**

用户问题 prompt：若 x + 3 = 10，求 x。

冷启动训练样本

```
|special_token|
首先理解题意：x + 3 = 10。  
目标是求未知数 x。

步骤 1：将等式中的未知数 x 单独放一边。  
x + 3 = 10  

步骤 2：将 +3 移到右边，变成减法。  
x = 10 - 3  

步骤 3：计算右边：10 - 3 = 7。  
因此 x = 7。

验证步骤：  
将 x = 7 代回原式：  
7 + 3 = 10，成立。  
说明推理无误。
|special_token|
x = 7
```

**数据生成方法**

首先让 deepseek-r1-zero生成一些长推理示例，然后专家手动挑选少量高质量范例，然后加入 prompt 作为 few-shot，让模型严格按照 `|special_token| reasoning |special_token| summary` 格式输出，同时设计提示词，让模型能够自动长推理，自我检查，验证，最终总结。这样就得到第一批冷启动 CoT 数据。人工标注者对这些推理进行清洗，精简，结构化整理，形成最终的冷启动数据。冷启动数据总共是数千条

##### 4.2 阶段2：推理导向的强化学习

> 奖励信号是推理任务的准确性和语言一致性奖励直接相加

在冷启动模型的基础上进行 RL 训练，侧重点是提升模型在推理任务上的性能。在这个阶段，会引入**语言一致性奖励**，该奖励根据思维链（CoT）中目标语言单词的比例来计算，以减少推理过程中的语言混合问题。（因为 deepseek-r1-zero 有很明显的输出混乱）

尽管消融实验表明，**语言一致性奖励**会导致模型性能略有下降，但它更符合人类的偏好，提高了内容的可读性。最终，通过将推理任务的准确性与语言一致性奖励直接相加，形成了综合的奖励函数。随后，对微调后的模型进行了强化学习（RL）训练，直至其在推理任务上达到收敛。

##### 4.3 阶段3：拒绝采样和 SFT

> 拒绝采样：利用上一步训练中生成的检查点，继续通过采样的方式生成更多推理轨迹。这一过程的核心是：
>
> 1. 生成一些推理提示（reasoning prompts）。
> 2. 通过拒绝采样（基于规则的奖励函数来评估）对模型生成的多种推理路径进行筛选，保留正确的推理
>
> 核心就是通过训练后的模型生成多个回答，然后根据奖励模型或者函数保留正确的回答，具体的方法是对于有确定规则的奖励函数那么就使用它，对于没有确定规则的奖励函数，如写作等问题，采用将正确答案和模型生成的答案一起送给 deepseek-v3 作为 llm-judge 来评判

使用上一阶段的 RL 模型进行拒绝采样，生成高质量的推理和非推理数据，并用这些数据对模型进行微调。侧重点是提升模型的综合能力，使其在写作、事实问答等多种任务上表现良好。

当 RL 训练接近收敛时，使用中间的 checkpoint 来采样监督微调（SFT）数据。与初期主要关注推理能力的冷启动数据不同，这一阶段加入了其他领域的数据，旨在增强模型在写作、角色扮演以及其他通用任务上的表现。具体的数据生成和模型微调步骤如下：

- 对于推理数据，构建推理 prompt，并从上述 RL 训练的 checkpoint 中进行拒绝采样，以生成推理轨迹。在之前的阶段，仅使用了基于规则的奖励来评估数据。然而，在这个阶段，通过添加其他数据来丰富数据集，其中部分数据使用了**生成奖励模型**，通过将真实值和模型预测输入 DeepSeek-V3 进行判断。同时，为了提升数据质量，过滤掉混合语言、长段落和代码块的思维链。对于每个提示，采样多个响应，并仅保留正确的响应。最终，收集了大约**60万**个与推理相关的训练样本。
- 对于非推理数据，如写作、问答、翻译等任务，使用 DeepSeek-V3 SFT 数据集的一部分。对于简单的 query，如“你好”，不使用思维链作为回答。经过筛选和整理，最终收集了大约**20万**个与推理无关的训练样本。

最终，使用大约**80万**个样本（60w推理+20w通用）对 DeepSeek-v3-Base 模型进行了两轮的 SFT。

##### 4.4 阶段4：所有场景下的强化学习

> **Diverse Prompt Distributions**（多样化提示分布）是指在模型训练中，使用 **各种不同类型的提示（prompts）** 来生成训练数据，以确保模型能够适应更广泛的任务和场景。

该阶段的目标是：

- 提高模型的 **推理能力**，使其在处理数学、代码和逻辑推理任务时表现得更加精准。
- 增强模型的 **有用性（helpfulness）**，使其生成的回答对用户更具实际价值。
- 减少和消除潜在的 **无害性（harmlessness）**，避免生成有偏见、有害或不适当的内容。

利用上个阶段获得的数据进行训练

##### 4.5 失败的尝试

**1. PRM 的挑战**

- 难以定义通用的、细粒度的推理步骤。
- 难以准确判断中间步骤的正确性，且自动标注方法效果不佳，人工标注又难以扩展。
- 模型化的 PRM 容易导致**奖励黑客**（Agent 利用奖励函数或环境中的漏洞来获取高奖励，而并未真正学习到预期行为。）行为，并且会增加额外的训练成本。

**2. MCTS 的挑战**

MCTS 被用来增强 **推理任务**，特别是针对 **生成推理步骤的任务**。具体应用过程如下：

- **分解答案**：为了解决复杂问题，DeepSeek 将一个复杂的答案分解为多个较小的推理步骤。这些步骤有助于模型更系统地探索解空间。
  - 例如，对于一个问题，模型不仅要给出最终答案，还需要通过多个推理步骤（如逻辑推理、验证等）逐步推导出正确答案。
- **使用预训练的价值模型引导搜索**：
  - MCTS 通过 **预训练的价值模型** 来 **引导推理过程**，帮助模型决定如何进行搜索。
  - 预训练价值模型的作用是提供对每个步骤的“评分”或“优先级”，指导模型做出更合理的推理路径选择。
- **迭代优化**：模型通过 MCTS 生成多个问题-答案对，这些数据用于训练 **演员模型（Actor Model）** 和 **价值模型（Value Model）**，不断优化模型的推理能力。

主要问题

- LLM 的 token 生成搜索空间巨大，远远超出棋类游戏，容易陷入局部最优解。
- 价值模型的训练非常困难，导致难以迭代提升。

![img](https://pic1.zhimg.com/v2-6a0549b43f4d5377b1c0cef626d4dd7e_r.jpg)

### 四、蒸馏技术

#### 1. 知识蒸馏简介

知识蒸馏使用的是Teacher—Student模型，其中teacher是“知识”的输出者，student是“知识”的接受者。知识蒸馏的过程分为2个阶段:

1. 原始模型训练: 训练"Teacher模型", 简称为Net-T，它的特点是模型相对复杂，也可以由多个分别训练的模型集成而成。我们对"Teacher模型"不作任何关于模型架构、参数量、是否集成方面的限制，唯一的要求就是，对于输入X, 其都能输出Y，其中Y经过softmax的映射，输出值对应相应类别的概率值。
2. 精简模型训练: 训练"Student模型", 简称为Net-S，它是参数量较小、模型结构相对简单的单模型。同样的，对于输入X，其都能输出Y，Y经过softmax映射后同样能输出对应相应类别的概率值。

一个很直白且高效的迁移泛化能力的方法就是：使用softmax层输出的类别的概率来作为“soft target”。

1. 传统training过程(**hard targets**): 对ground truth求极大似然
2. KD的training过程(**soft targets**): 用large model的class probabilities作为soft targets

softmax层的输出，除了正例之外，**负标签也带有大量的信息**，比如某些负标签对应的概率远远大于其他负标签。而在传统的训练过程(hard target)中，所有负标签都被统一对待。也就是说，**KD的训练方式使得每个样本给Net-S带来的信息量大于传统的训练方式**。

![image-20251119085147297](C:\Users\PC\AppData\Roaming\Typora\typora-user-images\image-20251119085147297.png)

先回顾一下原始的 softmax 函数：

$$
q_i = \frac{\exp(z_i)}{\sum_j \exp(z_j)}
$$

但是直接使用 softmax 层的输出值作为 soft target，这又会带来一个问题：当 softmax 输出的概率分布很接近于 0，负损失函数值很小，对损失函数的贡献非常小，小到可以忽略不计。因此，“温度”这个变量就应运而生了。

下面的公式表示了温度过后 softmax 函数：

$$
q_i = \frac{\exp(z_i / T)}{\sum_j \exp(z_j / T)}
$$

这里的 $T$ 就是温度。

原来的 softmax 函数是 $T = 1$ 的特殊例子。**温度**越高，softmax 的 output probability distribution 越接近于平坦，具有分布的翘曲，负标签携带的信息会被相对地放大，模型训练将更为负荷繁重。

#### 2. 具体步骤

![img](https://pic4.zhimg.com/v2-d01f5142d06aa27bc5e207831b5131d9_r.jpg)

训练 Net-T 的过程很简单，下面详细讲解第二步：高温蒸馏的过程。高温蒸馏过程的目标函数由 distill loss（对应 soft target）和 student loss（对应 hard target）加权得到，示意图如下。

$$
L = \alpha L_{\text{KD}} + \beta L_{\text{hard}}
$$

- $v_i$: Net-T 的 logits
- $z_i$: Net-S 的 logits
- $p_i^T$: Net-T 的温度 $T$ 下 softmax 输出在第 $i$ 类上的值
- $q_i^T$: Net-S 的温度 $T$ 下 softmax 输出在第 $i$ 类上的值
- $c_i$: 在类别 $i$ 上的 ground truth 值，$c_i \in \{0, 1\}$，正确标签为 1，负标签为 0
- $N$: 总样本数

Net-T 和 Net-S 同时输入训练集（这里可以直接复用训练 Net-T 用到的 training set），用 Net-T 产生的 softmax distribution (with high temperature) 来作为 soft target，Net-S 在相同温度下条件下的 softmax 输出和 soft target 的 cross entropy 就是 Loss 函数的第一部分 $L_{\text{soft}}$。可以用交叉熵损失，也可以用KL散度

$$
L_{\text{KD}} = T^2 \sum_{i=1}^{N} p_j^T [log(p_j^T)-log(q_j^T)], \quad p_i^T = \frac{\exp(v_i / T)}{\sum_k \exp(v_k / T)}, \quad q_i^T = \frac{\exp(z_i / T)}{\sum_k \exp(z_k / T)}
$$

Net-S 在 $T = 1$ 时的 softmax 输出和 ground truth 的 cross entropy 就是 Loss 函数的第二部分 $L_{\text{hard}}$。

$$
L_{\text{hard}} = - \sum_{i=1}^{N} c_j \log(q_j^1), \quad q_i^1 = \frac{\exp(z_i)}{\sum_k \exp(z_k)}
$$

第二部分 Loss $L_{\text{hard}}$ 的解释实际上是：Net-T 也有一定的错误造成；使用 ground truth 可以有效地传递给学生模型 Net-S 的信息。打个比方，老师虽然远离学生，但学生仍然能从老师的教学反馈中得到帮助；而当情况下学生在老师的指导下学习，就可以有效降低损失函数中的错误 "常温" 的可避免性。

#### 3. 分类

1. 软标签蒸馏 （soft-label）也就是logits蒸馏

   ![动图](https://pic3.zhimg.com/v2-7f36217f119d5e5c321f6cfa02997496_b.webp)

   - 使用一个已训练好的教师模型对整份语料生成 softmax 概率分布（即“软标签”）。
   - 同时也将相同的数据输入未经训练的学生模型，获取它的 softmax 概率分布。
   - 训练学生模型，使其输出尽可能匹配教师模型的概率分布。
   - 可见的 softmax 概率可以最大化地实现知识（或推理能力）的迁移。

   不过，这种方法有个问题：

   - 你必须能访问教师模型的权重，才能获取完整的输出概率分布。
   - 即使你能访问，还有另一个挑战：

   假设你的词表大小是 10 万个 token，语料总量为 5 万亿个 token。由于每个 token 都需要生成整个词表上的 softmax 概率，在 float8 精度下，存储所有“软标签”大约需要 **500 万 GB** 的内存。

2. 硬标签蒸馏（hard-label）也就是数据蒸馏

   ![动图](https://pica.zhimg.com/v2-afc7b5ad5b465e9b7e69bf4f16d832f6_b.webp)

   - 使用一个固定的已训练教师模型，只获取最终预测的 one-hot 输出 token（即“硬标签”）。
   - 使用未经训练的学生模型对相同数据获取 softmax 概率。
   - 训练学生模型，使其输出概率尽可能匹配教师模型的 one-hot 标签。

   DeepSeek 就是采用这种方式，将 DeepSeek-R1 蒸馏到了 Qwen 和 Llama 3.1 中。

3. 协同蒸馏（co-distillation）

   ![动图](https://pic1.zhimg.com/v2-f838aac249a040c5f4b79941040298aa_b.webp)

   - 从一个未经训练的教师模型和一个未经训练的学生模型开始。
   - 对当前 batch 的输入，两者都生成 softmax 概率分布。
   - 教师模型按常规使用“硬标签”训练。
   - 学生模型则训练使其概率分布尽可能匹配教师模型的 softmax 输出。

   Llama 4 就是用这种方式，从 Llama 4 Behemoth 中训练出了 Llama 4 Scout 和 Maverick。

   当然，在初始阶段，教师模型本身的 softmax 输出也不太准确。

   因此，学生模型的训练会同时参考教师的“软标签”与真实的“硬标签”。

#### 4. 具体示例

用 ResNet-18 作为教师蒸馏 CNN

```python
torch.manual_seed(args.seed)
if args.cuda:
    torch.cuda.manual_seed(args.seed)


kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}
train_loader = torch.utils.data.DataLoader(
    datasets.MNIST('./data_mnist', train=True, download=True,
                   transform=transforms.Compose([
                       transforms.ToTensor(),
                       transforms.Normalize((0.1307,), (0.3081,))
                   ])),
    batch_size=args.batch_size, shuffle=True, **kwargs)

test_loader = torch.utils.data.DataLoader(
    datasets.MNIST('./data_mnist', train=False, transform=transforms.Compose([
                       transforms.ToTensor(),
                       transforms.Normalize((0.1307,), (0.3081,))
                   ])),
    batch_size=args.test_batch_size, shuffle=True, **kwargs)


class teacherNet(nn.Module):
    def __init__(self):
        super(teacherNet, self).__init__()
        self.fc1 = nn.Linear(28 * 28, 1200)
        self.fc2 = nn.Linear(1200, 1200)
        self.fc3 = nn.Linear(1200, 10)

    def forward(self, x):
        x = x.view(-1, 28 * 28)
        x = F.relu(self.fc1(x))
        x = F.dropout(x, p=0.8, training=self.training)
        x = F.relu(self.fc2(x))
        x = F.dropout(x, p=0.8, training=self.training)
        x = self.fc3(x)
        return x

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(28 * 28, 800)
        self.fc2 = nn.Linear(800, 800)
        self.fc3 = nn.Linear(800, 10)

    def forward(self, x):
        x = x.view(-1, 28 * 28)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

teacher_model = teacherNet()
teacher_model.load_state_dict(torch.load('teacher_MLP.pth.tar'))


model = Net()
if args.cuda:
    model.cuda()

optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)

def distillation(y, labels, teacher_scores, T, alpha):
    return nn.KLDivLoss()(F.log_softmax(y/T), F.softmax(teacher_scores/T)) * (T*T * 2.0 * alpha) + F.cross_entropy(y, labels) * (1. - alpha)


def train(epoch, model, loss_fn):
    model.train()
    teacher_model.eval()
    for batch_idx, (data, target) in enumerate(train_loader):
        if args.cuda:
            data, target = data.cuda(), target.cuda()
        data, target = Variable(data), Variable(target)
        optimizer.zero_grad()
        output = model(data)
        teacher_output = teacher_model(data)
        teacher_output = teacher_output.detach()
        # teacher_output = Variable(teacher_output.data, requires_grad=False) #alternative approach to load teacher_output
        loss = loss_fn(output, target, teacher_output, T=20.0, alpha=0.7)
        loss.backward()
        optimizer.step()
        if batch_idx % args.log_interval == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * len(data), len(train_loader.dataset),
                100. * batch_idx / len(train_loader), loss.data[0]))

def train_evaluate(model):
    model.eval()
    train_loss = 0
    correct = 0
    for data, target in train_loader:
        if args.cuda:
            data, target = data.cuda(), target.cuda()
        data, target = Variable(data, volatile=True), Variable(target)
        output = model(data)
        train_loss += F.cross_entropy(output, target).data[0] # sum up batch loss
        pred = output.data.max(1, keepdim=True)[1]
        correct += pred.eq(target.data.view_as(pred)).cpu().sum()

    print('\nTrain set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
        train_loss, correct, len(train_loader.dataset),
        100. * correct / len(train_loader.dataset)))


def test(model):
    model.eval()
    test_loss = 0
    correct = 0
    for data, target in test_loader:
        if args.cuda:
            data, target = data.cuda(), target.cuda()
        data, target = Variable(data, volatile=True), Variable(target)
        output = model(data)
        # test_loss += F.cross_entropy(output, target).data[0] # sum up batch loss
        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability
        correct += pred.eq(target.data.view_as(pred)).cpu().sum()

    test_loss /= len(test_loader.dataset)
    print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
        test_loss, correct, len(test_loader.dataset),
        100. * correct / len(test_loader.dataset)))


for epoch in range(1, args.epochs + 1):
    train(epoch, model, loss_fn=distillation)
    train_evaluate(model)
    test(model)


torch.save(model.state_dict(), 'distill.pth.tar')
# the_model = Net()
# the_model.load_state_dict(torch.load('student.pth.tar'))

# test(the_model)
# for data, target in test_loader:
#     data, target = Variable(data, volatile=True), Variable(target)
#     teacher_out = the_model(data)
# print(teacher_out)
print("--- %s seconds ---" % (time.time() - start_time))
```





























