### 一、深度学习、强化学习、智能体基础

#### 1. 强化学习整体流程

强化学习的两个实体：**智能体（Agent）**与**环境（Environment）**

强化学习中两个实体的交互：

- **状态空间S**：S即为State，指环境中所有可能状态的集合
- **动作空间A**：A即为Action，指智能体所有可能动作的集合
- **奖励R：**R即为Reward，指智能体在环境的某一状态下所获得的奖励。
- **策略P：** 即Policy，决定在给定状态下采取哪个动作的策略
- **值函数V：** 即Value Function，估计某个动作或者状态的价值，即期望的未来汇报

![image-20251002100645336](assets/image-20251002100645336.png)

上面的奖励它表示环境进入状态 下的**即时奖励**。但如果只考虑即时奖励，目光似乎太短浅了：当下的状态和动作会影响到未来的状态和动作，进而影响到未来的整体收益。
所以，一种更好的设计方式是：**t时刻状态s的总收益 = 身处状态s能带来的即时收益 + 从状态s出发后能带来的未来收益。**写成表达式就是：$V_t = R_t + \gamma V_{t+1}$ 

#### 2. nlp中的强化学习

- 我们先喂给模型一个 prompt，期望它能产出符合人类喜好的 response
- 在 $t$ 时刻，模型根据上文，产出一个 token，这个 token 即对应着强化学习中的动作，我们记为 $A_t$。因此不难理解，在 NLP 语境下，强化学习任务的动作空间就对应着词表。
- 在 $t$ 时刻，模型产出 token $A_t$ 对应的即时收益为 $R_t$，总收益为 $V_t$（复习一下，$V_t$ 蕴含着“即时收益”与“未来收益”两个内容）。这个收益即可以理解为“对人类喜好的衡量”。此刻，模型的状态从 $S_t$ 变为 $S_{t+1}$，也就是从“上文”变成“上文 + 新产出的 token”
- 在 NLP 语境下，智能体是语言模型本身，环境则对应着它产出的语料

![image-20251002100710478](assets/image-20251002100710478.png)

#### 3. RLHF

如上图，**在RLHF-PPO阶段，一共有四个主要模型**，分别是：

- **Actor Model：演员模型**，这就是我们想要训练的目标语言模型，用 $SFT$ 阶段产生的模型对它进行初始化
- **Critic Model：评论家模型**，它的作用是预估总收益，从reward model初始化而来
- **Reward Model：奖励模型**，它的作用是计算即时收益 
- **Reference Model：参考模型**，它的作用是在RLHF阶段给语言模型增加一些“约束”，防止语言模型训歪（朝不受控制的方向更新，效果可能越来越差），也是用SFT阶段模型初始化，在训练过程中参数是冻结的

对Actor模型，我们喂给它一个prompt，它正常输出对应的response。那么response中每一个token肯定有它对应的log_prob结果，我们把这样的结果记为**log_probs**；对Ref模型，我们把Actor生成的"prompt + response"喂给它，那么它同样能给出每个token的log_prob结果，我们记其为**ref_log_probs**；那么这两个模型的输出分布相似度就可以用**`ref_log_probs - log_probs`**来衡量

![image-20250722153320724](assets/image-20250722153320724.png)

**损失函数计算：**

（1）直观设计

- Actor 接收到当前上文 $S_t$，产出 token $A_t$ （$P(A_t \mid S_t)$）
- Critic 根据 $S_t, A_t$，产出对总收益的预测 $V_t$
- 那么 Actor loss 可以设计为：

$$
actor\_loss = -\sum_{t \in response\_timestep} V_t \log P(A_t \mid S_t)
$$

求和符号表示我们只考虑 response 部分所有 token 的 loss，为了表达简便，我们先把这个求和符号略去（下文也是同理），也就是说：

$$
actor\_loss = -V_t \log P(A_t \mid S_t)
$$

我们希望 minimize 这个 actor\_loss。

（2）引入优势（Advantage）

在开始讲解之前，我们举个小例子：

假设在王者中，中路想支援发育路，这时中路有两种选择：1. 走自家野区。2. 走大龙路。  
中路选择走大龙路，当她做出这个决定后，Critic 告诉她可以收 1 个人头。  
结果，此刻对面打野正在自家采灵芝，对面也没有什么守塔英雄，中路一路直上，最终收割到 2 个人头。  
因为实际收到的人头比预期要多 1 个，中路尝到了“甜头”，所以她增大了“支援发育路走大龙路”的概率。  

这个多出来的“甜头”，就叫做“优势”（Advantage）。

---

对 NLP 任务来说，如果 Critic 对 $A_t$ 的总收益预测为 $V_t$，但实际执行 $A_t$ 后的总收益是  $R_t + \gamma * V_{t+1}$，我们就定义优势为：

$$
Adv_t = R_t + \gamma * V_{t+1} - V_t
$$

我们用 $Adv_t$ 替换掉 $V_t$，则此刻 actor\_loss 变为：

$$
actor\_loss = -Adv_t \log P(A_t \mid S_t)
$$
我们可以引入未来优势考量（对于未来的 $Adv_{t+1}$ 可以通过 $dp$ 算法求得，对于最后一个时刻未来收益和优势都是0倒推即可）：
$$
Adv_t = R_t + \gamma * V_{t+1} - V_t + \gamma * \lambda * Adv_{t+1}
$$
![image-20250723144537032](assets/image-20250723144537032.png)

- 第一步，我们准备一个batch的prompts
- 第二步，我们将这个batch的prompts喂给Actor模型，让它生成对应的responses
- 第三步，我们把prompt+responses喂给我们的Critic/Reward/Reference模型，让它生成用于计算actor/critic loss的数据，按照强化学习的术语，我们称这些数据为经验（experiences）。critic loss我们将在后文做详细讲解，目前我们只把目光聚焦到actor loss上
- 第四步，我们根据这些经验，实际计算出actor/critic loss，然后更新Actor和Critic模型

#### 4. DPO

直接将人类的偏好对用于模型的训练，以达到最小的损失满足

![图1：RLHF和DPO方法的比较](assets/img-0.jpeg)

#### 5. agent 设计模式

**反思（Reflection）**：这是指系统能够自我反省或自我评估。在智能体设计中，反思通常意味着智能体能够回顾自己的行为、决策和结果，从而进行调整和改进。

**工具使用（Tool use）**：智能体不仅仅是执行任务，还可以利用外部工具来帮助完成任务。工具使用可以是智能体利用某种设备或技术，来增强其能力和效率。

**规划（Planning）**：智能体能够为实现目标或任务进行规划，选择最佳路径或策略。这种设计模式强调如何通过预先设定的步骤来达成特定目标。

**多智能体协作（Multi-agent collaboration）**：这是指多个智能体之间的合作，共同完成某个任务或解决问题。多个智能体能够相互协作，协调工作以达成共同目标。

### 二、LLM 基础

#### 1. 常见性质

外推性：大模型在训练时和预测时的输入长度不一致，导致模型的泛化能力下降的问题。例如，如果一个模型在训练时只使用了512个 token 的文本，那么在预测时如果输入超过512个 token，模型可能无法正确处理。这就限制了大模型在处理长文本或多轮对话等任务时的效果。

涌现能力：当模型规模（参数量、训练数据量等）增大到某个阈值之后，会**突然**表现出一些在小模型中完全没有、甚至无法预期的复杂能力

### 三、LLM架构对比

#### 1. gpt系列对比

GPT1采用了Transformer架构，其中包括多头自注意力机制和前向神经网络。这使得GPT1可以在处理自然语言时捕捉长距离依赖性，并且具有高效的并行性。GPT-1使用了一种称为“生成式预训练”（Generative Pre-Training，GPT）的技术。预训练分为两个阶段：预训练和微调（fine-tuning）。在预训练阶段，GPT-1使用了大量的无标注文本数据集，例如维基百科和网页文本等。通过最大化预训练数据集上的log-likelihood来训练模型参数。在微调阶段，GPT-1将预训练模型的参数用于特定的自然语言处理任务，如文本分类和问答系统等。

![image-20250722141509300](assets/image-20250722141509300.png)

GPT-2主要解决的问题是如何利用大规模未标注的自然语言文本来预训练一个通用的语言模型，从而提高自然语言处理的能力。与GPT-1模型不同之处在于，GPT-2模型使用了更大的模型规模和更多的数据进行预训练，同时增加了许多新的预训练任务。GPT-2具有零样本学习的能力，能够在只看到少量样本的情况下学习和执行新任务。**其主要变化在于训练的数据集规模变大**

GPT-3使用了更深的网络层数和更宽的Transformer网络结构，模型更大，参数更多，表达能力和语言理解能力更强； - GPT-3在预训练阶段使用了更大规模的数据集，并采用了更多样化的预训练任务。 GPT-3的微调阶段采用了zero-shot学习和few-shot的方法，使得GPT-3具备更强的泛化能力和迁移学习能力

InstructGPT：语言模型扩大并不能代表它们会更好地按照用户的意图进行工作，大语言模型很可能会生成一些不真实的、有害的或者是没有帮助的答案。换句话说，这些模型和用户的意图并不一致（not aligned with their users）。**由此OpenAI提出了“align”的概念，即希望模型的输出与人类意图“对齐”，符合人类真实偏好。** 对齐就是让模型回答符合人类的伦理和喜好，而不是乱回答，这里采用了 $RLHF$ 基于人类反馈的强化学习策略。ChatGPT就是在GPT3.5上用指示学习和人类反馈的强化学习来知道模型训练的。

![img](assets/img-1.jpeg)

1. 收集**示范数据**，进行有监督微调`SFT`。 - 标注数据：根据prompts（提示，这里就是写的各种各样的问题），人类会撰写一系列demonstrations（演示）作为模型的期望输出（主要是英文）； - 模型微调：**将prompts和人类标注的答案拼在一起，作为人工标注的数据集**，然后使用这部分数据集对预训练的GPT-3进行监督微调，得到第一个模型`SFT`（supervised fine-tuning，有监督微调) - **因为问题和答案是拼在一起的，所以在 GPT 眼中都是一样的，都是给定一段话然后预测下一个词，所以在微调上跟之前的在别的地方做微调或者是做预训练没有任何区别。** 

2.收集**比较数据**，训练奖励模型`RM`。 - 生成式标注是很贵的一件事，所以第二步是进行排序式/判别式标注。用上一步得到的`SFT`模型生成各种问题的答案，标注者（labelers）会对这些输出进行比较和排序（由好到坏，比如下图D>C>A=B）。 - 基于这个数据集，用强化学习训练一个`RM`（reward model)。训练好了之后这个RM模型就可以对生成的答案进行打分，且打出的分数能够满足人工排序的关系。

3.使用强化学习的机制，优化`SFT`模型，得到最终的`RL`模型（InstructGPT)。 将`SFT`模型的输出输入`RM`进行打分，通过强化学习来优化`SFT`模型的参数

#### 2. clip

如下图，在训练阶段，用文本和图像匹配，将文本编码，然后也将图像编码，计算文本向量和图像向量夹角的余弦值，然后用去训练encoder。然后在文本创建 `a photo of a {object}` 和图片去预测相似程度，然后得到分类，这样就可以进行 `zero shot` 学习，即没有样本的情况下也可以学习

![img](assets/img-0-1760844263570-1.jpeg)

