## 大模型算法

### 一、PyTorch基础

#### 1. 模型评价指标

**混淆矩阵**

用来看每个类别之间的准确率，常用来做目标分类

![image-20251021090747400](assets/image-20251021090747400.png)

代码如下

```python
from sklearn.metrics import confusion_matrix
def compute_confusion_matrix(labels,pred_labels_list,gt_labels_list):
    pred_labels_list = np.asarray(pred_labels_list)
    gt_labels_list = np.assarray(gt_labels_list)
    matrix = confusion_matrix(test_label_list,
                              pred_label_list,
                              labels=labels)
    return matrix
```

**Overall Accuracy**

OA 代表了所有预测正确的样本占总样本的比例，公式为 $OA = \frac{TP+TN}{TP+TN+FP+FN}$ 

**Average accuracy**

$AA = \frac{(\frac{TP}{TP+FN} + \frac{TN}{TN+FP})}{2}$

**Recall**

$Recall = \frac{TP}{TP+FN}$，正样本被正确识别的比例

**Precision**

$Precsion = \frac{TP}{TP+FP}$，预测正确的正样本所占的比例

**F1**

$F1 = 2 \times \frac{P \times R}{P + R}$ ，召回率和精确率的加权平均，值越高表明鲁棒性越好

**PR曲线**

横轴是召回率，纵轴代表了P（精确率)，P-R曲线上的一个点代表着，在某一阈值下，模型将大于该阈值的结果判定为正样本，小于该阈值的结果判定为负样本，此时返回结果对应的召回率和精确率。整条P-R曲线是通过将阈值从高到低移动而生成的。原点附近代表当阈值最大时模型的精确率和召回率，在PR曲线我们可以从图中直观的看到某一个曲线被另外一条曲线完全包裹，所包围的面积大于另一条曲线包围的面积，举例图中可以说明A模型的性能优于B和C。

![image-20251021092018100](assets/image-20251021092018100.png)

**置信度**

在目标检测中，我们通常需要将边界框内物体划分为正样本和负样本。我们使用置信度这个指标来进行划分，当小于置信度设置的阈值判定为负样本（背景），大于置信度设置的阈值判定为正样本.

**IOU**

$IOU = \frac{A \bigcap B}{A \bigcup B}$ 分子部分是模型预测框与真实标注框之间的重叠区域，分母部分是两者的并集，预测框和实际框所占有的总区域，在实际模型识别时会根据我们自己设定合适的阈值来判定正负样本。

![image-20251021092234082](assets/image-20251021092234082.png)

**AP、mAP**

PR 曲线包围的面积，mAP是所有类的 AP 值的平均

**交叉熵损失和 KL 散度**

1. **信息熵（Entropy）**

信息熵用于表示随机变量不确定性的度量。它反映了信息源的混乱程度，也就是如果要描述 1 bit 的信息需要多少bit：

- **公式**：
  $$
  H(P) = - \sum_i P(x_i) \log P(x_i)
  $$

- **高熵**：当数据分布较为分散时，信息熵较高，表示不确定性大。

- **低熵**：当数据集中在某些特定值上时，信息熵较低，表示不确定性较小。

2. **交叉熵（Cross-Entropy）**

交叉熵衡量的是预测分布与真实分布之间的信息差异，常用于分类问题中

- **公式**：
  $$
  H(P, Q) = - \sum_i P(x_i) \log Q(x_i)
  $$

- **用途**：它用来衡量实际分布 $P$ 和预测分布 $Q$ 之间的差异。

- 在大模型中，通常  $P(x)=1$ ，即 token 应该输出是确定的

3. **KL 散度（Kullback-Leibler Divergence）**

KL 散度衡量的是从分布 $Q$ 到分布 $P$ 的信息损失：

- **公式**：
  $$
  D_{KL}(P \parallel Q) = \sum_i P(x_i) \log \frac{P(x_i)}{Q(x_i)}
  $$

- **KL 散度与交叉熵的关系**：
  $$
  D_{KL}(P \parallel Q) = H(P, Q) - H(Q)
  $$
  这表明，KL 散度等于交叉熵与真实分布的熵之间的差异。 KL 散度是表示用 $Q$ 去衡量 $P$ 的时候需要多用的比特数

  在大模型训练中，通常使用交叉熵作为损失函数，因为训练数据提供的真实分布 $P$ 是一个 one-hot 向量即模型明确告诉我们期望的 token 是哪一个，因此它的信息熵为 0

4. **KL 散度的物理意义**

- KL 散度表示从分布 $Q$ 编码真实分布 $P$ 所需的额外信息量。通常它是大于零的，且只有当 $P = Q$ 时，KL 散度等于零。

- **为什么 KL 不是一个真正的距离度量？**

  1. KL 散度不满足对称性，即 $KL(P \parallel Q) \neq KL(Q \parallel P)$。
  2. KL 散度不满足三角不等式。

  因此，KL 散度不是一个距离度量。

5. **KL 散度在损失函数中的应用**

KL 散度可以作为损失函数使用，尤其在训练模型时，KL 散度帮助优化模型的预测分布，使其尽可能接近真实分布。具体来说，当真实分布 $Q$ 固定时，KL 散度越小，模型预测的分布与真实分布越接近。

#### 2. 张量

创建

```python
import torch
# 随机创建一个四行三列的向量
x = torch.randn(4, 3)
x = torch.zeros(4, 3, dtype = torch.long)
x = torch.tensor([5.5, 3]) # 创建一个包含两个元素的向量
x = torch.randn_like(x, dtype = torch.float)
y = x.int() # 转换成整数类型
```

获取维度信息

```python
print(x.size())
print(x.shape)
```

```python
single_distance = torch.tensor(25) 
print(single_distance.shape)
# torch.tensor([]) 这是一个标量，最低维度是[1, 1]
with_batch = single_distance.unsqueeze(0)
# torch.tenosr([1])
ready_for_model = with_distance.unsqueeze(1)
# torch.tenosr([1, 1])
value = ready_for_model[0].item() 
```

常见 Tensor 的构建方法

| 函数                | 功能                                              |
| :------------------ | :------------------------------------------------ |
| Tensor(sizes)       | 基础构造函数                                      |
| tensor(data)        | 类似于np.array                                    |
| ones(sizes)         | 全1                                               |
| zeros(sizes)        | 全0                                               |
| eye(sizes)          | 对角为1，其余为0                                  |
| arange(s,e,step)    | 从s到e，步长为step                                |
| linspace(s,e,steps) | 从s到e，均匀分成step份                            |
| rand/randn(sizes)   | rand是[0,1)均匀分布；randn是服从N(0，1)的正态分布 |
| normal(mean,std)    | 正态分布(均值为mean，标准差是std)                 |
| randperm(m)         | 随机排列                                          |

操作

```python
import torch
y = torch.rand(4, 3)
print(x + y)
print(torch.add(x, y))
```

```python
x = torch.rand(4, 3)
print(x[:, 1])
# 输出
tensor([-0.0720,  0.0666,  1.0336, -0.6965])
# 索引出来的结果与原数据共享内存，修改一个另一个也会更改，如果不想更改使用copy()等方法
y = x[0, :]
```

```python
x = torch.randn(4, 4)
# torch.view 也是共享内存的
y = x.view(16) # 或者torch.reshape()
z = x.view(-1, 8)
print(x.size(), y.size(), z.size())
torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])
```

广播

当两个向量最后一维对齐，如果这一维度是相同的或者其中之一是1的时候，视为相同，可以将维度为1的那个张量广播

```python
x = torch.arange(1, 3).view(1, 2)
print(x)
y = torch.arange(1, 4).view(3, 1)
print(y)
print(x + y)
```

```test
tensor([[1, 2]])
tensor([[1],
        [2],
        [3]])
tensor([[2, 3],
        [3, 4],
        [4, 5]])
```

自动求导

默认 `requires_grad` 为 `False`

如果设置它的属性` .requires_grad` 为 `True`，那么它将会追踪对于该张量的所有操作。当完成计算后可以通过调用` .backward()`，来自动计算所有的梯度。这个张量的所有梯度将会自动累加到`.grad`属性。

在 y.backward() 时，如果 y 是标量，则不需要为 backward() 传入任何参数；否则，需要传入一个与 y 同形的Tensor。

代码块包装在 `with torch.no_grad(): `中，在评估模型时特别有用，因为模型可能具有 `requires_grad = True` 的可训练的参数，但是我们不需要在此过程中对他们进行梯度计算。

```python
x = torch.rann(4, 3, requires_grad = True)
```

每个张量都有一个`.grad_fn`属性，该属性引用了创建 `Tensor `自身的`Function`(除非这个张量是用户手动创建的，即这个张量的`grad_fn`是 `None` )。上面给出的例子中，张量由用户手动创建，因此grad_fn返回结果是None。

```python
y = x**2
print(y)

tensor([[1., 1.],
        [1., 1.]], grad_fn=<PowBackward0>)
```

反向传播是累加的

```python
# 再来反向传播⼀一次，注意grad是累加的
out2 = x.sum()
out2.backward()
print(x.grad)

out3 = x.sum()
x.grad.data.zero_()
out3.backward()
print(x.grad)
```

#### 3. 基本流程

自己定制 Dataset

```python
import os
import pandas as pd
from torchvision.io import read_image

class MyDataset(Dataset):
    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):
        """
        Args:
            annotations_file (string): Path to the csv file with annotations.
            img_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
            target_transform (callable, optional): Optional transform to be applied
                on the target.
        """
        self.img_labels = pd.read_csv(annotations_file)
        self.img_dir = img_dir
        self.transform = transform
        self.target_transform = target_transform

    def __len__(self):
        return len(self.img_labels)

    def __getitem__(self, idx):
        """
        Args:
            idx (int): Index
        """
        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])
        image = read_image(img_path)
        label = self.img_labels.iloc[idx, 1]
        if self.transform:
            image = self.transform(image)
        if self.target_transform:
            label = self.target_transform(label)
        return image, label
```

设计 dataloader

```python
transform = transforms.Compose([
  transforms.ToTensor(),
  transforms.Normalize((mean,),(std,))
])

dataset = SomeDataset('./data', train = True, download = True, transform = transform)

dataset_loader = DataLoader(train_dataset, batch_size = 32, shuffle = True)

for batch_index, (data, labels) in enumerate(train_loader):
  output = model(data)
```

神经网络的构造

`Module` 类是 `torch.nn` 模块里提供的一个模型构造类，是所有神经网络模块的基类，我们可以继承它来定义我们想要的模型。下面继承 Module 类构造多层感知机。这里定义的 MLP 类重载了 `Module` 类的 `__init__` 函数和 `forward` 函数。它们分别用于创建模型参数和定义前向计算（正向传播）。下面的 MLP 类定义了一个具有两个隐藏层的多层感知机。

```python
import torch
from torch import nn

class MLP(nn.Module):
  # 声明带有模型参数的层，这里声明了两个全连接层
  def __init__(self, **kwargs):
    # 调用MLP父类Block的构造函数来进行必要的初始化。这样在构造实例时还可以指定其他函数
    super(MLP, self).__init__(**kwargs)
    self.hidden = nn.Linear(784, 256)
    self.act = nn.ReLU()
    self.output = nn.Linear(256,10)
    
   # 定义模型的前向计算，即如何根据输入x计算返回所需要的模型输出
  def forward(self, x):
    o = self.act(self.hidden(x))
    return self.output(o)   
```

网络实例化

```python
X = torch.rand(2,784) # 设置一个随机的输入张量
net = MLP() # 实例化模型
print(net) # 打印模型
net(X) # 前向计算
```

```
MLP(
  (hidden): Linear(in_features=784, out_features=256, bias=True)
  (act): ReLU()
  (output): Linear(in_features=256, out_features=10, bias=True)
)
tensor([[ 0.0149, -0.2641, -0.0040,  0.0945, -0.1277, -0.0092,  0.0343,  0.0627,
         -0.1742,  0.1866],
        [ 0.0738, -0.1409,  0.0790,  0.0597, -0.1572,  0.0479, -0.0519,  0.0211,
         -0.1435,  0.1958]], grad_fn=<AddmmBackward>)
```

我们只需要定义 `forward` 函数，`backward`函数会在使用`autograd`时自动定义，`backward`函数用来计算导数。我们可以在 `forward` 函数中使用任何针对张量的操作和计算。

一个模型的可学习参数可以通过`net.parameters()`返回

```python
params = list(net.parameters())
print(len(params))
print(params[0].size())  # conv1的权重
```

`torch.nn`只支持小批量处理 (mini-batches）。整个 `torch.nn` 包只支持小批量样本的输入，不支持单个样本的输入。比如，`nn.Conv2d` 接受一个4维的张量，即`nSamples x nChannels x Height x Width `如果是一个单独的样本，只需要使用`input.unsqueeze(0)` 来添加一个“假的”批大小维度。

- `torch.Tensor` - 一个多维数组，支持诸如`backward()`等的自动求导操作，同时也保存了张量的梯度。
- `nn.Module `- 神经网络模块。是一种方便封装参数的方式，具有将参数移动到GPU、导出、加载等功能。
- `nn.Parameter `- 张量的一种，当它作为一个属性分配给一个`Module`时，它会被自动注册为一个参数。
- `autograd.Function` - 实现了自动求导前向和反向传播的定义，每个`Tensor`至少创建一个`Function`节点，该节点连接到创建`Tensor`的函数并对其历史进行编码。

**模型初始化**

先定义模型初始化参数

```python
def initialize_weights(model):
	for m in model.modules():
		# 判断是否属于Conv2d
		if isinstance(m, nn.Conv2d):
			torch.nn.init.zeros_(m.weight.data)
			# 判断是否有偏置
			if m.bias is not None:
				torch.nn.init.constant_(m.bias.data,0.3)
		elif isinstance(m, nn.Linear):
			torch.nn.init.normal_(m.weight.data, 0.1)
			if m.bias is not None:
				torch.nn.init.zeros_(m.bias.data)
		elif isinstance(m, nn.BatchNorm2d):
			m.weight.data.fill_(1) 		 
			m.bias.data.zeros_()	
```

然后定义模型

```python
# 模型的定义
class MLP(nn.Module):
  # 声明带有模型参数的层，这里声明了两个全连接层
  def __init__(self, **kwargs):
    # 调用MLP父类Block的构造函数来进行必要的初始化。这样在构造实例时还可以指定其他函数
    super(MLP, self).__init__(**kwargs)
    self.hidden = nn.Conv2d(1,1,3)
    self.act = nn.ReLU()
    self.output = nn.Linear(10,1)
    
   # 定义模型的前向计算，即如何根据输入x计算返回所需要的模型输出
  def forward(self, x):
    o = self.act(self.hidden(x))
    return self.output(o)

mlp = MLP()
print(mlp.hidden.weight.data)
print("-------初始化-------")

mlp.apply(initialize_weights)
# 或者initialize_weights(mlp)
print(mlp.hidden.weight.data)
```

首先应该设置模型的状态：如果是训练状态，那么模型的参数应该支持反向传播的修改；如果是验证/测试状态，则不应该修改模型参数。在PyTorch中，模型的状态设置非常简便，如下的两个操作二选一即可：

```
model.train()   # 训练状态
model.eval()   # 验证/测试状态
```

我们前面在DataLoader构建完成后介绍了如何从中读取数据，在训练过程中使用类似的操作即可，区别在于此时要用for循环读取DataLoader中的全部数据。

```
for data, label in train_loader:
```

之后将数据放到GPU上用于后续计算，此处以.cuda()为例

```
data, label = data.cuda(), label.cuda()
```

开始用当前批次数据做训练时，应当先将优化器的梯度置零：

```
optimizer.zero_grad()
```

之后将data送入模型中训练：

```
output = model(data)
```

根据预先定义的criterion计算损失函数：

```
loss = criterion(output, label)
```

将loss反向传播回网络：

```
loss.backward()
```

使用优化器更新模型参数：

```
optimizer.step()
```

这样一个训练过程就完成了，后续还可以计算模型准确率等指标，这部分会在下一节的图像分类实战中加以介绍。

验证/测试的流程基本与训练过程一致，不同点在于：

- 需要预先设置torch.no_grad，以及将model调至eval模式
- 不需要将优化器的梯度置零
- 不需要将loss反向回传到网络
- 不需要更新optimizer

完整的训练和验证流程

```python
def train(epoch):
    model.train()
    train_loss = 0
    for data, label in train_loader:
        data, label = data.cuda(), label.cuda()
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, label)
        loss.backward()
        optimizer.step()
        train_loss += loss.item()*data.size(0)
    train_loss = train_loss/len(train_loader.dataset)
		print('Epoch: {} \tTraining Loss: {:.6f}'.format(epoch, train_loss))
    

def val(epoch):       
    model.eval()
    val_loss = 0
    with torch.no_grad():
        for data, label in val_loader:
            data, label = data.cuda(), label.cuda()
            output = model(data)
            preds = torch.argmax(output, 1)
            loss = criterion(output, label)
            val_loss += loss.item()*data.size(0)
            running_accu += torch.sum(preds == label.data)
    val_loss = val_loss/len(val_loader.dataset)
    print('Epoch: {} \tTraining Loss: {:.6f}'.format(epoch, val_loss))
```

模型保存

```python
from torchvision import models
model = models.resnet152(pretrained=True)
save_dir = './resnet152.pth'

# 保存整个模型
torch.save(model, save_dir)
# 保存模型权重
torch.save(model.state_dict, save_dir)
```

#### 4. 神经网络常见的层

* **无参数层**

```python
import torch
from torch import nn

class MyLayer(nn.Module):
    def __init__(self, **kwargs):
        super(MyLayer, self).__init__(**kwargs)
    def forward(self, x):
        return x - x.mean()  
```

* **含模型参数的层**

`Parameter` 类其实是 `Tensor` 的子类，如果一个 `Tensor` 是 `Parameter` ，那么它会⾃动被添加到模型的参数列表里。所以在⾃定义含模型参数的层时，我们应该将参数定义成 `Parameter` ，除了直接定义成 `Parameter` 类外，还可以使⽤ `ParameterList` 和 `ParameterDict` 分别定义参数的列表和字典。

> torch.mm 是矩阵的乘积，不支持广播只支持二维的，torch.matmul是支持广播和多维的

```python
class MyListDense(nn.Module):
    def __init__(self):
        super(MyListDense, self).__init__()
        self.params = nn.ParameterList([nn.Parameter(torch.randn(4, 4)) for i in range(3)])
        self.params.append(nn.Parameter(torch.randn(4, 1)))

    def forward(self, x):
        for i in range(len(self.params)):
            x = torch.mm(x, self.params[i])
        return x
net = MyListDense()
print(net)
```

```python
class MyDictDense(nn.Module):
    def __init__(self):
        super(MyDictDense, self).__init__()
        self.params = nn.ParameterDict({
                'linear1': nn.Parameter(torch.randn(4, 4)),
                'linear2': nn.Parameter(torch.randn(4, 1))
        })
        self.params.update({'linear3': nn.Parameter(torch.randn(4, 2))}) # 新增

    def forward(self, x, choice='linear1'):
        return torch.mm(x, self.params[choice])

net = MyDictDense()
print(net)
```

* **二维卷积层**

```python
import torch
from torch import nn

# 卷积运算（二维互相关）
def corr2d(X, K): 
    h, w = K.shape
    X, K = X.float(), K.float()
    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            Y[i, j] = (X[i: i + h, j: j + w] * K).sum()
    return Y

# 二维卷积层
class Conv2D(nn.Module):
    def __init__(self, kernel_size):
        super(Conv2D, self).__init__()
        self.weight = nn.Parameter(torch.randn(kernel_size))
        self.bias = nn.Parameter(torch.randn(1))

    def forward(self, x):
        return corr2d(x, self.weight) + self.bias
```

### 二、深度学习、强化学习、智能体基础

#### 1. 强化学习整体流程

强化学习的两个实体：**智能体（Agent）**与**环境（Environment）**

强化学习中两个实体的交互，下面就是马尔可夫决策过程（MDP）的五元组：

- **状态空间S**：S即为State，指环境中所有可能状态的集合
- **动作空间A**：A即为Action，指智能体所有可能动作的集合
- **奖励R：**R即为Reward，指智能体在环境的某一状态下所获得的奖励。
- **策略P：** 即Policy，决定在给定状态下采取哪个动作的策略
- **衰减 $\gamma$ ：**衰减系数

![image-20251002100645336](assets/image-20251002100645336.png)

上面的奖励它表示环境进入状态 下的**即时奖励**。但如果只考虑即时奖励，目光似乎太短浅了：当下的状态和动作会影响到未来的状态和动作，进而影响到未来的整体收益。
所以，一种更好的设计方式是：**t时刻状态s的总收益 = 身处状态s能带来的即时收益 + 从状态s出发后能带来的未来收益。**写成表达式就是：$V_t = R_t + \gamma V_{t+1}$ 

**回报 $G_t$：** 从时刻 $t$ 起的累计折扣奖励：$G_t = \sum_{k = 0}^{\infty}{{\gamma}^k R_{t+k}}$  

- **状态价值函数** $V_\theta(s)$: 在策略 $\pi$ 下，从状态 $s$ 开始的期望回报：

$$
V_\theta(s) = \mathbb{E}_\pi [G_t | s_t = s]
$$

从 $V_\theta(s)$ 的定义出发：
$$
V_\theta(s) = \mathbb{E}_\pi [G_t | s_t = s]
$$

将 $G_t$ 展开为奖励和未来回报：

$$
G_t = R_t + \gamma G_{t+1}
$$

代入后得到：

$$
V_\theta(s) = \mathbb{E}_\pi [R_t + \gamma G_{t+1} | S_t = s]
$$

根据期望的线性性质：

$$
V_\theta(s) = \mathbb{E}_\pi [R_t | S_t = s] + \gamma \mathbb{E}_\pi [G_{t+1} | S_t = s]
$$
即时奖励期望：
$$
\mathbb{E}_\pi [R_t | S_t = s] = \sum_a \pi(a | s) \sum_{s'} P(s' | s, a) R(s, a, s')
$$

直观理解就是所有可能状态的概率 $P(s' | s, a)$ 与对应状态的奖励 $R(s, a, s')$ 的期望与策略（Policy）模型在该状态 $s$ 下做出动作 $a$ 的概率之和。

未来回报期望：
$$
\mathbb{E}_\pi [G_{t+1} | S_t = s] = \sum_a \pi(a | s) \sum_{s'} P(s' | s, a) V_\theta(s')
$$

直观理解就是所有可能状态的概率 $P(s' | s, a)$ 与转移状态对应的状态价值函数 $V_\theta(s')$ 的期望与策略（Policy）模型在该状态 $s$ 下做出动作 $a$ 的概率之和。

合并后得到贝尔曼方程：
$$
V_\theta(s) = \sum_a \pi(a | s) \sum_{s'} P(s' | s, a) [R(s, a, s') + \gamma V_\theta(s')]
$$

- **动作价值函数** $Q_\theta(s,a)$: 在策略 $\pi$ 下，从状态 $s$ 执行动作 $a$ 后的期望回报：

$$
Q_\theta(s, a) = \mathbb{E}_\pi [G_t | s_t = s, a_t = a]
$$

类比地，从 $Q_\theta(s, a)$ 出发：

$$
Q_\theta(s, a) = \mathbb{E}_\pi [R_t + \gamma G_{t+1} | S_t = s, A_t = a]
$$

由于动作 $a$ 已确定，直接对下一个状态 $s'$ 求期望：

$$
Q_\theta(s, a) = \sum_{s'} P(s' | s, a) \left[ R(s, a, s') + \gamma \mathbb{E}_\pi [ G_{t+1} | S_{t+1} = s' ] \right]
$$

注意到：

$$
\mathbb{E}_\pi [ G_{t+1} | S_{t+1} = s' ] = V_\theta(s')
$$

状态价值函数的定义为：

$$
V_\theta(s') = \sum_{a'} \pi(a' | s') Q_\theta(s', a')
$$

直观理解就是在状态 $s'$ 下，动作价值函数的所有动作对应的期望是状态价值函数，因为：

$$
Q_\theta(s, a) = \sum_{s'} P(s' | s, a) \left[ R(s, a, s') + \gamma \sum_{a'} \pi(a' | s') Q_\theta(s', a') \right]
$$


#### 2. nlp中的强化学习

- 我们先喂给模型一个 prompt，期望它能产出符合人类喜好的 response
- 在 $t$ 时刻，模型根据上文，产出一个 token，这个 token 即对应着强化学习中的动作，我们记为 $A_t$。因此不难理解，在 NLP 语境下，强化学习任务的动作空间就对应着词表。
- 在 $t$ 时刻，模型产出 token $A_t$ 对应的即时收益为 $R_t$，总收益为 $V_t$（复习一下，$V_t$ 蕴含着“即时收益”与“未来收益”两个内容）。这个收益即可以理解为“对人类喜好的衡量”。此刻，模型的状态从 $S_t$ 变为 $S_{t+1}$，也就是从“上文”变成“上文 + 新产出的 token”
- 在 NLP 语境下，智能体是语言模型本身，环境则对应着它产出的语料

![image-20251002100710478](assets/image-20251002100710478.png)

#### 3. RLHF

如上图，**在RLHF-PPO阶段，一共有四个主要模型**，分别是：

- **Actor Model：演员模型**，这就是我们想要训练的目标语言模型，用 $SFT$ 阶段产生的模型对它进行初始化
- **Critic Model：评论家模型**，它的作用是预估总收益，从reward model初始化而来
- **Reward Model：奖励模型**，它的作用是计算即时收益 
- **Reference Model：参考模型**，它的作用是在RLHF阶段给语言模型增加一些“约束”，防止语言模型训歪（朝不受控制的方向更新，效果可能越来越差），也是用SFT阶段模型初始化，在训练过程中参数是冻结的

对Actor模型，我们喂给它一个prompt，它正常输出对应的response。那么response中每一个token肯定有它对应的log_prob结果，我们把这样的结果记为**log_probs**；对Ref模型，我们把Actor生成的"prompt + response"喂给它，那么它同样能给出每个token的log_prob结果，我们记其为**ref_log_probs**；那么这两个模型的输出分布相似度就可以用**`ref_log_probs - log_probs`**来衡量

> 可能会奇怪为什么参考模型输入是 prompt + response ，而不是 prompt 然后自己输出相应的 token 的概率，因为这些参考模型，奖励模型，评价模型考虑的是通过 策略模型 的输出来进行评价，比如我喜欢吃苹果，prompt是我喜欢，然后response是吃苹果，而如果参考模型得到我喜欢，输出第一个是喝，那就已经偏离了，我们无法评价输出效果如何，应该是看一下输出吃的概率，然后利用我喜欢吃继续，而不是用这个模型的最大概率继续

![image-20250722153320724](assets/image-20250722153320724.png)

**损失函数计算：**

（1）直观设计

- Actor 接收到当前上文 $S_t$，产出 token $A_t$ （$P(A_t \mid S_t)$）
- Critic 根据 $S_t, A_t$，产出对总收益的预测 $V_t$
- 那么 Actor loss 可以设计为：

$$
actor\_loss = -\sum_{t \in response\_timestep} V_t \log P(A_t \mid S_t)
$$

求和符号表示我们只考虑 response 部分所有 token 的 loss，为了表达简便，我们先把这个求和符号略去（下文也是同理），也就是说：

$$
actor\_loss = -V_t \log P(A_t \mid S_t)
$$

我们希望 minimize 这个 actor\_loss。直观理解就是当 $V_t$ 大于 0 时，意味着 Critic 对于 Actor 给予了正反馈，那我们就需要在训练中提高该 token 的输出概率

（2）引入优势（Advantage）

对 NLP 任务来说，如果 Critic 对 $A_t$ 的总收益预测为 $V_t$，但实际执行 $A_t$ 后的总收益是  $R_t + \gamma * V_{t+1}$，我们就定义优势为：

$$
Adv_t = R_t + \gamma * V_{t+1} - V_t
$$

我们用 $Adv_t$ 替换掉 $V_t$，则此刻 actor\_loss 变为：

$$
actor\_loss = -Adv_t \log P(A_t \mid S_t)
$$
$R_t$ 应该表示每个Actor产出token 带来的即时收益，其中 T 表示最后一个时刻

![image-20251110123106732](assets/image-20251110123106732.png)

奖励函数我们采用下面的设计
$$
R_t = -kl\_ctl * \left( \log \frac{P(A_t|S_t)}{P_{ref}(A_t|S_t)} \right), \quad t \neq T
$$

$$
R_t = -kl\_ctl * \left( \log \frac{P(A_t|S_t)}{P_{ref}(A_t|S_t)} \right) + R_t, \quad t = T
$$

- **kl_ctl**: 常量，可以理解成一个控制比值的缩放因子，在 deepspeed-chat 中默认值设为 0.1。
- **$$ - \log \frac{P(A_t|S_t)}{P_{ref}(A_t|S_t)} $$**: 这一项你是非常熟悉的，就是我们在 3.2 部分介绍的 Actor 和 Ref 模型间的 KL 散度，写成更容易理解的形式，就是 **ref_log_probs - log_probs**。在 3.2 中我们说过，为了防止模型过拟合，我们需要把这个 KL 散度加入 loss 计算中，所以这里我们就做这件事。

我们可以看出当 $t \ne T$ 时，更关系 Actor 是否在 Ref 约束下生成 token，而最后一个时刻，我们还要考虑即时收益

这是因为在Reward模型训练阶段，就是用这个位置的 来表示对完整的prompt + response的奖励预测（但不妨碍你理解成是执行完 的即时奖励），然后用这个指标来做模型eval的（但是Reward训练阶段算loss时，还是考虑了response部分所有token输出的reward值）。所以到了RLHF的场景下，其余时刻的即时奖励，我们就用“Actor是否遵循了Ref的约束”来进行评价

我们可以继续加入未来收益作为考量
$$
Adv_t = (R_t + \gamma  V_{t+1} - V_t) + \gamma \lambda  Adv_{t+1}
$$
注意到，对于最后一个时刻 ，它的未来收益和未来优势都是0，也就是 ，这是可以直接算出来的。所以可以用动态规划反推

![image-20250723144537032](assets/image-20250723144537032.png)

- 第一步，我们准备一个batch的prompts
- 第二步，我们将这个batch的prompts喂给Actor模型，让它生成对应的responses
- 第三步，我们把prompt+responses喂给我们的Critic/Reward/Reference模型，让它生成用于计算actor/critic loss的数据，按照强化学习的术语，我们称这些数据为经验（experiences）。critic loss我们将在后文做详细讲解，目前我们只把目光聚焦到actor loss上
- 第四步，我们根据这些经验，实际计算出actor/critic loss，然后更新Actor和Critic模型

#### 4. DPO

直接将人类的偏好对用于模型的训练，以达到最小的损失满足

![图1：RLHF和DPO方法的比较](assets/img-0.jpeg)

### 三、LLM 基础

#### 1. 模型架构

![image-20251108204949938](assets/image-20251108204949938.png)

1. **编码器 (Encoder)** ：任务是“**理解**”输入的整个句子。它会读取所有输入词元(这个概念会在3.2.2节介绍)，最终为每个词元生成一个富含上下文信息的向量表示。
2. **解码器 (Decoder)** ：任务是“**生成**”目标句子。它会参考自己已经生成的前文，并“咨询”编码器的理解结果，来生成下一个词。

```python
import torch
import torch.nn as nn
import math

# --- 占位符模块，将在后续小节中实现 ---

class PositionalEncoding(nn.Module):
    """
    位置编码模块
    """
    def forward(self, x):
        pass

class MultiHeadAttention(nn.Module):
    """
    多头注意力机制模块
    """
    def forward(self, query, key, value, mask):
        pass

class PositionWiseFeedForward(nn.Module):
    """
    位置前馈网络模块
    """
    def forward(self, x):
        pass

# --- 编码器核心层 ---

class EncoderLayer(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout):
        super(EncoderLayer, self).__init__()
        self.self_attn = MultiHeadAttention() # 待实现
        self.feed_forward = PositionWiseFeedForward() # 待实现
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, mask):
        # 残差连接与层归一化将在 3.1.2.4 节中详细解释
        # 1. 多头自注意力
        attn_output = self.self_attn(x, x, x, mask)
        x = self.norm1(x + self.dropout(attn_output))

        # 2. 前馈网络
        ff_output = self.feed_forward(x)
        x = self.norm2(x + self.dropout(ff_output))

        return x

# --- 解码器核心层 ---

class DecoderLayer(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout):
        super(DecoderLayer, self).__init__()
        self.self_attn = MultiHeadAttention() # 待实现
        self.cross_attn = MultiHeadAttention() # 待实现
        self.feed_forward = PositionWiseFeedForward() # 待实现
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, encoder_output, src_mask, tgt_mask):
        # 1. 掩码多头自注意力 (对自己)
        attn_output = self.self_attn(x, x, x, tgt_mask)
        x = self.norm1(x + self.dropout(attn_output))

        # 2. 交叉注意力 (对编码器输出)
        cross_attn_output = self.cross_attn(x, encoder_output, encoder_output, src_mask)
        x = self.norm2(x + self.dropout(cross_attn_output))

        # 3. 前馈网络
        ff_output = self.feed_forward(x)
        x = self.norm3(x + self.dropout(ff_output))

        return x
```

现在，我们来填充骨架中最关键的模块，注意力机制。

想象一下我们阅读这个句子：“The agent learns because **it** is intelligent.”。当我们读到加粗的 "**it**" 时，为了理解它的指代，我们的大脑会不自觉地将更多的注意力放在前面的 "agent" 这个词上。**自注意力 (Self-Attention)** 机制就是对这种现象的数学建模。它允许模型在处理序列中的每一个词时，都能兼顾句子中的所有其他词，并为这些词分配不同的“注意力权重”。权重越高的词，代表其与当前词的关联性越强，其信息也应该在当前词的表示中占据更大的比重。

为了实现上述过程，自注意力机制为每个输入的词元向量引入了三个可学习的角色：

- **查询 (Query, Q)**：代表当前词元，它正在主动地“查询”其他词元以获取信息。
- **键 (Key, K)**：代表句子中可被查询的词元“标签”或“索引”。
- **值 (Value, V)**：代表词元本身所携带的“内容”或“信息”。

这三个向量都是由原始的词嵌入向量乘以三个不同的、可学习的权重矩阵 ($W^Q,W^K,W^V$) 得到的。整个计算过程可以分为以下几步，我们可以把它想象成一次高效的开卷考试：

- 准备“考题”和“资料”：对于句子中的每个词，都通过权重矩阵生成其 $Q,K,V$ 向量。
- 计算相关性得分：要计算词 $A$ 的新表示，就用词 $A$ 的 $Q$ 向量，去和句子中所有词（包括 $A$ 自己）的 $K$ 向量进行点积运算。这个得分反映了其他词对于理解词 $A$ 的重要性。
- 稳定化与归一化：将得到的所有分数除以一个缩放因子$\sqrt{d_k}$），以防止梯度过小，然后用Softmax函数将分数转换成总和为1的权重，也就是归一化的过程。
- 加权求和：将上一步得到的权重分别乘以每个词对应的V*V*向量，然后将所有结果相加。最终得到的向量，就是词 $A$ 融合了全局上下文信息后的新表示。

这个过程可以用一个简洁的公式来概括：
$$
Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V
$$


如果只进行一次上述的注意力计算（即单头），模型可能会只学会关注一种类型的关联。比如，在处理 "it" 时，可能只学会了关注主语。但语言中的关系是复杂的，我们希望模型能同时关注多种关系（如指代关系、时态关系、从属关系等）。多头注意力机制应运而生。它的思想很简单：把一次做完变成分成几组，分开做，再合并。

它将原始的 Q, K, V 向量在维度上切分成 h 份（h 就是“头”数），每一份都独立地进行一次单头注意力的计算。这就好比让 h 个不同的“专家”从不同的角度去审视句子，每个专家都能捕捉到一种不同的特征关系。最后，将这 h 个专家的“意见”（即输出向量）拼接起来，再通过一个线性变换进行整合，就得到了最终的输出。

```python
class MultiHeadAttention(nn.Module):
    """
    多头注意力机制模块
    """
    def __init__(self, d_model, num_heads):
        super(MultiHeadAttention, self).__init__()
        assert d_model % num_heads == 0, "d_model 必须能被 num_heads 整除"

        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads

        # 定义 Q, K, V 和输出的线性变换层
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)

    def scaled_dot_product_attention(self, Q, K, V, mask=None):
        # 1. 计算注意力得分 (QK^T)
        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)

        # 2. 应用掩码 (如果提供)
        if mask is not None:
            # 将掩码中为 0 的位置设置为一个非常小的负数，这样 softmax 后会接近 0
            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)

        # 3. 计算注意力权重 (Softmax)
        attn_probs = torch.softmax(attn_scores, dim=-1)

        # 4. 加权求和 (权重 * V)
        output = torch.matmul(attn_probs, V)
        return output

    def split_heads(self, x):
        # 将输入 x 的形状从 (batch_size, seq_length, d_model)
        # 变换为 (batch_size, num_heads, seq_length, d_k)
        batch_size, seq_length, d_model = x.size()
        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)

    def combine_heads(self, x):
        # 将输入 x 的形状从 (batch_size, num_heads, seq_length, d_k)
        # 变回 (batch_size, seq_length, d_model)
        batch_size, num_heads, seq_length, d_k = x.size()
        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)

    def forward(self, Q, K, V, mask=None):
        # 1. 对 Q, K, V 进行线性变换
        Q = self.split_heads(self.W_q(Q))
        K = self.split_heads(self.W_k(K))
        V = self.split_heads(self.W_v(V))

        # 2. 计算缩放点积注意力
        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)

        # 3. 合并多头输出并进行最终的线性变换
        output = self.W_o(self.combine_heads(attn_output))
        return output

```

在每个 Encoder 和 Decoder 层中，多头注意力子层之后都跟着一个**逐位置前馈网络(Position-wise Feed-Forward Network, FFN)** 。如果说注意力层的作用是从整个序列中“动态地聚合”相关信息，那么前馈网络的作用从这些聚合后的信息中提取更高阶的特征。

这个名字的关键在于“逐位置”。它意味着这个前馈网络会独立地作用于序列中的每一个词元向量。换句话说，对于一个长度为 `seq_len` 的序列，这个 FFN 实际上会被调用 `seq_len` 次，每次处理一个词元。重要的是，所有位置共享的是同一组网络权重。这种设计既保持了对每个位置进行独立加工的能力，又大大减少了模型的参数量。这个网络的结构非常简单，由两个线性变换和一个 ReLU 激活函数组成：
$$
FFN(x) = max(0, xW_1+b_1)W_2+b_2
$$
其中，$x$ 是注意力子层的输出。$W_1,b_1,W_2,b_2$ 是可学习的参数。通常，第一个线性层的输出维度 `d_ff` 会远大于输入的维度 `d_model`（例如 `d_ff = 4 * d_model`），经过 ReLU 激活后再通过第二个线性层映射回 `d_model` 维度。这种“先扩大再缩小”的模式，也被称为瓶颈结构，被认为有助于模型学习更丰富的特征表示。

```python
class PositionWiseFeedForward(nn.Module):
    """
    位置前馈网络模块
    """
    def __init__(self, d_model, d_ff, dropout=0.1):
        super(PositionWiseFeedForward, self).__init__()
        self.linear1 = nn.Linear(d_model, d_ff)
        self.dropout = nn.Dropout(dropout)
        self.linear2 = nn.Linear(d_ff, d_model)
        self.relu = nn.ReLU()

    def forward(self, x):
        # x 形状: (batch_size, seq_len, d_model)
        x = self.linear1(x)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.linear2(x)
        # 最终输出形状: (batch_size, seq_len, d_model)
        return x

```

在 Transformer 的每个编码器和解码器层中，所有子模块（如多头注意力和前馈网络）都被一个 `Add & Norm` 操作包裹。这个组合是为了保证 Transformer 能够稳定训练。

这个操作由两个部分组成：

- **残差连接 (Add)**：该操作将子模块的输入 `x` 直接加到该子模块的输出 `Sublayer(x)` 上。这一结构解决了深度神经网络中的**梯度消失 (Vanishing Gradients)** 问题。在反向传播时，梯度可以绕过子模块直接向前传播，从而保证了即使网络层数很深，模型也能得到有效的训练。其公式可以表示为：$Output = x + sublayer(x)$
- **层归一化 (Norm)**：该操作对单个样本的所有特征进行归一化，使其均值为0，方差为1。这解决了模型训练过程中的**内部协变量偏移 (Internal Covariate Shift)** 问题，使每一层的输入分布保持稳定，从而加速模型收敛并提高训练的稳定性。

我们已经了解，Transformer 的核心是自注意力机制，它通过计算序列中任意两个词元之间的关系来捕捉依赖。然而，这种计算方式有一个固有的问题：它本身不包含任何关于词元顺序或位置的信息。对于自注意力来说，“agent learns” 和 “learns agent” 这两个序列是完全等价的，因为它只关心词元之间的关系，而忽略了它们的排列。为了解决这个问题，Transformer 引入了**位置编码 (Positional Encoding)** 。

位置编码的核心思想是，为输入序列中的每一个词元嵌入向量，都额外加上一个能代表其绝对位置和相对位置信息的“位置向量”。这个位置向量不是通过学习得到的，而是通过一个固定的数学公式直接计算得出。这样一来，即使两个词元（例如，两个都叫 `agent` 的词元）自身的嵌入是相同的，但由于它们在句子中的位置不同，它们最终输入到 Transformer 模型中的向量就会因为加上了不同的位置编码而变得独一无二。原论文中提出的位置编码使用正弦和余弦函数来生成，其公式如下：
$$
PE_{(pos, 2i)} = sin(\frac{pos}{10000^{\frac{2i}{d_{model}}}})
$$

$$
PE_{(pos, 2i+1)} = cos(\frac{pos}{10000^{\frac{2i}{d_{model}}}})
$$

```Python
class PositionalEncoding(nn.Module):
    """
    为输入序列的词嵌入向量添加位置编码。
    """
    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)

        # 创建一个足够长的位置编码矩阵
        position = torch.arange(max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))

        # pe (positional encoding) 的大小为 (max_len, d_model)
        pe = torch.zeros(max_len, d_model)

        # 偶数维度使用 sin, 奇数维度使用 cos
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        # 将 pe 注册为 buffer，这样它就不会被视为模型参数，但会随模型移动（例如 to(device)）
        self.register_buffer('pe', pe.unsqueeze(0))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x.size(1) 是当前输入的序列长度
        # 将位置编码加到输入向量上
        x = x + self.pe[:, :x.size(1)]
        return self.dropout(x)
```

Decoder-Only 架构的工作模式被称为**自回归 (Autoregressive)** 。这个听起来很专业的术语，其实描述了一个非常简单的过程：

1. 给模型一个起始文本（例如 “Datawhale Agent is”）。
2. 模型预测出下一个最有可能的词（例如 “a”）。
3. 模型将自己刚刚生成的词 “a” 添加到输入文本的末尾，形成新的输入（“Datawhale Agent is a”）。
4. 模型基于这个新输入，再次预测下一个词（例如 “powerful”）。
5. 不断重复这个过程，直到生成完整的句子或达到停止条件。

模型就像一个在玩“文字接龙”的游戏，它不断地“回顾”自己已经写下的内容，然后思考下一个字该写什么。

你可能会问，解码器是如何保证在预测第 `t` 个词时，不去“偷看”第 `t+1` 个词的答案呢？

答案就是**掩码自注意力 (Masked Self-Attention)** 。在 Decoder-Only 架构中，这个机制变得至关重要。它的工作原理非常巧妙：

在自注意力机制计算出注意力分数矩阵（即每个词对其他所有词的关注度得分）之后，但在进行 Softmax 归一化之前，模型会应用一个“掩码”。这个掩码会将所有位于当前位置之后（即目前尚未观测到）的词元对应的分数，替换为一个非常大的负数。当这个带有负无穷分数的矩阵经过 Softmax 函数时，这些位置的概率就会变为 0。这样一来，模型在计算任何一个位置的输出时，都从数学上被阻止了去关注它后面的信息。这种机制保证了模型在预测下一个词时，能且仅能依赖它已经见过的、位于当前位置之前的所有信息，从而确保了预测的公平性和逻辑的连贯性。

**Decoder-Only 架构的优势**

这种看似简单的架构，却带来了巨大的成功，其优势在于：

- **训练目标统一**：模型的唯一任务就是“预测下一个词”，这个简单的目标非常适合在海量的无标注文本数据上进行预训练。
- **结构简单，易于扩展**：更少的组件意味着更容易进行规模化扩展。今天的 GPT-4、Llama 等拥有数千亿甚至万亿参数的巨型模型，都是基于这种简洁的架构。
- **天然适合生成任务**：其自回归的工作模式与所有生成式任务（对话、写作、代码生成等）完美契合，这也是它能成为构建通用智能体基础的核心原因。

总而言之，从 Transformer 的解码器演变而来的 Decoder-Only 架构，通过“预测下一个词”这一简单的范式，开启了我们今天所处的大语言模型时代。

在GPT的训练中，模型的目标是**预测下一个token**。所以：

- **输入**：模型的输入是文本的一部分，通常是**一个token序列**（例如“我 喜欢 吃”）。
- **输出**：模型生成的输出是该序列的下一个token预测（例如“苹果”）。

**训练时使用的是原始文本中的真实token**作为目标，而不是模型自己生成的token。也就是说，GPT在训练时并不会将之前生成的内容作为输入，而是依赖**真实的标注文本**（即训练集中的真实token）来计算损失。

在GPT的训练过程中，通常使用**交叉熵损失（Cross-Entropy Loss）**来衡量预测的概率分布与真实目标分布之间的差异。对于每个时间步，损失函数计算模型的输出概率分布与真实token之间的差异。

对于每个位置 $t$，假设模型预测的token为 $y_t'$（预测的token），而真实的目标token为 $y_t$（真实标签）。交叉熵损失定义为：
$$
L(\theta) = -\sum_{t =1}^{T}{log P(y_t|x_1,x_2,……,x_{t-1};\theta)}
$$
一个 $batch$ 所有 $token$ 预测结束后计算平均交叉熵损失

#### 2. 常见性质

##### 2.1 **外推性**

大模型在训练时和预测时的输入长度不一致，导致模型的泛化能力下降的问题。例如，如果一个模型在训练时只使用了512个 token 的文本，那么在预测时如果输入超过512个 token，模型可能无法正确处理。这就限制了大模型在处理长文本或多轮对话等任务时的效果。

##### 2.2 **涌现能力**

当模型规模（参数量、训练数据量等）增大到某个阈值之后，会**突然**表现出一些在小模型中完全没有、甚至无法预期的复杂能力

##### 2.3 **模型幻觉（Hallucination）**

通常指的是大语言模型生成的内容与客观事实、用户输入或上下文信息相矛盾，或者生成了不存在的事实、实体或事件。幻觉的本质是模型在生成过程中，过度自信地“编造”了信息，而非准确地检索或推理。根据其表现形式，幻觉可以被分为多种类型[11]，例如：

- **事实性幻觉 (Factual Hallucinations)** ： 模型生成与现实世界事实不符的信息。
- **忠实性幻觉 (Faithfulness Hallucinations)** ： 在文本摘要、翻译等任务中，生成的内容未能忠实地反映源文本的含义。
- **内在幻觉 (Intrinsic Hallucinations)** ： 模型生成的内容与输入信息直接矛盾。

幻觉的产生是多方面因素共同作用的结果。首先，训练数据中可能包含错误或矛盾的信息。其次，模型的自回归生成机制决定了它只是在预测下一个最可能的词元，而没有内置的事实核查模块。最后，在面对需要复杂推理的任务时，模型可能会在逻辑链条中出错，从而“编造”出错误的结论。例如：一个旅游规划 Agent，可能会为你推荐一个现实中不存在的景点，或者预订一个航班号错误的机票。

此外，大语言模型还面临着知识时效性不足和训练数据中存在的偏见等挑战。大语言模型的能力来源于其训练数据。这意味着模型所掌握的知识是其训练数据收集时的最新材料。对于在此日期之后发生的事件、新出现的概念或最新的事实，模型将无法感知或正确回答。与此同时训练数据往往包含了人类社会的各种偏见和刻板印象。当模型在这些数据上学习时，它不可避免地会吸收并反映出这些偏见[12]。

为了提高大语言模型的可靠性，研究人员和开发者正在积极探索多种检测和缓解幻觉的方法：

1. **数据层面**： 通过高质量数据清洗、引入事实性知识以及强化学习与人类反馈 (RLHF) 等方式[13]，从源头减少幻觉。
2. **模型层面**： 探索新的模型架构，或让模型能够表达其对生成内容的不确定性。
3. **推理与生成层面**：
   1. **检索增强生成 (Retrieval-Augmented Generation, RAG)** [14]： 这是目前缓解幻觉的有效方法之一。RAG 系统通过在生成之前从外部知识库（如文档数据库、网页）中检索相关信息，然后将检索到的信息作为上下文，引导模型生成基于事实的回答。
   2. **多步推理与验证**： 引导模型进行多步推理，并在每一步进行自我检查或外部验证。
   3. **引入外部工具**： 允许模型调用外部工具（如搜索引擎、计算器、代码解释器）来获取实时信息或进行精确计算。

尽管幻觉问题短期内难以完全消除，但通过上述的策略，可以显著降低其发生频率和影响，提高大语言模型在实际应用中的可靠性和实用性。

##### 2.4 温度

`Temperature`：温度是控制模型输出 “随机性” 与 “确定性” 的关键参数。其原理是引入温度系数 $T> 0$，将 Softmax 改写为 $P_{i}^{(T)} = \frac{e^{z_i}}{\sum_{j=1}^{k}{e^{\frac{z_j}{T}}}}$ 

当T变小时，分布“更加陡峭”，高概率项权重进一步放大，生成更“保守”且重复率更高的文本。当T变大时，分布“更加平坦”，低概率项权重提升，生成更“多样”但可能出现不连贯的内容。

- 低温度（0 ⩽ Temperature < 0.3）时输出更 “精准、确定”。适用场景： 事实性任务：如问答、数据计算、代码生成； 严谨性场景：法律条文解读、技术文档撰写、学术概念解释等场景。
- 中温度（0.3 ⩽ Temperature < 0.7）：输出 “平衡、自然”。适用场景： 日常对话：如客服交互、聊天机器人； 常规创作：如邮件撰写、产品文案、简单故事创作。
- 高温度（0.7 ⩽ Temperature < 1）：输出 “创新、发散”。适用场景： 创意性任务：如诗歌创作、科幻故事构思、广告 slogan brainstorm、艺术灵感启发； 发散性思考。

#### 3. 文本嵌入与分词

##### 3.1 嵌入 embedding

$one-hot$ 独热编码存在的问题：维度灾难（维度和词表大小一样）；两个词相似程度没法表示

![image-20251109090504433](assets/image-20251109090504433.png)

词向量训练方法：Word2Vec, 这种方法它有一个重要假设，就是文本中离得越近的词语，相似度越高，有两种典型的方法，一个是CBOW，另一个是 Skip-gram

**CBOW** 方法，就是用两边词预测中间词，比如要预测"有"，它的独热编码是（000100），那就把 "爱" （001000）和 "温"（000010） 通过 WordEmbedding，WordEmbedding 是 一个 $5 \times 2$ 的矩阵，那么爱假设是 [0.5 0.3]，温是 [0.1 0.4]，然后把它们经过一个 Linear 层变成六维度的，比如 [0.1 0.2 -0.2 0.8 0.1 1.2]，然后计算爱和温的两个输出向量的平均值，经过softmax后找到概率最大的位置，应该是（000100），然后根据交叉熵损失进行训练，因为 $y_i$ 是独热的，只有正确词（比如“有”）的那个位置是 1，所以实际上：
$$
L = -\log(\hat{y}_{\text{“有”}})
$$
**Skip-Gram** 方法，通过中间词去预测上下文，它是将中间词进行编码得到softmax输出，找到最大概率的词看看是不是它旁边的词

Skip-Gram 是反过来，用一个中心词预测它的上下文。

例如中心词是 “注”，上下文是 [“关”, “意”]。

- 输入：中心词 “注” 的独热编码

- WordEmbedding 层：将“注”映射到向量 $\mathbf{v}_{注}$

- 输出层：预测每一个上下文词的概率
  $$
  P(\text{关}|\text{注}) = \text{softmax}(W' \cdot \mathbf{v}_{注})
  $$

  $$
  P(\text{意}|\text{注}) = \text{softmax}(W' \cdot \mathbf{v}_{注})
  $$

对于给定中心词 $c$ 和上下文词 $o$，希望最大化：
$$
P(o|c) = \frac{\exp(\mathbf{v}'_o \cdot \mathbf{v}_c)}{\sum_{w=1}^V \exp(\mathbf{v}'_w \cdot \mathbf{v}_c)}
$$
于是损失是负对数似然：
$$
L = -\log P(o|c)
$$
![image-20251109094618619](assets/image-20251109094618619.png)

但是这种方法都会通过 softmax 计算，当词表很大时，计算时长会增加很多，所以考虑利用负采样的方法

即对于 关 和 注 都进行编码，然后计算他们的点积，之后通过 sigmoid函数，概率大于0.5就输出1，小于0.5就输出0，比如我要训练注，如果和 关 点积那我希望是大于0.5的，而如果和一个不相关的比如 温 点积，我希望是小于0.5的

##### 3.2 分词 Tokenization

将文本序列转换为数字序列的过程，就叫做**分词 (Tokenization)** 。**分词器 (Tokenizer)** 的作用，就是定义一套规则，将原始文本切分成一个个最小的单元，我们称之为**词元 (Token)** 。

早期的自然语言处理任务可能会采用简单的分词策略：

- **按词分词 (Word-based)** ：直接用空格或标点符号将句子切分成单词。这种方法很直观，但会面临“词表爆炸”的问题。一个语言的词汇量是巨大的，如果每个词都作为一个独立的词元，词表会变得难以管理。更糟糕的是，模型将无法处理任何未在词表中出现过的词，例如 “DatawhaleAgent”。
- **按字符分词 (Character-based)** ：将文本切分成单个字符。这种方法词表很小（例如英文字母、数字和标点），不存在 OOV 问题。但它的缺点是，单个字符大多不具备独立的语义，模型需要花费更多的精力去学习如何将字符组合成有意义的词，导致学习效率低下。
- **按字节编词（BBPE）：** BBPE 用 byte （字节）构建最基础词表，将 BPE 从字符级别扩展到字节级别，在字节序列上使用 BPE 算法进行相邻合并。BBPE与BPE构建词表的流程基本一致，只是在字节级别进行合并，需要在BPE的基础上把单个字符转化为单个字节再进行合并。

字节对编码 (Byte-Pair Encoding, BPE) 是最主流的子词分词算法之一，GPT系列模型就采用了这种算法。其核心思想非常简洁，可以理解为一个“贪心”的合并过程：

1. **初始化**：将词表初始化为所有在语料库中出现过的基本字符。
2. **迭代合并**：在语料库上，统计所有相邻词元对的出现频率，找到频率最高的一对，将它们合并成一个新的词元，并加入词表。
3. **重复**：重复第 2 步，直到词表大小达到预设的阈值。

```python
import re, collections

def get_stats(vocab):
    """统计词元对频率"""
    pairs = collections.defaultdict(int)
    for word, freq in vocab.items():
        symbols = word.split()
        for i in range(len(symbols)-1):
            pairs[symbols[i],symbols[i+1]] += freq
    return pairs

def merge_vocab(pair, v_in):
    """合并词元对"""
    v_out = {}
    bigram = re.escape(' '.join(pair))
    p = re.compile(r'(?<!\S)' + bigram + r'(?!\S)')
    for word in v_in:
        w_out = p.sub(''.join(pair), word)
        v_out[w_out] = v_in[word]
    return v_out

# 准备语料库，每个词末尾加上</w>表示结束，并切分好字符
vocab = {'h u g </w>': 1, 'p u g </w>': 1, 'p u n </w>': 1, 'b u n </w>': 1}
num_merges = 4 # 设置合并次数

for i in range(num_merges):
    pairs = get_stats(vocab)
    if not pairs:
        break
    best = max(pairs, key=pairs.get)
    vocab = merge_vocab(best, vocab)
    print(f"第{i+1}次合并: {best} -> {''.join(best)}")
    print(f"新词表（部分）: {list(vocab.keys())}")
    print("-" * 20)

>>>
第1次合并: ('u', 'g') -> ug
新词表（部分）: ['h ug </w>', 'p ug </w>', 'p u n </w>', 'b u n </w>']
--------------------
第2次合并: ('ug', '</w>') -> ug</w>
新词表（部分）: ['h ug</w>', 'p ug</w>', 'p u n </w>', 'b u n </w>']
--------------------
第3次合并: ('u', 'n') -> un
新词表（部分）: ['h ug</w>', 'p ug</w>', 'p un </w>', 'b un </w>']
--------------------
第4次合并: ('un', '</w>') -> un</w>
新词表（部分）: ['h ug</w>', 'p ug</w>', 'p un</w>', 'b un</w>']
--------------------

```

#### 4. 复杂度与优化

1. **自注意力机制（Self-Attention）复杂度**

在Transformer架构中，推理的核心操作是**自注意力机制**（Self-Attention）。其复杂度通常是计算每个token与所有其他tokens之间关系的过程。具体来说，自注意力机制的计算复杂度通常是**O(n²)**，其中 **n** 是输入序列的长度。

- **原因**：每个token都需要与其他所有token进行交互（即计算注意力权重），因此对于每个token，都会有一个 **n x n** 的计算矩阵。

- **公式**：
  $$
  O(\text{Complexity}) = O(n^2 \cdot d)
  $$
  其中，$n$ 是输入序列的长度，$d$ 是每个token的维度（通常是模型的隐藏层维度）。

- **解释**：例如，假设我们有一个包含1000个token的输入序列，每个token的维度是1024。那么，单次自注意力计算的复杂度将是 $O(1000^2 \cdot 1024)$，即每次推理需要进行约1亿次计算。

2. **前馈神经网络（Feed-Forward Networks）复杂度**

在Transformer模型的每一层中，除了自注意力计算，还会有一个**前馈神经网络**（Feed-Forward Network）。该部分的计算复杂度是与每个token的维度和网络层数有关的。

- **公式**：
  $$
  O(\text{Feed-Forward}) = O(n \cdot d^2)
  $$
  其中，$n$ 是输入序列长度，$d$ 是每个token的维度（通常是隐藏层的维度）。

- **解释**：假设我们每个token的维度是1024，输入序列长度是1000。前馈网络的计算复杂度通常是与 $d^2$ 成比例的，因此，对于这个例子，计算复杂度将是 $O(1000 \cdot 1024^2)$。

3. **总的推理复杂度**

在一个Transformer模型中，通常包含多个层（例如，12层、24层或更多）。每层都包含自注意力机制和前馈神经网络。因此，总的推理复杂度通常是**每层复杂度的叠加**。

- **公式**：
  $$
  O(\text{Total Complexity}) = L \cdot O(n^2 \cdot d + n \cdot d^2)
  $$
  其中，$L$ 是模型的层数，$n$ 是序列长度，$d$ 是每个token的维度。

- **解释**：假设有12层Transformer模型，每层的计算复杂度为 $O(n^2 \cdot d + n \cdot d^2)$，并且输入序列长度是1000，token维度是1024。那总的计算复杂度将是每层的复杂度乘以层数12。

4. **空间复杂度**

推理时的**内存消耗**也是非常重要的，因为长序列输入会占用更多的内存，特别是在需要存储中间计算结果时。

- 对于自注意力层，需要存储每一层的注意力矩阵，其内存需求是 $O(n^2 \cdot d)$，因为每个token与所有其他tokens的关系需要在内存中存储。
- 如果模型非常大，特别是在长序列处理时，内存消耗会非常高。

5. KV 缓存优化

**KV Cache** 是Transformer **Decoder** 中用于加速推理的一种机制。它通过缓存**键（Key）**和值（Value）来避免在生成新token时重新计算先前的键值对。这样可以显著提高模型的推理速度，因为在生成每个新token时，模型只需关注当前token，而无需重新计算所有先前token之间的关系。使用KV缓存时，**前面生成的token的K和V可以被重复利用**，从而加速生成过程。

![image-20251108201407552](assets/image-20251108201407552.png)

![image-20251108201413803](assets/image-20251108201413803.png)

![image-20251108201421295](assets/image-20251108201421295.png)

![image-20251108201439123](assets/image-20251108201439123.png)

### 三、LLM架构对比

#### 1. gpt系列对比

GPT1采用了Transformer架构，其中包括多头自注意力机制和前向神经网络。这使得GPT1可以在处理自然语言时捕捉长距离依赖性，并且具有高效的并行性。GPT-1使用了一种称为“生成式预训练”（Generative Pre-Training，GPT）的技术。预训练分为两个阶段：预训练和微调（fine-tuning）。在预训练阶段，GPT-1使用了大量的无标注文本数据集，例如维基百科和网页文本等。通过最大化预训练数据集上的log-likelihood来训练模型参数。在微调阶段，GPT-1将预训练模型的参数用于特定的自然语言处理任务，如文本分类和问答系统等。

![image-20250722141509300](assets/image-20250722141509300.png)

GPT-2主要解决的问题是如何利用大规模未标注的自然语言文本来预训练一个通用的语言模型，从而提高自然语言处理的能力。与GPT-1模型不同之处在于，GPT-2模型使用了更大的模型规模和更多的数据进行预训练，同时增加了许多新的预训练任务。GPT-2具有零样本学习的能力，能够在只看到少量样本的情况下学习和执行新任务。**其主要变化在于训练的数据集规模变大**

GPT-3使用了更深的网络层数和更宽的Transformer网络结构，模型更大，参数更多，表达能力和语言理解能力更强； - GPT-3在预训练阶段使用了更大规模的数据集，并采用了更多样化的预训练任务。 GPT-3的微调阶段采用了zero-shot学习和few-shot的方法，使得GPT-3具备更强的泛化能力和迁移学习能力

InstructGPT：语言模型扩大并不能代表它们会更好地按照用户的意图进行工作，大语言模型很可能会生成一些不真实的、有害的或者是没有帮助的答案。换句话说，这些模型和用户的意图并不一致（not aligned with their users）。**由此OpenAI提出了“align”的概念，即希望模型的输出与人类意图“对齐”，符合人类真实偏好。** 对齐就是让模型回答符合人类的伦理和喜好，而不是乱回答，这里采用了 $RLHF$ 基于人类反馈的强化学习策略。ChatGPT就是在GPT3.5上用指示学习和人类反馈的强化学习来知道模型训练的。

![img](assets/img-1.jpeg)

1. 收集**示范数据**，进行有监督微调`SFT`。 - 标注数据：根据prompts（提示，这里就是写的各种各样的问题），人类会撰写一系列demonstrations（演示）作为模型的期望输出（主要是英文）； - 模型微调：**将prompts和人类标注的答案拼在一起，作为人工标注的数据集**，然后使用这部分数据集对预训练的GPT-3进行监督微调，得到第一个模型`SFT`（supervised fine-tuning，有监督微调) - **因为问题和答案是拼在一起的，所以在 GPT 眼中都是一样的，都是给定一段话然后预测下一个词，所以在微调上跟之前的在别的地方做微调或者是做预训练没有任何区别。** 

2.收集**比较数据**，训练奖励模型`RM`。 - 生成式标注是很贵的一件事，所以第二步是进行排序式/判别式标注。用上一步得到的`SFT`模型生成各种问题的答案，标注者（labelers）会对这些输出进行比较和排序（由好到坏，比如下图D>C>A=B）。 - 基于这个数据集，用强化学习训练一个`RM`（reward model)。训练好了之后这个RM模型就可以对生成的答案进行打分，且打出的分数能够满足人工排序的关系。

3.使用强化学习的机制，优化`SFT`模型，得到最终的`RL`模型（InstructGPT)。 将`SFT`模型的输出输入`RM`进行打分，通过强化学习来优化`SFT`模型的参数

#### 2. clip

如下图，在训练阶段，用文本和图像匹配，将文本编码，然后也将图像编码，计算文本向量和图像向量夹角的余弦值，然后用去训练encoder。然后在文本创建 `a photo of a {object}` 和图片去预测相似程度，然后得到分类，这样就可以进行 `zero shot` 学习，即没有样本的情况下也可以学习

![img](assets/img-0-1760844263570-1.jpeg)

