## 大模型算法

### 一、PyTorch、Transformers

#### 1. 模型评价指标

##### 1.1 **混淆矩阵**

用来看每个类别之间的准确率，常用来做目标分类

![image-20251021090747400](assets/image-20251021090747400.png)

代码如下

```python
from sklearn.metrics import confusion_matrix
def compute_confusion_matrix(labels,pred_labels_list,gt_labels_list):
    pred_labels_list = np.asarray(pred_labels_list)
    gt_labels_list = np.assarray(gt_labels_list)
    matrix = confusion_matrix(test_label_list,
                              pred_label_list,
                              labels=labels)
    return matrix
```

##### 1.2  Overall Accuracy

OA 代表了所有预测正确的样本占总样本的比例，公式为 $OA = \frac{TP+TN}{TP+TN+FP+FN}$ 

##### 1.3 **Average accuracy**

$AA = \frac{(\frac{TP}{TP+FN} + \frac{TN}{TN+FP})}{2}$

##### 1.4 **Recall**

$Recall = \frac{TP}{TP+FN}$，正样本被正确识别的比例

##### 1.5 **Precision**

$Precsion = \frac{TP}{TP+FP}$，预测正确的正样本所占的比例

##### 1.6 **F1**

$F1 = 2 \times \frac{P \times R}{P + R}$ ，召回率和精确率的加权平均，值越高表明鲁棒性越好

##### 1.7 **PR曲线**

横轴是召回率，纵轴代表了P（精确率)，P-R曲线上的一个点代表着，在某一阈值下，模型将大于该阈值的结果判定为正样本，小于该阈值的结果判定为负样本，此时返回结果对应的召回率和精确率。整条P-R曲线是通过将阈值从高到低移动而生成的。原点附近代表当阈值最大时模型的精确率和召回率，在PR曲线我们可以从图中直观的看到某一个曲线被另外一条曲线完全包裹，所包围的面积大于另一条曲线包围的面积，举例图中可以说明A模型的性能优于B和C。

![image-20251021092018100](assets/image-20251021092018100.png)

##### 1.8 **置信度**

在目标检测中，我们通常需要将边界框内物体划分为正样本和负样本。我们使用置信度这个指标来进行划分，当小于置信度设置的阈值判定为负样本（背景），大于置信度设置的阈值判定为正样本.

##### 1.9 **IOU**

$IOU = \frac{A \bigcap B}{A \bigcup B}$ 分子部分是模型预测框与真实标注框之间的重叠区域，分母部分是两者的并集，预测框和实际框所占有的总区域，在实际模型识别时会根据我们自己设定合适的阈值来判定正负样本。

![image-20251021092234082](assets/image-20251021092234082.png)

##### 1.10 **AP、mAP**

PR 曲线包围的面积，mAP是所有类的 AP 值的平均

##### 1.11 **交叉熵损失和 KL 散度**

1. **信息熵（Entropy）**

信息熵用于表示随机变量不确定性的度量。它反映了信息源的混乱程度，也就是如果要描述 1 bit 的信息需要多少bit：

- **公式**：
  $$
  H(P) = - \sum_i P(x_i) \log P(x_i)
  $$

- **高熵**：当数据分布较为分散时，信息熵较高，表示不确定性大。

- **低熵**：当数据集中在某些特定值上时，信息熵较低，表示不确定性较小。

2. **交叉熵（Cross-Entropy）**

交叉熵衡量的是预测分布与真实分布之间的信息差异，常用于分类问题中

- **公式**：
  $$
  H(P, Q) = - \sum_i P(x_i) \log Q(x_i)
  $$

- **用途**：它用来衡量实际分布 $P$ 和预测分布 $Q$ 之间的差异。

- 在大模型中，通常  $P(x)=1$ ，即 token 应该输出是确定的

3. **KL 散度（Kullback-Leibler Divergence）**

KL 散度衡量的是从分布 $Q$ 到分布 $P$ 的信息损失：

- **公式**：
  $$
  D_{KL}(P \parallel Q) = \sum_i P(x_i) \log \frac{P(x_i)}{Q(x_i)}
  $$

- **KL 散度与交叉熵的关系**：
  $$
  D_{KL}(P \parallel Q) = H(P, Q) - H(Q)
  $$
  这表明，KL 散度等于交叉熵与真实分布的熵之间的差异。 KL 散度是表示用 $Q$ 去衡量 $P$ 的时候需要多用的比特数

  在大模型训练中，通常使用交叉熵作为损失函数，因为训练数据提供的真实分布 $P$ 是一个 one-hot 向量即模型明确告诉我们期望的 token 是哪一个，因此它的信息熵为 0

4. **KL 散度的物理意义**

- KL 散度表示从分布 $Q$ 编码真实分布 $P$ 所需的额外信息量。通常它是大于零的，且只有当 $P = Q$ 时，KL 散度等于零。

- **为什么 KL 不是一个真正的距离度量？**

  1. KL 散度不满足对称性，即 $KL(P \parallel Q) \neq KL(Q \parallel P)$。
  2. KL 散度不满足三角不等式。

  因此，KL 散度不是一个距离度量。

5. **KL 散度在损失函数中的应用**

KL 散度可以作为损失函数使用，尤其在训练模型时，KL 散度帮助优化模型的预测分布，使其尽可能接近真实分布。具体来说，当真实分布 $Q$ 固定时，KL 散度越小，模型预测的分布与真实分布越接近。

##### 1.12 困惑度

困惑度（Perplexity）是语言模型对文本的预测“困惑”程度。具体来说，它是模型在给定前文（上下文）后，预测下一个词的概率分布的不确定性。

数学上，困惑度可以定义为：
$$
\text{PPL}(x) = 2^{H(x)}
$$
其中：

- $H(x)$ 是模型对序列 $x$ 的交叉熵（cross-entropy），即衡量模型预测分布与真实分布之间差异的指标。
- $x$ 是模型预测的输入文本序列。

交叉熵越小，说明模型的预测越接近实际的词分布，因此困惑度越低。

**计算困惑度**

1. **交叉熵（Cross-Entropy）**：交叉熵衡量的是模型输出的概率分布与实际分布之间的差异，定义为：
   $$
   H(x) = - \frac{1}{N} \sum_{i=1}^{N} \log P(y_i | x_{<i})
   $$
   其中 $y_i$ 是目标词，$x_{ 是前文（上下文），$P(y_i | x_{ 是模型预测的概率。

2. **从交叉熵到困惑度**：困惑度是交叉熵的指数形式，即：
   $$
   \text{PPL}(x) = 2^{H(x)}
   $$
   困惑度反映的是模型对于一个文本序列的预测能力，较低的困惑度意味着模型较少“困惑”。

##### 11.3 ROC-AUC

1. **ROC 曲线（Receiver Operating Characteristic Curve）**

ROC 曲线是通过绘制不同分类阈值下的假阳性率（False Positive Rate, FPR）和真正率（True Positive Rate, TPR）来评估模型的性能。

- **真正率（TPR）**，也叫**召回率**（Recall），表示正确预测为正类的样本占所有正类样本的比例：
  $$
  \text{TPR} = \frac{\text{TP}}{\text{TP} + \text{FN}}
  $$
  其中，TP 是真正例（True Positive），FN 是假负例（False Negative）。

- **假阳性率（FPR）**，表示错误地将负类预测为正类的比例：
  $$
  \text{FPR} = \frac{\text{FP}}{\text{FP} + \text{TN}}
  $$
  其中，FP 是假正例（False Positive），TN 是真负例（True Negative）。

ROC 曲线通过改变分类阈值（通常是模型输出的概率）并计算每个阈值下的 TPR 和 FPR 来绘制。横轴是 FPR，纵轴是 TPR。理想的 ROC 曲线会尽量接近左上角，即高真正率和低假阳性率。

2. **AUC（Area Under the Curve）**

AUC 是 ROC 曲线下的面积，表示模型的总体分类性能。

- **AUC 值**在 [0, 1] 之间：
  - AUC = 1：模型完美区分正负类，所有正类样本的预测概率都高于负类样本。
  - AUC = 0.5：模型表现得像随机猜测一样，正负类样本没有区别。
  - AUC < 0.5：模型表现比随机猜测还差，通常表示模型的预测结果与标签完全相反。

#### 2. 张量

##### 2.1 创建张量

```python
import torch
# 随机创建一个四行三列的向量
x = torch.randn(4, 3)
x = torch.zeros(4, 3, dtype = torch.long)
x = torch.tensor([5.5, 3]) # 创建一个包含两个元素的向量
x = torch.randn_like(x, dtype = torch.float)
y = x.int() # 转换成整数类型
```

获取维度信息

```python
print(x.size())
print(x.shape)
```

```python
single_distance = torch.tensor(25) 
print(single_distance.shape)
# torch.tensor([]) 这是一个标量，最低维度是[1, 1]
with_batch = single_distance.unsqueeze(0)
# torch.tenosr([1])
ready_for_model = with_distance.unsqueeze(1)
# torch.tenosr([1, 1])
value = ready_for_model[0].item() 
```

常见 Tensor 的构建方法

| 函数                | 功能                                                    |
| :------------------ | :------------------------------------------------------ |
| Tensor(sizes)       | 基础构造函数                                            |
| tensor(data)        | 类似于np.array，data是数组格式，直接将array转换为tensor |
| from_numpy(array)   | 将numpy类型的数组转换为tensor                           |
| ones(sizes)         | 全1                                                     |
| zeros(sizes)        | 全0                                                     |
| eye(sizes)          | 对角为1，其余为0                                        |
| arange(s,e,step)    | 从s到e，步长为step                                      |
| linspace(s,e,steps) | 从s到e，均匀分成step份                                  |
| rand/randn(sizes)   | rand是[0,1)均匀分布；randn是服从N(0，1)的正态分布       |
| normal(mean,std)    | 正态分布(均值为mean，标准差是std)                       |
| randperm(m)         | 随机排列                                                |

##### 2.2 算术操作

```python
import torch
y = torch.rand(4, 3)
print(x + y)
print(torch.add(x, y))
```

```python
x = torch.rand(4, 3)
print(x[:, 1])
# 输出
tensor([-0.0720,  0.0666,  1.0336, -0.6965])
# 索引出来的结果与原数据共享内存，修改一个另一个也会更改，如果不想更改使用copy()等方法
y = x[0, :]
```

```python
x = torch.randn(4, 4)
# torch.view 也是共享内存的
y = x.view(16) # 或者torch.reshape()
z = x.view(-1, 8)
print(x.size(), y.size(), z.size())
torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])
```

##### 2.3 广播

当两个向量最后一维对齐，如果这一维度是相同的或者其中之一是1的时候，视为相同，可以将维度为1的那个张量广播

```python
x = torch.arange(1, 3).view(1, 2)
print(x)
y = torch.arange(1, 4).view(3, 1)
print(y)
print(x + y)
```

```test
tensor([[1, 2]])
tensor([[1],
        [2],
        [3]])
tensor([[2, 3],
        [3, 4],
        [4, 5]])
```

##### 2.4 自动求导

默认 `requires_grad` 为 `False`

如果设置它的属性` .requires_grad` 为 `True`，那么它将会追踪对于该张量的所有操作。当完成计算后可以通过调用` .backward()`，来自动计算所有的梯度。这个张量的所有梯度将会自动累加到`.grad`属性。

在 y.backward() 时，如果 y 是标量，则不需要为 backward() 传入任何参数；否则，需要传入一个与 y 同形的Tensor。

代码块包装在 `with torch.no_grad(): `中，在评估模型时特别有用，因为模型可能具有 `requires_grad = True` 的可训练的参数，但是我们不需要在此过程中对他们进行梯度计算。

```python
x = torch.rann(4, 3, requires_grad = True)
```

每个张量都有一个`.grad_fn`属性，该属性引用了创建 `Tensor `自身的`Function`(除非这个张量是用户手动创建的，即这个张量的`grad_fn`是 `None` )。上面给出的例子中，张量由用户手动创建，因此grad_fn返回结果是None。

```python
y = x**2
print(y)

tensor([[1., 1.],
        [1., 1.]], grad_fn=<PowBackward0>)
```

反向传播是累加的

```python
# 再来反向传播⼀一次，注意grad是累加的
out2 = x.sum()
out2.backward()
print(x.grad)

out3 = x.sum()
x.grad.data.zero_()
out3.backward()
print(x.grad)
```

##### 2.5 调整张量形状

有时我们需要对张量的形状进行调整，Pytorch 共提供了 4 种调整张量形状的函数，分别为：

- **形状转换 `view`** 将张量转换为新的形状，需要保证总的元素个数不变，例如：

  ```python
  >>> x = torch.tensor([1, 2, 3, 4, 5, 6])
  >>> print(x, x.shape)
  tensor([1, 2, 3, 4, 5, 6]) torch.Size([6])
  >>> x.view(2, 3) # shape adjusted to (2, 3)
  tensor([[1, 2, 3],
          [4, 5, 6]])
  >>> x.view(3, 2) # shape adjusted to (3, 2)
  tensor([[1, 2],
          [3, 4],
          [5, 6]])
  >>> x.view(-1, 3) # -1 means automatic inference
  tensor([[1, 2, 3],
          [4, 5, 6]])
  ```

  进行 view 操作的张量必须是连续的 (contiguous)，可以调用 `is_conuous` 来判断张量是否连续；如果非连续，需要先通过 `contiguous` 函数将其变为连续的。也可以直接调用 Pytorch 新提供的 `reshape` 函数，它与 `view` 功能几乎一致，并且能够自动处理非连续张量。

- **转置 `transpose`** 交换张量中的两个维度，参数为相应的维度：

  ```python
  >>> x = torch.tensor([[1, 2, 3], [4, 5, 6]])
  >>> x
  tensor([[1, 2, 3],
          [4, 5, 6]])
  >>> x.transpose(0, 1)
  tensor([[1, 4],
          [2, 5],
          [3, 6]])
  ```

- **交换维度 `permute`** 与 `transpose` 函数每次只能交换两个维度不同，`permute` 可以直接设置新的维度排列方式：

  ```python
  >>> x = torch.tensor([[[1, 2, 3], [4, 5, 6]]])
  >>> print(x, x.shape)
  tensor([[[1, 2, 3],
           [4, 5, 6]]]) torch.Size([1, 2, 3])
  >>> x = x.permute(2, 0, 1)
  >>> print(x, x.shape)
  tensor([[[1, 4]],
    
          [[2, 5]],
    
          [[3, 6]]]) torch.Size([3, 1, 2])
  ```

##### 2.6 降维与升维

有时为了计算需要对一个张量进行降维或升维。例如神经网络通常只接受一个批次 (batch) 的样例作为输入，如果只有 1 个输入样例，就需要手工添加一个 batch 维度。具体地：

- **升维** `torch.unsqueeze(input, dim, out=None)` 在输入张量的 dim 位置插入一维，与索引一样，dim 值也可以为负数；
- **降维** `torch.squeeze(input, dim=None, out=None)` 在不指定 dim 时，张量中所有形状为 1 的维度都会被删除，例如 (A,1,B,1,C) 会变成 (A,B,C) ；当给定 dim 时，只会删除给定的维度（形状必须为 1），例如对于 (A,1,B) ，`squeeze(input, dim=0)` 会保持张量不变，只有 `squeeze(input, dim=1)` 形状才会变成 (A,B)

```python
>>> a = torch.tensor([1, 2, 3, 4])
>>> a.shape
torch.Size([4])
>>> b = torch.unsqueeze(a, dim=0)
>>> print(b, b.shape)
tensor([[1, 2, 3, 4]]) torch.Size([1, 4])
>>> b = a.unsqueeze(dim=0) # another way to unsqueeze tensor
>>> print(b, b.shape)
tensor([[1, 2, 3, 4]]) torch.Size([1, 4])
>>> c = b.squeeze()
>>> print(c, c.shape)
tensor([1, 2, 3, 4]) torch.Size([4])
```

#### 3. 基本流程

自己定制 Dataset

```python
import os
import pandas as pd
from torchvision.io import read_image

class MyDataset(Dataset):
    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):
        """
        Args:
            annotations_file (string): Path to the csv file with annotations.
            img_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
            target_transform (callable, optional): Optional transform to be applied
                on the target.
        """
        self.img_labels = pd.read_csv(annotations_file)
        self.img_dir = img_dir
        self.transform = transform
        self.target_transform = target_transform

    def __len__(self):
        return len(self.img_labels)

    def __getitem__(self, idx):
        """
        Args:
            idx (int): Index
        """
        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])
        image = read_image(img_path)
        label = self.img_labels.iloc[idx, 1]
        if self.transform:
            image = self.transform(image)
        if self.target_transform:
            label = self.target_transform(label)
        return image, label
```

设计 dataloader

```python
transform = transforms.Compose([
  transforms.ToTensor(),
  transforms.Normalize((mean,),(std,))
])

dataset = SomeDataset('./data', train = True, download = True, transform = transform)

dataset_loader = DataLoader(train_dataset, batch_size = 32, shuffle = True)

for batch_index, (data, labels) in enumerate(train_loader):
  output = model(data)
```

神经网络的构造

`Module` 类是 `torch.nn` 模块里提供的一个模型构造类，是所有神经网络模块的基类，我们可以继承它来定义我们想要的模型。下面继承 Module 类构造多层感知机。这里定义的 MLP 类重载了 `Module` 类的 `__init__` 函数和 `forward` 函数。它们分别用于创建模型参数和定义前向计算（正向传播）。下面的 MLP 类定义了一个具有两个隐藏层的多层感知机。

```python
import torch
from torch import nn

class MLP(nn.Module):
  # 声明带有模型参数的层，这里声明了两个全连接层
  def __init__(self, **kwargs):
    # 调用MLP父类Block的构造函数来进行必要的初始化。这样在构造实例时还可以指定其他函数
    super(MLP, self).__init__(**kwargs)
    self.hidden = nn.Linear(784, 256)
    self.act = nn.ReLU()
    self.output = nn.Linear(256,10)
    
   # 定义模型的前向计算，即如何根据输入x计算返回所需要的模型输出
  def forward(self, x):
    o = self.act(self.hidden(x))
    return self.output(o)   
```

网络实例化

```python
X = torch.rand(2,784) # 设置一个随机的输入张量
net = MLP() # 实例化模型
print(net) # 打印模型
net(X) # 前向计算
```

```
MLP(
  (hidden): Linear(in_features=784, out_features=256, bias=True)
  (act): ReLU()
  (output): Linear(in_features=256, out_features=10, bias=True)
)
tensor([[ 0.0149, -0.2641, -0.0040,  0.0945, -0.1277, -0.0092,  0.0343,  0.0627,
         -0.1742,  0.1866],
        [ 0.0738, -0.1409,  0.0790,  0.0597, -0.1572,  0.0479, -0.0519,  0.0211,
         -0.1435,  0.1958]], grad_fn=<AddmmBackward>)
```

我们只需要定义 `forward` 函数，`backward`函数会在使用`autograd`时自动定义，`backward`函数用来计算导数。我们可以在 `forward` 函数中使用任何针对张量的操作和计算。

一个模型的可学习参数可以通过`net.parameters()`返回

```python
params = list(net.parameters())
print(len(params))
print(params[0].size())  # conv1的权重
```

`torch.nn`只支持小批量处理 (mini-batches）。整个 `torch.nn` 包只支持小批量样本的输入，不支持单个样本的输入。比如，`nn.Conv2d` 接受一个4维的张量，即`nSamples x nChannels x Height x Width `如果是一个单独的样本，只需要使用`input.unsqueeze(0)` 来添加一个“假的”批大小维度。

- `torch.Tensor` - 一个多维数组，支持诸如`backward()`等的自动求导操作，同时也保存了张量的梯度。
- `nn.Module `- 神经网络模块。是一种方便封装参数的方式，具有将参数移动到GPU、导出、加载等功能。
- `nn.Parameter `- 张量的一种，当它作为一个属性分配给一个`Module`时，它会被自动注册为一个参数。
- `autograd.Function` - 实现了自动求导前向和反向传播的定义，每个`Tensor`至少创建一个`Function`节点，该节点连接到创建`Tensor`的函数并对其历史进行编码。

**模型初始化**

先定义模型初始化参数

```python
def initialize_weights(model):
	for m in model.modules():
		# 判断是否属于Conv2d
		if isinstance(m, nn.Conv2d):
			torch.nn.init.zeros_(m.weight.data)
			# 判断是否有偏置
			if m.bias is not None:
				torch.nn.init.constant_(m.bias.data,0.3)
		elif isinstance(m, nn.Linear):
			torch.nn.init.normal_(m.weight.data, 0.1)
			if m.bias is not None:
				torch.nn.init.zeros_(m.bias.data)
		elif isinstance(m, nn.BatchNorm2d):
			m.weight.data.fill_(1) 		 
			m.bias.data.zeros_()	
```

然后定义模型

```python
# 模型的定义
class MLP(nn.Module):
  # 声明带有模型参数的层，这里声明了两个全连接层
  def __init__(self, **kwargs):
    # 调用MLP父类Block的构造函数来进行必要的初始化。这样在构造实例时还可以指定其他函数
    super(MLP, self).__init__(**kwargs)
    self.hidden = nn.Conv2d(1,1,3)
    self.act = nn.ReLU()
    self.output = nn.Linear(10,1)
    
   # 定义模型的前向计算，即如何根据输入x计算返回所需要的模型输出
  def forward(self, x):
    o = self.act(self.hidden(x))
    return self.output(o)

mlp = MLP()
print(mlp.hidden.weight.data)
print("-------初始化-------")

mlp.apply(initialize_weights)
# 或者initialize_weights(mlp)
print(mlp.hidden.weight.data)
```

首先应该设置模型的状态：如果是训练状态，那么模型的参数应该支持反向传播的修改；如果是验证/测试状态，则不应该修改模型参数。在PyTorch中，模型的状态设置非常简便，如下的两个操作二选一即可：

```
model.train()   # 训练状态
model.eval()   # 验证/测试状态
```

我们前面在DataLoader构建完成后介绍了如何从中读取数据，在训练过程中使用类似的操作即可，区别在于此时要用for循环读取DataLoader中的全部数据。

```
for data, label in train_loader:
```

之后将数据放到GPU上用于后续计算，此处以.cuda()为例

```
data, label = data.cuda(), label.cuda()
```

开始用当前批次数据做训练时，应当先将优化器的梯度置零：

```
optimizer.zero_grad()
```

之后将data送入模型中训练：

```
output = model(data)
```

根据预先定义的criterion计算损失函数：

```
loss = criterion(output, label)
```

将loss反向传播回网络：

```
loss.backward()
```

使用优化器更新模型参数：

```
optimizer.step()
```

这样一个训练过程就完成了，后续还可以计算模型准确率等指标，这部分会在下一节的图像分类实战中加以介绍。

验证/测试的流程基本与训练过程一致，不同点在于：

- 需要预先设置torch.no_grad，以及将model调至eval模式
- 不需要将优化器的梯度置零
- 不需要将loss反向回传到网络
- 不需要更新optimizer

完整的训练和验证流程

```python
def train(epoch):
    model.train()
    train_loss = 0
    for data, label in train_loader:
        data, label = data.cuda(), label.cuda()
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, label)
        loss.backward()
        optimizer.step()
        train_loss += loss.item()*data.size(0)
    train_loss = train_loss/len(train_loader.dataset)
		print('Epoch: {} \tTraining Loss: {:.6f}'.format(epoch, train_loss))
    

def val(epoch):       
    model.eval()
    val_loss = 0
    with torch.no_grad():
        for data, label in val_loader:
            data, label = data.cuda(), label.cuda()
            output = model(data)
            preds = torch.argmax(output, 1)
            loss = criterion(output, label)
            val_loss += loss.item()*data.size(0)
            running_accu += torch.sum(preds == label.data)
    val_loss = val_loss/len(val_loader.dataset)
    print('Epoch: {} \tTraining Loss: {:.6f}'.format(epoch, val_loss))
```

模型保存

```python
from torchvision import models
model = models.resnet152(pretrained=True)
save_dir = './resnet152.pth'

# 保存整个模型
torch.save(model, save_dir)
# 保存模型权重
torch.save(model.state_dict, save_dir)
```

#### 4. 神经网络常见的层

* **无参数层**

```python
import torch
from torch import nn

class MyLayer(nn.Module):
    def __init__(self, **kwargs):
        super(MyLayer, self).__init__(**kwargs)
    def forward(self, x):
        return x - x.mean()  
```

* **含模型参数的层**

`Parameter` 类其实是 `Tensor` 的子类，如果一个 `Tensor` 是 `Parameter` ，那么它会⾃动被添加到模型的参数列表里。所以在⾃定义含模型参数的层时，我们应该将参数定义成 `Parameter` ，除了直接定义成 `Parameter` 类外，还可以使⽤ `ParameterList` 和 `ParameterDict` 分别定义参数的列表和字典。

> torch.mm 是矩阵的乘积，不支持广播只支持二维的，torch.matmul是支持广播和多维的

```python
class MyListDense(nn.Module):
    def __init__(self):
        super(MyListDense, self).__init__()
        self.params = nn.ParameterList([nn.Parameter(torch.randn(4, 4)) for i in range(3)])
        self.params.append(nn.Parameter(torch.randn(4, 1)))

    def forward(self, x):
        for i in range(len(self.params)):
            x = torch.mm(x, self.params[i])
        return x
net = MyListDense()
print(net)
```

```python
class MyDictDense(nn.Module):
    def __init__(self):
        super(MyDictDense, self).__init__()
        self.params = nn.ParameterDict({
                'linear1': nn.Parameter(torch.randn(4, 4)),
                'linear2': nn.Parameter(torch.randn(4, 1))
        })
        self.params.update({'linear3': nn.Parameter(torch.randn(4, 2))}) # 新增

    def forward(self, x, choice='linear1'):
        return torch.mm(x, self.params[choice])

net = MyDictDense()
print(net)
```

* **二维卷积层**

```python
import torch
from torch import nn

# 卷积运算（二维互相关）
def corr2d(X, K): 
    h, w = K.shape
    X, K = X.float(), K.float()
    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            Y[i, j] = (X[i: i + h, j: j + w] * K).sum()
    return Y

# 二维卷积层
class Conv2D(nn.Module):
    def __init__(self, kernel_size):
        super(Conv2D, self).__init__()
        self.weight = nn.Parameter(torch.randn(kernel_size))
        self.bias = nn.Parameter(torch.randn(1))

    def forward(self, x):
        return corr2d(x, self.weight) + self.bias
```

#### 5. pipeline 

Transformers 库最基础的对象就是 `pipeline()` 函数，它封装了预训练模型和对应的前处理和后处理环节。我们只需输入文本，就能得到预期的答案。目前常用的 [pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines) 有：

- `feature-extraction` （获得文本的向量化表示）
- `fill-mask` （填充被遮盖的词、片段）
- `ner`（命名实体识别）
- `question-answering` （自动问答）
- `sentiment-analysis` （情感分析）
- `summarization` （自动摘要）
- `text-generation` （文本生成）
- `translation` （机器翻译）
- `zero-shot-classification` （零训练样本分类）

```python
from transformers import pipeline

classifier = pipeline("sentiment-analysis")
result = classifier("I've been waiting for a HuggingFace course my whole life.")
print(result)
results = classifier(
  ["I've been waiting for a HuggingFace course my whole life.", "I hate this so much!"]
)
print(results)
```

```
No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)

[{'label': 'POSITIVE', 'score': 0.9598048329353333}]
[{'label': 'POSITIVE', 'score': 0.9598048329353333}, {'label': 'NEGATIVE', 'score': 0.9994558691978455}]

```

pipeline会自动加载模型，实际他背后经过了三个步骤

![image-20251123201947558](assets/image-20251123201947558.png)

因为神经网络模型无法直接处理文本，因此首先需要通过**预处理**环节将文本转换为模型可以理解的数字。具体地，我们会使用每个模型对应的分词器 (tokenizer) 来进行：

1. 将输入切分为词语、子词或者符号（例如标点符号），统称为 **tokens**；
2. 根据模型的词表将每个 token 映射到对应的 token 编号（就是一个数字）；
3. 根据模型的需要，添加一些额外的输入。

我们对输入文本的预处理需要与模型自身预训练时的操作完全一致，只有这样模型才可以正常地工作。注意，每个模型都有特定的预处理操作。这里我们使用 `AutoTokenizer` 类和它的 `from_pretrained()` 函数，它可以自动根据模型 checkpoint 名称来获取对应的分词器。

```python
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.",
    "I hate this so much!",
]
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt")
print(inputs)

# output
{
    'input_ids': tensor([
        [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172, 2607,  2026,  2878,  2166,  1012,   102],
        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,
             0,     0,     0,     0,     0,     0]
    ]), 
    'attention_mask': tensor([
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]
    ])
}
```

```python
from transformers import AutoModel

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModel.from_pretrained(checkpoint)
outputs = model(**inputs)
print(outputs.last_hidden_state.shape)

# 输出是torch.Size([2, 16, 768])，分别是batch_size,Sequence length，Hidden size 
```

然后 `output.logits` 输出的是这种 (下面是两句话，做情感分析的，所以一个tensor对应两个维度)

```
tensor([[-1.5607,  1.6123],
        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward0>)
```

还需要经过一个softmax处理，得到最终的概率输出

#### 6. 模型

除了像之前使用 `AutoModel` 根据 checkpoint 自动加载模型以外，我们也可以直接使用模型对应的 `Model` 类，例如 BERT 对应的就是 `BertModel`：

```python
from transformers import BertModel

model = BertModel.from_pretrained("bert-base-cased")
```

注意，**在大部分情况下，我们都应该使用 `AutoModel` 来加载模型。**这样如果我们想要使用另一个模型（比如把 BERT 换成 RoBERTa），只需修改 checkpoint，其他代码可以保持不变。

保存模型通过调用 `Model.save_pretrained()` 函数实现，例如保存加载的 BERT 模型：

```python
from transformers import AutoModel

model = AutoModel.from_pretrained("bert-base-cased")
model.save_pretrained("./models/bert-base-cased/")
```

这会在保存路径下创建两个文件：

- *config.json*：模型配置文件，存储模型结构参数，例如 Transformer 层数、特征空间维度等；
- *pytorch_model.bin*：又称为 state dictionary，存储模型的权重。

简单来说，配置文件记录模型的**结构**，模型权重记录模型的**参数**，这两个文件缺一不可。我们自己保存的模型同样通过 `Model.from_pretrained()` 加载，只需要传递保存目录的路径。

#### 7. 分词器

##### 7.1 分词器的加载与保存

由于神经网络模型不能直接处理文本，因此我们需要先将文本转换为数字，这个过程被称为**编码 (Encoding)**，其包含两个步骤：

1. 使用分词器 (tokenizer) 将文本按词、子词、字符切分为 tokens；
2. 将所有的 token 映射到对应的 token ID。

分词器的加载与保存与模型相似，使用 `Tokenizer.from_pretrained()` 和 `Tokenizer.save_pretrained()` 函数。例如加载并保存 BERT 模型的分词器：

```python
from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained("bert-base-cased")
tokenizer.save_pretrained("./models/bert-base-cased/")
```

同样地，在大部分情况下我们都应该使用 `AutoTokenizer` 来加载分词器：

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
tokenizer.save_pretrained("./models/bert-base-cased/")
```

调用 `Tokenizer.save_pretrained()` 函数会在保存路径下创建三个文件：

- *special_tokens_map.json*：映射文件，里面包含 unknown token 等特殊字符的映射关系；
- *tokenizer_config.json*：分词器配置文件，存储构建分词器需要的参数；
- *vocab.txt*：词表，一行一个 token，行号就是对应的 token ID（从 0 开始）。

##### 7.2 编码文本（拆分步骤和包调用）

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

sequence = "Using a Transformer network is simple"
tokens = tokenizer.tokenize(sequence)

print(tokens)

# ['using', 'a', 'transform', '##er', 'network', 'is', 'simple']
```

可以看到，BERT 分词器采用的是子词切分策略，它会不断切分词语直到获得词表中的 token，例如 “transformer” 会被切分为 “transform” 和 “##er”。

然后，我们通过 `convert_tokens_to_ids()` 将切分出的 tokens 转换为对应的 token IDs：

```python
ids = tokenizer.convert_tokens_to_ids(tokens)

print(ids)
[7993, 170, 13809, 23763, 2443, 1110, 3014]
```

还可以通过 `encode()` 函数将这两个步骤合并，并且 `encode()` 会自动添加模型需要的特殊 token，例如 BERT 分词器会分别在序列的首尾添加 和 ：

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

sequence = "Using a Transformer network is simple"
sequence_ids = tokenizer.encode(sequence)

print(sequence_ids)
[101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102]
```

其中 101 和 102 分别是 [cls] 和 [sep] 对应的 token IDs。即开始和结束的字符

注意，上面这些只是为了演示。**在实际编码文本时，最常见的是直接使用分词器进行处理**，这样不仅会返回分词后的 token IDs，还包含模型需要的其他输入。例如 BERT 分词器还会自动在输入中添加 `token_type_ids` 和 `attention_mask`：

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
tokenized_text = tokenizer("Using a Transformer network is simple")
print(tokenized_text)
{'input_ids': [101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102], 
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

##### 7.3 文本解码

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])
print(decoded_string)

decoded_string = tokenizer.decode([101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102])
print(decoded_string)


# Using a transformer network is simple
# [CLS] Using a Transformer network is simple [SEP]
```

##### 7.4 padding

按批输入多段文本产生的一个直接问题就是：batch 中的文本有长有短，而输入张量必须是严格的二维矩形，维度为 ，即每一段文本编码后的 token IDs 数量必须一样多。例如下面的 ID 列表是无法转换为张量的：

```python
batched_ids = [
    [200, 200, 200],
    [200, 200]
]
```

我们需要通过 Padding 操作，在短序列的结尾填充特殊的 padding token，使得 batch 中所有的序列都具有相同的长度，例如：

```python
padding_id = 100

batched_ids = [
    [200, 200, 200],
    [200, 200, padding_id],
]
```

模型的 padding token ID 可以通过其分词器的 `pad_token_id` 属性获得。下面我们尝试将两段文本分别以独立以及 batch 的形式送入到模型中：

```python
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence1_ids = [[200, 200, 200]]
sequence2_ids = [[200, 200]]
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

print(model(torch.tensor(sequence1_ids)).logits)
print(model(torch.tensor(sequence2_ids)).logits)
print(model(torch.tensor(batched_ids)).logits)
tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward0>)
tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)
tensor([[ 1.5694, -1.3895],
        [ 1.3374, -1.2163]], grad_fn=<AddmmBackward0>)
```

> 问题出现了，使用 padding token 填充的序列的结果竟然与其单独送入模型时不同！
>
> 这是因为模型默认会编码输入序列中的所有 token 以建模完整的上下文，因此这里会将填充的 padding token 也一同编码进去，从而生成不同的语义表示。

因此，在进行 Padding 操作时，我们必须明确告知模型哪些 token 是我们填充的，它们不应该参与编码。这就需要使用到 Attention Mask 了，在前面的例子中相信你已经多次见过它了。

##### 7.5 Attention mask

下面是掩码的例子

```python
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence1_ids = [[200, 200, 200]]
sequence2_ids = [[200, 200]]
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]
batched_attention_masks = [
    [1, 1, 1],
    [1, 1, 0],
]

print(model(torch.tensor(sequence1_ids)).logits)
print(model(torch.tensor(sequence2_ids)).logits)
outputs = model(
    torch.tensor(batched_ids), 
    attention_mask=torch.tensor(batched_attention_masks))
print(outputs.logits)
```

##### 7.6 直接使用分词器

```python
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

sequences = [
    "I've been waiting for a HuggingFace course my whole life.", 
    "So have I!"
]

model_inputs = tokenizer(sequences)
print(model_inputs)
```

输出就包括了 Input IDs 和 attention_mask

```
{'input_ids': [
    [101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], 
    [101, 2061, 2031, 1045, 999, 102]], 
 'attention_mask': [
    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 
    [1, 1, 1, 1, 1, 1]]
}
```

**Padding 操作**通过 `padding` 参数来控制：

- `padding="longest"`： 将序列填充到当前 batch 中最长序列的长度；
- `padding="max_length"`：将所有序列填充到模型能够接受的最大长度，例如 BERT 模型就是 512。

```python
model_inputs = tokenizer(sequences, padding="longest")
model_inputs = tokenizer(sequences, padding="max_length")
```

**截断操作**通过 `truncation` 参数来控制，如果 `truncation=True`，那么大于模型最大接受长度的序列都会被截断，例如对于 BERT 模型就会截断长度超过 512 的序列。此外，也可以通过 `max_length` 参数来控制截断长度：

```python
model_inputs = tokenizer(sequences, max_length=8, truncation=True)
```

分词器还可以通过 `return_tensors` 参数指定返回的张量格式：设为 `pt` 则返回 PyTorch 张量；`tf` 则返回 TensorFlow 张量，`np` 则返回 NumPy 数组。

```python
model_inputs = tokenizer(sequences, padding=True, return_tensors="pt")
```

实际使用时一般是

```python
tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")
```

##### 7.7  编码句子对

```python
from transformers import AutoTokenizer

checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

sentence1_list = ["First sentence.", "This is the second sentence.", "Third one."]
sentence2_list = ["First sentence is short.", "The second sentence is very very very long.", "ok."]

tokens = tokenizer(
    sentence1_list,
    sentence2_list,
    padding=True,
    truncation=True,
    return_tensors="pt"
)
print(tokens)
print(tokens['input_ids'].shape)

# 输出如下 上下对应一个句子对，比如 first sentence 和 first sentence is short 是一对，它们构成了 tensor 的第一行，token_type_ids是用来划分问题和答案的
{'input_ids': tensor([
        [ 101, 2034, 6251, 1012,  102, 2034, 6251, 2003, 2460, 1012,  102,    0,
            0,    0,    0,    0,    0,    0],
        [ 101, 2023, 2003, 1996, 2117, 6251, 1012,  102, 1996, 2117, 6251, 2003,
         2200, 2200, 2200, 2146, 1012,  102],
        [ 101, 2353, 2028, 1012,  102, 7929, 1012,  102,    0,    0,    0,    0,
            0,    0,    0,    0,    0,    0]]), 
 'token_type_ids': tensor([
        [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 
 'attention_mask': tensor([
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])
}
torch.Size([3, 18])
```

##### 7.8 添加新token

Transformers 库提供了两种方式来添加新 token，分别是：

- **[`add_tokens()`](https://huggingface.co/docs/transformers/v4.25.1/en/internal/tokenization_utils#transformers.SpecialTokensMixin.add_tokens) 添加普通 token：**参数是新 token 列表，如果 token 不在词表中，就会被添加到词表的最后。

  ```python
  checkpoint = "bert-base-uncased"
  tokenizer = AutoTokenizer.from_pretrained(checkpoint)
      
  num_added_toks = tokenizer.add_tokens(["new_token1", "my_new-token2"])
  print("We have added", num_added_toks, "tokens")
  ```

  ```
  We have added 2 tokens
  ```

  为了防止 token 已经包含在词表中，我们还可以预先对新 token 列表进行过滤：

  ```python
  new_tokens = ["new_token1", "my_new-token2"]
  new_tokens = set(new_tokens) - set(tokenizer.vocab.keys())
  tokenizer.add_tokens(list(new_tokens))
  ```

- **[`add_special_tokens()`](https://huggingface.co/docs/transformers/v4.25.1/en/internal/tokenization_utils#transformers.SpecialTokensMixin.add_special_tokens) 添加特殊 token：**参数是包含特殊 token 的字典，键值只能从 `bos_token`, `eos_token`, `unk_token`, `sep_token`, `pad_token`, `cls_token`, `mask_token`, `additional_special_tokens` 中选择。同样地，如果 token 不在词表中，就会被添加到词表的最后。添加后，还可以通过特殊属性来访问这些 token，例如 `tokenizer.cls_token` 就指向 cls token。

  ```python
  checkpoint = "bert-base-uncased"
  tokenizer = AutoTokenizer.from_pretrained(checkpoint)
      
  special_tokens_dict = {"cls_token": "[MY_CLS]"}
      
  num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)
  print("We have added", num_added_toks, "tokens")
      
  assert tokenizer.cls_token == "[MY_CLS]"
  ```

  ```
  We have added 1 tokens
  ```

  我们也可以使用 `add_tokens()` 添加特殊 token，只需要额外设置参数 `special_tokens=True`：

  ```python
  checkpoint = "bert-base-uncased"
  tokenizer = AutoTokenizer.from_pretrained(checkpoint)
      
  num_added_toks = tokenizer.add_tokens(["[NEW_tok1]", "[NEW_tok2]"])
  num_added_toks = tokenizer.add_tokens(["[NEW_tok3]", "[NEW_tok4]"], special_tokens=True)
      
  print("We have added", num_added_toks, "tokens")
  print(tokenizer.tokenize('[NEW_tok1] Hello [NEW_tok2] [NEW_tok3] World [NEW_tok4]!'))
  ```

  ```
  We have added 2 tokens
  ['[new_tok1]', 'hello', '[new_tok2]', '[NEW_tok3]', 'world', '[NEW_tok4]', '!']
  ```

  > 特殊 token 的标准化 (normalization) 与普通 token 有一些不同，比如不会被小写。
  >
  > 这里我们使用的是不区分大小写的 BERT 模型，因此分词后添加的普通 token 和 都被处理为了小写，而添加的特殊 token 和 则保持大写。

  **调整 embedding 矩阵**

  > 向词表中添加新 token 后，必须重置模型 embedding 矩阵的大小，也就是向矩阵中添加新 token 对应的 embedding，这样模型才可以正常工作，将 token 映射到对应的 embedding。

  调整 embedding 矩阵通过 `resize_token_embeddings()` 函数来实现，例如对于前面的例子：

  ```python
  from transformers import AutoTokenizer, AutoModel
  
  checkpoint = "bert-base-uncased"
  tokenizer = AutoTokenizer.from_pretrained(checkpoint)
  model = AutoModel.from_pretrained(checkpoint)
  
  print('vocabulary size:', len(tokenizer))
  num_added_toks = tokenizer.add_tokens(['[ENT_START]', '[ENT_END]'], special_tokens=True)
  print("After we add", num_added_toks, "tokens")
  print('vocabulary size:', len(tokenizer))
  
  model.resize_token_embeddings(len(tokenizer))
  print(model.embeddings.word_embeddings.weight.size())
  
  # Randomly generated matrix
  print(model.embeddings.word_embeddings.weight[-2:, :])
  
  # Output
  vocabulary size: 30522
  After we add 2 tokens
  vocabulary size: 30524
  torch.Size([30524, 768])
  
  tensor([[-0.0325, -0.0224,  0.0044,  ..., -0.0088, -0.0078, -0.0110],
          [-0.0005, -0.0167, -0.0009,  ...,  0.0110, -0.0282, -0.0013]],
         grad_fn=<SliceBackward0>)
  ```

  可以看到，在添加 [ENT_START] 和 [ENT-END] 之后，分词器的词表大小从 30522 增加到了 30524，模型 embedding 矩阵的大小也成功调整为了 。

  > 在默认情况下，新添加 token 的 embedding 是随机初始化的。

  我们尝试打印出新添加 token 对应的 embedding（新 token 会添加在词表的末尾，因此只需打印出最后两行）。如果你多次运行上面的代码，就会发现每次打印出的 [ENT_START] 和 [ENT-END] 的 embedding 是不同的。

#### 8. 模型调用

加载模型和分词器，如果仅作推理开启评判模式

```python
# 1. 模型路径配置
model_path = "/mnt/public/wwj/zhaozq/cy/model/Qwen/Qwen3-8B"

print(f"Loading model from {model_path}...")

# 2. 加载 Tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)

# 3. 加载 Model
# 使用 device_map="auto" 可以自动分配显存，也可以指定 "cuda"
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=torch.bfloat16,
    device_map="auto", 
    trust_remote_code=True
)
model.eval()
```

构建提示词，数据预处理

```python
# 4. 构造 Prompt
query = "What are the limitations of energy harvesting from natural sources?"

messages = [{
    "role": "user",
    "content": f"""
    You are an expert in telecommunications knowledge assessment. Given question related to telecommunications, you need to answer it.

    #### Question
    {query}
    """
}]

# 5. 数据预处理
# step 1: 将 messages 转为 prompt 字符串
# 注意：add_generation_prompt=True 会自动添加 <|im_start|>assistant 引导模型输出
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True,
)

# step 2: 将字符串转为 Tensor 并移动到 GPU
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
```

Qwen3 默认会开启推理模式，该模型提供显示关闭的方法，最后模型输出只会输出答案，下面是推理生成

`with torch.no_grad()` 不会计算梯度图信息，节省显存

```python
# 6. 推理生成
with torch.no_grad():
    generated_ids = model.generate(
        model_inputs.input_ids,
        attention_mask=model_inputs.attention_mask,
        max_new_tokens=2048,
        temperature=0.7,   # 建议稍微降低温度以获得更稳定的回答
        top_p=0.9,
        do_sample=True,
        pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id
    )

# 7. 解码输出
# 为了只显示回答，我们需要切片去掉输入部分的 token
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]

response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
```

上面的参数解析：

1. `do_sample=True`

- **解释**：是否启用采样（sampling）策略。
- **作用**：如果设置为 `True`，模型会根据概率分布从可能的词汇中采样生成下一个 token。这使得生成的文本更具多样性。如果设置为 `False`，则模型会使用贪心搜索（greedy search）策略，即选择概率最大的词生成文本。
  - **采样**：可以生成更具创意或多样性的文本，但可能会包含更多的不确定性或不连贯的内容。
  - **贪心搜索**：生成的文本通常更加精确，但可能会较为单一、缺乏多样性。

2. `temperature=0.6`

- **解释**：`temperature` 控制采样时的随机性，影响生成文本的多样性。
- **作用**：`temperature` 越低，模型生成的文本越确定，重复性高；`temperature` 越高，文本越随机且多样化。`temperature=0.6` 是一个适中的值，意味着生成的文本既有一定的多样性，但不至于太随机。
  - **值范围**：一般来说，`temperature` 在 0 到 1 之间调整，0 表示完全确定，1 表示最大随机性。

3. `top_p=0.9`

- **解释**：`top_p` 也叫做核采样（nucleus sampling），控制生成过程中考虑的概率分布的范围。
- **作用**：`top_p` 定义了一个累计概率阈值。模型会从概率累计到 `p` 的 token 中采样（即选择这些 token 中的词）。`top_p=0.95` 表示模型会从使累计概率为 95% 的 token 中进行采样，而丢弃其他低概率的选项。（意思就是我们把概率从大到小排序，然后累加到0.95的时候，在数组里的token可以被使用）
  - **top_p=1** 表示无条件地从所有可能的 token 中选择。
  - **top_p=0.9** 或更低的值将限制选择的词汇数量，使生成的文本更加精确和连贯。

4. `top_k=20`

- **解释**：`top_k` 限制了模型在生成时考虑的候选 token 数量。模型只会从概率最高的 `top_k` 个词中选择下一个词。
- **作用**：`top_k=20` 表示模型每次生成时会从概率排名前 20 的词中选择一个词。与 `top_p` 类似，`top_k` 控制了生成过程的多样性，但它基于候选词数量限制，而不是累计概率。
  - 如果设置为较小的 `k`，生成的文本将更加集中和精确。

5. `min_p=0`

- **解释**：`min_p` 控制生成时的最小概率阈值。它通常与 `top_p` 或 `top_k` 配合使用，限制生成时考虑的最小概率。
- **作用**：`min_p=0` 表示没有设置最低概率，实际上这会使得生成的过程不受最低概率限制。`min_p` 允许你设置一个最低的概率阈值，低于此阈值的 token 将不会被考虑。例如，如果设置 `min_p=0.05`，那么只有概率大于或等于 5% 的候选词才会被选中。

### 二、强化学习

#### 1. 马尔可夫决策过程

强化学习的两个实体：**智能体（Agent）**与**环境（Environment）**

强化学习中两个实体的交互，下面就是马尔可夫决策过程（MDP）的五元组：

- **状态空间S**：S即为State，指环境中所有可能状态的集合
- **动作空间A**：A即为Action，指智能体所有可能动作的集合
- **奖励R：**R即为Reward，指智能体在环境的某一状态下所获得的奖励。
- **策略P：** 即Policy，决定在给定状态下采取哪个动作的策略
- **衰减 $\gamma$ ：**衰减系数

![image-20251002100645336](assets/image-20251002100645336.png)

给定一个随机策略 $\pi(a_t \mid s_t)$，智能体与环境交互会产生一个状态、动作、奖励序列，也称为一次转移（transition），其形式为
 $(s_t, a_t, r_t, s_{t+1})$。

- $a_t$ 由策略 $\pi$ 根据当前状态 $s_t$ 采样得到。
- $s_{t+1}$ 由状态转移概率 $p_S$ 根据当前状态 $s_t$ 和动作 $a_t$ 采样得到。
- $r_t$ 由奖励概率 $p_R$ 根据当前状态 $s_t$、动作 $a_t$ 和下一状态 $s_{t+1}$ 采样得到。

在策略 $\pi$ 下，生成一条长度为 $T$ 的轨迹
$$
\tau = (s_0, a_0, r_0, s_1, a_1, r_1, s_2, \ldots, s_T)
$$
的概率为：
$$
P(\tau \mid \pi) 
= p(s_0) \prod_{t=0}^{T-1} \pi(a_t \mid s_t)\, p_S(s_{t+1} \mid s_t, a_t)\, p_R(r_t \mid s_t, a_t, s_{t+1}).
$$
**POMDP** 是指部分可观测马尔可夫决策过程，即智能体只能根据动作执行后的观测来判断当前的状态，而无法对全局进行整体感知

状态转移函数为 $p_s(s_{t + 1} |s_t, a_t) = E_{{\epsilon}_{t+1}^s}[T(s_{t+1}= W(s_t, a_t, {\epsilon}_{t + 1}^s))]$ ，即在 $s_t$ 状态执行动作 $a_t$ 后状态为 $s_{t + 1}$ 的概率，用期望是因为加入了随机噪声，有一些确定性场景，比如大模型生成中，$a_t$ 为生成下一个token，那么下一个 $s_{t+1}$ 就是确定的了

智能体目标本质就是实现下面的均衡

- **探索（Exploration）**：尝试不熟悉的动作（action）、状态（state）或策略（policy）。通过探索，智能体可以获取更多环境信息，为未来决策打下基础。
- **利用（Exploitation）**：在已有经验或当前认知下，选择看上去收益最高的动作，进一步提高目标回报（reward）。

强化学习最终目标是使得 $J(\theta) = E_{\tau \sim \pi(\theta)}[G(\tau)]$ 最大，即在满足策略分布的情况下的轨迹所获得的回报的均值最大化

**nlp中的强化学习**

- 我们先喂给模型一个 prompt，期望它能产出符合人类喜好的 response
- 在 $t$ 时刻，模型根据上文，产出一个 token，这个 token 即对应着强化学习中的动作，我们记为 $A_t$。因此不难理解，在 NLP 语境下，强化学习任务的动作空间就对应着词表。
- 在 $t$ 时刻，模型产出 token $A_t$ 对应的即时收益为 $R_t$，总收益为 $V_t$（复习一下，$V_t$ 蕴含着“即时收益”与“未来收益”两个内容）。这个收益即可以理解为“对人类喜好的衡量”。此刻，模型的状态从 $S_t$ 变为 $S_{t+1}$，也就是从“上文”变成“上文 + 新产出的 token”
- 在 NLP 语境下，智能体是语言模型本身，环境则对应着它产出的语料

![image-20251002100710478](assets/image-20251002100710478.png)

#### 2. 贝尔曼方程

这一部分定义了强化学习中如何进行评估以及后面损失函数的设计，奖励是固定的，一次行动环境就会给一个固定的奖励信号；回报是所有奖励的加权和；价值是从当前状态或者动作对未来的预估

##### 2.1 奖励

上面的奖励它表示环境进入状态 下的**即时奖励**。奖励由奖励模型提供，获取奖励的方式有

##### 2.2 回报

但如果只考虑即时奖励，目光似乎太短浅了：当下的状态和动作会影响到未来的状态和动作，进而影响到未来的整体收益。所以，一种更好的设计方式是：**t时刻状态s的总收益 = 身处状态s能带来的即时收益 + 从状态s出发后能带来的未来收益。**写成表达式就是：$V_t = R_t + \gamma V_{t+1}$ 

**回报 $G_t$：** 从时刻 $t$ 起的累计折扣奖励：$G_t = \sum_{k = 0}^{\infty}{{\gamma}^k R_{t+k}}$  

##### 2.3 价值

- **状态价值函数** $V_\theta(s)$: 在策略 $\pi$ 下，从状态 $s$ 开始的期望回报：

$$
V_\theta(s) = \mathbb{E}_\pi [G_t | s_t = s]
$$

从 $V_\theta(s)$ 的定义出发：
$$
V_\theta(s) = \mathbb{E}_\pi [G_t | s_t = s]
$$

将 $G_t$ 展开为奖励和未来回报：

$$
G_t = R_t + \gamma G_{t+1}
$$

代入后得到：

$$
V_\theta(s) = \mathbb{E}_\pi [R_t + \gamma G_{t+1} | S_t = s]
$$

根据期望的线性性质：

$$
V_\theta(s) = \mathbb{E}_\pi [R_t | S_t = s] + \gamma \mathbb{E}_\pi [G_{t+1} | S_t = s]
$$
即时奖励期望：
$$
\mathbb{E}_\pi [R_t | S_t = s] = \sum_a \pi(a | s) \sum_{s'} P(s' | s, a) R(s, a, s')
$$

直观理解就是所有可能状态的概率 $P(s' | s, a)$ 与对应状态的奖励 $R(s, a, s')$ 的期望与策略（Policy）模型在该状态 $s$ 下做出动作 $a$ 的概率之和。

未来回报期望：
$$
\mathbb{E}_\pi [G_{t+1} | S_t = s] = \sum_a \pi(a | s) \sum_{s'} P(s' | s, a) V_\theta(s')
$$

直观理解就是所有可能状态的概率 $P(s' | s, a)$ 与转移状态对应的状态价值函数 $V_\theta(s')$ 的期望与策略（Policy）模型在该状态 $s$ 下做出动作 $a$ 的概率之和。

合并后得到贝尔曼方程：
$$
V_\theta(s) = \sum_a \pi(a | s) \sum_{s'} P(s' | s, a) [R(s, a, s') + \gamma V_\theta(s')]
$$

- **动作价值函数** $Q_\theta(s,a)$: 在策略 $\pi$ 下，从状态 $s$ 执行动作 $a$ 后的期望回报：

$$
Q_\theta(s, a) = \mathbb{E}_\pi [G_t | s_t = s, a_t = a]
$$

类比地，从 $Q_\theta(s, a)$ 出发：

$$
Q_\theta(s, a) = \mathbb{E}_\pi [R_t + \gamma G_{t+1} | S_t = s, A_t = a]
$$

由于动作 $a$ 已确定，直接对下一个状态 $s'$ 求期望：

$$
Q_\theta(s, a) = \sum_{s'} P(s' | s, a) \left[ R(s, a, s') + \gamma \mathbb{E}_\pi [ G_{t+1} | S_{t+1} = s' ] \right]
$$

注意到：

$$
\mathbb{E}_\pi [ G_{t+1} | S_{t+1} = s' ] = V_\theta(s')
$$

状态价值函数的定义为：

$$
V_\theta(s') = \sum_{a'} \pi(a' | s') Q_\theta(s', a')
$$

直观理解就是在状态 $s'$ 下，动作价值函数的所有动作对应的期望是状态价值函数，因为：

$$
Q_\theta(s, a) = \sum_{s'} P(s' | s, a) \left[ R(s, a, s') + \gamma \sum_{a'} \pi(a' | s') Q_\theta(s', a') \right]
$$

##### 2.5 优势

因为状态价值函数是对未来的一个预测，都是不确定的，我们就引入优势，假设当前执行了动作 $a_t$ 后得到的收益可能比在当前状态下的收益多多少，公式可以写为 $Adv_t = Q_{\theta}(s, a) - V_{\theta}(s)$ ，这里 $V_\pi(s)$ 充当了一个「基线」的作用，因为它可以看作是在给定策略、给定状态下，所有动作对应 $Q_\pi(s, a)$ 的期望值：
$$
V_\pi(s) = \mathbb{E}_{\pi(a \mid s)} \big[ Q_\pi(s, a) \big].
$$
通常来说，优势函数 $A_\pi(s, a)$ 表示的是：在状态 $s$ 执行动作 $a$ 比按照策略 $\pi$ 行动要更好还是更差。

我们也可以通过时序差分分解，时序差分就是正常情况我们要计算回报需要计算完整路径，但是计算量太大了，我们就用预估的下一步价值函数和当前动作的回报作为 Q
$$
Adv_t = R_t + \gamma * V_{t+1} - V_t
$$

**广义优势估计**

刚才我们使用 TD 误差来近似优势函数，它是一种**单步TD**方法。为了更好地估计优势函数，我们可以使用 **n 步回报**：
$$
R_t^{(n)} = r_t + \gamma r_{t+1} + ... + \gamma^{n-1}r_{t + n - 1} + \gamma ^nV(s_{t+n})
$$
为了更灵活地进行偏差-方差权衡，GAE可以使用「加权平均」的技巧
$$
A_t^{GAE(\lambda)} = \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l}
$$

数学形式：
$$
A_t^{\text{GAE}} = \delta_t + (\gamma \lambda) \delta_{t+1} 
 + (\gamma \lambda)^2 \delta_{t+2} + \cdots
$$

- $\gamma$（gamma）：折扣因子
- $\lambda$（lambda）：控制偏差/方差的关键参数
  - λ = 1 → 类似 MC 回报，高方差低偏差
  - λ = 0 → TD(0)，低方差高偏差
  - λ ∈ (0,1) → 在中间平衡

PPO 默认 λ ≈ 0.95。

实际代码中一般是：
$$
A_t = \delta_t + \gamma \lambda A_{t+1}
$$
你可以把 GAE 理解成：

> “用很多不同 horizon 的估计来平均优势，每个 horizon 按 λ 进行加权。”

比如：

- 1-step TD
- 2-step TD
- 3-step TD
- ⋯
- 全回报（MC）

然后按权重组合：
$$
\text{GAE} = (1-\lambda)\sum_{k=1}^{\infty} \lambda^{k-1} A^{(k)}.
$$
这是 “加权多步 TD”。

##### 2.6 贝尔曼最优方程

对于 MDP 问题，肯定会存在一个确定性的最优策略，就是总会是 $\sum_{s'} P(s' | s, a) [R(s, a, s') + \gamma V_\theta(s')]$ 这个最大的那个动作，这个时候最优策略 ${\pi}_*(a|s)$  就是一个 one-hot 分布，可以用 max 算子来改写上面的方程 
$$
V_{{\pi}_*}(s) = max_a \sum_{s'} P(s' | s, a) [R(s, a, s') + \gamma V_\theta(s')]
$$


#### 3. 致命三元组

在强化学习中，还有一个著名的「致命三元组」（deadly triad）问题，指的是当以下三个因素同时存在时，学习过程可能会变得不稳定甚至发散：

- **自举 (bootstrapping)** ：用估计值来更新估计值，如时序差分 (Temporal Difference, TD) 学习。
- **函数近似 (function approximation)** ：使用神经网络等函数近似器来表示价值函数。
- **Off-policy 学习** ：使用行为策略产生的数据来更新目标策略，如Q学习。

#### 4. 基于价值的强化学习

##### 4.1 动态规划算法

将 bellman 方程更新为
$$
V_{k+1}(s) = \sum_{a} \pi(a | s) \sum_{s'} P(s' | s, a) [R(s, a, s') + \gamma V_k(s')]
$$
其中 $V_k(s)$ 是第 k 次迭代时状态 s 的价值函数估计

**策略迭代算法** （PI）

策略评估：在给定一个策略（初始可能就是一个随机策略）的情况下，计算该策略下各个状态的价值函数。就是上面那个DP过程。

策略控制：利用上一步求解出的价值函数，进行对策略的「贪婪化」，得到一个新的策略 $\pi'$ ，${\pi}^{'}(s) = arg max_a \sum_{s'} P(s' | s, a) [R(s, a, s') + \gamma V_{\pi}(s')]$ ，要保证每次改进都不得劣于之前的策略，策略的改进一定会收敛

**价值迭代算法** （VI）

最大化操作以及隐含了策略改进，不需要显示地维护状态
$$
V_{k+1}(s) = max_a \sum_{s'} P(s' | s, a) [R(s, a, s') + \gamma V_k(s')]
$$
VI每次更新都在「当前价值函数」的基础上进行「极大化」，这在前期能迅速地把价值函数拉向最优值附近。但到达后期，需要许多额外迭代次数来「细调」价值函数，可能出现「振荡」或「缓慢逼近」的情况。如果状态空间较大，价值迭代在后期趋近最优时往往会变得「较慢」。

而PI在策略改进阶段，一旦整体策略改变，就能跳到一个较高的收益水平。从某种意义上讲，策略评估阶段给出一个相对更加「精确」的价值函数，再进行一次「彻底的」策略改进 ，往往收敛所需的「轮数」（大循环数）较少。

也就是说，PI是少次的「大步跳跃」，而VI是多次的「小步快跑」。

策略迭代方法，

```python
def policy_iteration(env, gamma=0.9, theta=1e-5, max_iter=1000):
    """
    使用策略迭代求解迷宫。
    gamma: 折扣因子(小于1，避免负循环)
    theta: 收敛阈值
    max_iter: 策略迭代的最大迭代次数，防止死循环
    """

    # 1) 收集所有非墙壁状态
    states = []
    for r in range(env.maze_size[0]):
        for c in range(env.maze_size[1]):
            if env.maze[r, c] != -1: 
                states.append((r, c))

    actions = [0, 1, 2, 3]  # 上、下、左、右

    def step_in_model(state, action):
        # 终点不需要再动
        if state == env.goal_pos:
            return state, 0.0, True

        directions = {
            0: (-1, 0),
            1: (1, 0),
            2: (0, -1),
            3: (0, 1)
        }
        move = directions[action]
        new_state = (state[0] + move[0], state[1] + move[1])

        if not (0 <= new_state[0] < env.maze_size[0] and 0 <= new_state[1] < env.maze_size[1]):
            # 越界
            return state, -5, False
        if env.maze[new_state] == -1:
            # 撞墙
            return state, -5, False

        # 正常移动
        reward = -1
        done = False
        if new_state == env.goal_pos:
            reward = 10
            done = True
        return new_state, reward, done

    # 2) 初始化策略、价值函数
    pi = {}
    V = {}
    for s in states:
        if s == env.goal_pos:
            pi[s] = None
            V[s] = 0.0
        else:
            pi[s] = np.random.choice(actions)
            V[s] = 0.0

    # 3) 策略迭代
    iter_count = 0
    while True:
        iter_count += 1
        if iter_count > max_iter:
            print("超过最大迭代次数，提前退出，可能未完全收敛。")
            break

        # ========== (A) 策略评估 ==========
        while True:
            delta = 0
            for s in states:
                if s == env.goal_pos:
                    continue
                v_old = V[s]
                a = pi[s]
                s_next, r, done = step_in_model(s, a)
                if done:
                    V[s] = r
                else:
                    V[s] = r + gamma * V[s_next]
                delta = max(delta, abs(V[s] - v_old))
               
            # 判断是不是所有的值都已经收敛了，即是否价值函数已经逼近真正的价值函数
            if delta < theta:
                break

        # ========== (B) 策略改进 ==========
        policy_stable = True
        for s in states:
            if s == env.goal_pos:
                continue
            old_a = pi[s]

            best_a = None
            best_q = float('-inf')
            for a in actions:
                s_next, r, done = step_in_model(s, a)
                q_sa = r if done else (r + gamma * V[s_next])
                if q_sa > best_q:
                    best_q = q_sa
                    best_a = a

            pi[s] = best_a
            if best_a != old_a:
                policy_stable = False

        if policy_stable:
            print(f"策略在迭代 {iter_count} 次后稳定。")
            break

    return pi, V
```

##### 4.2 蒙特卡罗算法

我们可以发现每次迭代需要遍历动作空间和状态空间，这个会有问题，比如环境是未知的，或者计算量很大，所以这种方法通常不可行

蒙特卡洛（Monte Carlo，MC）方法就是一种**通过多次采样来估计期望值**的方法。在强化学习中，它通过与环境的实际交互，获取状态、动作和奖励的序列（称为**经验**，也就是样本），并基于这些经验来**估计价值函数**。与DP相比，蒙特卡洛方法不需要环境的完整模型，因此更适用于实际场景。

更具体地说，对于一个状态 $s$，我们进行多次试验（或者说，多个回合）：

1. 在每一次试验中，智能体从状态 $s$ 出发，按照策略 $\pi$ 与环境交互，直到到达终止状态（或者折扣因子 $\gamma ^ t$ 已经小到可以忽略不计），这一次交互过程也称作rollout（中文称 展开策略），而得到的「状态、动作、奖励」序列称作「轨迹」（trajectory）。
2. 记录下此次试验中，从状态 $s$ 开始到终止状态的所有奖励，计算该轨迹的折扣回报 $G_t$。
3. 重复多次试验，计算所有回报的平均值，就可以得到对价值函数的一个近似值：$V_\pi(s) = \frac{1}{n}\sum_{t=1}^nG_t$ 

使用增量式计算，在 MC 方法中，通常会进行大量的实验，每次都重新计算平均值效率比较地，我们可以将上述过程写成更新迭代的公式 $V(s_t) \leftarrow V(s_t) + \eta[G_t - V(s_t)]$，在实际操作中通常采用动作价值函数的增量计算来进行更新迭代

在上面介绍的最基本的MC方法中，我们其实隐含了**在策略（On-Policy）** 的假设：所谓「在策略」，或者「同策略」，指的是，与环境交互（生成样本）的**行为策略（behavior policy）** 和 评估价值（更新Q函数）的**目标策略（target policy）** 是同一个策略。

下面具体算法中，epsilon_greedy_policy，有一个贪心策略采样，是因为智能体可能会陷入一个较优环境，导致环境探索不足，因此采用随机贪心选择一些点

```python
def mc_control_on_policy(env, num_episodes=5000, gamma=1.0, epsilon=0.1):
    """
    基于第一访问蒙特卡洛的 on-policy 控制（ε-贪心）。
    :param env: 自定义迷宫环境
    :param num_episodes: 训练的回合数
    :param gamma: 折扣因子
    :param epsilon: 探索率
    :return: Q, 最优的状态-动作价值函数
    """
    # Q 表示状态-动作价值函数，大小为 [行, 列, 动作数]
    Q = np.zeros((env.maze_size[0], env.maze_size[1], env.action_space.n))

    # 这里使用一个字典来存储每个状态-动作对的回报（列表），方便后续取平均做更新
    returns = dict()
    for r in range(env.maze_size[0]):
        for c in range(env.maze_size[1]):
            for a in range(env.action_space.n):
                returns[((r, c), a)] = []

    def epsilon_greedy_policy(state):
        """
        给定当前的 Q 和 explored state, 采用 ε-贪心策略选择动作
        """
        r, c = state
        if random.random() < epsilon:
            # 随机探索
            return np.random.choice(env.action_space.n)
        else:
            # 贪心选择
            return np.argmax(Q[r, c])

    for episode in range(num_episodes):
        # 生成一条回合（episode）
        state = env.reset()
        episode_trace = []  # 存储 (state, action, reward) 元组

        done = False
        while not done:
            action = epsilon_greedy_policy(tuple(state))
            next_state, reward, done, _ = env.step(action)
            episode_trace.append((tuple(state), action, reward))
            state = next_state

        # 回溯回合，更新 Q
        visited_state_actions = set()
        G = 0  # 从后往前计算折扣回报
        # 在这里从后向前计算更简洁（若想从前向后可先沿 episode_trace 再次扫一遍计算回报）
        for t in reversed(range(len(episode_trace))):
            s_t, a_t, r_t = episode_trace[t]
            G = gamma * G + r_t
            # 检查是否是该回合中首次出现的 (s_t, a_t)
            if (s_t, a_t) not in visited_state_actions:
                visited_state_actions.add((s_t, a_t))
                returns[(s_t, a_t)].append(G)
                # 增量方式更新 Q(s, a)
                Q[s_t[0], s_t[1], a_t] = np.mean(returns[(s_t, a_t)])
    return Q

if __name__ == "__main__":
    # 创建环境
    env = MazeEnv()

    # 使用蒙特卡洛方法进行训练
    Q = mc_control_on_policy(env, num_episodes=3000, gamma=1.0, epsilon=0.1)

    # 打印最终学到的 Q
    print("训练结束后学到的状态-动作价值函数 Q：")
    for r in range(env.maze_size[0]):
        for c in range(env.maze_size[1]):
            print(f"State=({r},{c}) -> Q={Q[r, c]}")
        print()

    # 根据学到的 Q 构造出一个贪心策略并测试
    def greedy_policy(state):
        return np.argmax(Q[state[0], state[1]])

    # 测试智能体在环境中的表现
    state = env.reset()
    env.render()
    done = False
    step_count = 0
    while not done and step_count < 50:  # 做一个简单的步数限制，防止卡死
        action = greedy_policy(tuple(state))
        next_state, reward, done, _ = env.step(action)
        state = next_state
        env.render()
        step_count += 1

    if tuple(state) == env.goal_pos:
        print("智能体成功到达目标！")
    else:
        print("智能体未能到达目标。")
```

##### 4.3 重要性采样

**1. 强化学习中的重要性采样**

在强化学习中，我们通常希望评估一个目标策略 $\pi$ 的表现，或者根据目标策略优化智能体的行为。但是，在很多情况下，直接按照目标策略采样数据可能不现实或不高效。此时，我们可以使用与目标策略不同的行为策略 $b$ 来生成数据。

重要性采样的核心思想是利用行为策略产生的数据，通过加权（重要性权重），将这些数据转换为目标策略下的有效数据。

设 p(x) 为目标策略，q(x) 为行为策略，q(x) 比较容易采样 

根据期望的定义，我们可以将 p(x) 下的期望值写成（连续变量版，如果是离散的，把积分换成求和）：

\[
\mathbb{E}_{x\sim p(x)}[f(x)] = \int f(x)p(x)\, dx
\]

如果从 p(x) 中采样比较困难，我们可以引入另一个分布 q(x)，并对上述式进行如下变换：

\[
\mathbb{E}_{x\sim p(x)}[f(x)] = \int f(x)\frac{p(x)}{q(x)}q(x)\, dx
\]

上述可以看做是函数 \( f(x)\frac{p(x)}{q(x)} \) 在分布 q(x) 下的期望，所以可以将其写为：

\[
\mathbb{E}_{x\sim p(x)}[f(x)] = \mathbb{E}_{x\sim q(x)}\!\left[f(x)\frac{p(x)}{q(x)}\right]
\]

其中，\(\frac{p(x)}{q(x)}\) 被称为**重要性权重**。

**2. 重要性采样公式**

设定以下符号：

- **目标策略** $\pi$：我们希望评估或优化的策略。
- **行为策略** $b$：我们用来生成数据的策略。
- **轨迹** $\tau = (s_0, a_0, r_1, s_1, a_1, r_2, \dots)$：一系列状态、动作和奖励。
- **重要性采样权重**：每个轨迹的权重为 $\frac{\pi(a_t | s_t)}{b(a_t | s_t)}$，即目标策略和行为策略选择相同动作的概率之比。

重要性采样的基本估计公式为：
$$
J(\pi) = \mathbb{E}_{b} \left[ \frac{\pi(a_t|s_t)}{b(a_t|s_t)} r_t \right]
$$
其中，$r_t$ 是从状态 $s_t$ 到下一个状态的奖励。其中 $\frac{\pi(a_t|s_t)}{b(a_t|s_t)}$ 被称为重要性权重，即目标策略和行为策略的比值，该值较大时，会导致估计值的方差很大，造成算法的不稳定，可以通过截断重要性采样来解决（即 clip）

> 重要性采样的方差为：
> $$
> \mathrm{Var}(\hat{\mu}) = \frac{1}{N}
> \left( 
> \mathbb{E}_{q}\big[(f(x) w(x))^2\big] 
> - (\mathbb{E}_p[f(x)])^2
> \right)
> $$
> 我们可以发现当 f(x)w(x) 很大时，会导致方程很大，w(x) 就是重要性采样权重
> $$
> (f(x)w(x))^2 = f(x)^2\, w(x)^2
> $$

在离策略学习中，重要性采样帮助我们调整行为策略和目标策略之间的差异，从而更准确地估计目标策略的表现。

##### 4.4 时序差分（Q-learning）

因为 MC 方法是通过采样来进行策略评估，所以需要等一个完整的回合结束才能知道具体的回报是多少，学习效率比较低，因此我们采用时序差分思路，将dp和mc思路结合
$$
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \eta[r_t + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)]
$$
![img](assets/v2-76418e27e9511afaabb247d762e1679c_r.jpg)

**SARSA 算法**

利用上面的公式进行更新，是on-policy算法，所以只能探索到被访问的点，可以利用 $\epsilon - greedy$ 算法折中。下面公式中 $\delta_{t}$ 是 TD 误差，也可以理解为优势
$$
\delta_t =  r_t + \gamma \ Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)
$$
**Q-learning算法**

就是用另一个行为策略先探索，找到一些 $max Q(s_{t+1}, a')$ 用来后面的训练更新

概述：**Q-learning** 是一种基于价值的 **强化学习** 算法，用于解决 **离策略（off-policy）** 的问题。它的目标是通过智能体与环境的交互来学习最优策略，即找到一个策略，使得在任何状态下，智能体采取某个动作时能最大化长期回报。

基本思想：Q-learning 通过不断更新每个 **状态-动作对 (s, a)** 的 **Q值**，来估计智能体在当前状态下采取某个动作后，能获得的最大长期回报。Q-learning 使用的是 **Bellman 方程** 来更新 Q 值，更新公式如下：
$$
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left( r_t + \gamma \max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t) \right)
$$

- $Q(s_t, a_t)$ 是状态 $s_t$ 下，采取动作 $a_t$ 的 Q 值。
- $\alpha$ 是学习率，控制新信息对 Q 值的影响。
- $r_t$ 是在状态 $s_t$ 采取动作 $a_t$ 后得到的即时奖励。
- $\gamma$ 是折扣因子，控制未来奖励的价值。
- $\max_{a'} Q(s_{t+1}, a')$ 是从新状态 $s_{t+1}$ 出发，选择能够最大化长期回报的动作的 Q 值。

Q-learning 的关键思想是 **最大化长期回报**，并通过不断迭代更新 Q 值，使得 Q 值趋于最优。

**步骤**

1. **初始化 Q 表**：
    初始化 Q 表，其中 Q 值通常被初始化为任意值（例如 0），或者一个较小的随机数。
2. **选择动作**：
    在每个时间步 $t$，智能体根据当前的状态 $s_t$ 选择一个动作 $a_t$。选择动作时，可以使用 **ε-贪心策略**（ε-greedy strategy）：
   - 以概率 $1 - \epsilon$ 选择具有最大 Q 值的动作（即贪心选择）。
   - 以概率 $\epsilon$ 随机选择一个动作（即探索）。
3. **执行动作并更新 Q 值**：
    执行选择的动作 $a_t$，然后根据获得的奖励 $r_t$ 和下一状态 $s_{t+1}$，使用 Q-learning 更新公式更新 Q 值。
4. **重复直到收敛**：
    重复步骤 2 和 3，直到达到预定的停止条件（例如，达到最大回合数，或者 Q 值的变化小于某个阈值）。

```python
# 初始化 Q 表为零，或小的随机值
Q = np.zeros((num_states, num_actions))

# 循环直到收敛
for episode in range(num_episodes):
    # 重置环境，获取初始状态
    state = env.reset()
    
    # 每个回合的时间步骤
    done = False
    while not done:
        # 以 ε-贪心策略选择动作
        if np.random.uniform(0, 1) < epsilon:
            action = np.random.choice(num_actions)  # 探索
        else:
            action = np.argmax(Q[state])  # 利用当前 Q 值选择最大动作

        # 执行动作并获得奖励和下一个状态
        next_state, reward, done, _ = env.step(action)
        
        # 更新 Q 值
        Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])
        
        # 更新当前状态
        state = next_state
```

##### 4.5 DQN

我们发现上面利用表格维护 $[s_t, a_t] \rightarrow q_t$ ，因为在二维地图中状态和动作的维度都比较小，所以可以，但是当维度很大的时候用表格就不合适了，所以考虑利用神经网络来拟合，即对于状态-动作对 $(s, a)$ 作为输入，神经网络的输出为 $Q_w(s, a)$ ，我们使用 MSE 作为损失函数训练，其中我们把 $r + \gamma max_{a'}Q_w(s', a')$ 作为目标值
$$
L(w) = E_{(s, a, r, s')} [(r + \gamma max_{a'}Q_w(s', a') - Q_w(s, a))^2]
$$
我们如果给上面那个损失函数求导，会遇到一个问题，「估计值」和「目标值」都是神经网络计算出来的，这个和「监督学习」是有差别的：在监督学习中，目标值是固定的标签

这会带来训练的不稳定的偏差。为了解决这个问题，我们可以「假装」目标值是固定的，也就是说，在给损失函数求梯度时，我们认为 $r + max_{a'}Q_w(s', a')$ 是个常数，而只给 $Q_w(s, a)$  求导，然后像标准梯度下降一样对当前估计值计算梯度，然后更新参数。

因为我们只计算了函数的一部分梯度，而不是整个函数的梯度，所以这种方法称为半梯度法（semi-gradient）。当然这种妥协不是没有代价的，由于忽略了目标值对参数的依赖性，这种更新方式可能导致收敛性问题。

**数据相关性**

第一个问题就是「数据相关性」。在传统的监督学习中，我们通常假设数据之间是「独立同分布」的，样本之间没有依赖性。但是，在Q-Learning这种TD学习中，数据是智能体与环境进行交互，产生一系列的「经验样本」，这些经验样本之间存在很强的「相关性」。例如，智能体连续向左移动几次，则这些经验样本在状态、动作和奖励上都非常相似。

高度相关的样本会使得模型在短时间内接触到相似的输入模式，导致模型参数更新的方向单一且不稳定。如果连续的样本都指向同一个方向的梯度，模型很容易陷入局部最优解。另外，这可能还会让算法的泛化性降低。

**经验回放**

为了解决数据相关性问题，实际中使用的DQN（Deep Q-Network）通常会使用**经验回放机制**（Experience Replay）：将智能体与环境交互的经验 $(s, a', r, s')$ 存储到一个缓冲区中，然后从缓冲区中随机采样一批经验来更新 Q 网络。

这样有什么好处呢？随机采样打破了样本之间的时间相关性，使得模型在训练时接触到的样本不再是连续的序列，而是来自不同时间点的样本，从而降低了样本之间的**时间相关性**。同时，回放缓存中存储了过去多个时间步的经验，这使得每次训练使用的样本具有更高的**多样性**，有助于模型学习到更稳健的特征。

但是 DQN 存在致命三元组问题，因此我们训练两个网络

1. **主网络（Online Network）** ：用于估计当前状态动作的Q值，并根据梯度下降进行参数更新。
2. **目标网络（Target Network）** ：用于计算TD目标，它的参数会滞后于主网络，从而提供更稳定的目标值。

```python
def train_dqn(model_name):
    env = MazeEnv()
    # 定义超参数
    num_episodes = 500
    batch_size = 32
    gamma = 0.99
    lr = 1e-3

    # epsilon 贪心相关参数
    epsilon_start = 1.0
    epsilon_end = 0.01
    epsilon_decay = 300  # 调整衰减速度

    target_update_interval = 50  # 每隔多少个 episode 同步一次目标网络
    replay_buffer_capacity = 10000

    # 创建网络
    policy_net = DQN()
    target_net = DQN()
    target_net.load_state_dict(policy_net.state_dict())
    target_net.eval()

    # 优化器
    optimizer = optim.Adam(policy_net.parameters(), lr=lr)

    # 经验回放缓冲区
    replay_buffer = ReplayBuffer(replay_buffer_capacity)

    # 记录奖励信息
    all_rewards = []

    # 训练过程
    for episode in range(num_episodes):
        state = env.reset()
        state = torch.FloatTensor(state).unsqueeze(0)
        episode_reward = 0
        done = False

        # 计算当前 epsilon
        epsilon = epsilon_end + (epsilon_start - epsilon_end) * \
            np.exp(-1. * episode / epsilon_decay)

        while not done:
            # 根据 epsilon 贪心选择动作
            if random.random() < epsilon:
                action = env.action_space.sample()
            else:
                with torch.no_grad():
                    q_values = policy_net(state)
                    action = q_values.argmax(dim=1).item()

            # 与环境进行一步交互
            next_state, reward, done, _ = env.step(action)
            next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)

            # 将 transition 存到经验回放中
            replay_buffer.push(
                state.squeeze(0).numpy(),
                action,
                reward,
                next_state_tensor.squeeze(0).numpy(),
                done
            )

            episode_reward += reward
            state = next_state_tensor

            # 每步都尝试训练（如果缓冲区够大）
            if len(replay_buffer) >= batch_size:
                # 从回放缓冲区采样
                states_b, actions_b, rewards_b, next_states_b, dones_b = replay_buffer.sample(batch_size)
                states_b = torch.FloatTensor(states_b)
                actions_b = torch.LongTensor(actions_b)
                rewards_b = torch.FloatTensor(rewards_b)
                next_states_b = torch.FloatTensor(next_states_b)
                dones_b = torch.FloatTensor(dones_b)

                # 计算 Q(s, a)
                q_values = policy_net(states_b)
                # 选出与动作对应的 Q-value
                q_values = q_values.gather(1, actions_b.unsqueeze(1)).squeeze(1)

                # 计算 Q'(s', a') 来 更新目标
                with torch.no_grad():
                    # 使用target_net来计算 max Q'(s', a')
                    next_q_values = target_net(next_states_b)
                    max_next_q_values = next_q_values.max(dim=1)[0]
                    # 如果结束，那么目标是 reward；否则是 reward + gamma * max Q'(s', a')
                    target_q_values = rewards_b + gamma * (1 - dones_b) * max_next_q_values

                # 计算损失
                loss = nn.MSELoss()(q_values, target_q_values)

                # 反向传播和更新
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

        all_rewards.append(episode_reward)

        # 每隔一段时间更新目标网络
        if (episode + 1) % target_update_interval == 0:
            target_net.load_state_dict(policy_net.state_dict())

        # 打印训练信息
        print(f"Episode {episode+1}, Epsilon: {epsilon:.3f}, Reward: {episode_reward}")

    # 保存训练好的网络
    torch.save(policy_net.state_dict(), model_name)
    print(f"model saved: {model_name}")

    return all_rewards
```

##### 4.6 $TD(\lambda)$ 

TD 方法不必等待整个回合结束，只进行单步采样，就可以更新价值函数，但是只考虑一步会导致无法准确预测未来的收益，因此我们采用 n 步回报
$$
G_{t:t+n} = r_t + \gamma r_{t+1}+...+\gamma^{n-1} r_{t+n - 1} + \gamma ^{n}V(s_{t+n})
$$
但是，n 是一个超参数，需要人为指定。不同的任务可能需要不同的 n 值，如何选择合适的 n 值是一个挑战。而且，如果选择**固定的n**，意味着所有状态的更新都使用相同步长的回报，这不够灵活。有些状态可能需要更长远的回报，而另一些状态可能只需要较短的回报。

为了解决上述问题，我们在「n步回报」的思想之上，再引入一个「λ 回报」的定义，将所有 n 步回报进行加权平均的回报：
$$
G_t^\lambda = (1-\lambda) \sum_{n=1}^{\infty} \lambda ^{n-1} G_{t:t+n}
$$
$\lambda$ 是一个介于 0 和 1 之间的参数，它控制了不同 n 步回报的权重。

- 当 λ 接近 0 时，短步回报的权重较高，学习方式更接近 TD 方法
- 当 λ 接近 1 时，长步回报的权重较高，学习方式更接近 MC 方法

#### 5. 基于策略的强化学习

基于价值（Value-based）的RL方法，他们是先学习状态价值函数V或状态-动作价值函数Q，然后根据价值函数**间接**指导策略的改进。而基于策略的RL，是对策略进行参数化，**直接**进行优化，没有V或Q做「中间商」。

基于策略的RL可以更好的处理「连续动作空间」的问题。什么是连续动作空间？以我们之前用的走迷宫游戏为例，在一个状态下，智能体都只有上下左右四种动作可以选择，这就是一个典型的「离散」的动作空间；与之相对，**连续动作空间**指的是智能体在每个时间步可以选择的动作是连续的，而不是离散的。例如，在控制问题中，动作可能是施加在机器人关节上的力或扭矩，这些值可以在某个范围内连续变化。

因为基于策略的RL没有价值函数V或者Q这个「中间商」，直接用一个可优化的函数去计算策略 ${\pi}_{\theta}(a|s)$，其中 $\theta$ 是策略的参数。例如，我们可以使用神经网络来建模策略，其中网络的输入是状态 $s$，输出是每个动作的概率。通过调整神经网络的参数 $\theta$，我们可以改变策略的行为。

##### 5.1 策略梯度定理

最大化目标函数，需要计算关于策略梯度 $\theta$ 的梯度 $\nabla_\theta J(\theta) = \nabla_\theta E_{\tau \sim \pi_\theta} [ G(\tau) ]$ ，我们可以将策略梯度改写为下面的形式：$\nabla_\theta J(\theta) = \nabla_\theta E_{\tau \sim \pi_\theta} [\sum_{t=0}^T G(\tau) \nabla_{\theta} log {\pi}_{\theta}(a_t|s_t) ]$，期望的梯度难以计算，我们可以用MC方法，即采样来近似梯度

其中，$\tau$ 表示智能体在策略下与环境交互产生的一个轨迹 (trajectory)，轨迹由一系列的状态和动作组成，表示为 $\tau = (s_0, a_0, s_1, a_1, ...,s_T, a_T)$ ，$G(\tau)$ 就是这个轨迹总的回报

##### 5.2 REINFORCE 算法

原始PG算法的一个主要问题是，它使用了不相关的奖励信息。在上述代码的计算梯度时，我们使用了总回报 $G(\tau)$ 作为权重，它包含了整个轨迹的回报，也就是说，站在时刻 t 的视角去看，这个总回报既包含了时刻t之前的奖励，也包含了时刻t之后的奖励。

这不太符合我们的直觉：我们一个朴素的直觉是，当前时刻的动作选择，**只应该考虑当前以及未来的奖励，而不应该考虑过去的奖励**。过去的就让它过去，不要让它影响我们当下的决策，为了解决这个问题， REINFORCE 使用了 $G_t$ 来代替 $G_\tau$ 作为权重
$$
G_t = \sum_{k =t}^T\gamma ^k r_k
$$
因此REINFORCE算法的梯度计算公式如下：
$$
\nabla_\theta J(\theta) = \nabla_\theta E_{\tau \sim \pi_\theta} [\sum_{t=0}^T G(t) \nabla_{\theta} log {\pi}_{\theta}(a_t|s_t) ]
$$
但是通过采样轨迹来估算策略梯度会有较高的方差，因此我们采用一个基线的概念，即用 $G_t - b(s_t)$ 代替 $G_t$ ，而这个基线可以用 $V(s)$ 来表示，这里就是优势的概念，损失函数定义为
$$
L(\theta) = - \mathbb{E}_{\tau \sim \pi_\theta} \left[ G(t) \log \pi_\theta(a_t | s_t) \right]
$$

```python
class PolicyNetwork(nn.Module):
    def __init__(self, state_dim=2, action_dim=4, hidden_dim=64):
        super(PolicyNetwork, self).__init__()
        # 输入: (x, y) 2维状态
        # 输出: 对应 4 个离散动作的概率分布
        self.layers = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim),
            nn.Softmax(dim=-1)  # 输出动作概率
        )

    def forward(self, x):
        return self.layers(x)


# ------------------------------
# 一些工具函数
# ------------------------------
def compute_discounted_returns(rewards, gamma=0.99):
    """计算从每个时间步开始的折扣回报 G_t。"""
    discounted_returns = []
    G = 0
    # 从后往前计算
    for r in reversed(rewards):
        G = r + gamma * G
        discounted_returns.insert(0, G)  # 头部插入
    return discounted_returns


# ------------------------------
# REINFORCE 训练函数
# ------------------------------
def train_reinforce(
    num_episodes=500,
    gamma=0.99,
    lr=1e-3,
    render_interval=0,
    model_save_path=None
):
    """
    使用 REINFORCE 方法训练策略网络。
    参数:
    - num_episodes: 训练的总回合数
    - gamma: 折扣因子
    - lr: 学习率
    - render_interval: 若 > 0，则每隔多少回合渲染一次迷宫
    - model_save_path: 若不为 None, 则在训练结束保存模型到该路径
    """
    env = MazeEnv()
    policy_net = PolicyNetwork()
    optimizer = optim.Adam(policy_net.parameters(), lr=lr)

    reward_history = []

    for i_episode in range(num_episodes):
        state = env.reset()
        log_probs = []
        rewards = []

        done = False
        step_count = 0

        # ----------------------------
        # 生成一条完整的回合(episode)
        # ----------------------------
        while not done:
            # 是否需要渲染
            if render_interval > 0 and (i_episode + 1) % render_interval == 0:
                env.render()

            # 转换状态为张量
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            # 前向传播, 得到动作概率分布
            action_probs = policy_net(state_tensor)
            dist = Categorical(action_probs)
            # 依据分布采样动作
            action = dist.sample()
            # 记录该动作的对数概率，以用于梯度更新
            log_prob = dist.log_prob(action)
            # 与环境交互
            next_state, reward, done, _ = env.step(action.item())

            log_probs.append(log_prob)
            rewards.append(reward)

            state = next_state
            step_count += 1

        # -----------------------------
        # 计算回合总折扣回报并回传梯度
        # -----------------------------
        discounted_returns = compute_discounted_returns(rewards, gamma)

        # 标准化 returns (可选, 常见做法)
        discounted_returns = torch.FloatTensor(discounted_returns)
        discounted_returns = (discounted_returns - discounted_returns.mean()) / \
                             (discounted_returns.std() + 1e-9)

        # 计算 loss = - Σ (log_pi(a_t|s_t) * Gt)
        loss = 0
        for log_prob, Gt in zip(log_probs, discounted_returns):
            loss += -log_prob * Gt

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        episode_reward = sum(rewards)
        reward_history.append(episode_reward)

        print(f"Episode {i_episode+1}/{num_episodes}, Reward: {episode_reward:.2f}")

    # 如果指定了保存路径，保存训练好的策略网络
    if model_save_path is not None:
        torch.save(policy_net.state_dict(), model_save_path)
        print(f"模型已保存到 {model_save_path}")

    return reward_history
```

##### 5.3 Actor-Critic 方法

Actor-Critic 方法的核心思想是：**利用 Critic 网络来评估当前策略的好坏，然后 Actor 网络根据 Critic 的评估结果来更新策略**。它是一种结合了策略梯度方法和时序差分学习 (Temporal Difference Learning，TD Learning) 的方法。因为 REINFORCE 需要完整的回报来ji'saun

- **Actor (策略)**：Actor 是一个策略网络 $\pi_\theta(a|s)$，它接收当前状态 $s$ 作为输入，输出一个动作 $a$ 的概率分布。Actor 的目标是学习一个好的策略，使得智能体可以获得尽可能高的回报。
- **Critic (值函数)**：Critic 是一个值函数网络 $Q_w(s, a)$ 或 $V_w(s)$，它接收当前状态 $s$（和动作 $a$）作为输入，输出一个对当前状态（或状态-动作对）的评估值。Critic 的目标是准确地评估当前策略的好坏。

**A2C（Advantage Actor-Critic）**

用优势函数作为 Critic，$A(s, a) = Q(s, a) - V(s)$ ，但是如果用 Q 函数来计算优势，还需要一个额外的 Q 函数网络，通常用 TD 误差来近似优势函数，即 $\delta_t = r_t + \gamma V(S_{t+1}) - V(s_t)$  

有一些技巧提高训练

1. **Stop-gradient：** 在更新 Actor 网络时，我们需要使用优势函数来计算策略梯度。为了避免 Critic 网络的更新影响 Actor 网络的更新，我们使用 stop-gradient 操作符来阻止梯度信息流向 Critic 网络。通俗地说，就是**在更新 Actor 网络时，我们把 Critic 网络的输出看作常数，忽略 Critic 网络参数的变化**。
2. **熵正则化项：** 为了**鼓励探索**，我们可以在 Actor 网络的损失函数中添加一个熵正则化项。熵正则化项可以鼓励 Actor 网络输出更加均匀的动作分布，从而避免陷入局部最优。
3. **共享网络：** 为了提高训练效率，我们可以让 Actor 网络和 Critic 网络共享一部分网络参数。比如，可以让它们共享卷积层或全连接层，然后分别使用不同的输出层。
4. **并行交互**：使用多个同步的智能体与环境交互，并统一进行参数更新。

```python
class ActorCritic(nn.Module):
    """
    一个简单的Actor-Critic结构:
      - actor_head: 输出对各动作的概率分布 (logits)
      - critic_head: 输出该状态的价值 V(s)
    """
    def __init__(self, state_dim=2, action_dim=4, hidden_dim=64):
        super(ActorCritic, self).__init__()
        self.shared = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )
        # Actor 部分输出对每个动作的logits (还需经过Softmax)
        self.actor_head = nn.Linear(hidden_dim, action_dim)
        # Critic 部分输出状态价值 V(s)
        self.critic_head = nn.Linear(hidden_dim, 1)

    def forward(self, x):
        """
        前向传播：
        输入： x (batch_size, state_dim)
        输出： actor_logits (batch_size, action_dim),
              critic_value (batch_size, 1)
        """
        shared_out = self.shared(x)
        actor_logits = self.actor_head(shared_out)
        critic_value = self.critic_head(shared_out)
        return actor_logits, critic_value


# --------------------------------------------------------------------------------
# 3. 训练 Actor-Critic
# --------------------------------------------------------------------------------
def train_actor_critic():
    env = MazeEnv()

    # 超参数
    num_episodes = 300
    gamma = 0.99
    lr = 1e-3

    # 创建网络
    model = ActorCritic(state_dim=2, action_dim=4, hidden_dim=64)
    optimizer = optim.Adam(model.parameters(), lr=lr)

    # 用于记录所有回合的总奖励
    all_rewards = []

    for episode in range(num_episodes):
        state = env.reset()
        state = torch.FloatTensor(state).unsqueeze(0)  # shape: (1,2)
        done = False

        # 记录一整个回合的 (state, action, reward, log_probs, values)
        transitions = []
        episode_reward = 0

        while not done:
            # 前向传播，得到logits和价值
            logits, value = model(state)   # logits: shape (1, action_dim)
            # 根据logits采样动作
            dist = torch.distributions.Categorical(logits=logits)
            action = dist.sample()
            log_prob = dist.log_prob(action)

            # 与环境交互
            next_state, reward, done, _ = env.step(action.item())
            next_state_t = torch.FloatTensor(next_state).unsqueeze(0)

            transitions.append({
                'state': state,
                'action': action,
                'reward': reward,
                'log_prob': log_prob,
                'value': value
            })

            episode_reward += reward
            state = next_state_t

        # 回合结束后，计算 returns 并更新网络
        # 1) 计算每个时间步的回报 G_t
        returns = []
        G = 0
        for t in reversed(transitions):
            G = t['reward'] + gamma * G
            returns.insert(0, G)
        returns = torch.FloatTensor(returns).unsqueeze(1)  # shape: (T,1)

        # 2) 计算 Actor 和 Critic 的loss并反向传播
        actor_loss = 0
        critic_loss = 0
        for i, trans in enumerate(transitions):
            advantage = returns[i] - trans['value']
            # Actor损失: -log(pi(a|s)) * advantage (策略梯度)
            actor_loss += -trans['log_prob'] * advantage.detach()
            # Critic损失: MSE( V(s) - G_t )
            critic_loss += advantage.pow(2)

        loss = actor_loss + critic_loss
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        all_rewards.append(episode_reward)
        # 打印训练信息
        print(f"Episode {episode+1}, Reward: {episode_reward}")

    # 训练结束后，保存模型
    torch.save(model.state_dict(), 'actor_critic_model.pth')
    return all_rewards


# --------------------------------------------------------------------------------
# 4. 使用训练好的 Actor-Critic 拟合器进行测试
# --------------------------------------------------------------------------------
def test_actor_critic(model_path='actor_critic_model.pth', num_episodes=1):
    """
    使用训练好的Actor-Critic模型在迷宫环境中测试 num_episodes 次，
    并通过 env.render() 在控制台打印出路径。
    参数:
        model_path: str, 已保存的模型文件路径，例如 'actor_critic_model.pth'
        num_episodes: int, 测试的回合数
    """
    env = MazeEnv()

    # 构建网络并加载参数
    model = ActorCritic(state_dim=2, action_dim=4, hidden_dim=64)
    model.load_state_dict(torch.load(model_path))
    model.eval()

    for episode in range(num_episodes):
        state = env.reset()
        state = torch.FloatTensor(state).unsqueeze(0)
        done = False
        episode_reward = 0
        print(f"===== 测试 Episode {episode + 1} 开始 =====")

        while not done:
            env.render()
            with torch.no_grad():
                logits, _ = model(state)
                dist = torch.distributions.Categorical(logits=logits)
                action = dist.sample()

            next_state, reward, done, _ = env.step(action.item())
            episode_reward += reward
            state = torch.FloatTensor(next_state).unsqueeze(0)

        # 渲染最终状态
        env.render()
        print(f"Episode {episode + 1} 结束，总奖励: {episode_reward}\n")


# --------------------------------------------------------------------------------
# 5. 运行示例
# --------------------------------------------------------------------------------
if __name__ == "__main__":
    # 1) 训练
    #   如果你已经训练过并保存了模型，可以注释掉此行并只执行测试。
    rewards = train_actor_critic()

    # 2) 测试
    #   如果已经存在 "actor_critic_model.pth"，可以直接进行测试
    #   也可以在训练完后直接测试
    test_actor_critic(model_path='actor_critic_model.pth', num_episodes=3)
```

##### 5.5 策略梯度总结

我们将 Actor 的策略梯度写成一个更通用的形式
$$
\nabla J(\theta) = \mathbb{E}\left[\sum_{t=0}^{\infty} \Psi_t \nabla \log \pi_\theta(a_t | s_t)\right]
$$

1. **蒙特卡洛估计（Monte Carlo Estimate）**：
   $$
   \Psi_t = \sum_{i=t}^{\infty} \gamma^i r_i
   $$
   这表示累计的回报，通过回报的累计折扣计算。

2. **带偏的蒙特卡洛估计**：
   $$
   \Psi_t = \sum_{i=t}^{\infty} \gamma^i r_i - V(s_t)
   $$
   这里使用了状态值函数 $V(s_t)$ 来减少估计的方差。

3. **优势函数**：
   $$
   \Psi_t = A_w(s_t, a_t)
   $$
   这表示优势函数，它衡量某个特定动作相对于基准策略的优势。

4. **Q函数**：
   $$
   \Psi_t = Q_w(s_t, a_t)
   $$
   这里是 Q 函数，用来估计在某个状态-动作对下的价值。

5. **TD误差**：
   $$
   \Psi_t = r_t + \gamma V(s_{t+1}) - V(s_t)
   $$
   这表示时间差分（TD）误差，它是通过当前奖励和估计的下一个状态值来调整当前状态的价值。

##### 5.6 最大熵策略（off-policy）

普通 RL 的目标是：
$$
J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}\Big[\sum_{t=0}^\infty \gamma^t r(s_t, a_t)\Big]
$$
SAC 把目标改成「**奖励 + 熵**」：
$$
J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}\Bigg[\sum_{t=0}^\infty 
\gamma^t \Big(r(s_t, a_t) + \alpha H(\pi_\theta(\cdot|s_t))\Big)\Bigg]
$$
其中

- $H(\pi_\theta(\cdot|s_t))$ 是策略在状态 $s_t$ 下的 **熵 entropy**：
   熵越大，说明动作越随机、越分散 → 探索更强
- $\alpha$ 是一个超参数 / 温度参数：
  - α 大：更偏向**高熵**（更随机、更多探索）
  - α 小：更偏向**高回报**（趋向确定性策略）

#### 6. 基于模型的强化学习

##### 6.1 模型

**动态模型**描述了在给定状态和动作的情况下，环境将如何转移到下一个状态，也可以称作 **“状态转移函数”**。它可以表示为一个概率分布：
$$
P(s' \mid s, a)
$$
表示在状态 $s$ 下采取动作 $a$ 后，转移到状态 $s'$ 的概率。

在实际应用中，我们通常使用 **参数化的模型**（如神经网络或高斯过程）来近似这个概率分布。

例如，我们可以用一个神经网络来表示动态模型，输入为当前状态 $s$ 和动作 $a$，输出为下一个状态的预测：
$$
s' = f_\theta(s, a)
$$
其中 $f_\theta$ 表示参数为 $\theta$ 的神经网络。 学习过程的目标是找到合适的 $\theta$，使得模型预测的 $s'$ 尽可能接近真实的下一个状态。

**奖励模型**描述了在给定状态和动作的情况下，智能体将获得多少奖励。 它可以表示为一个函数：
$$
R(s, a)
$$
表示在状态 $s$ 下采取动作 $a$ 后获得的奖励。与动态模型类似，我们也可以使用参数化的模型（如神经网络）来近似奖励函数。

例如，我们可以使用神经网络表示奖励模型，输入为当前状态 $s$ 和动作 $a$，输出为奖励的预测值：
$$
r = r_\phi(s, a)
$$
其中 $r_\phi$ 表示参数为 $\phi$ 的神经网络。 学习过程的目标是找到合适的 $\phi$，使得模型预测的奖励 $r$ 尽可能接近真实奖励。

环境模型的学习通常可以使用 **监督学习** 的方法。通过与环境的交互，我们收集到一系列数据，例如：
$$
(s_t, a_t, r_t, s_{t+1})
$$
然后我们就可以用这些数据训练动态模型和奖励模型。

训练时，将当前状态 $s_t$ 和动作 $a_t$ 作为输入，将下一个状态 $s_{t+1}$ 作为标签，训练模型预测下一状态。

同理，也可以训练奖励模型：将当前状态 $s_t$ 和动作 $a_t$ 作为输入、奖励 $r_t$ 作为标签，训练模型预测奖励。

##### 6.2 MCTS 

每一次“模拟 / rollout”都会经历下面 4 步，在这棵搜索树上走一圈：

1. **Selection 选择**
2. **Expansion 扩展**
3. **Simulation / Rollout 模拟**
4. **Backpropagation 回传**

你可以把它看成在树上不断地“走下去 -> 长出新节点 -> 随机试一试 -> 把结果往上传”。

**Selection（在已有树中向下走）**

从根节点开始，沿着树往下走，每一步选一个“最有前途”的子节点。

怎么判断“有前途”？
 典型做法：**UCB / UCT 公式**（Upper Confidence Bound for Trees）

对于节点 $s$ 的某个动作 $a$，定义：
$$
\text{UCT}(s,a) = Q(s,a) + c \sqrt{\frac{\ln N(s)}{N(s,a) + 1}}
$$

- $Q(s,a)$：当前为止，这个动作的平均回报（ exploitation 利用）
- $N(s)$：节点 $s$ 被访问的次数
- $N(s,a)$：在 $s$ 选 $a$ 的次数
- $c$：探索强度超参数（ exploration 强度）

行为逻辑：

- 前半部分 $Q(s,a)$：选当前看起来“平均成绩高”的动作（利用）
- 后半部分：优先去探索访问次数少的动作（探索）

如此反复，从根往下选到一个“还可以再展开的节点”。

**Expansion（扩展新节点）**

当你走到一个还未完全展开的节点时：

- 从这个状态下选一个“还没出现在树中的动作/后继”
- 把它添加为新的子节点

这一步就是“**让树长出新枝**”。

**Simulation / Rollout（从新节点往后随机构造一条轨迹）**

从新节点开始，你一般不会再用精细的搜索，而是：

- 用一个简单策略 / 随机策略
- 一路往下 roll 到终局（或固定步数）

然后得到一个“模拟结果”（赢/输、得分、reward、是否解出题目等等）。
 这个结果就是一次蒙特卡洛样本。

在大模型推理里，这一步可以变成：

- 让 LLM 继续“自由补全答案”直到结束；
- 或用某种“草率思考”的便宜推理。

**Backpropagation（回传）**

把刚才 simulation 得到的结果，从叶子往根节点**逐层回传**：

对路径上的每个节点、对应的动作：

- 访问次数 $N(\cdot)$ 加 1
- 累积回报更新，用来计算新的 $Q(s,a)$

这样，对“表现一直不错”的分支：

- 平均回报会高，UCT 分数更高 → 下次更常被选择
- 树会在这些区域不断加深、加密

```python
import gym
from gym import spaces
import numpy as np
import copy


class MazeEnv(gym.Env):
    """
    自定义迷宫环境，继承自 gym.Env
    """
    metadata = {'render.modes': ['human']}

    def __init__(self):
        super(MazeEnv, self).__init__()
        # 定义动作空间和状态空间
        # 动作空间：上、下、左、右
        self.action_space = spaces.Discrete(4)
        # 状态空间：智能体在迷宫中的位置（二维坐标）
        self.maze_size = (5, 5)
        self.observation_space = spaces.Box(low=0, high=4, shape=(2,), dtype=np.int32)

        # 定义迷宫（0 表示空地，-1 表示墙壁）
        self.maze = np.zeros(self.maze_size)
        self.maze[0, 3] = -1  # 墙壁位置
        self.maze[1, 1] = -1
        self.maze[1, 3] = -1
        self.maze[2, 1] = -1
        self.maze[3, 3] = -1
        self.maze[4, 1] = -1

        # 起点和终点
        self.start_pos = (0, 0)
        self.goal_pos = (0, 4)

        # 智能体初始位置
        self.agent_pos = self.start_pos

    def step(self, action):
        """
        执行动作
        """
        directions = {
            0: (-1, 0),  # 上
            1: (1, 0),   # 下
            2: (0, -1),  # 左
            3: (0, 1)    # 右
        }
        # 根据动作计算新的位置
        move = directions[action]
        new_pos = (self.agent_pos[0] + move[0], self.agent_pos[1] + move[1])

        # 默认奖励和结束标志
        reward = -1
        done = False

        # 检查新位置是否在迷宫范围内
        if (0 <= new_pos[0] < self.maze_size[0]) and (0 <= new_pos[1] < self.maze_size[1]):
            # 检查新位置是否是墙壁
            if self.maze[new_pos] == -1:
                # 撞到墙壁
                reward = -5
            else:
                # 合法移动，更新位置
                self.agent_pos = new_pos
        else:
            # 越界
            reward = -5

        # 检查是否到达终点
        if self.agent_pos == self.goal_pos:
            reward = 10
            done = True

        obs = np.array(self.agent_pos)
        info = {}
        return obs, reward, done, info

    def reset(self):
        """
        重置环境到初始状态
        """
        self.agent_pos = self.start_pos
        return np.array(self.agent_pos)

    def render(self, mode='human'):
        """
        渲染迷宫环境
        """
        maze_render = np.copy(self.maze)
        maze_render[self.agent_pos] = 2  # 智能体的位置
        maze_render[self.start_pos] = 3  # 起点
        maze_render[self.goal_pos] = 4   # 终点

        symbol_map = {
            -1: 'W',  # 墙壁
            0: ' ',   # 空地
            2: 'A',   # 智能体
            3: 'S',   # 起点
            4: 'G'    # 终点
        }
        print("\n".join(["".join([symbol_map[item] for item in row]) for row in maze_render]))
        print("\n")


class MCTSNode:
    """
    MCTS搜索树上的节点
    """
    def __init__(self, state, parent=None):
        """
        state: 表示智能体在迷宫中的 (row, col) 坐标
        parent: 指向父节点
        """
        self.state = state
        self.parent = parent
        self.children = {}     # action -> MCTSNode
        self.visits = 0        # 该节点被访问次数
        self.value_sum = 0.0   # 该节点从后续模拟中获得的累积价值总和

    def is_leaf(self):
        return len(self.children) == 0

    @property
    def q_value(self):
        """
        节点的平均价值
        """
        if self.visits == 0:
            return 0.0
        return self.value_sum / self.visits


class MCTS:
    """
    基本的蒙特卡洛树搜索，使用UCB作为选择策略。
    """
    def __init__(self, env, ucb_c=1.4, rollout_limit=20):
        """
        env: 迷宫环境
        ucb_c: UCB公式中的探索常数
        rollout_limit: 在模拟(rollout)时的最大步数限制
        """
        self.env = env
        self.ucb_c = ucb_c
        self.rollout_limit = rollout_limit

    def search(self, root_state, n_simulations=100):
        """
        在给定状态下做多次模拟搜索，返回在该状态下最优动作。
        root_state: (row, col)
        n_simulations: MCTS迭代次数
        """
        # 创建根节点
        self.root = MCTSNode(root_state)

        # 重复进行MCTS模拟
        for _ in range(n_simulations):
            node = self._select(self.root)
            reward = self._simulate(node)
            self._backpropagate(node, reward)

        # 在根节点选择访问次数最多的动作
        best_action, best_child = None, None
        max_visits = -1
        for action, child in self.root.children.items():
            if child.visits > max_visits:
                max_visits = child.visits
                best_action = action
                best_child = child
        return best_action

    def _select(self, node):
        """
        选择节点，使用UCB公式在树中向下寻找可以扩展的节点。
        如果节点是叶子，则在此进行扩展（创建子节点）。
        """
        while True:
            if node.is_leaf():
                # 节点尚未扩展就进行扩展
                if node.visits == 0:
                    # 如果该节点从未被访问过，直接返回用来进行模拟
                    return node
                else:
                    # 否则进行扩展
                    self._expand(node)
                    return node
            else:
                # 若不是叶子节点，则基于UCB选择最优子节点
                node = self._ucb_select(node)
                # 如果某个子节点还没被真正模拟过，可直接返回
                if node.visits == 0:
                    return node

    def _expand(self, node):
        """
        对节点node进行扩展。对其可能的所有动作都创建子节点。
        """
        row, col = node.state
        for action in range(self.env.action_space.n):
            # 对每个action都做一次假设，看看能走到哪里
            next_state, reward, done = self._transition(node.state, action)
            # 只要下一个状态有效（不一定非得是非墙，一般撞墙也可以算一种状态，只是reward不同）
            # 这里可以自行判断是否要跳过撞墙/越界的动作
            if True:  
                child_node = MCTSNode(next_state, parent=node)
                node.children[action] = child_node

    def _simulate(self, node):
        """
        从给定节点的状态开始，执行随机策略进行rollout，返回累积奖励。
        为了让示例简单，我们不在树中进行深层次搜索，而是随机走 rollout_limit 步或直到done。
        """
        total_reward = 0.0
        current_state = node.state
        env_copy = copy.deepcopy(self.env)
        env_copy.agent_pos = current_state  # 在复制的环境中把智能体位置设在node对应的状态

        for _ in range(self.rollout_limit):
            # 如果已经到达终点，则停止
            if (env_copy.agent_pos == env_copy.goal_pos):
                total_reward += 10  # 终点奖励
                break

            action = env_copy.action_space.sample()  # 随机动作
            obs, reward, done, _ = env_copy.step(action)
            total_reward += reward
            if done:
                break
        return total_reward

    def _backpropagate(self, node, reward):
        """
        将模拟的返回奖励回传到根节点
        """
        while node is not None:
            node.visits += 1
            node.value_sum += reward
            node = node.parent

    def _ucb_select(self, node):
        """
        在node的所有子节点中使用UCB准则选择一个节点
        UCB = Q + c * sqrt( ln(parent_visits) / (1+child_visits) )
        """
        best_score = -float('inf')
        best_child = None

        for action, child in node.children.items():
            # 如果还没访问过，那么 UCB 公式中 visits=0，需要做一个平滑
            q_value = child.q_value
            ucb_explore = self.ucb_c * np.sqrt(np.log(node.visits + 1) / (child.visits + 1e-8))
            ucb_score = q_value + ucb_explore
            if ucb_score > best_score:
                best_score = ucb_score
                best_child = child
        return best_child

    def _transition(self, state, action):
        """
        对于给定状态和动作，在已知的迷宫模型中计算下一状态和奖励、是否结束。
        因为我们是model-based，所以可以直接复用环境的逻辑。
        """
        row, col = state
        env_copy = copy.deepcopy(self.env)
        env_copy.agent_pos = (row, col)
        obs, reward, done, _ = env_copy.step(action)
        next_state = tuple(obs)
        return next_state, reward, done


def main():
    env = MazeEnv()
    mcts_planner = MCTS(env, ucb_c=1.0, rollout_limit=10)
    
    obs = env.reset()
    done = False
    step_count = 0
    
    while not done and step_count < 50:  # 最多走50步
        env.render()
        current_state = tuple(obs)
        
        # 进行若干次MCTS模拟，得到最优动作
        best_action = mcts_planner.search(root_state=current_state, n_simulations=50)
        
        # 在真实环境中执行该动作
        obs, reward, done, info = env.step(best_action)
        step_count += 1

    env.render()
    if env.agent_pos == env.goal_pos:
        print("成功到达终点，用时步数:", step_count)
    else:
        print("未能在限制步数内到达终点。")


if __name__ == "__main__":
    main()
```

#### 7. RL for LLM 

##### 7.1 RLHF-PPO

如上图，**在RLHF-PPO阶段，一共有四个主要模型**，分别是：

- **Policy Model (策略模型，Actor)**，这就是我们想要训练的目标语言模型，用 $SFT$ 阶段产生的模型对它进行初始化
- **Value Model (价值模型，Critic)**，它的作用是预估总收益，从reward model初始化而来
- **Reward Model：奖励模型**，它的作用是计算即时收益 
- **Reference Model：参考模型**，它的作用是在RLHF阶段给语言模型增加一些“约束”，防止语言模型训歪（朝不受控制的方向更新，效果可能越来越差），也是用SFT阶段模型初始化，在训练过程中参数是冻结的

对Actor模型，我们喂给它一个prompt，它正常输出对应的response。那么response中每一个token肯定有它对应的log_prob结果，我们把这样的结果记为**log_probs**；对Ref模型，我们把Actor生成的"prompt + response"喂给它，那么它同样能给出每个token的log_prob结果，我们记其为**ref_log_probs**；那么这两个模型的输出分布相似度就可以用**`ref_log_probs - log_probs`**来衡量

> 可能会奇怪为什么参考模型输入是 prompt + response ，而不是 prompt 然后自己输出相应的 token 的概率，因为这些参考模型，奖励模型，评价模型考虑的是通过 策略模型 的输出来进行评价，比如我喜欢吃苹果，prompt是我喜欢，然后response是吃苹果，而如果参考模型得到我喜欢，输出第一个是喝，那就已经偏离了，我们无法评价输出效果如何，应该是看一下输出吃的概率，然后利用我喜欢吃继续，而不是用这个模型的最大概率继续

![image-20250722153320724](assets/image-20250722153320724.png)

**损失函数计算：**

（1）直观设计

- Actor 接收到当前上文 $S_t$，产出 token $A_t$ （$P(A_t \mid S_t)$）
- Critic 根据 $S_t, A_t$，产出对总收益的预测 $V_t$
- 那么 Actor loss 可以设计为：

$$
actor\_loss = -\sum_{t \in response\_timestep} V_t \log P(A_t \mid S_t)
$$

求和符号表示我们只考虑 response 部分所有 token 的 loss，为了表达简便，我们先把这个求和符号略去（下文也是同理），也就是说：

$$
actor\_loss = -V_t \log P(A_t \mid S_t)
$$

我们希望 minimize 这个 actor\_loss。直观理解就是当 $V_t$ 大于 0 时，意味着 Critic 对于 Actor 给予了正反馈，那我们就需要在训练中提高该 token 的输出概率

（2）引入优势（Advantage）

对 NLP 任务来说，如果 Critic 对 $A_t$ 的总收益预测为 $V_t$，但实际执行 $A_t$ 后的总收益是  $R_t + \gamma * V_{t+1}$，我们就定义优势为：

$$
Adv_t = R_t + \gamma * V_{t+1} - V_t
$$

我们用 $Adv_t$ 替换掉 $V_t$，则此刻 actor\_loss 变为：

$$
actor\_loss = -Adv_t \log P(A_t \mid S_t)
$$
$R_t$ 应该表示每个Actor产出token 带来的即时收益，其中 T 表示最后一个时刻

![image-20251110123106732](assets/image-20251110123106732.png)

奖励函数我们采用下面的设计
$$
R_t = -kl\_ctl * \left( \log \frac{P(A_t|S_t)}{P_{ref}(A_t|S_t)} \right), \quad t \neq T
$$

$$
R_t = -kl\_ctl * \left( \log \frac{P(A_t|S_t)}{P_{ref}(A_t|S_t)} \right) + R_t, \quad t = T
$$

- **kl_ctl**: 常量，可以理解成一个控制比值的缩放因子，在 deepspeed-chat 中默认值设为 0.1。
- **$$ - \log \frac{P(A_t|S_t)}{P_{ref}(A_t|S_t)} $$**: 这一项你是非常熟悉的，就是 Actor 和 Ref 模型间的 KL 散度，写成更容易理解的形式，就是 **ref_log_probs - log_probs**。在 3.2 中我们说过，为了防止模型过拟合，我们需要把这个 KL 散度加入 loss 计算中，所以这里我们就做这件事。

我们可以看出当 $t \ne T$ 时，更关系 Actor 是否在 Ref 约束下生成 token，而最后一个时刻，我们还要考虑即时收益

这是因为在Reward模型训练阶段，就是用这个位置的 来表示对完整的prompt + response的奖励预测（但不妨碍你理解成是执行完 的即时奖励），然后用这个指标来做模型eval的（但是Reward训练阶段算loss时，还是考虑了response部分所有token输出的reward值）。所以到了RLHF的场景下，其余时刻的即时奖励，我们就用“Actor是否遵循了Ref的约束”来进行评价

我们可以继续加入未来收益作为考量（GAE广义优势估计）
$$
Adv_t = (R_t + \gamma  V_{t+1} - V_t) + \gamma \lambda  Adv_{t+1}
$$
注意到，对于最后一个时刻 ，它的未来收益和未来优势都是0，也就是 ，这是可以直接算出来的。所以可以用动态规划反推 

![image-20250723144537032](assets/image-20250723144537032.png)

- 第一步，我们准备一个batch的prompts
- 第二步，我们将这个batch的prompts喂给Actor模型，让它生成对应的responses
- 第三步，我们把prompt+responses喂给我们的Critic/Reward/Reference模型，让它生成用于计算actor/critic loss的数据，按照强化学习的术语，我们称这些数据为经验（experiences）。critic loss我们将在后文做详细讲解，目前我们只把目光聚焦到actor loss上
- 第四步，我们根据这些经验，实际计算出actor/critic loss，然后更新Actor和Critic模型

我们可能会发现一个 batch 的经验被用来 n 次更新，因为在强化学习中收集一个batch非常耗时

我们假设最开始吃batch，吐出经验的actor叫 $Actor_{old}$ ，而在伪代码中，每次做完ppo_epochs而更新的actor叫 $Actor_{new}$ ，那么我们只要尽量保证每次更新后的 $Actor_{new}$ 能模仿最开始的那个 $Actor_{old}$
$$
actor\_loss = -Adv_tlog\frac{P(A_t|S_t)}{P_{old}(A_t|S_t)}
$$
所以这个公式从直觉上也可以理解成：在Actor想通过模拟交互的方式，使用一个batch的经验值更新自己时，它需要收到真正吃到batch的那个时刻的Actor的约束，这样才能在有效利用batch，提升训练速度的基础上，保持训练的稳定。

但是如果 $Actor_{old}$ 约束能力不够，可以通过裁剪（clip）解决

我们给 $\frac{P(A_t|S_t)}{P_{old}(A_t|S_t)}$ 设置一个范围，例如 $(0.8, 1.2)$

对于 Critic loss，公式为 $Critic\_loss = (R_t + \gamma \times V_{t + 1} - V_{t})^2$

```python
import gym
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import random
from collections import deque

# 设置随机种子，便于复现
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)


# ------------------------------------------------------------------------------
# 1. 改进后的迷宫环境：调整每步的惩罚以减小负奖励
# ------------------------------------------------------------------------------
class MazeEnv(gym.Env):
    """
    自定义迷宫环境，继承自 gym.Env
    """
    metadata = {'render.modes': ['human']}

    def __init__(self):
        super(MazeEnv, self).__init__()
        # 定义动作空间和状态空间
        self.action_space = gym.spaces.Discrete(4)  # 上、下、左、右
        self.maze_size = (5, 5)
        self.observation_space = gym.spaces.Box(
            low=0, high=4, shape=(2,), dtype=np.int32
        )

        # 定义迷宫（0 表示空地，-1 表示墙壁）
        self.maze = np.zeros(self.maze_size)
        self.maze[0, 3] = -1
        self.maze[1, 1] = -1
        self.maze[1, 3] = -1
        self.maze[2, 1] = -1
        self.maze[3, 3] = -1
        self.maze[4, 1] = -1

        # 起点和终点
        self.start_pos = (0, 0)
        self.goal_pos = (0, 4)
        # 智能体初始位置
        self.agent_pos = self.start_pos

    def step(self, action):
        # 定义动作对应的移动
        directions = {
            0: (-1, 0),  # 上
            1: (1, 0),   # 下
            2: (0, -1),  # 左
            3: (0, 1)    # 右
        }
        move = directions[action]
        new_pos = (self.agent_pos[0] + move[0], self.agent_pos[1] + move[1])

        # 调整后的奖励/惩罚
        step_penalty = -0.1    # 每步行动的负奖励
        wall_penalty = -1      # 撞墙或越界惩罚
        goal_reward = 10       # 到达终点

        reward = step_penalty
        done = False

        # 检查新位置是否在迷宫范围内
        if (0 <= new_pos[0] < self.maze_size[0]) and (0 <= new_pos[1] < self.maze_size[1]):
            # 检查新位置是否是墙壁
            if self.maze[new_pos] == -1:
                # 撞到墙壁
                reward += wall_penalty
            else:
                self.agent_pos = new_pos  # 更新位置
        else:
            # 越界
            reward += wall_penalty

        # 是否到达终点
        if self.agent_pos == self.goal_pos:
            reward += goal_reward
            done = True

        obs = np.array(self.agent_pos)
        info = {}
        return obs, reward, done, info

    def reset(self):
        self.agent_pos = self.start_pos
        return np.array(self.agent_pos)

    def render(self, mode='human'):
        maze_render = np.copy(self.maze)
        maze_render[self.agent_pos] = 2  # 智能体
        maze_render[self.start_pos] = 3  # 起点
        maze_render[self.goal_pos] = 4   # 终点

        symbol_map = {
            -1: 'W',  # 墙壁
            0: ' ',   # 空地
            2: 'A',   # 智能体
            3: 'S',   # 起点
            4: 'G'    # 终点
        }
        print("\n".join(["".join([symbol_map[item] for item in row]) for row in maze_render]))
        print("\n")


# ------------------------------------------------------------------------------
# 2. ActorCritic 模型
# ------------------------------------------------------------------------------
class ActorCritic(nn.Module):
    def __init__(self, state_dim=2, action_dim=4, hidden_dim=64):
        super(ActorCritic, self).__init__()

        # 公共特征提取层
        self.base = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )

        # Actor 分支：输出对每个动作的 logits
        self.actor = nn.Linear(hidden_dim, action_dim)

        # Critic 分支：输出状态价值 V(s)
        self.critic = nn.Linear(hidden_dim, 1)

    def forward(self, x):
        features = self.base(x)
        logits = self.actor(features)
        state_value = self.critic(features)
        return logits, state_value


# ------------------------------------------------------------------------------
# 3. Rollout Buffer
# ------------------------------------------------------------------------------
class RolloutBuffer:
    def __init__(self):
        self.states = []
        self.actions = []
        self.log_probs = []
        self.rewards = []
        self.is_done = []
        self.values = []

    def clear(self):
        self.states.clear()
        self.actions.clear()
        self.log_probs.clear()
        self.rewards.clear()
        self.is_done.clear()
        self.values.clear()

    def add(self, state, action, log_prob, reward, done, value):
        self.states.append(state)
        self.actions.append(action)
        self.log_probs.append(log_prob)
        self.rewards.append(reward)
        self.is_done.append(done)
        self.values.append(value)

    def get_size(self):
        return len(self.states)


# ------------------------------------------------------------------------------
# 4. PPO 核心
# ------------------------------------------------------------------------------
class PPOTrainer:
    def __init__(
        self,
        state_dim=2,
        action_dim=4,
        hidden_dim=64,
        gamma=0.99,
        lr=3e-4,
        clip_eps=0.2,
        update_epochs=5,
        lmbda=0.95,
        vf_coef=0.5,
        ent_coef=0.02  # 略微增大，促进探索
    ):
        self.gamma = gamma
        self.clip_eps = clip_eps
        self.update_epochs = update_epochs
        self.lmbda = lmbda
        self.vf_coef = vf_coef
        self.ent_coef = ent_coef

        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.ac = ActorCritic(state_dim, action_dim, hidden_dim).to(self.device)
        self.optimizer = optim.Adam(self.ac.parameters(), lr=lr)

    def select_action(self, state):
        """
        输入单个state (numpy array)，输出一个action，以及log_prob等信息
        """
        state_t = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        logits, value = self.ac(state_t)
        dist = torch.distributions.Categorical(logits=logits)
        action = dist.sample()
        log_prob = dist.log_prob(action)
        return action.item(), log_prob.item(), value.item()

    def get_value(self, state):
        """
        给定一个状态，返回 Critic 估计的价值
        """
        state_t = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        with torch.no_grad():
            _, value = self.ac(state_t)
        return value.item()

    def compute_gae(self, rewards, values, dones, final_value):
        """
        使用 GAE-lambda 计算优势和回报
        如果最后状态没 done，就用 final_value 作为 bootstrap
        """
        advantages = np.zeros_like(rewards, dtype=np.float32)
        returns = np.zeros_like(rewards, dtype=np.float32)
        gae = 0.0

        values = np.append(values, [final_value])

        for t in reversed(range(len(rewards))):
            delta = rewards[t] + self.gamma * values[t + 1] * (1 - dones[t]) - values[t]
            gae = delta + self.gamma * self.lmbda * (1 - dones[t]) * gae
            advantages[t] = gae
            returns[t] = gae + values[t]
        return advantages, returns

    def update(self, buffer: RolloutBuffer, final_value):
        states = torch.FloatTensor(buffer.states).to(self.device)
        actions = torch.LongTensor(buffer.actions).to(self.device)
        old_log_probs = torch.FloatTensor(buffer.log_probs).to(self.device)
        rewards = np.array(buffer.rewards, dtype=np.float32)
        dones = np.array(buffer.is_done, dtype=np.float32)
        values = np.array(buffer.values, dtype=np.float32)

        # 计算 GAE
        advantages, returns = self.compute_gae(rewards, values, dones, final_value)
        advantages = torch.FloatTensor(advantages).to(self.device)
        returns = torch.FloatTensor(returns).to(self.device)

        for _ in range(self.update_epochs):
            logits, value_pred = self.ac(states)
            dist = torch.distributions.Categorical(logits=logits)
            new_log_probs = dist.log_prob(actions)
            entropy = dist.entropy().mean()

            # 计算比率 ratio
            ratio = torch.exp(new_log_probs - old_log_probs)

            # PPO Clip
            surr1 = ratio * advantages
            surr2 = torch.clamp(ratio, 1.0 - self.clip_eps, 1.0 + self.clip_eps) * advantages
            policy_loss = -torch.min(surr1, surr2).mean()

            # Critic loss
            value_pred = value_pred.squeeze(-1)
            value_loss = nn.MSELoss()(value_pred, returns)

            loss = policy_loss + self.vf_coef * value_loss - self.ent_coef * entropy

            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()


def train_ppo(num_episodes=600, max_steps_per_episode=100):
    """
    使用 PPO 训练迷宫环境：
    - 增加训练回合数
    - 每回合限制 100 步
    """
    env = MazeEnv()

    trainer = PPOTrainer(
        state_dim=2,
        action_dim=4,
        hidden_dim=64,
        gamma=0.99,
        lr=3e-4,
        clip_eps=0.2,
        update_epochs=5,
        lmbda=0.95,
        vf_coef=0.5,
        ent_coef=0.02  # 提高熵系数，引导更多探索
    )

    rollout_buffer = RolloutBuffer()
    all_rewards = []

    for episode in range(num_episodes):
        state = env.reset()
        episode_reward = 0
        rollout_buffer.clear()

        for step in range(max_steps_per_episode):
            action, log_prob, value = trainer.select_action(state)
            next_state, reward, done, _ = env.step(action)

            rollout_buffer.add(
                state=state,
                action=action,
                log_prob=log_prob,
                reward=reward,
                done=float(done),
                value=value
            )
            state = next_state
            episode_reward += reward

            if done:
                break

        # 如果回合中途没有 done，就从 Critic 得到 bootstrap 价值
        if done:
            final_value = 0.0
        else:
            final_value = trainer.get_value(state)

        # 用 rollouts 中的数据更新网络
        trainer.update(rollout_buffer, final_value)
        all_rewards.append(episode_reward)

        # 打印训练信息
        if (episode+1) % 10 == 0:
            avg_rew = np.mean(all_rewards[-10:])
            print(f"Episode {episode+1}/{num_episodes} | Reward: {episode_reward:.2f} | Avg10: {avg_rew:.2f}")

    # 训练结束后，保存模型参数
    torch.save(trainer.ac.state_dict(), "ppo_actor_critic_fixed.pth")
    return all_rewards


# ------------------------------------------------------------------------------
# 测试：加载训练好的模型，在迷宫中走若干回合并渲染
# ------------------------------------------------------------------------------
def test_ppo(model_path="ppo_actor_critic_fixed.pth", num_episodes=3):
    env = MazeEnv()

    state_dim = 2
    action_dim = 4
    ac = ActorCritic(state_dim=state_dim, action_dim=action_dim)
    ac.load_state_dict(torch.load(model_path))
    ac.eval()

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    ac.to(device)

    for epi in range(num_episodes):
        state = env.reset()
        done = False
        episode_reward = 0

        print(f"===== 测试 Episode {epi+1} =====")
        while not done:
            env.render()

            state_t = torch.FloatTensor(state).unsqueeze(0).to(device)
            with torch.no_grad():
                logits, critic_value = ac(state_t)
                dist = torch.distributions.Categorical(logits=logits)
                action = dist.sample().item()

            next_state, reward, done, _ = env.step(action)
            state = next_state
            episode_reward += reward

        env.render()
        print(f"Episode {epi+1} 结束，回合总奖励: {episode_reward}\n")


if __name__ == "__main__":
    # 1. 训练
    rewards = train_ppo(num_episodes=600, max_steps_per_episode=100)

    # 2. 测试
    test_ppo("ppo_actor_critic_fixed.pth", num_episodes=3)
```

##### 7.2 DPO

直接将人类的偏好对用于模型的训练，以达到最小的损失满足，属于离线策略，DPO可以通过人类偏好数据，用二元交叉熵对策略进行优化，而不需要多次进行在线数据采样进行优化。其中，$y_w$ 为偏好数据，$y_l$ 为非偏好数据。公式设计的目标就是最大化偏好数据
$$
\mathcal{L}_{DPO}(\pi_{\theta}; \pi_{\text{ref}}) = -\mathbb{E}_{(x, y_w, y_l) \sim D} \left[ \log \sigma \left( \beta \log \frac{\pi_{\theta}(y_w | x)}{\pi_{\text{ref}}(y_w | x)} - \beta \log \frac{\pi_{\theta}(y_l | x)}{\pi_{\text{ref}}(y_l | x)} \right) \right]
$$
![图1：RLHF和DPO方法的比较](assets/img-0.jpeg)

##### 7.3 GRPO

步骤：1.生成补全：对于一个提示词生成G个回复；2.计算优势值：对于每一个G序列，使用奖励模型计算奖励，通常奖励模型是基于同一问题的输出之间的比较数据集进行训练的——优势的计算反映了这些相对 比较，$A_{i,t} = \frac{r_i-mean(r)}{std(r)}$ ；3.估算KL散度：$
D_{\text{KL}}[\pi \theta \parallel \pi_{\text{ref}}] = \frac{\pi_{\text{ref}}(\omega_i | q, O_i, < t)}{\pi_\theta(\omega_i | q, O_i, < t)} - \log \frac{\pi_{\text{ref}}(\omega_i | q, O_i, < t)}{\pi_\theta(\omega_i | q, O_i, < t)} - 1
$ ； 4. 计算损失
$$
\mathcal{L}_{\text{GRPO}}(\theta) = -\frac{1}{G} \sum_{i=1}^G \frac{1}{|\omega_i|} \sum_{t=1}^{|\omega_i|} \left[ \min \left( \frac{\pi_\theta(\omega_{i,t} | q, O_i, < t)}{\pi_{\theta_{\text{old}}}(\omega_{i,t} | q, O_i, < t)}, 1 - \epsilon, 1 + \epsilon \right) \hat{A}_{i,t} - \beta D_{\text{KL}}[\pi_\theta \parallel \pi_{\text{ref}}] \right]
$$
![img](assets/v2-6e7e78165b1af96e810c84cf82fc7a94_720w.webp)

GRPO通过优化PPO算法，解决了计算优势值时需要同时依赖奖励模型（reward model）和价值模型（value model）的问题，成功移除了value model（价值模型），显著降低了推理时的内存占用和时间开销。**Advantage（优势值）**的核心价值在于为模型输出提供更精准的评估，不仅衡量答案的绝对质量，还通过相对比较（与其他回答的对比）来更全面地定位其优劣。

![img](assets/v2-c1ae49558e634c81987d370fc78a5cbb_r.jpg)

#### 8. 模仿学习

如果没有奖励信号，那么可以通过模仿学习，核心思想就是观察专家的行为，学习一个策略，能够像专家一样完成任务

##### 8.1 行为克隆 (Behavioral Cloning, BC)

行为克隆（Behavioral Cloning, BC）是最简单、最直接的模仿学习方法。它的核心思想是将模仿学习视为一个监督学习问题，即：给定一系列专家演示数据，学习一个从状态到动作的映射。

假设我们有一组专家演示数据 \( D = \{(s_i, a_i)\}_{i=1}^N \)，其中 \( s_i \) 表示状态，\( a_i \) 表示专家在状态 \( s_i \) 下采取的动作。行为克隆的目标是学习一个策略 \( \pi(a|s) \)，使得给定状态 \(s\) 时，策略能够预测出与专家相近的动作。

行为克隆的目标是最小化损失函数：

$$
L(\theta) = \mathbb{E}_{(s, a) \sim D} \left[ l(\pi(a | s), a) \right]
$$

其中，\( l(\cdot, \cdot) \) 表示一个合适的损失函数，例如交叉熵损失（Cross-Entropy Loss）或均方误差损失（Mean Squared Error Loss）。

行为克隆的原理非常简单，可以利用现有的监督学习算法，易于理解和实现。但它存在一些问题，例如：

- **没有考虑到环境的动态性**：行为克隆将每个状态-动作对视为独立的样本，忽略了模仿学习中状态转移的关系，可能会导致环境状态的分布发生偏移。
- **泛化能力差**：行为克隆容易出现过拟合问题，难以泛化到新的状态或动作。
- **协变量偏移 (Covariate Shift)**：训练数据来自专家策略，而实际运行时中智能体可能遇到到不同的状态分布，导致性能下降。

解决协变量偏移的方法

- **数据增量 (Data Augmentation)**：通过对专家演示数据进行变换，增加数据的多样性，从而提升模型的泛化能力。例如，可以在天气的变换中加入一些雨天、雾天等状态。
- **Dagger 算法 (Dataset Aggregation)**：Dagger 算法是一种交互式的学习方法，通过不断从环境中收集新的数据，并将其加入到训练集中，从而避免协变量偏移的问题。
- **具有交互式学习算法**：除了 Dagger 算法之外，还有许多其他的交互式学习方法，例如策略迭代等，均能够缓解协变量偏移的问题。

### 三、LLM 基础

#### 1. 模型架构

![image-20251108204949938](assets/image-20251108204949938.png)

1. **编码器 (Encoder)** ：任务是“**理解**”输入的整个句子。它会读取所有输入词元(这个概念会在3.2.2节介绍)，最终为每个词元生成一个富含上下文信息的向量表示。
2. **解码器 (Decoder)** ：任务是“**生成**”目标句子。它会参考自己已经生成的前文，并“咨询”编码器的理解结果，来生成下一个词。

```python
import torch
import torch.nn as nn
import math

# --- 占位符模块，将在后续小节中实现 ---

class PositionalEncoding(nn.Module):
    """
    位置编码模块
    """
    def forward(self, x):
        pass

class MultiHeadAttention(nn.Module):
    """
    多头注意力机制模块
    """
    def forward(self, query, key, value, mask):
        pass

class PositionWiseFeedForward(nn.Module):
    """
    位置前馈网络模块
    """
    def forward(self, x):
        pass

# --- 编码器核心层 ---

class EncoderLayer(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout):
        super(EncoderLayer, self).__init__()
        self.self_attn = MultiHeadAttention() # 待实现
        self.feed_forward = PositionWiseFeedForward() # 待实现
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, mask):
        # 残差连接与层归一化将在 3.1.2.4 节中详细解释
        # 1. 多头自注意力
        attn_output = self.self_attn(x, x, x, mask)
        x = self.norm1(x + self.dropout(attn_output))

        # 2. 前馈网络
        ff_output = self.feed_forward(x)
        x = self.norm2(x + self.dropout(ff_output))

        return x

# --- 解码器核心层 ---

class DecoderLayer(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout):
        super(DecoderLayer, self).__init__()
        self.self_attn = MultiHeadAttention() # 待实现
        self.cross_attn = MultiHeadAttention() # 待实现
        self.feed_forward = PositionWiseFeedForward() # 待实现
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, encoder_output, src_mask, tgt_mask):
        # 1. 掩码多头自注意力 (对自己)
        attn_output = self.self_attn(x, x, x, tgt_mask)
        x = self.norm1(x + self.dropout(attn_output))

        # 2. 交叉注意力 (对编码器输出)
        cross_attn_output = self.cross_attn(x, encoder_output, encoder_output, src_mask)
        x = self.norm2(x + self.dropout(cross_attn_output))

        # 3. 前馈网络
        ff_output = self.feed_forward(x)
        x = self.norm3(x + self.dropout(ff_output))

        return x
```

现在，我们来填充骨架中最关键的模块，注意力机制。

想象一下我们阅读这个句子：“The agent learns because **it** is intelligent.”。当我们读到加粗的 "**it**" 时，为了理解它的指代，我们的大脑会不自觉地将更多的注意力放在前面的 "agent" 这个词上。**自注意力 (Self-Attention)** 机制就是对这种现象的数学建模。它允许模型在处理序列中的每一个词时，都能兼顾句子中的所有其他词，并为这些词分配不同的“注意力权重”。权重越高的词，代表其与当前词的关联性越强，其信息也应该在当前词的表示中占据更大的比重。

为了实现上述过程，自注意力机制为每个输入的词元向量引入了三个可学习的角色：

- **查询 (Query, Q)**：代表当前词元，它正在主动地“查询”其他词元以获取信息。
- **键 (Key, K)**：代表句子中可被查询的词元“标签”或“索引”。
- **值 (Value, V)**：代表词元本身所携带的“内容”或“信息”。

这三个向量都是由原始的词嵌入向量乘以三个不同的、可学习的权重矩阵 ($W^Q,W^K,W^V$) 得到的。整个计算过程可以分为以下几步，我们可以把它想象成一次高效的开卷考试：

- 准备“考题”和“资料”：对于句子中的每个词，都通过权重矩阵生成其 $Q,K,V$ 向量。
- 计算相关性得分：要计算词 $A$ 的新表示，就用词 $A$ 的 $Q$ 向量，去和句子中所有词（包括 $A$ 自己）的 $K$ 向量进行点积运算。这个得分反映了其他词对于理解词 $A$ 的重要性。
- 稳定化与归一化：将得到的所有分数除以一个缩放因子$\sqrt{d_k}$），以防止梯度过小，然后用Softmax函数将分数转换成总和为1的权重，也就是归一化的过程。
- 加权求和：将上一步得到的权重分别乘以每个词对应的V*V*向量，然后将所有结果相加。最终得到的向量，就是词 $A$ 融合了全局上下文信息后的新表示。

这个过程可以用一个简洁的公式来概括：
$$
Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V
$$


如果只进行一次上述的注意力计算（即单头），模型可能会只学会关注一种类型的关联。比如，在处理 "it" 时，可能只学会了关注主语。但语言中的关系是复杂的，我们希望模型能同时关注多种关系（如指代关系、时态关系、从属关系等）。多头注意力机制应运而生。它的思想很简单：把一次做完变成分成几组，分开做，再合并。

它将原始的 Q, K, V 向量在维度上切分成 h 份（h 就是“头”数），每一份都独立地进行一次单头注意力的计算。这就好比让 h 个不同的“专家”从不同的角度去审视句子，每个专家都能捕捉到一种不同的特征关系。最后，将这 h 个专家的“意见”（即输出向量）拼接起来，再通过一个线性变换进行整合，就得到了最终的输出。

```python
class MultiHeadAttention(nn.Module):
    """
    多头注意力机制模块
    """
    def __init__(self, d_model, num_heads):
        super(MultiHeadAttention, self).__init__()
        assert d_model % num_heads == 0, "d_model 必须能被 num_heads 整除"

        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads

        # 定义 Q, K, V 和输出的线性变换层
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)

    def scaled_dot_product_attention(self, Q, K, V, mask=None):
        # 1. 计算注意力得分 (QK^T)
        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)

        # 2. 应用掩码 (如果提供)
        if mask is not None:
            # 将掩码中为 0 的位置设置为一个非常小的负数，这样 softmax 后会接近 0
            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)

        # 3. 计算注意力权重 (Softmax)
        attn_probs = torch.softmax(attn_scores, dim=-1)

        # 4. 加权求和 (权重 * V)
        output = torch.matmul(attn_probs, V)
        return output

    def split_heads(self, x):
        # 将输入 x 的形状从 (batch_size, seq_length, d_model)
        # 变换为 (batch_size, num_heads, seq_length, d_k)
        batch_size, seq_length, d_model = x.size()
        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)

    def combine_heads(self, x):
        # 将输入 x 的形状从 (batch_size, num_heads, seq_length, d_k)
        # 变回 (batch_size, seq_length, d_model)
        batch_size, num_heads, seq_length, d_k = x.size()
        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)

    def forward(self, Q, K, V, mask=None):
        # 1. 对 Q, K, V 进行线性变换
        Q = self.split_heads(self.W_q(Q))
        K = self.split_heads(self.W_k(K))
        V = self.split_heads(self.W_v(V))

        # 2. 计算缩放点积注意力
        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)

        # 3. 合并多头输出并进行最终的线性变换
        output = self.W_o(self.combine_heads(attn_output))
        return output

```

在每个 Encoder 和 Decoder 层中，多头注意力子层之后都跟着一个**逐位置前馈网络(Position-wise Feed-Forward Network, FFN)** 。如果说注意力层的作用是从整个序列中“动态地聚合”相关信息，那么前馈网络的作用从这些聚合后的信息中提取更高阶的特征。

这个名字的关键在于“逐位置”。它意味着这个前馈网络会独立地作用于序列中的每一个词元向量。换句话说，对于一个长度为 `seq_len` 的序列，这个 FFN 实际上会被调用 `seq_len` 次，每次处理一个词元。重要的是，所有位置共享的是同一组网络权重。这种设计既保持了对每个位置进行独立加工的能力，又大大减少了模型的参数量。这个网络的结构非常简单，由两个线性变换和一个 ReLU 激活函数组成：
$$
FFN(x) = max(0, xW_1+b_1)W_2+b_2
$$
其中，$x$ 是注意力子层的输出。$W_1,b_1,W_2,b_2$ 是可学习的参数。通常，第一个线性层的输出维度 `d_ff` 会远大于输入的维度 `d_model`（例如 `d_ff = 4 * d_model`），经过 ReLU 激活后再通过第二个线性层映射回 `d_model` 维度。这种“先扩大再缩小”的模式，也被称为瓶颈结构，被认为有助于模型学习更丰富的特征表示。

```python
class PositionWiseFeedForward(nn.Module):
    """
    位置前馈网络模块
    """
    def __init__(self, d_model, d_ff, dropout=0.1):
        super(PositionWiseFeedForward, self).__init__()
        self.linear1 = nn.Linear(d_model, d_ff)
        self.dropout = nn.Dropout(dropout)
        self.linear2 = nn.Linear(d_ff, d_model)
        self.relu = nn.ReLU()

    def forward(self, x):
        # x 形状: (batch_size, seq_len, d_model)
        x = self.linear1(x)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.linear2(x)
        # 最终输出形状: (batch_size, seq_len, d_model)
        return x

```

在 Transformer 的每个编码器和解码器层中，所有子模块（如多头注意力和前馈网络）都被一个 `Add & Norm` 操作包裹。这个组合是为了保证 Transformer 能够稳定训练。

这个操作由两个部分组成：

- **残差连接 (Add)**：该操作将子模块的输入 `x` 直接加到该子模块的输出 `Sublayer(x)` 上。这一结构解决了深度神经网络中的**梯度消失 (Vanishing Gradients)** 问题。在反向传播时，梯度可以绕过子模块直接向前传播，从而保证了即使网络层数很深，模型也能得到有效的训练。其公式可以表示为：$Output = x + sublayer(x)$
- **层归一化 (Norm)**：该操作对单个样本的所有特征进行归一化，使其均值为0，方差为1。这解决了模型训练过程中的**内部协变量偏移 (Internal Covariate Shift)** 问题，使每一层的输入分布保持稳定，从而加速模型收敛并提高训练的稳定性。

我们已经了解，Transformer 的核心是自注意力机制，它通过计算序列中任意两个词元之间的关系来捕捉依赖。然而，这种计算方式有一个固有的问题：它本身不包含任何关于词元顺序或位置的信息。对于自注意力来说，“agent learns” 和 “learns agent” 这两个序列是完全等价的，因为它只关心词元之间的关系，而忽略了它们的排列。为了解决这个问题，Transformer 引入了**位置编码 (Positional Encoding)** 。

位置编码的核心思想是，为输入序列中的每一个词元嵌入向量，都额外加上一个能代表其绝对位置和相对位置信息的“位置向量”。这个位置向量不是通过学习得到的，而是通过一个固定的数学公式直接计算得出。这样一来，即使两个词元（例如，两个都叫 `agent` 的词元）自身的嵌入是相同的，但由于它们在句子中的位置不同，它们最终输入到 Transformer 模型中的向量就会因为加上了不同的位置编码而变得独一无二。原论文中提出的位置编码使用正弦和余弦函数来生成，其公式如下：
$$
PE_{(pos, 2i)} = sin(\frac{pos}{10000^{\frac{2i}{d_{model}}}})
$$

$$
PE_{(pos, 2i+1)} = cos(\frac{pos}{10000^{\frac{2i}{d_{model}}}})
$$

```Python
class PositionalEncoding(nn.Module):
    """
    为输入序列的词嵌入向量添加位置编码。
    """
    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)

        # 创建一个足够长的位置编码矩阵
        position = torch.arange(max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))

        # pe (positional encoding) 的大小为 (max_len, d_model)
        pe = torch.zeros(max_len, d_model)

        # 偶数维度使用 sin, 奇数维度使用 cos
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        # 将 pe 注册为 buffer，这样它就不会被视为模型参数，但会随模型移动（例如 to(device)）
        self.register_buffer('pe', pe.unsqueeze(0))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x.size(1) 是当前输入的序列长度
        # 将位置编码加到输入向量上
        x = x + self.pe[:, :x.size(1)]
        return self.dropout(x)
```

Decoder-Only 架构的工作模式被称为**自回归 (Autoregressive)** 。这个听起来很专业的术语，其实描述了一个非常简单的过程：

1. 给模型一个起始文本（例如 “Datawhale Agent is”）。
2. 模型预测出下一个最有可能的词（例如 “a”）。
3. 模型将自己刚刚生成的词 “a” 添加到输入文本的末尾，形成新的输入（“Datawhale Agent is a”）。
4. 模型基于这个新输入，再次预测下一个词（例如 “powerful”）。
5. 不断重复这个过程，直到生成完整的句子或达到停止条件。

模型就像一个在玩“文字接龙”的游戏，它不断地“回顾”自己已经写下的内容，然后思考下一个字该写什么。

你可能会问，解码器是如何保证在预测第 `t` 个词时，不去“偷看”第 `t+1` 个词的答案呢？

答案就是**掩码自注意力 (Masked Self-Attention)** 。在 Decoder-Only 架构中，这个机制变得至关重要。它的工作原理非常巧妙：

在自注意力机制计算出注意力分数矩阵（即每个词对其他所有词的关注度得分）之后，但在进行 Softmax 归一化之前，模型会应用一个“掩码”。这个掩码会将所有位于当前位置之后（即目前尚未观测到）的词元对应的分数，替换为一个非常大的负数。当这个带有负无穷分数的矩阵经过 Softmax 函数时，这些位置的概率就会变为 0。这样一来，模型在计算任何一个位置的输出时，都从数学上被阻止了去关注它后面的信息。这种机制保证了模型在预测下一个词时，能且仅能依赖它已经见过的、位于当前位置之前的所有信息，从而确保了预测的公平性和逻辑的连贯性。

**Decoder-Only 架构的优势**

这种看似简单的架构，却带来了巨大的成功，其优势在于：

- **训练目标统一**：模型的唯一任务就是“预测下一个词”，这个简单的目标非常适合在海量的无标注文本数据上进行预训练。
- **结构简单，易于扩展**：更少的组件意味着更容易进行规模化扩展。今天的 GPT-4、Llama 等拥有数千亿甚至万亿参数的巨型模型，都是基于这种简洁的架构。
- **天然适合生成任务**：其自回归的工作模式与所有生成式任务（对话、写作、代码生成等）完美契合，这也是它能成为构建通用智能体基础的核心原因。

总而言之，从 Transformer 的解码器演变而来的 Decoder-Only 架构，通过“预测下一个词”这一简单的范式，开启了我们今天所处的大语言模型时代。

在GPT的训练中，模型的目标是**预测下一个token**。所以：

- **输入**：模型的输入是文本的一部分，通常是**一个token序列**（例如“我 喜欢 吃”）。
- **输出**：模型生成的输出是该序列的下一个token预测（例如“苹果”）。

**训练时使用的是原始文本中的真实token**作为目标，而不是模型自己生成的token。也就是说，GPT在训练时并不会将之前生成的内容作为输入，而是依赖**真实的标注文本**（即训练集中的真实token）来计算损失。

在GPT的训练过程中，通常使用**交叉熵损失（Cross-Entropy Loss）**来衡量预测的概率分布与真实目标分布之间的差异。对于每个时间步，损失函数计算模型的输出概率分布与真实token之间的差异。

对于每个位置 $t$，假设模型预测的token为 $y_t'$（预测的token），而真实的目标token为 $y_t$（真实标签）。交叉熵损失定义为：
$$
L(\theta) = -\sum_{t =1}^{T}{log P(y_t|x_1,x_2,……,x_{t-1};\theta)}
$$
一个 $batch$ 所有 $token$ 预测结束后计算平均交叉熵损失

#### 2. 常见性质

##### 2.1 **外推性**

大模型在训练时和预测时的输入长度不一致，导致模型的泛化能力下降的问题。例如，如果一个模型在训练时只使用了512个 token 的文本，那么在预测时如果输入超过512个 token，模型可能无法正确处理。这就限制了大模型在处理长文本或多轮对话等任务时的效果。

##### 2.2 **涌现能力**

当模型规模（参数量、训练数据量等）增大到某个阈值之后，会**突然**表现出一些在小模型中完全没有、甚至无法预期的复杂能力

##### 2.3 **模型幻觉（Hallucination）**

通常指的是大语言模型生成的内容与客观事实、用户输入或上下文信息相矛盾，或者生成了不存在的事实、实体或事件。幻觉的本质是模型在生成过程中，过度自信地“编造”了信息，而非准确地检索或推理。根据其表现形式，幻觉可以被分为多种类型[11]，例如：

- **事实性幻觉 (Factual Hallucinations)** ： 模型生成与现实世界事实不符的信息。
- **忠实性幻觉 (Faithfulness Hallucinations)** ： 在文本摘要、翻译等任务中，生成的内容未能忠实地反映源文本的含义。
- **内在幻觉 (Intrinsic Hallucinations)** ： 模型生成的内容与输入信息直接矛盾。

幻觉的产生是多方面因素共同作用的结果。首先，训练数据中可能包含错误或矛盾的信息。其次，模型的自回归生成机制决定了它只是在预测下一个最可能的词元，而没有内置的事实核查模块。最后，在面对需要复杂推理的任务时，模型可能会在逻辑链条中出错，从而“编造”出错误的结论。例如：一个旅游规划 Agent，可能会为你推荐一个现实中不存在的景点，或者预订一个航班号错误的机票。

此外，大语言模型还面临着知识时效性不足和训练数据中存在的偏见等挑战。大语言模型的能力来源于其训练数据。这意味着模型所掌握的知识是其训练数据收集时的最新材料。对于在此日期之后发生的事件、新出现的概念或最新的事实，模型将无法感知或正确回答。与此同时训练数据往往包含了人类社会的各种偏见和刻板印象。当模型在这些数据上学习时，它不可避免地会吸收并反映出这些偏见[12]。

为了提高大语言模型的可靠性，研究人员和开发者正在积极探索多种检测和缓解幻觉的方法：

1. **数据层面**： 通过高质量数据清洗、引入事实性知识以及强化学习与人类反馈 (RLHF) 等方式[13]，从源头减少幻觉。
2. **模型层面**： 探索新的模型架构，或让模型能够表达其对生成内容的不确定性。
3. **推理与生成层面**：
   1. **检索增强生成 (Retrieval-Augmented Generation, RAG)** [14]： 这是目前缓解幻觉的有效方法之一。RAG 系统通过在生成之前从外部知识库（如文档数据库、网页）中检索相关信息，然后将检索到的信息作为上下文，引导模型生成基于事实的回答。
   2. **多步推理与验证**： 引导模型进行多步推理，并在每一步进行自我检查或外部验证。
   3. **引入外部工具**： 允许模型调用外部工具（如搜索引擎、计算器、代码解释器）来获取实时信息或进行精确计算。

尽管幻觉问题短期内难以完全消除，但通过上述的策略，可以显著降低其发生频率和影响，提高大语言模型在实际应用中的可靠性和实用性。

##### 2.4 温度

`Temperature`：温度是控制模型输出 “随机性” 与 “确定性” 的关键参数。其原理是引入温度系数 $T> 0$，将 Softmax 改写为 $P_{i}^{(T)} = \frac{e^{z_i}}{\sum_{j=1}^{k}{e^{\frac{z_j}{T}}}}$ 

当T变小时，分布“更加陡峭”，高概率项权重进一步放大，生成更“保守”且重复率更高的文本。当T变大时，分布“更加平坦”，低概率项权重提升，生成更“多样”但可能出现不连贯的内容。

- 低温度（0 ⩽ Temperature < 0.3）时输出更 “精准、确定”。适用场景： 事实性任务：如问答、数据计算、代码生成； 严谨性场景：法律条文解读、技术文档撰写、学术概念解释等场景。
- 中温度（0.3 ⩽ Temperature < 0.7）：输出 “平衡、自然”。适用场景： 日常对话：如客服交互、聊天机器人； 常规创作：如邮件撰写、产品文案、简单故事创作。
- 高温度（0.7 ⩽ Temperature < 1）：输出 “创新、发散”。适用场景： 创意性任务：如诗歌创作、科幻故事构思、广告 slogan brainstorm、艺术灵感启发； 发散性思考。

#### 3. 文本嵌入与分词

N-gram 模型是 NLP 领域中一种基于统计的语言模型，广泛应用于语音识别、手写识别、拼写纠错、机器翻译和搜索引擎等众多任务。N-gram模型的核心思想是基于马尔可夫假设，即一个词的出现概率仅依赖于它前面的N-1个词。这里的N代表连续出现单词的数量，可以是任意正整数。例如，当N=1时，模型称为unigram，仅考虑单个词的概率；当N=2时，称为bigram，考虑前一个词来估计当前词的概率；当N=3时，称为trigram，考虑前两个词来估计第三个词的概率，以此类推N-gram。

N-gram模型通过条件概率链式规则来估计整个句子的概率。具体而言，对于给定的一个句子，模型会计算每个N-gram出现的条件概率，并将这些概率相乘以得到整个句子的概率。例如，对于句子“The quick brown fox”，作为trigram模型，我们会计算 P("brown"∣"The","quick")*P*("*b**ro**w**n*"∣"*T**h**e*","*q**u**i**c**k*")、P("fox"∣"quick","brown")*P*("*f**o**x*"∣"*q**u**i**c**k*","*b**ro**w**n*")等概率，并将它们相乘。

N-gram的优点是实现简单、容易理解，在许多任务中效果不错。但当N较大时，会出现数据稀疏性问题。模型的参数空间会急剧增大，相同的N-gram序列出现的概率变得非常低，导致模型无法有效学习，模型泛化能力下降。此外，N-gram模型忽略了词之间的范围依赖关系，无法捕捉到句子中的复杂结构和语义信息。

尽管存在局限性，N-gram模型由于其简单性和实用性，在许多 NLP 任务中仍然被广泛使用。在某些应用中，结合N-gram模型和其他技术（如深度学习模型）可以获得更好的性能。

##### 3.1 嵌入 embedding

$one-hot$ 独热编码存在的问题：维度灾难（维度和词表大小一样）；两个词相似程度没法表示

![image-20251109090504433](assets/image-20251109090504433.png)

词向量训练方法：Word2Vec, 这种方法它有一个重要假设，就是文本中离得越近的词语，相似度越高，有两种典型的方法，一个是CBOW，另一个是 Skip-gram

**CBOW** 方法，就是用两边词预测中间词，比如要预测"有"，它的独热编码是（000100），那就把 "爱" （001000）和 "温"（000010） 通过 WordEmbedding，WordEmbedding 是 一个 $5 \times 2$ 的矩阵，那么爱假设是 [0.5 0.3]，温是 [0.1 0.4]，然后把它们经过一个 Linear 层变成六维度的，比如 [0.1 0.2 -0.2 0.8 0.1 1.2]，然后计算爱和温的两个输出向量的平均值，经过softmax后找到概率最大的位置，应该是（000100），然后根据交叉熵损失进行训练，因为 $y_i$ 是独热的，只有正确词（比如“有”）的那个位置是 1，所以实际上：
$$
L = -\log(\hat{y}_{\text{“有”}})
$$
**Skip-Gram** 方法，通过中间词去预测上下文，它是将中间词进行编码得到softmax输出，找到最大概率的词看看是不是它旁边的词

Skip-Gram 是反过来，用一个中心词预测它的上下文。

例如中心词是 “注”，上下文是 [“关”, “意”]。

- 输入：中心词 “注” 的独热编码

- WordEmbedding 层：将“注”映射到向量 $\mathbf{v}_{注}$

- 输出层：预测每一个上下文词的概率
  $$
  P(\text{关}|\text{注}) = \text{softmax}(W' \cdot \mathbf{v}_{注})
  $$

  $$
  P(\text{意}|\text{注}) = \text{softmax}(W' \cdot \mathbf{v}_{注})
  $$

对于给定中心词 $c$ 和上下文词 $o$，希望最大化：
$$
P(o|c) = \frac{\exp(\mathbf{v}'_o \cdot \mathbf{v}_c)}{\sum_{w=1}^V \exp(\mathbf{v}'_w \cdot \mathbf{v}_c)}
$$
于是损失是负对数似然：
$$
L = -\log P(o|c)
$$
![image-20251109094618619](assets/image-20251109094618619.png)

但是这种方法都会通过 softmax 计算，当词表很大时，计算时长会增加很多，所以考虑利用负采样的方法

即对于 关 和 注 都进行编码，然后计算他们的点积，之后通过 sigmoid函数，概率大于0.5就输出1，小于0.5就输出0，比如我要训练注，如果和 关 点积那我希望是大于0.5的，而如果和一个不相关的比如 温 点积，我希望是小于0.5的

##### 3.2 分词 Tokenization

将文本序列转换为数字序列的过程，就叫做**分词 (Tokenization)** 。**分词器 (Tokenizer)** 的作用，就是定义一套规则，将原始文本切分成一个个最小的单元，我们称之为**词元 (Token)** 。

早期的自然语言处理任务可能会采用简单的分词策略：

- **按词分词 (Word-based)** ：直接用空格或标点符号将句子切分成单词。这种方法很直观，但会面临“词表爆炸”的问题。一个语言的词汇量是巨大的，如果每个词都作为一个独立的词元，词表会变得难以管理。更糟糕的是，模型将无法处理任何未在词表中出现过的词，例如 “DatawhaleAgent”。
- **按字符分词 (Character-based)** ：将文本切分成单个字符。这种方法词表很小（例如英文字母、数字和标点），不存在 OOV 问题。但它的缺点是，单个字符大多不具备独立的语义，模型需要花费更多的精力去学习如何将字符组合成有意义的词，导致学习效率低下。
- **按字节编词（BBPE）：** BBPE 用 byte （字节）构建最基础词表，将 BPE 从字符级别扩展到字节级别，在字节序列上使用 BPE 算法进行相邻合并。BBPE与BPE构建词表的流程基本一致，只是在字节级别进行合并，需要在BPE的基础上把单个字符转化为单个字节再进行合并。

字节对编码 (Byte-Pair Encoding, BPE) 是最主流的子词分词算法之一，GPT系列模型就采用了这种算法。其核心思想非常简洁，可以理解为一个“贪心”的合并过程：

1. **初始化**：将词表初始化为所有在语料库中出现过的基本字符。
2. **迭代合并**：在语料库上，统计所有相邻词元对的出现频率，找到频率最高的一对，将它们合并成一个新的词元，并加入词表。
3. **重复**：重复第 2 步，直到词表大小达到预设的阈值。

```python
import re, collections

def get_stats(vocab):
    """统计词元对频率"""
    pairs = collections.defaultdict(int)
    for word, freq in vocab.items():
        symbols = word.split()
        for i in range(len(symbols)-1):
            pairs[symbols[i],symbols[i+1]] += freq
    return pairs

def merge_vocab(pair, v_in):
    """合并词元对"""
    v_out = {}
    bigram = re.escape(' '.join(pair))
    p = re.compile(r'(?<!\S)' + bigram + r'(?!\S)')
    for word in v_in:
        w_out = p.sub(''.join(pair), word)
        v_out[w_out] = v_in[word]
    return v_out

# 准备语料库，每个词末尾加上</w>表示结束，并切分好字符
vocab = {'h u g </w>': 1, 'p u g </w>': 1, 'p u n </w>': 1, 'b u n </w>': 1}
num_merges = 4 # 设置合并次数

for i in range(num_merges):
    pairs = get_stats(vocab)
    if not pairs:
        break
    best = max(pairs, key=pairs.get)
    vocab = merge_vocab(best, vocab)
    print(f"第{i+1}次合并: {best} -> {''.join(best)}")
    print(f"新词表（部分）: {list(vocab.keys())}")
    print("-" * 20)

>>>
第1次合并: ('u', 'g') -> ug
新词表（部分）: ['h ug </w>', 'p ug </w>', 'p u n </w>', 'b u n </w>']
--------------------
第2次合并: ('ug', '</w>') -> ug</w>
新词表（部分）: ['h ug</w>', 'p ug</w>', 'p u n </w>', 'b u n </w>']
--------------------
第3次合并: ('u', 'n') -> un
新词表（部分）: ['h ug</w>', 'p ug</w>', 'p un </w>', 'b un </w>']
--------------------
第4次合并: ('un', '</w>') -> un</w>
新词表（部分）: ['h ug</w>', 'p ug</w>', 'p un</w>', 'b un</w>']
--------------------

```

#### 4. 复杂度与优化

1. **自注意力机制（Self-Attention）复杂度**

在Transformer架构中，推理的核心操作是**自注意力机制**（Self-Attention）。其复杂度通常是计算每个token与所有其他tokens之间关系的过程。具体来说，自注意力机制的计算复杂度通常是**O(n²)**，其中 **n** 是输入序列的长度。

- **原因**：每个token都需要与其他所有token进行交互（即计算注意力权重），因此对于每个token，都会有一个 **n x n** 的计算矩阵。

- **公式**：
  $$
  O(\text{Complexity}) = O(n^2 \cdot d)
  $$
  其中，$n$ 是输入序列的长度，$d$ 是每个token的维度（通常是模型的隐藏层维度）。

- **解释**：例如，假设我们有一个包含1000个token的输入序列，每个token的维度是1024。那么，单次自注意力计算的复杂度将是 $O(1000^2 \cdot 1024)$，即每次推理需要进行约1亿次计算。

2. **前馈神经网络（Feed-Forward Networks）复杂度**

在Transformer模型的每一层中，除了自注意力计算，还会有一个**前馈神经网络**（Feed-Forward Network）。该部分的计算复杂度是与每个token的维度和网络层数有关的。

- **公式**：
  $$
  O(\text{Feed-Forward}) = O(n \cdot d^2)
  $$
  其中，$n$ 是输入序列长度，$d$ 是每个token的维度（通常是隐藏层的维度）。

- **解释**：假设我们每个token的维度是1024，输入序列长度是1000。前馈网络的计算复杂度通常是与 $d^2$ 成比例的，因此，对于这个例子，计算复杂度将是 $O(1000 \cdot 1024^2)$。

3. **总的推理复杂度**

在一个Transformer模型中，通常包含多个层（例如，12层、24层或更多）。每层都包含自注意力机制和前馈神经网络。因此，总的推理复杂度通常是**每层复杂度的叠加**。

- **公式**：
  $$
  O(\text{Total Complexity}) = L \cdot O(n^2 \cdot d + n \cdot d^2)
  $$
  其中，$L$ 是模型的层数，$n$ 是序列长度，$d$ 是每个token的维度。

- **解释**：假设有12层Transformer模型，每层的计算复杂度为 $O(n^2 \cdot d + n \cdot d^2)$，并且输入序列长度是1000，token维度是1024。那总的计算复杂度将是每层的复杂度乘以层数12。

4. **空间复杂度**

推理时的**内存消耗**也是非常重要的，因为长序列输入会占用更多的内存，特别是在需要存储中间计算结果时。

- 对于自注意力层，需要存储每一层的注意力矩阵，其内存需求是 $O(n^2 \cdot d)$，因为每个token与所有其他tokens的关系需要在内存中存储。
- 如果模型非常大，特别是在长序列处理时，内存消耗会非常高。

5. KV 缓存优化

**KV Cache** 是Transformer **Decoder** 中用于加速推理的一种机制。它通过缓存**键（Key）**和值（Value）来避免在生成新token时重新计算先前的键值对。这样可以显著提高模型的推理速度，因为在生成每个新token时，模型只需关注当前token，而无需重新计算所有先前token之间的关系。使用KV缓存时，**前面生成的token的K和V可以被重复利用**，从而加速生成过程。

![image-20251108201407552](assets/image-20251108201407552.png)

![image-20251108201413803](assets/image-20251108201413803.png)

![image-20251108201421295](assets/image-20251108201421295.png)

![image-20251108201439123](assets/image-20251108201439123.png)

#### 5. 模型下载

要通过 `curl` 完全下载 Hugging Face 上的模型及其所需的全部文件，并在后续的代码中从本地加载这些文件，通常你需要下载以下文件：

1. **模型权重**（例如 `pytorch_model.bin` 或 `tf_model.h5`）。
2. **分词器文件**（例如 `vocab.json`, `merges.txt`, `tokenizer_config.json` 等）。
3. **配置文件**（例如 `config.json`，包含模型架构信息）。

**步骤 1：查找模型的下载链接**

首先，确保你了解模型在 Hugging Face 上的存储位置（例如 `Qwen/Qwen3-8B` 模型）。你可以通过浏览模型页面找到相关文件的 URL，或者直接通过 Hugging Face API 获取文件。

例如，假设你下载的是 `Qwen/Qwen3-8B` 模型，文件可能存放在如下路径：

- `config.json`：包含模型配置。
- `pytorch_model.bin`：模型权重文件。
- `tokenizer_config.json`、`vocab.json`、`merges.txt`：分词器相关文件。

**步骤 2：使用 `curl` 下载模型和相关文件**

使用 `curl` 下载所有必要的文件。以下是具体命令（注意替换为你具体的模型路径）：

**`-C -`**：开启断点续传，`-` 表示继续从中断的地方下载。

**`-o model.bin`**：指定下载文件保存的路径和文件名。

**`https://huggingface.co/Qwen/Qwen3-8B/resolve/main/pytorch_model.bin`**：这是模型文件的下载 URL，`curl` 会从该 URL 下载模型文件。如果链接是 HTTPS 或 HTTP，`curl` 会根据文件内容自动处理。

```bash
# 下载配置文件
curl -C - -o config.json https://huggingface.co/Qwen/Qwen3-8B/resolve/main/config.json

# 下载模型权重文件
curl -C - -o pytorch_model.bin https://huggingface.co/Qwen/Qwen3-8B/resolve/main/pytorch_model.bin

# 下载分词器文件
curl -C - -o tokenizer_config.json https://huggingface.co/Qwen/Qwen3-8B/resolve/main/tokenizer_config.json
curl -C - -o vocab.json https://huggingface.co/Qwen/Qwen3-8B/resolve/main/vocab.json
curl -C - -o merges.txt https://huggingface.co/Qwen/Qwen3-8B/resolve/main/merges.txt
```

**说明**：

- **`config.json`**：包含模型的架构配置。
- **`pytorch_model.bin`**：模型的权重文件。
- **`tokenizer_config.json`**、**`vocab.json`** 和 **`merges.txt`**：这些文件是分词器需要的文件，包含词汇表、分词器的配置和合并规则。

**步骤 3：将文件保存在本地并加载模型**

在下载完这些文件之后，你可以将它们放在一个本地文件夹中，例如 `./model_directory/`，然后使用 `transformers` 库从本地加载这些文件。

**本地加载模型和分词器的代码示例：**

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

model_path="./model_directory"

# 从本地加载分词器
tokenizer = AutoTokenizer.from_pretrained(model_path)

# 从本地加载模型
model = AutoModelForCausalLM.from_pretrained(model_path)

# 使用模型进行推理
messages = [
    {"role": "user", "content": "Who are you?"}
]
inputs = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt=True,
    tokenize=True,
    return_dict=True,
    return_tensors="pt",
).to(model.device)

# 生成输出
outputs = model.generate(**inputs, max_new_tokens=40)
print(tokenizer.decode(outputs[0][inputs["input_ids"].shape[-1]:]))
```

**步骤 4：确保本地文件的完整性**

为了确保从 Hugging Face 下载的所有文件都正确无误，你可以检查模型文件夹中的文件是否齐全，确保包括以下几类文件：

- `config.json`：模型架构配置文件。
- `pytorch_model.bin`（或类似的文件，如 `tf_model.h5`，具体取决于模型框架）。
- `tokenizer_config.json`、`vocab.json`、`merges.txt` 等分词器相关文件。

**使用 Hugging Face CLI 下载模型（另一种方式）**

你也可以使用 Hugging Face 提供的 `huggingface-cli` 工具来下载模型和所有相关文件。

1. 安装 `huggingface-cli`：

```bash
pip install huggingface_hub
```

2. 使用 CLI 下载模型：

```bash
huggingface-cli download Qwen/Qwen3-8B --local-dir ./my_model_dir
```

3. 配置环境变量

```bash
export HF_ENDPOIN='https://hf-mirror.com'
```

这会自动将模型及其所有相关文件下载到当前目录，并且 Hugging Face 会将这些文件结构化地存储到本地，方便之后直接加载。

首先，确保你了解模型在 Hugging Face 上的存储位置（例如 `Qwen/Qwen3-8B` 模型）。你可以通过浏览模型页面找到相关文件的 URL，或者直接通过 Hugging Face API 获取文件。

例如，假设你下载的是 `Qwen/Qwen3-8B` 模型，文件可能存放在如下路径：

- `config.json`：包含模型配置。
- `pytorch_model.bin`：模型权重文件。
- `tokenizer_config.json`、`vocab.json`、`merges.txt`：分词器相关文件。

使用modelscope下载

```
modelscope download --model qwen/Qwen-7B-Chat --local_dir ./my_models
```

指定卡进行训练

```
CUDA_VISIBLE_DEVICES=1,2 python xxx.py
```



### 三、LLM架构对比

#### 1. gpt系列对比

GPT1采用了Transformer架构，其中包括多头自注意力机制和前向神经网络。这使得GPT1可以在处理自然语言时捕捉长距离依赖性，并且具有高效的并行性。GPT-1使用了一种称为“生成式预训练”（Generative Pre-Training，GPT）的技术。预训练分为两个阶段：预训练和微调（fine-tuning）。在预训练阶段，GPT-1使用了大量的无标注文本数据集，例如维基百科和网页文本等。通过最大化预训练数据集上的log-likelihood来训练模型参数。在微调阶段，GPT-1将预训练模型的参数用于特定的自然语言处理任务，如文本分类和问答系统等。

![image-20250722141509300](assets/image-20250722141509300.png)

GPT-2主要解决的问题是如何利用大规模未标注的自然语言文本来预训练一个通用的语言模型，从而提高自然语言处理的能力。与GPT-1模型不同之处在于，GPT-2模型使用了更大的模型规模和更多的数据进行预训练，同时增加了许多新的预训练任务。GPT-2具有零样本学习的能力，能够在只看到少量样本的情况下学习和执行新任务。**其主要变化在于训练的数据集规模变大**

GPT-3使用了更深的网络层数和更宽的Transformer网络结构，模型更大，参数更多，表达能力和语言理解能力更强； - GPT-3在预训练阶段使用了更大规模的数据集，并采用了更多样化的预训练任务。 GPT-3的微调阶段采用了zero-shot学习和few-shot的方法，使得GPT-3具备更强的泛化能力和迁移学习能力

InstructGPT：语言模型扩大并不能代表它们会更好地按照用户的意图进行工作，大语言模型很可能会生成一些不真实的、有害的或者是没有帮助的答案。换句话说，这些模型和用户的意图并不一致（not aligned with their users）。**由此OpenAI提出了“align”的概念，即希望模型的输出与人类意图“对齐”，符合人类真实偏好。** 对齐就是让模型回答符合人类的伦理和喜好，而不是乱回答，这里采用了 $RLHF$ 基于人类反馈的强化学习策略。ChatGPT就是在GPT3.5上用指示学习和人类反馈的强化学习来知道模型训练的。

![img](assets/img-1.jpeg)

1. 收集**示范数据**，进行有监督微调`SFT`。 - 标注数据：根据prompts（提示，这里就是写的各种各样的问题），人类会撰写一系列demonstrations（演示）作为模型的期望输出（主要是英文）； - 模型微调：**将prompts和人类标注的答案拼在一起，作为人工标注的数据集**，然后使用这部分数据集对预训练的GPT-3进行监督微调，得到第一个模型`SFT`（supervised fine-tuning，有监督微调) - **因为问题和答案是拼在一起的，所以在 GPT 眼中都是一样的，都是给定一段话然后预测下一个词，所以在微调上跟之前的在别的地方做微调或者是做预训练没有任何区别。** 

2. 收集**比较数据**，训练奖励模型`RM`。 - 生成式标注是很贵的一件事，所以第二步是进行排序式/判别式标注。用上一步得到的`SFT`模型生成各种问题的答案，标注者（labelers）会对这些输出进行比较和排序（由好到坏，比如下图D>C>A=B）。 - 基于这个数据集，用强化学习训练一个`RM`（reward model)。训练好了之后这个RM模型就可以对生成的答案进行打分，且打出的分数能够满足人工排序的关系。

3. 使用强化学习的机制，优化`SFT`模型，得到最终的`RL`模型（InstructGPT)。 将`SFT`模型的输出输入`RM`进行打分，通过强化学习来优化`SFT`模型的参数

#### 2. clip

如下图，在训练阶段，用文本和图像匹配，将文本编码，然后也将图像编码，计算文本向量和图像向量夹角的余弦值，然后用去训练encoder。然后在文本创建 `a photo of a {object}` 和图片去预测相似程度，然后得到分类，这样就可以进行 `zero shot` 学习，即没有样本的情况下也可以学习

![img](assets/img-0-1760844263570-1.jpeg)

#### 3. Deepseek-r1-zero

> SFT 监督微调，大量数据可能会造成模型思维固化，熵降低，少量数据可以使得模型学习领域知识
>
> 训练思路：跳过 SFT 冷启动，直接用 AIME 数据集在 Deepseek-v3 进行 RL 训练，设计提示词见上（强调模型输出思考标签），然后奖励设计是看最后答案是否正确（数学问题），代码运行能否通过测试样例（编程问题），也就是准确性设计，还有就是过程中是否使用了 `<think>` 这样的标签，训练采用 GRPO 训练，奖励模型直接用上述评判标准，参考模型和策略模型都是 Deepseek-v3

DeepSeek-R1-Zero 直接在基础模型上应用强化学习，不使用任何 SFT 数据（不用冷启动）。 为了训练 DeepSeek-R1-Zero，deepseek 采用了一种基于规则的奖励系统，该系统主要由两种奖励组成：

- **准确率奖励**：准确率奖励模型评估响应是否正确。例如，在具有确定性结果的数学问题中，模型需要以指定的格式（box）提供最终答案，从而能够通过基于规则的验证来可靠地确认正确性。同样，对于 LeetCode 问题，可以使用编译器根据预定义的测试用例生成反馈。
- **格式奖励**: 除了准确性奖励模型，还采用了一种格式奖励模型，要求模型将其思考过程放在 `<think>` 和` </think>` 标签之间。

需要强调的是：deepseek 在训练 DeepSeek-R1-Zero 时**没有使用结果奖励（ORM）或者过程奖励（PRM）**。

这里主要通过「结果判定」的方式：对于数学题、编程题等有客观正确答案的任务，可以把最终答案与标准结果对比给出奖励。虽没有逐步的过程标注，但最终答案正确与否足以在 RL 中当作回报（Reward）来引导模型学会更好的推理。

部分中间也会酌情使用**格式奖励**，用来约束模型输出思考过程，这是一种「作弊少、易维护」的思路。

在训练过程中设计的提示词，prompt 会用具体的问题替代

```text
A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. User: prompt. Assistant:
```

随着训练轮次的提升，我们可以看出模型在没有明确指导的情况下，自主学习并改进

> 在大规模强化学习中，模型的「思考过程」会不断与最终的**正确率奖励**相互作用。当模型最初得出的答案并未得到较高奖励时，它会在后续的推理中「**回头反省**」，尝试补充或修正先前的思路，从而获得更高的奖励。随着强化学习的迭代，这种「**主动回溯、推翻先前想法并重新推理**」的行为逐渐巩固，便在输出中表现为所谓的「**aha moment**」。本质上，这是 RL 为模型「留出了」足够的思考和试错空间，当模型自行发现更优思路时，就会出现类似人类「恍然大悟」的瞬间。

![image-20251118084018287](C:\Users\PC\AppData\Roaming\Typora\typora-user-images\image-20251118084018287.png)

#### 4. Deepseek-r1

##### 4.1 阶段1: 冷启动

冷启动阶段使用少量高质量的 CoT 数据对基础模型进行微调，作为 RL 训练的初始起点。侧重点是让模型掌握基本的 CoT 推理能力，并使模型的输出更具可性。

DeepSeek-R1 使用冷启动数据的主要目的是为了解决 DeepSeek-R1-Zero 在训练早期出现的训练不稳定问题。

为了获取这些数据，deepseek 探索了几种策略：利用长思维回答（COT）作为 few-shot 示例，直接提示模型生成包含反思和验证步骤的详细答案，以及收集 DeepSeek-R1-Zero 的输出并通过人工标注者进行细化。最终收集了数千条冷启动数据，用以微调 DeepSeek-V3-Base 作为 RL 训练的起点。DeepSeek-R1 创建的冷启动数据采用了一种可读模式，明确将输出格式定义为：`|special_token|<reasoning_process>|special_token<summary>`。

**数据示例：**

用户问题 prompt：若 x + 3 = 10，求 x。

冷启动训练样本

```
|special_token|
首先理解题意：x + 3 = 10。  
目标是求未知数 x。

步骤 1：将等式中的未知数 x 单独放一边。  
x + 3 = 10  

步骤 2：将 +3 移到右边，变成减法。  
x = 10 - 3  

步骤 3：计算右边：10 - 3 = 7。  
因此 x = 7。

验证步骤：  
将 x = 7 代回原式：  
7 + 3 = 10，成立。  
说明推理无误。
|special_token|
x = 7
```

**数据生成方法**

首先让 deepseek-r1-zero生成一些长推理示例，然后专家手动挑选少量高质量范例，然后加入 prompt 作为 few-shot，让模型严格按照 `|special_token| reasoning |special_token| summary` 格式输出，同时设计提示词，让模型能够自动长推理，自我检查，验证，最终总结。这样就得到第一批冷启动 CoT 数据。人工标注者对这些推理进行清洗，精简，结构化整理，形成最终的冷启动数据。冷启动数据总共是数千条

##### 4.2 阶段2：推理导向的强化学习

> 奖励信号是推理任务的准确性和语言一致性奖励直接相加

在冷启动模型的基础上进行 RL 训练，侧重点是提升模型在推理任务上的性能。在这个阶段，会引入**语言一致性奖励**，该奖励根据思维链（CoT）中目标语言单词的比例来计算，以减少推理过程中的语言混合问题。（因为 deepseek-r1-zero 有很明显的输出混乱）

尽管消融实验表明，**语言一致性奖励**会导致模型性能略有下降，但它更符合人类的偏好，提高了内容的可读性。最终，通过将推理任务的准确性与语言一致性奖励直接相加，形成了综合的奖励函数。随后，对微调后的模型进行了强化学习（RL）训练，直至其在推理任务上达到收敛。

##### 4.3 阶段3：拒绝采样和 SFT

> 拒绝采样：利用上一步训练中生成的检查点，继续通过采样的方式生成更多推理轨迹。这一过程的核心是：
>
> 1. 生成一些推理提示（reasoning prompts）。
> 2. 通过拒绝采样（基于规则的奖励函数来评估）对模型生成的多种推理路径进行筛选，保留正确的推理
>
> 核心就是通过训练后的模型生成多个回答，然后根据奖励模型或者函数保留正确的回答，具体的方法是对于有确定规则的奖励函数那么就使用它，对于没有确定规则的奖励函数，如写作等问题，采用将正确答案和模型生成的答案一起送给 deepseek-v3 作为 llm-judge 来评判

使用上一阶段的 RL 模型进行拒绝采样，生成高质量的推理和非推理数据，并用这些数据对模型进行微调。侧重点是提升模型的综合能力，使其在写作、事实问答等多种任务上表现良好。

当 RL 训练接近收敛时，使用中间的 checkpoint 来采样监督微调（SFT）数据。与初期主要关注推理能力的冷启动数据不同，这一阶段加入了其他领域的数据，旨在增强模型在写作、角色扮演以及其他通用任务上的表现。具体的数据生成和模型微调步骤如下：

- 对于推理数据，构建推理 prompt，并从上述 RL 训练的 checkpoint 中进行拒绝采样，以生成推理轨迹。在之前的阶段，仅使用了基于规则的奖励来评估数据。然而，在这个阶段，通过添加其他数据来丰富数据集，其中部分数据使用了**生成奖励模型**，通过将真实值和模型预测输入 DeepSeek-V3 进行判断。同时，为了提升数据质量，过滤掉混合语言、长段落和代码块的思维链。对于每个提示，采样多个响应，并仅保留正确的响应。最终，收集了大约**60万**个与推理相关的训练样本。
- 对于非推理数据，如写作、问答、翻译等任务，使用 DeepSeek-V3 SFT 数据集的一部分。对于简单的 query，如“你好”，不使用思维链作为回答。经过筛选和整理，最终收集了大约**20万**个与推理无关的训练样本。

最终，使用大约**80万**个样本（60w推理+20w通用）对 DeepSeek-v3-Base 模型进行了两轮的 SFT。

##### 4.4 阶段4：所有场景下的强化学习

> **Diverse Prompt Distributions**（多样化提示分布）是指在模型训练中，使用 **各种不同类型的提示（prompts）** 来生成训练数据，以确保模型能够适应更广泛的任务和场景。

该阶段的目标是：

- 提高模型的 **推理能力**，使其在处理数学、代码和逻辑推理任务时表现得更加精准。
- 增强模型的 **有用性（helpfulness）**，使其生成的回答对用户更具实际价值。
- 减少和消除潜在的 **无害性（harmlessness）**，避免生成有偏见、有害或不适当的内容。

利用上个阶段获得的数据进行训练

##### 4.5 失败的尝试

**1. PRM 的挑战**

- 难以定义通用的、细粒度的推理步骤。
- 难以准确判断中间步骤的正确性，且自动标注方法效果不佳，人工标注又难以扩展。
- 模型化的 PRM 容易导致**奖励黑客**（Agent 利用奖励函数或环境中的漏洞来获取高奖励，而并未真正学习到预期行为。）行为，并且会增加额外的训练成本。

**2. MCTS 的挑战**

MCTS 被用来增强 **推理任务**，特别是针对 **生成推理步骤的任务**。具体应用过程如下：

- **分解答案**：为了解决复杂问题，DeepSeek 将一个复杂的答案分解为多个较小的推理步骤。这些步骤有助于模型更系统地探索解空间。
  - 例如，对于一个问题，模型不仅要给出最终答案，还需要通过多个推理步骤（如逻辑推理、验证等）逐步推导出正确答案。
- **使用预训练的价值模型引导搜索**：
  - MCTS 通过 **预训练的价值模型** 来 **引导推理过程**，帮助模型决定如何进行搜索。
  - 预训练价值模型的作用是提供对每个步骤的“评分”或“优先级”，指导模型做出更合理的推理路径选择。
- **迭代优化**：模型通过 MCTS 生成多个问题-答案对，这些数据用于训练 **演员模型（Actor Model）** 和 **价值模型（Value Model）**，不断优化模型的推理能力。

主要问题

- LLM 的 token 生成搜索空间巨大，远远超出棋类游戏，容易陷入局部最优解。
- 价值模型的训练非常困难，导致难以迭代提升。

![img](https://pic1.zhimg.com/v2-6a0549b43f4d5377b1c0cef626d4dd7e_r.jpg)

#### 5. Qwen 系列

##### 5.1 Qwen3-8B-Base 模型

- 

### 四、蒸馏技术

#### 1. 知识蒸馏简介

知识蒸馏使用的是Teacher—Student模型，其中teacher是“知识”的输出者，student是“知识”的接受者。知识蒸馏的过程分为2个阶段:

1. 原始模型训练: 训练"Teacher模型", 简称为Net-T，它的特点是模型相对复杂，也可以由多个分别训练的模型集成而成。我们对"Teacher模型"不作任何关于模型架构、参数量、是否集成方面的限制，唯一的要求就是，对于输入X, 其都能输出Y，其中Y经过softmax的映射，输出值对应相应类别的概率值。
2. 精简模型训练: 训练"Student模型", 简称为Net-S，它是参数量较小、模型结构相对简单的单模型。同样的，对于输入X，其都能输出Y，Y经过softmax映射后同样能输出对应相应类别的概率值。

一个很直白且高效的迁移泛化能力的方法就是：使用softmax层输出的类别的概率来作为“soft target”。

1. 传统training过程(**hard targets**): 对ground truth求极大似然
2. KD的training过程(**soft targets**): 用large model的class probabilities作为soft targets

softmax层的输出，除了正例之外，**负标签也带有大量的信息**，比如某些负标签对应的概率远远大于其他负标签。而在传统的训练过程(hard target)中，所有负标签都被统一对待。也就是说，**KD的训练方式使得每个样本给Net-S带来的信息量大于传统的训练方式**。

![image-20251119085147297](C:\Users\PC\AppData\Roaming\Typora\typora-user-images\image-20251119085147297.png)

先回顾一下原始的 softmax 函数：

$$
q_i = \frac{\exp(z_i)}{\sum_j \exp(z_j)}
$$

但是直接使用 softmax 层的输出值作为 soft target，这又会带来一个问题：当 softmax 输出的概率分布很接近于 0，负损失函数值很小，对损失函数的贡献非常小，小到可以忽略不计。因此，“温度”这个变量就应运而生了。

下面的公式表示了温度过后 softmax 函数：

$$
q_i = \frac{\exp(z_i / T)}{\sum_j \exp(z_j / T)}
$$

这里的 $T$ 就是温度。

原来的 softmax 函数是 $T = 1$ 的特殊例子。**温度**越高，softmax 的 output probability distribution 越接近于平坦，具有分布的翘曲，负标签携带的信息会被相对地放大，模型训练将更为负荷繁重。

#### 2. 具体步骤

![img](https://pic4.zhimg.com/v2-d01f5142d06aa27bc5e207831b5131d9_r.jpg)

训练 Net-T 的过程很简单，下面详细讲解第二步：高温蒸馏的过程。高温蒸馏过程的目标函数由 distill loss（对应 soft target）和 student loss（对应 hard target）加权得到，示意图如下。

$$
L = \alpha L_{\text{KD}} + \beta L_{\text{hard}}
$$

- $v_i$: Net-T 的 logits
- $z_i$: Net-S 的 logits
- $p_i^T$: Net-T 的温度 $T$ 下 softmax 输出在第 $i$ 类上的值
- $q_i^T$: Net-S 的温度 $T$ 下 softmax 输出在第 $i$ 类上的值
- $c_i$: 在类别 $i$ 上的 ground truth 值，$c_i \in \{0, 1\}$，正确标签为 1，负标签为 0
- $N$: 总样本数

Net-T 和 Net-S 同时输入训练集（这里可以直接复用训练 Net-T 用到的 training set），用 Net-T 产生的 softmax distribution (with high temperature) 来作为 soft target，Net-S 在相同温度下条件下的 softmax 输出和 soft target 的 cross entropy 就是 Loss 函数的第一部分 $L_{\text{soft}}$。可以用交叉熵损失，也可以用KL散度

$$
L_{\text{KD}} = T^2 \sum_{i=1}^{N} p_j^T [log(p_j^T)-log(q_j^T)], \quad p_i^T = \frac{\exp(v_i / T)}{\sum_k \exp(v_k / T)}, \quad q_i^T = \frac{\exp(z_i / T)}{\sum_k \exp(z_k / T)}
$$

Net-S 在 $T = 1$ 时的 softmax 输出和 ground truth 的 cross entropy 就是 Loss 函数的第二部分 $L_{\text{hard}}$。

$$
L_{\text{hard}} = - \sum_{i=1}^{N} c_j \log(q_j^1), \quad q_i^1 = \frac{\exp(z_i)}{\sum_k \exp(z_k)}
$$

第二部分 Loss $L_{\text{hard}}$ 的解释实际上是：Net-T 也有一定的错误造成；使用 ground truth 可以有效地传递给学生模型 Net-S 的信息。打个比方，老师虽然远离学生，但学生仍然能从老师的教学反馈中得到帮助；而当情况下学生在老师的指导下学习，就可以有效降低损失函数中的错误 "常温" 的可避免性。

#### 3. 分类

1. 软标签蒸馏 （soft-label）也就是logits蒸馏

   ![动图](https://pic3.zhimg.com/v2-7f36217f119d5e5c321f6cfa02997496_b.webp)

   - 使用一个已训练好的教师模型对整份语料生成 softmax 概率分布（即“软标签”）。
   - 同时也将相同的数据输入未经训练的学生模型，获取它的 softmax 概率分布。
   - 训练学生模型，使其输出尽可能匹配教师模型的概率分布。
   - 可见的 softmax 概率可以最大化地实现知识（或推理能力）的迁移。

   不过，这种方法有个问题：

   - 你必须能访问教师模型的权重，才能获取完整的输出概率分布。
   - 即使你能访问，还有另一个挑战：

   假设你的词表大小是 10 万个 token，语料总量为 5 万亿个 token。由于每个 token 都需要生成整个词表上的 softmax 概率，在 float8 精度下，存储所有“软标签”大约需要 **500 万 GB** 的内存。

2. 硬标签蒸馏（hard-label）也就是数据蒸馏

   ![动图](https://pica.zhimg.com/v2-afc7b5ad5b465e9b7e69bf4f16d832f6_b.webp)

   - 使用一个固定的已训练教师模型，只获取最终预测的 one-hot 输出 token（即“硬标签”）。
   - 使用未经训练的学生模型对相同数据获取 softmax 概率。
   - 训练学生模型，使其输出概率尽可能匹配教师模型的 one-hot 标签。

   DeepSeek 就是采用这种方式，将 DeepSeek-R1 蒸馏到了 Qwen 和 Llama 3.1 中。

3. 协同蒸馏（co-distillation）

   ![动图](https://pic1.zhimg.com/v2-f838aac249a040c5f4b79941040298aa_b.webp)

   - 从一个未经训练的教师模型和一个未经训练的学生模型开始。
   - 对当前 batch 的输入，两者都生成 softmax 概率分布。
   - 教师模型按常规使用“硬标签”训练。
   - 学生模型则训练使其概率分布尽可能匹配教师模型的 softmax 输出。

   Llama 4 就是用这种方式，从 Llama 4 Behemoth 中训练出了 Llama 4 Scout 和 Maverick。

   当然，在初始阶段，教师模型本身的 softmax 输出也不太准确。

   因此，学生模型的训练会同时参考教师的“软标签”与真实的“硬标签”。

#### 4. 具体示例

用 ResNet-18 作为教师蒸馏 CNN

```python
torch.manual_seed(args.seed)
if args.cuda:
    torch.cuda.manual_seed(args.seed)


kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}
train_loader = torch.utils.data.DataLoader(
    datasets.MNIST('./data_mnist', train=True, download=True,
                   transform=transforms.Compose([
                       transforms.ToTensor(),
                       transforms.Normalize((0.1307,), (0.3081,))
                   ])),
    batch_size=args.batch_size, shuffle=True, **kwargs)

test_loader = torch.utils.data.DataLoader(
    datasets.MNIST('./data_mnist', train=False, transform=transforms.Compose([
                       transforms.ToTensor(),
                       transforms.Normalize((0.1307,), (0.3081,))
                   ])),
    batch_size=args.test_batch_size, shuffle=True, **kwargs)


class teacherNet(nn.Module):
    def __init__(self):
        super(teacherNet, self).__init__()
        self.fc1 = nn.Linear(28 * 28, 1200)
        self.fc2 = nn.Linear(1200, 1200)
        self.fc3 = nn.Linear(1200, 10)

    def forward(self, x):
        x = x.view(-1, 28 * 28)
        x = F.relu(self.fc1(x))
        x = F.dropout(x, p=0.8, training=self.training)
        x = F.relu(self.fc2(x))
        x = F.dropout(x, p=0.8, training=self.training)
        x = self.fc3(x)
        return x

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(28 * 28, 800)
        self.fc2 = nn.Linear(800, 800)
        self.fc3 = nn.Linear(800, 10)

    def forward(self, x):
        x = x.view(-1, 28 * 28)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

teacher_model = teacherNet()
teacher_model.load_state_dict(torch.load('teacher_MLP.pth.tar'))


model = Net()
if args.cuda:
    model.cuda()

optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)

def distillation(y, labels, teacher_scores, T, alpha):
    return nn.KLDivLoss()(F.log_softmax(y/T), F.softmax(teacher_scores/T)) * (T*T * 2.0 * alpha) + F.cross_entropy(y, labels) * (1. - alpha)


def train(epoch, model, loss_fn):
    model.train()
    teacher_model.eval()
    for batch_idx, (data, target) in enumerate(train_loader):
        if args.cuda:
            data, target = data.cuda(), target.cuda()
        data, target = Variable(data), Variable(target)
        optimizer.zero_grad()
        output = model(data)
        teacher_output = teacher_model(data)
        teacher_output = teacher_output.detach()
        # teacher_output = Variable(teacher_output.data, requires_grad=False) #alternative approach to load teacher_output
        loss = loss_fn(output, target, teacher_output, T=20.0, alpha=0.7)
        loss.backward()
        optimizer.step()
        if batch_idx % args.log_interval == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * len(data), len(train_loader.dataset),
                100. * batch_idx / len(train_loader), loss.data[0]))

def train_evaluate(model):
    model.eval()
    train_loss = 0
    correct = 0
    for data, target in train_loader:
        if args.cuda:
            data, target = data.cuda(), target.cuda()
        data, target = Variable(data, volatile=True), Variable(target)
        output = model(data)
        train_loss += F.cross_entropy(output, target).data[0] # sum up batch loss
        pred = output.data.max(1, keepdim=True)[1]
        correct += pred.eq(target.data.view_as(pred)).cpu().sum()

    print('\nTrain set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
        train_loss, correct, len(train_loader.dataset),
        100. * correct / len(train_loader.dataset)))


def test(model):
    model.eval()
    test_loss = 0
    correct = 0
    for data, target in test_loader:
        if args.cuda:
            data, target = data.cuda(), target.cuda()
        data, target = Variable(data, volatile=True), Variable(target)
        output = model(data)
        # test_loss += F.cross_entropy(output, target).data[0] # sum up batch loss
        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability
        correct += pred.eq(target.data.view_as(pred)).cpu().sum()

    test_loss /= len(test_loader.dataset)
    print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
        test_loss, correct, len(test_loader.dataset),
        100. * correct / len(test_loader.dataset)))


for epoch in range(1, args.epochs + 1):
    train(epoch, model, loss_fn=distillation)
    train_evaluate(model)
    test(model)


torch.save(model.state_dict(), 'distill.pth.tar')
# the_model = Net()
# the_model.load_state_dict(torch.load('student.pth.tar'))

# test(the_model)
# for data, target in test_loader:
#     data, target = Variable(data, volatile=True), Variable(target)
#     teacher_out = the_model(data)
# print(teacher_out)
print("--- %s seconds ---" % (time.time() - start_time))
```

### 五、推理模型设计

#### 1. RLHF 

基于人类反馈的强化学习，通过设计偏好对来进行奖励模型的设计，通过监督数据来训练奖励模型，可以让大模型的输出满足人类的偏好，同时也可以适合一些开发问题的处理，通过偏好对来进行更优答案的选择

RLHF是该领域的一项开创性工作。它通过收集人类对模型不同输出的偏好排序，训练一个奖励模型（Reward Model）。这个奖励模型随后作为“代理裁判”，在强化学习过程中为LLM的输出打分，引导LLM生成更符合人类偏好的内容。然而，RLHF的成功建立在大量高质量的人类标注数据之上，这是一个劳动密集型且成本高昂的过程。

#### 2. RLVR

为了降低对人类标注的依赖，研究者们提出了RLVR。RLVR适用于那些答案可以被程序自动验证的领域，例如数学问题（验证最终答案是否正确）和代码生成（执行代码看是否通过单元测试）。通过这种方式，可以自动获得一个清晰、客观的奖励信号。尽管RLVR在特定领域取得了巨大成功，但其应用场景受限，在开放问题场景应用较为困难。

在开放问题场景下，我们通常思考可以通过设置 rubrics 来进行评判，但是专家数据难以收集，并且不可能为所有问题都覆盖，所以我们需要考虑如何根据有限的标注来进行训练对比，通常可以通过构建 向量知识库，通过检索手段获得一些相似的rubrics，然后把他们作为 few-shot 以及其他精心设计的提示词来生成该问题对应的 rubrics。

之后大模型输出答案后就需要一个 llm-as-judge 裁判模型来评判大模型输出是否满足条件，但是裁判模型答准率也会受到影响，即裁判模型一般需要特定微调（即rubrics，answer，以及answer是否满足rubrics这样构建的数据集来进行微调）才行，并且可能会出现幻觉，造成裁判模型答准率受限。

**RLVR 的核心在于验证器的设计，在确定答案问题中，验证器可直接为是否正确回答（比如数学答案是否正确，代码输出是否可以通过单元测试）；在开放问题中，验证器可以为 rubrics 和 裁判模型共同组成，来设计奖励信号**

在最近的研究中，有研究称 SFT 可以提高模型的 pass@k 能力，RL 可以提高模型的 pass@1 能力，也就是模型能够更高效的找到推理路径，但是推理路径是由 SFT 阶段训练出来的

#### 3. RLIF

RLIF的核心思想是，模型在生成答案的过程中，其内部状态本身就蕴含着丰富的、可用于自我改进的信号。例如，模型对于下一步生成哪个词元（token）的不确定性程度，或者对于整个生成轨迹的整体置信度，都可以被量化并用作奖励信号。

RLIF的最大优势在于其**无监督**的特性。它摆脱了对外部标注数据或验证器的依赖，使得在任何领域进行模型的自我优化成为可能，极大地降低了训练成本并提升了方法的通用性。

* 自我确定性（self-certainty）: **自我确定性 (Self-Certainty)**: 自我确定性衡量的是模型输出概率分布的“自信程度”。它通常通过计算模型在每个生成步骤的输出概率分布与一个均匀分布之间的KL散度（Kullback-Leibler Divergence）来量化。如果模型的输出概率高度集中在少数几个词元上（即“尖锐”分布），那么它与均匀分布的KL散度就大，表明模型对此步生成非常“确定”。反之，如果概率分布较为平坦，则KL散度小，表示模型较为“犹豫”。RLIF的目标是最大化这种自我确定性，即鼓励模型生成它“认为”最可能的序列。 其奖励函数可以表示为：
  $$
  r_{\text{self-certainty}}(x, y) = \frac{1}{|y|} \sum_{t=1}^{|y|} D_{\text{KL}}(U \parallel \pi_{\theta}(\cdot | x, y_{<t}))
  $$
  其中 $y$ 是生成的序列， $|y|$ 是序列长度，$x$ 是输入，$\pi_{\theta}$ 是策略，$U$ 是一个均匀分布

* **词元级熵 (Token-Level Entropy)**: 熵是信息论中衡量不确定性的经典指标。词元级熵计算的是模型在每个生成步骤的输出概率分布的熵。低熵意味着低不确定性（分布尖锐），高熵则意味着高不确定性（分布平坦）。在RLIF中，通常使用**负熵**作为奖励，即最小化每一步生成的不确定性。 其奖励函数为：
  $$
  r_{\text{token-entropy}}(x, y) = \frac{-1}{|y|} \sum_{t=1}^{|y|} H(\pi_{\theta}(\cdot | x, y_{<t}))
  $$
  $H(x)$ 是香农熵，表示信息不确定性，即 $H(x) = -\sum_{i=1}^{n}P(x_i)log_2P(x_i)$

* **轨迹级熵 (Trajectory-Level Entropy)**: 与词元级熵关注每一步不同，轨迹级熵衡量的是整个生成序列 $y$ 的总概率（或对数概率）。一个序列的整体概率越高，其轨迹级熵（这里指负对数概率）就越低。RLIF的目标是最大化整个序列的概率，等价于最小化轨迹级熵。 其奖励函数可以表示为（经过归一化）：
  $$
  r_{\text{traj-entropy}}(x, y) = \frac{1}{|y|} \log \pi_{\theta}(y | x)
  $$

以上三种方式的本质都是最小化策略熵，通过实验可以看出，会出现性能先升后降的问题，在早期（通常前20个时间步）效果会有提升，后面会慢慢降低，可能是因为策略熵降低导致模型过度自信。

研究人员引入了“过渡词”（transitional words）的概念。这些词，如 "但是 (but)"、"等等 (wait)"、"让我检查一下 (let me check...)"、"或者 (alternatively)" 等，在复杂推理过程中扮演着至关重要的角色。它们通常标志着模型正在进行自我修正、探索不同思路或对中间步骤进行反思。这些词汇在生成时通常伴随着较高的熵，因为模型在这些节点上正处于决策的不确定状态。

实验分析发现，随着RLIF训练的进行（即策略熵的不断降低），这些关键的“过渡词”出现的频率显著下降。模型不再“犹豫”和“反思”，而是倾向于生成一条直接、简短、看似自信的推理路径。

**RLIF 成功的关键在于模型初始的高熵分布，对于一些instruct模型，初始时熵已经比较低，则会破坏原来的推理轨迹，所以一般在 base 模型直接进行强化学习训练，跳过冷启动阶段**

#### 4. RLPR

RLPR 的思路基于这样一个基本的洞察：**LLM 生成正确自由形式答案的内在概率，直接反映了其自身对推理质量的评估。** 这个思路其实在最近的一些研究中经常出现。

为什么 LLM 的「内在概率」能作为奖励信号？

1. **「自信」的体现：** LLM 在生成每一个 token 时，都会计算出其在当前上下文中最有可能出现的下一个 token 及其概率。如果 LLM 在生成一段推理过程 $z$ 之后，能够以**高概率**继续生成出与**标准答案** $y^*$ 高度吻合的 token 序列，这说明 LLM 内部的知识和逻辑流与标准答案是高度一致的，它对自己的推理结果「很有信心」。这种「信心」正是其推理过程高质量的体现。
2. **对语义和逻辑的捕捉：** LLM 通过海量数据预训练，已经掌握了复杂的语言模式、语义关系和世界知识。当它生成一个 token 时，其概率分布反映了该 token 在当前语境下的「合理性」。如果一个推理过程 $z$ 能够有效地引导模型走向语义和逻辑都正确的方向，那么后续生成标准答案的 token 概率自然会很高。
3. **细粒度评估：** 传统的外部验证器通常只能给出「对」或「错」的二元奖励。而 LLM 的内在概率是连续的，可以提供**更细粒度的奖励**。即使模型生成的答案并非完美，但如果大部分 token 与标准答案语义接近，它也能获得一个相对较高的平均概率，从而得到部分奖励。这有助于模型在学习过程中获得更丰富的梯度信号，从而更好地优化其推理能力。

举个例子，假设问题是：`请用一句话描述什么是光合作用？` **标准答案 $y^* $ ：** `光合作用是植物利用光能将二氧化碳和水转化为有机物并释放氧气的过程。`

- **场景 A：LLM 进行了高质量推理**
  - LLM 内部经过一系列思考（例如，先想到「植物」，再想到「光能」，再想到「二氧化碳和水」，最后想到「有机物」和「氧气」），生成了推理过程  $z$，然后输出了答案。
  - 当我们将标准答案 $y^* $ 喂给它，让它计算生成这些 token 的概率时，由于其内部状态与正确答案高度匹配，它会以极高的概率生成 中的每一个 token。例如，在生成「氧气」这个词时，模型会觉得这个词出现的概率非常高。
  - 所有 token 概率的平均值会很高，奖励也很高。
- **场景 B：LLM 进行了低质量推理**
  - LLM 的推理过程 $z^*$ 混乱，或者思考方向错误（例如，它想到了动物呼吸）。
  - 当我们将标准答案 $y*$ 喂给它，让它计算生成这些 token 的概率时，由于其内部状态与正确答案不匹配，它会觉得生成 中的某些 token（例如「植物」、「二氧化碳」）的概率非常低。
  - 所有 token 概率的平均值会很低，奖励也很低。

也就是说，RLPR 将 LLM 自身作为「奖励评估者」，从而完全摆脱了对外部验证器的依赖。

![image-20251129201508064](assets/image-20251129201508064.png)

主要技术：

**奖励的基准分数**

LLM 生成答案 $y^* $ 是一系列 token $y_1^*$，$y_2^*$，……，$y_N^*$ 的序列，每个token都有一个输出概率 $p_i$，RLPR 选了一个鲁棒的聚合方式，直接计算所有 token 概率平均值，即 $r = \frac{1}{N}\sum_{i=1}^{N}p_i$ ，这种方法对低概率不敏感，如果直接采用序列似然即将概率相乘然后开N次根那么会对低概率很敏感。

即使我们使用了平均概率，计算出的奖励 $r$ 仍然可能包含「水分」。这个「水分」来源于问题 $Q$ 和参考答案 $y$ 本身的特性，而不是 LLM 推理过程 $z$ 带来的真正贡献。

例如，对于一个非常简单、常识性的问题，LLM 可能无需任何复杂推理，就能够极高概率直接给出正确答案。这种高概率答案模式呈现出“先验知识”的体貌，并且其「推理能力」的体现。我们希望奖励函数激励的是模型通过推理带来的能力提升。

RLPR 引入了一个基准分数 $r'$，它的计算方式是：在没有中间推理 $z$ 的情况下，模型直接生成参考答案 $y$ 的概率。

也就是说，我们给 LLM 看问题 $Q$ 和标准答案 $y^*$，让它计算生成 $y^*$ 的概率。这个 $r'$ 就代表 LLM 在没有任何额外推理帮助下，仅凭其预训练知识对答案案的「基准分」或「先验概念」。

然后，RLPR 的去偏移 $r$ 定义为：

$$
\hat{r} = \text{clip}(0, 1, r - r')
$$

这里，clip 是一个裁剪函数，将结果限制在 $[0, 1]$ 之间。

这个 $\hat{r}$ 衡量的是：LLM 在生成推理 $z$ 之后，生成正确答案 $y^* $ 的概率相比没有推理 $z$ 时提得多多少。这是我们旨在激励的“推理能力”。

**标准差过滤**

强化学习训练本身就容易不稳定。同时，不是所有训练样本都对模型学习有益。

- **太简单的样本**：模型已经轻松给出正确答案，每次都能够得到高奖励，奖励标准差很小。继续训练这些样本，模型可能仍然学习不到有用的东西，甚至可能超出模型当前的能力范围，或者奖励值与其预测结果偏差过大，无法提供有效帮助。强化学习可能影响模型的效果。
  
- **太难的样本**：模型目前完全无法给出正确答案，每次都得到低奖励，奖励标准差很大。这些样本本可能超出模型目前的能力范围，或者奖励值与其预测结果偏差过大，无法提供有效帮助。强化学习可能影响模型的效果。

传统的 RLVR 可以使用“准概率过渡”，即过渡时完全对或完全错的样本。但 RLPR 的奖励率是连续的，无法直接使用这种二元过渡。

RLPR 的启示是：如果一个提示 (prompt) 对应的奖励值标准差很低，这意味着模型对这个提示的响应要么总是好（太简单），要么总是差（太难）。这些样本提供的学习信号弱，应当减少训练。

更进一步，RLPR 引入了**自适应阈值**：

- 过渡阈值 $\beta$ 不是固定的，而是动态调整的。

- 它使用指数移动平均 (EMA) 来跟踪训练过程中所有样本奖励标准差的整体分布。

这样，训练策略的自适应调整，始终让模型在“有挑战性但可学习”的样本上进行训练。通过这种“自适应增强学习”机制，RLPR 能够高效地利用训练数据，将计算资源用于模型能够做出更清晰信号的样本上，从而显著提升训练的稳定性和最终性能。

### 六、verl框架

> 安装时特别注意 [flash-attn](https://github.com/kingbri1/flash-attention/releases) 版本， 根据 torch ，cuda 和 pip debug（查看python版本）三部分组成

#### 1. 常用训练脚本

1. 在 $vllm$ 中需要设置 `rollout.mode=sync`，也就是下面两个配置要同时出现

   ```bash
   actor_rollout_ref.rollout.name=vllm \
   actor_rollout_ref.rollout.mode=sync \
   ```

2. `tensor_model_paraller_size` 设为和 GPU 卡数一样，比如模型的某个矩阵维度是 512 维，那么每张卡就是放 128 个参数

3. `data.train.batch_size` 是一轮的数据量 ，`mini_batch_size` 就是把一轮 `batch_size` 分成多次训练，然后梯度叠加。比如 `batch_size` 是1024，`mini_batch_size` 是256，那么就是分成四次训练，然后不是每次都更新梯度，而是累加后更新。主要是为了减少显存占用。

   `data.train.batch_size * actor_rollout.ref.rollout.n` 是总共的轨迹数，一个样本被用来 `actor_rollout.ref.rollout.n` 次更新

4. `prompt `必须是 `message_list `格式也就是 ` {"role":"user","content":"hello"}` 这种格式

5. 对于数学问题 `response_length `可以适当扩大，防止推理被截断。`max_num_batched_tokens = max_prompt_length + max_response_length`

   `data.filter_overlong_prompts` 为是否过滤长提示词，即超过最大长度的提示词会被过滤掉，`truncation` 就是输入超限出现截断的时候会报错

6. `enforce_eager=true` 减少显存但是会变慢

7. `per_gpu` 的参数计算方式为 `batch_size * gpu`
8. `fsdp_config.param_offload=True` 移动参数到 cpu，也是节省显存的

9. KL 散度，第一个参数为是否在 actor 里面使用 KL 散度，GRPO 默认是 True，第二个参数为 KL 散度在总损失函数的系数，第三个参数是 KL 散度的类型

   ```bash
   actor_rollout_ref.actor.use_kl_loss=True \
   actor_rollout_ref.actor.kl_loss_coef=0.001 \
   actor_rollout_ref.actor.kl_loss_type=low_var_kl
   ```

10. `actor_rollout_ref.rollout.gpu_memory_utilization` 这里是告诉 vllm 模型加载和推理缓存可以占用显存的比例

11. `actor_rollout.ref.rollout.n` 是指对一个 prompt 采样几次，GRPO 一般选大于 1 的数字

下面是一个具体的例子

```bash
# Tested successfully on the hiyouga/verl:ngc-th2.6.0-cu126-vllm0.8.4-flashinfer0.2.2-cxx11abi0 image.
# It outperforms the Qwen2 7B base model by two percentage points on the test set of GSM8K.

set -x

per_gpu=1

python3 -m verl.trainer.main_ppo \
    algorithm.adv_estimator=grpo \
    data.train_files=/mnt/public/wwj/zhaozq/verl/data/gsm8k/train.parquet \
    data.val_files=/mnt/public/wwj/zhaozq/verl/data/gsm8k/test.parquet \
    data.train_batch_size=1024 \
    data.max_prompt_length=512 \
    data.max_response_length=1024 \
    data.filter_overlong_prompts=True \
    data.truncation='error' \
    actor_rollout_ref.model.path=/mnt/public/wwj/zhaozq/model/Qwen/Qwen3-8B \
    actor_rollout_ref.actor.optim.lr=1e-6 \
    actor_rollout_ref.model.use_remove_padding=True \
    actor_rollout_ref.actor.ppo_mini_batch_size=256 \
    actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=${per_gpu} \
    actor_rollout_ref.actor.use_kl_loss=True \
    actor_rollout_ref.actor.kl_loss_coef=0.001 \
    actor_rollout_ref.actor.kl_loss_type=low_var_kl \
    actor_rollout_ref.actor.entropy_coeff=0 \
    actor_rollout_ref.model.enable_gradient_checkpointing=True \
    actor_rollout_ref.actor.fsdp_config.param_offload=False \ 
    actor_rollout_ref.actor.fsdp_config.optimizer_offload=False \
    actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=${per_gpu} \
    actor_rollout_ref.rollout.tensor_model_parallel_size=4 \
    actor_rollout_ref.rollout.name=vllm \
    actor_rollout_ref.rollout.mode=sync \  
    actor_rollout_ref.rollout.gpu_memory_utilization=0.6 \
    actor_rollout_ref.rollout.n=4 \
    actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=${per_gpu} \
    actor_rollout_ref.ref.fsdp_config.param_offload=True \
    algorithm.use_kl_in_reward=False \
    trainer.critic_warmup=0 \
    trainer.logger='["console"]' \
    trainer.project_name='verl_grpo_example_gsm8k' \
    trainer.experiment_name='qwen3_8b_function_rm' \
    trainer.n_gpus_per_node=4 \
    trainer.nnodes=1 \
    trainer.save_freq=20 \
    trainer.test_freq=5 \
    trainer.total_epochs=15 $@
```

```bash
#!/usr/bin/env bash
set -xeuo pipefail

## !!!!!!!important!!!!!!
## set the following environment variables on all your nodes
# env_vars:
#   CUDA_DEVICE_MAX_CONNECTIONS: "1"
#   NCCL_NVLS_ENABLE: "0"
#   VLLM_USE_V1: 1
# install mbridge=0.1.13 on all your node with the following command:
# pip3 install git+https://github.com/ISEEKYAN/mbridge




enable_overlong_buffer=True
overlong_buffer_len=$((1024 * 1))
overlong_penalty_factor=1.0

loss_agg_mode="token-mean"

train_prompt_bsz=${TRAIN_BS:-32}
n_resp_per_prompt=8
train_prompt_mini_bsz=16

# minimum nodes need for qwen3-235B-A22B
NNODES=${NNODES:-8}
# Paths

RAY_DATA_HOME=${RAY_DATA_HOME:-"${HOME}/verl"}

MODEL_PATH=$RAY_DATA_HOME/models/Qwen3-235B-A22B

# Algorithm
temperature=1.0
top_p=1.0
top_k=-1 # 0 for HF rollout, -1 for vLLM rollout
val_top_p=0.7
# Performance Related Parameter
use_dynamic_bsz=True
actor_ppo_max_token_len=$((16384 * 10 / 10))
infer_ppo_max_token_len=$((16384  * 1))
offload=True
OPTIM_OFFLOAD=${OPTIM_OFFLOAD:-False}
gen_tp=16
train_tp=${TP:-4}
train_pp=${PP:-8}

EP=${EP:-8}
ETP=1
CP=1
optimizer_offload_fraction=${OFFLOAD_FRACTION:-0.5}
last_layer=${LAST_LAYER:-10}

project_name='verl-qwen3'
exp_name="235B-${NNODES}-pp${train_pp}-tp${train_tp}-ep${EP}-actor-length${actor_ppo_max_token_len}"
CKPTS_DIR="/vllm-workspace/models/qiming3_RL_235B_debug"

# TODO: support cuda graph for rollout by setting the following config
    # actor_rollout_ref.rollout.cudagraph_capture_sizes=[1,2,4,8,16,32]
    # actor_rollout_ref.rollout.enforce_eager=False

python3 -m verl.trainer.main_ppo \
    --config-path=config \
    --config-name='ppo_megatron_trainer.yaml' \
    data.train_files="/vllm-workspace/verl-main/data/math_tele_data.parquet" \
    data.val_files="/vllm-workspace/verl-main/data/math_tele_data_val.parquet" \
    custom_reward_function.path=/vllm-workspace/verl-main/verl/utils/reward_score/chinatele_reward.py \
    custom_reward_function.name=compute_score \
    data.prompt_key=prompt \
    data.truncation='left' \
    data.max_prompt_length=1000 \
    data.max_response_length=10 \
    data.train_batch_size=4 \
    actor_rollout_ref.rollout.n=16 \
    actor_rollout_ref.rollout.name=vllm \
    actor_rollout_ref.rollout.enforce_eager=True \
    algorithm.adv_estimator=grpo \
    algorithm.use_kl_in_reward=False \
    algorithm.kl_ctrl.kl_coef=0.0 \
    actor_rollout_ref.model.use_fused_kernels=True \
    actor_rollout_ref.actor.megatron.us	e_mbridge=True \
    actor_rollout_ref.actor.use_kl_loss=True \
    actor_rollout_ref.actor.kl_loss_coef=0.001 \
    actor_rollout_ref.actor.clip_ratio_low=0.2 \
    actor_rollout_ref.actor.clip_ratio_high=0.28 \
    actor_rollout_ref.actor.clip_ratio_c=10.0 \
    actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=1 \
    actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=1 \
    actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=1 \
    actor_rollout_ref.actor.use_dynamic_bsz=False \
    actor_rollout_ref.ref.log_prob_use_dynamic_bsz=False \
    actor_rollout_ref.rollout.log_prob_use_dynamic_bsz=False \
    actor_rollout_ref.actor.ppo_max_token_len_per_gpu=16384 \
    actor_rollout_ref.ref.log_prob_max_token_len_per_gpu=16384 \
    actor_rollout_ref.rollout.log_prob_max_token_len_per_gpu=16384 \
    actor_rollout_ref.model.path="/vllm-workspace/models/Qwen3-235B-A22B-Thinking-2507" \
    actor_rollout_ref.actor.optim.lr=1e-6 \
    actor_rollout_ref.actor.optim.lr_warmup_steps=10 \
    actor_rollout_ref.actor.optim.weight_decay=0.1 \
    +actor_rollout_ref.actor.optim.override_optimizer_config.optimizer_offload_fraction=${optimizer_offload_fraction} \
    +actor_rollout_ref.actor.optim.override_optimizer_config.overlap_cpu_optimizer_d2h_h2d=True \
    +actor_rollout_ref.actor.optim.override_optimizer_config.use_precision_aware_optimizer=True \
    +actor_rollout_ref.actor.optim.override_optimizer_config.optimizer_cpu_offload=True \
    actor_rollout_ref.actor.ppo_mini_batch_size=4 \
    actor_rollout_ref.actor.megatron.param_offload=${offload} \
    actor_rollout_ref.actor.megatron.optimizer_offload=${OPTIM_OFFLOAD} \
    actor_rollout_ref.actor.megatron.grad_offload=${offload} \
    actor_rollout_ref.actor.megatron.pipeline_model_parallel_size=${train_pp} \
    actor_rollout_ref.actor.megatron.tensor_model_parallel_size=${train_tp} \
    actor_rollout_ref.actor.megatron.expert_model_parallel_size=$EP \
    actor_rollout_ref.actor.megatron.expert_tensor_parallel_size=$ETP \
    actor_rollout_ref.actor.megatron.context_parallel_size=${CP} \
    actor_rollout_ref.actor.entropy_coeff=0 \
    actor_rollout_ref.actor.optim.clip_grad=1.0 \
    actor_rollout_ref.actor.loss_agg_mode=${loss_agg_mode} \
    actor_rollout_ref.rollout.gpu_memory_utilization=0.85 \
    actor_rollout_ref.rollout.tensor_model_parallel_size=16 \
    actor_rollout_ref.rollout.enable_chunked_prefill=True \
    actor_rollout_ref.rollout.max_num_batched_tokens=16384 \
    actor_rollout_ref.rollout.temperature=${temperature} \
    actor_rollout_ref.rollout.top_p=${top_p} \
    actor_rollout_ref.rollout.top_k=${top_k} \
    actor_rollout_ref.rollout.max_model_len=16384 \
    actor_rollout_ref.rollout.name=vllm \
    actor_rollout_ref.rollout.ignore_eos=False \
    actor_rollout_ref.rollout.dtype=bfloat16 \
    actor_rollout_ref.rollout.enforce_eager=true\
    actor_rollout_ref.nccl_timeout=1200 \
    actor_rollout_ref.rollout.val_kwargs.temperature=${temperature} \
    actor_rollout_ref.rollout.val_kwargs.top_p=${val_top_p} \
    actor_rollout_ref.rollout.val_kwargs.top_k=${top_k} \
    actor_rollout_ref.rollout.val_kwargs.do_sample=True \
    actor_rollout_ref.rollout.val_kwargs.n=1 \
    actor_rollout_ref.ref.megatron.pipeline_model_parallel_size=${train_pp} \
    actor_rollout_ref.ref.megatron.tensor_model_parallel_size=${train_tp} \
    actor_rollout_ref.ref.megatron.expert_model_parallel_size=$EP \
    actor_rollout_ref.ref.megatron.expert_tensor_parallel_size=$ETP \
    actor_rollout_ref.ref.megatron.context_parallel_size=${CP} \
    actor_rollout_ref.ref.megatron.param_offload=True \
    +actor_rollout_ref.actor.megatron.override_transformer_config.apply_rope_fusion=False \
    +actor_rollout_ref.actor.megatron.override_transformer_config.moe_router_dtype=fp32 \
    +actor_rollout_ref.actor.megatron.override_transformer_config.moe_shared_expert_overlap=False \
    +actor_rollout_ref.actor.megatron.override_transformer_config.moe_enable_deepep=True \
    +actor_rollout_ref.actor.megatron.override_transformer_config.moe_token_dispatcher_type=flex \
    +actor_rollout_ref.actor.megatron.override_transformer_config.recompute_method=uniform \
    +actor_rollout_ref.actor.megatron.override_transformer_config.recompute_granularity=full \
    +actor_rollout_ref.actor.megatron.override_transformer_config.recompute_num_layers=1 \
    +actor_rollout_ref.actor.megatron.override_transformer_config.gradient_accumulation_fusion=True \
    +actor_rollout_ref.actor.megatron.override_transformer_config.moe_permute_fusion=True \
    +actor_rollout_ref.actor.megatron.override_transformer_config.account_for_embedding_in_pipeline_split=False \
    +actor_rollout_ref.actor.megatron.override_transformer_config.account_for_loss_in_pipeline_split=False \
    +actor_rollout_ref.actor.megatron.override_transformer_config.num_layers_in_last_pipeline_stage=${last_layer} \
    reward_model.reward_manager=naive \
    trainer.logger=['console'] \
    trainer.project_name="${project_name}" \
    trainer.experiment_name="${exp_name}" \
    trainer.n_gpus_per_node=8 \
    trainer.nnodes="${NNODES}" \
    trainer.val_before_train=False \
    trainer.test_freq=100 \
    trainer.save_freq=100 \
    trainer.total_epochs=10 \
    trainer.default_local_dir="${CKPTS_DIR}" \
    trainer.resume_mode=auto \
    trainer.log_val_generations=10 $@
```

#### 2. 数据集

在 verl 脚本中需要的数据格式为 parquet 格式，即按照列去存储，有点类似于 Excel 表格的形式

转换为 parquet 的格式，注意代码样例在 `examples/data_preprocess/gsm8k.py`

```python
train_dataset = dataset["train"]
test_dataset = dataset["test"]

instruction_following = 'Let\'s think step by step and output the final answer after "####".'

# add a row to each data item that represents a unique id
def make_map_fn(split):
    def process_fn(example, idx):
        question_raw = example.pop("question")

        question = question_raw + " " + instruction_following

        answer_raw = example.pop("answer")
        solution = extract_solution(answer_raw)
        data = {
            "data_source": data_source,
            "prompt": [
                {
                    "role": "user",
                    "content": question,
                }
            ],
            "ability": "math",
            "reward_model": {"style": "rule", "ground_truth": solution},
            "extra_info": {
                "split": split,
                "index": idx,
                "answer": answer_raw,
                "question": question_raw,
            },
        }
        return data

    return process_fn

train_dataset = train_dataset.map(function=make_map_fn("train"), with_indices=True)
test_dataset = test_dataset.map(function=make_map_fn("test"), with_indices=True)
```

然后利用命令行转换

```python
python3 verl/examples/data_preprocess/gsm8k.py --local_dir ./data/gsm8k
# 同样的，对于Math数据集，也可以使用
python3 verl/examples/data_preprocess/math_dataset.py --local_dir ./data/math
```

查看验证方法

```bash
python
import pandas as pd
hf = pd.read_parquet("file")
hf.head()
```

#### 3. 核心奖励流程分析

##### 3.1 重写 default 

在 `verl/utils/reward_score/__init__.py` 中修改 `default_compute_score` 方法，改成我们自己的方法即可

如果是自己训练的生成式奖励模型，可以通过发送 $API$ 请求到通过 $vllm$ 部署的模型。可以包装一个类放在这个文件里面专门用于请求

##### 3.2 重写 reward_manager 或者重写 reward_score

在 PPO 训练脚本 `main_ppo.py` 的入口点，实现了一个 `RewardManager` ，它利用预先实现的奖励函数来计算每个响应的分数

```python
reward_fn = load_reward_manager(config, tokenizer, num_examine=0, **config.reward_model.get("reward_kwargs", {})) # 159行
val_reward_fn = load_reward_manager(config, tokenizer, num_examine=1)
```

在 `verl/verl/trainer/ppo/reward.py` 下的 `load_reward_manager` 是加载奖励函数的具体实现：

```python
def load_reward_manager(
    config: DictConfig, tokenizer: Any, num_examine: int, **reward_kwargs: Any
) -> AbstractRewardManager:
    """
    Load and initialize a reward manager based on the configuration.

    Args:
        config: PPO trainer configuration object containing reward_model fields.
        tokenizer: Tokenizer object used for processing text.
        num_examine: Number of samples to examine.
        **reward_kwargs: Additional keyword arguments for the reward manager.

    Returns:
        An instance of the specified reward manager class.
    """

    # Try to get a custom reward function based on the configuration
    # user defined reward manager can be registered in custom_reward_fn
    compute_score = get_custom_reward_fn(config)
    final_compute_score = compute_score

    # The list of pre-defined reward managers are defined in `verl/workers/reward_manager/`:
    # naive: NaiveRewardManager
    # prime: PrimeRewardManager
    # batch: BatchRewardManager
    # dapo: DAPORewardManager
    # Note(haibin.lin): For custom reward managers, please make sure they are imported and
    # registered via `verl.workers.reward_manager.register`
    # By default reward_manager is set to naive (NaiveRewardManager)
    reward_manager_name = config.reward_model.get("reward_manager", "naive")
    reward_manager_cls = get_reward_manager_cls(reward_manager_name)

    if compute_score is None:
        sandbox_config = config.reward_model.get("sandbox_fusion")
        sandbox_url = sandbox_config.get("url") if sandbox_config else None
        memory_limit_mb = sandbox_config.get("memory_limit_mb", 1024) if sandbox_config else 1024
        if sandbox_url:
            sandbox_manager = multiprocessing.Manager()
            # Create a semaphore to control concurrent access to the sandbox
            _concurrent_semaphore = sandbox_manager.Semaphore(sandbox_config.get("max_concurrent", 64))
            final_compute_score = partial(
                default_compute_score,
                sandbox_fusion_url=sandbox_url,
                concurrent_semaphore=_concurrent_semaphore,
                memory_limit_mb=memory_limit_mb,
            )
        else:
            final_compute_score = default_compute_score

    # Instantiate and return the reward manager with the specified parameters
    # RewardLoopManagerBase subclasses (like RateLimitedRewardLoopManager) don't accept num_examine
    # while AbstractRewardManager subclasses (like NaiveRewardManager) do
    if RewardLoopManagerBase is not None and issubclass(reward_manager_cls, RewardLoopManagerBase):
        # RewardLoopManagerBase-based managers use a different signature
        return reward_manager_cls(
            config=config,
            tokenizer=tokenizer,
            compute_score=final_compute_score,
            **reward_kwargs,
        )
    else:
        # Traditional AbstractRewardManager-based managers
        return reward_manager_cls(
            tokenizer=tokenizer,
            num_examine=num_examine,
            compute_score=final_compute_score,
            reward_fn_key=config.data.reward_fn_key,
            **reward_kwargs,
        )
```

下面是逐段解释

1. 函数签名 (118-120行)

   `def load_reward_manager(  *config*: DictConfig, *tokenizer*: Any, *num_examine*: int, ***reward_kwargs*: Any) -> AbstractRewardManager:`

- config: 配置对象，包含奖励模型相关设置

- tokenizer: 分词器

- num_examine: 检查样本数量

- **reward_kwargs: 额外关键字参数

2. 加载自定义奖励函数 (134-137行)

   `compute_score = get_custom_reward_fn(config)`

   `final_compute_score = compute_score`

- 尝试从配置加载自定义奖励函数（如示例中的 chinatele_reward.py），这个对应启动脚本的下面部分

```python
custom_reward_function.path=/vllm-workspace/verl-main/verl/utils/reward_score/chinatele_reward.py \
custom_reward_function.name=compute_score \
```

- 若未配置，compute_score 为 None

3. 获取奖励管理器类 (139-148行)

   `reward_manager_name = config.reward_model.get("reward_manager", "naive")`

   `reward_manager_cls = get_reward_manager_cls(reward_manager_name)`

- 从配置读取管理器类型（默认 "naive"）

- 预定义类型：naive、prime、batch、dapo

- 通过 get_reward_manager_cls 获取对应的类
- 也可以自己设置奖励管理器类，对应启动脚本的下面部分

```bash
reward_model.reward_manager=prob 
```

4. 设置默认奖励计算函数 (150-165行)

如果没有自定义奖励函数，则设置默认函数：

```python
if compute_score is None:
    sandbox_config = config.reward_model.get("sandbox_fusion")
    sandbox_url = sandbox_config.get("url") if sandbox_config else None
    memory_limit_mb = sandbox_config.get("memory_limit_mb", 1024) if sandbox_config else 1024
    if sandbox_url:
        sandbox_manager = multiprocessing.Manager()
        # Create a semaphore to control concurrent access to the sandbox
        _concurrent_semaphore = sandbox_manager.Semaphore(sandbox_config.get("max_concurrent", 64))
        final_compute_score = partial(
            default_compute_score,
            sandbox_fusion_url=sandbox_url,
            concurrent_semaphore=_concurrent_semaphore,
            memory_limit_mb=memory_limit_mb,
        )
    else:
        final_compute_score = default_compute_score
```

- 若配置了 sandbox_fusion，创建带并发控制的沙箱版本

- 否则使用 default_compute_score

5. 实例化奖励管理器 (167-186行)

根据管理器类型选择不同的初始化方式：

方式一：RewardLoopManagerBase 子类 (170-177行)

```python
if RewardLoopManagerBase is not None and issubclass(reward_manager_cls, RewardLoopManagerBase):
        # RewardLoopManagerBase-based managers use a different signature
        return reward_manager_cls(
            config=config,
            tokenizer=tokenizer,
            compute_score=final_compute_score,
            **reward_kwargs,
        )
```

- 传入 config，不传 num_examine

方式二：传统 AbstractRewardManager 子类 (178-186行)

```python
else:
    # Traditional AbstractRewardManager-based managers
    return reward_manager_cls(
        tokenizer=tokenizer,
        num_examine=num_examine,
        compute_score=final_compute_score,
        reward_fn_key=config.data.reward_fn_key,
        **reward_kwargs,
    )
```

- 传入 num_examine 和 reward_fn_key

在 `RewardManager` 中，实现了一个 `__call__` 函数来计算每个回复的分数

```python
 def __call__(self, data: DataProto, return_dict: bool = False) -> torch.Tensor | dict[str, Any]:
        """We will expand this function gradually based on the available datasets"""

        # If there is rm score, we directly return rm score. Otherwise, we compute via rm_score_fn
        if "rm_scores" in data.batch.keys():
            if return_dict:
                reward_extra_keys = data.meta_info.get("reward_extra_keys", [])
                reward_extra_info = {key: data.non_tensor_batch[key] for key in reward_extra_keys}
                return {"reward_tensor": data.batch["rm_scores"], "reward_extra_info": reward_extra_info}
            else:
                return data.batch["rm_scores"]

        reward_tensor = torch.zeros_like(data.batch["responses"], dtype=torch.float32)
        reward_extra_info = defaultdict(list)

        already_print_data_sources = {}

        for i in range(len(data)):
            data_item = data[i]  # DataProtoItem

            prompt_ids = data_item.batch["prompts"]

            prompt_length = prompt_ids.shape[-1]

            valid_prompt_length = data_item.batch["attention_mask"][:prompt_length].sum()
            valid_prompt_ids = prompt_ids[-valid_prompt_length:]

            response_ids = data_item.batch["responses"]
            valid_response_length = data_item.batch["attention_mask"][prompt_length:].sum()
            valid_response_ids = response_ids[:valid_response_length]

            # decode
            prompt_str = self.tokenizer.decode(valid_prompt_ids, skip_special_tokens=True)
            response_str = self.tokenizer.decode(valid_response_ids, skip_special_tokens=True)

            ground_truth = data_item.non_tensor_batch["reward_model"]["ground_truth"]
            data_source = data_item.non_tensor_batch[self.reward_fn_key]
            extra_info = data_item.non_tensor_batch.get("extra_info", {})
            num_turns = data_item.non_tensor_batch.get("__num_turns__", None)
            rollout_reward_scores = data_item.non_tensor_batch.get("reward_scores", {})
            extra_info["num_turns"] = num_turns
            extra_info["rollout_reward_scores"] = rollout_reward_scores

            score = self.compute_score(
                data_source=data_source,
                solution_str=response_str,
                ground_truth=ground_truth,
                extra_info=extra_info,
            )

            if isinstance(score, dict):
                reward = score["score"]
                # Store the information including original reward
                for key, value in score.items():
                    reward_extra_info[key].append(value)
            else:
                reward = score

            reward_tensor[i, valid_response_length - 1] = reward

            if data_source not in already_print_data_sources:
                already_print_data_sources[data_source] = 0

            if already_print_data_sources[data_source] < self.num_examine:
                already_print_data_sources[data_source] += 1
                print("[prompt]", prompt_str)
                print("[response]", response_str)
                print("[ground_truth]", ground_truth)
                if isinstance(score, dict):
                    for key, value in score.items():
                        print(f"[{key}]", value)
                else:
                    print("[score]", score)

        if return_dict:
            return {
                "reward_tensor": reward_tensor,
                "reward_extra_info": reward_extra_info,
            }
        else:
            return reward_tensor
```

也可以通过 reward 模型来计算奖励，可以配置下面的项

```python
reward_model.enable=True \
reward_model.model.path="./models/FsfairX-LLaMA3-RM-v0.1" \
reward_model.model.use_remove_padding=True \
reward_model.model.fsdp_config.param_offload=True \
reward_model.micro_batch_size_per_gpu=32 \
```

#### 4. 模型保存与还原

GRPO 过程结束后，verl 保存的 checkpoint 并不是 huggingface的形式，而是同优化器状态一起保存，用下面的代码可以还原为 Huggingface 的格式

```python
from typing import List, Tuple, Dict
import re
import os
import torch
import argparse
from transformers import AutoConfig, AutoModelForCausalLM, AutoModelForTokenClassification, AutoTokenizer
from concurrent.futures import ThreadPoolExecutor
from torch.distributed._tensor import DTensor, Shard, Placement


def merge_by_placement(tensors: List[torch.Tensor], placement: Placement):
    if placement.is_replicate():
        return tensors[0]
    elif placement.is_partial():
        raise NotImplementedError("Partial placement is not supported yet")
    elif placement.is_shard():
        return torch.cat(tensors, dim=placement.dim).contiguous()
    else:
        raise ValueError(f"Unsupported placement: {placement}")


if __name__ == '__main__':
    step = 100
    local_dir = f"output/Qwen2.5-7B-GRPO/global_step_{step}/actor"  # 这里需要替换为绝对路径
    hf_path = f"output/Qwen2.5-7B-GRPO/global_step_{step}/actor/huggingface"  # 这里需要替换为绝对路径
    output_path = f"models/Qwen2.5-7B-Instruct-GRPO"  # 这里需要替换为绝对路径

    # copy rank zero to find the shape of (dp, fsdp)
    rank = 0
    world_size = 0
    for filename in os.listdir(local_dir):
        match = re.match(r"model_world_size_(\d+)_rank_0\.pt", filename)
        if match:
            world_size = match.group(1)  
            break  
    assert world_size, "No model file with the proper format"

    state_dict = torch.load(os.path.join(local_dir, f'model_world_size_{world_size}_rank_{rank}.pt'), map_location='cpu')
    pivot_key = sorted(list(state_dict.keys()))[0]
    weight = state_dict[pivot_key]
    assert isinstance(weight, torch.distributed._tensor.DTensor)
    # get sharding info
    device_mesh = weight.device_mesh
    mesh = device_mesh.mesh
    mesh_dim_names = device_mesh.mesh_dim_names

    print(f'Got device mesh {mesh}, mesh_dim_names {mesh_dim_names}')

    assert mesh_dim_names in (
        ('fsdp',),
    ), f'Unsupported mesh_dim_names {mesh_dim_names}'

    if 'tp' in mesh_dim_names:
        # fsdp * tp
        total_shards = mesh.shape[-1] * mesh.shape[-2]
        mesh_shape = (mesh.shape[-2], mesh.shape[-1])
    else:
        # fsdp
        total_shards = mesh.shape[-1]
        mesh_shape = (mesh.shape[-1],)

    print(f'Processing model shards with {total_shards} {mesh_shape} in total')

    model_state_dict_lst = []
    model_state_dict_lst.append(state_dict)
    model_state_dict_lst.extend([""] * (total_shards - 1))

    def process_one_shard(rank):
        model_path = os.path.join(local_dir, f'model_world_size_{world_size}_rank_{rank}.pt')
        state_dict = torch.load(model_path, map_location='cpu', weights_only=False)
        model_state_dict_lst[rank] = state_dict
        return state_dict

    with ThreadPoolExecutor(max_workers=min(32, os.cpu_count())) as executor:
        for rank in range(1, total_shards):
            executor.submit(process_one_shard, rank)
    state_dict = {}
    param_placements: Dict[str, List[Placement]] = {}
    keys = set(model_state_dict_lst[0].keys())
    for key in keys:
        state_dict[key] = []
        for model_state_dict in model_state_dict_lst:
            try:
                tensor = model_state_dict.pop(key)
            except:
                print("-"*30)
                print(model_state_dict)
            if isinstance(tensor, DTensor):
                state_dict[key].append(tensor._local_tensor.bfloat16())
                placements = tuple(tensor.placements)
                # replicated placement at dp dimension can be discarded
                if mesh_dim_names[0] == 'dp':
                    placements = placements[1:]
                if key not in param_placements:
                    param_placements[key] = placements
                else:
                    assert param_placements[key] == placements
            else:
                state_dict[key] = tensor.bfloat16()

    del model_state_dict_lst

    for key in sorted(state_dict):
        if not isinstance(state_dict[key], list):
            print(f"No need to merge key {key}")
            continue
        # merge shards
        placements: Tuple[Shard] = param_placements[key]
        if len(mesh_shape) == 1:
            # 1-D list, FSDP without TP
            assert len(placements) == 1
            shards = state_dict[key]
            state_dict[key] = merge_by_placement(shards, placements[0])
        else:
            # 2-D list, FSDP + TP
            raise NotImplementedError("FSDP + TP is not supported yet")

    print('Writing to local disk')
    hf_path = os.path.join(local_dir, 'huggingface')
    config = AutoConfig.from_pretrained(hf_path)

    if 'ForTokenClassification' in config.architectures[0]:
        auto_model = AutoModelForTokenClassification
    elif 'ForCausalLM' in config.architectures[0]:
        auto_model = AutoModelForCausalLM
    else:
        raise NotImplementedError(f'Unknown architecture {config["architectures"]}')

    with torch.device('meta'):
        model = auto_model.from_config(config, torch_dtype=torch.bfloat16)
    model.to_empty(device='cpu')

    print(f'Saving model to {output_path}')
    tokenizer = AutoTokenizer.from_pretrained(hf_path)
    tokenizer.save_pretrained(output_path)
    model.save_pretrained(output_path, state_dict=state_dict)
```

#### 5. 采样关键文件

##### 5.1 data.batch字段创建时间线

```
1. Rollout阶段（生成响应）
   ├─ prompts     
   ├─ responses   
   ├─ input_ids  
   ├─ attention_mask
   └─ position_ids

2. 概率计算阶段
   ├─ old_log_probs (标准)

3. 奖励计算阶段
   ├─ token_level_scores
   ├─ token_level_rewards
   ├─ advantages
   └─ returns

4. Critic阶段（如果使用）
   └─ values

5. 参考策略阶段（如果使用）
   └─ ref_log_prob
```

##### 5.2 Rollout 阶段（生成响应）

在 **verl/workers/rollout/** 下面各种类就是实现采样的部分，包括 vllm，hf原生方法等采样策略

##### 5.3 概率计算阶段

这里有计算概率的部分，即上表中的概率计算阶段，可以重写计算方法

1. **verl/workers/fsdp_workers.py**

- 作用：FSDP（Fully Sharded Data Parallel）工作器，管理分布式训练中的 Actor/Critic/Ref 模型

- 功能：

- 使用 PyTorch FSDP 进行模型分片和分布式训练

- 管理 Actor、Critic、Ref 模型的初始化和训练

- 处理模型加载、保存、检查点

- 支持 FSDP1 和 FSDP2

2. **verl/workers/actor/dp_actor.py**

- 作用：数据并行（Data Parallel）Actor 实现

- 功能：

- 基于 FSDP 的数据并行 Actor

- 支持移除 padding、Ulysses 序列并行等优化

- 计算 log probability 和 entropy

- 执行 PPO 更新

##### 5.4 奖励阶段

**1 `token_level_scores` - Token级别奖励分数**

**定义位置：**

- `verl/trainer/ppo/ray_trainer.py` 第1959行

**形状：** `[batch_size, response_length]`

**用途：** 存储每个token位置的奖励分数（通常只有最后一个有效token有非零值）

**创建流程：**

```python
# ray_trainer.py 第1929行
reward_tensor, scoreB_tensor, scoreA_tensor, format_reward_tensor, extracted_answer_list = self.reward_fn(batch)
batch.batch['token_level_scores'] = reward_tensor
```

**2 `token_level_rewards` - Token级别最终奖励**

**定义位置：**

- `verl/trainer/ppo/ray_trainer.py` 第1968行

**形状：** `[batch_size, response_length]`

**用途：** 经过KL惩罚后的最终奖励

**创建流程：**

```python
# ray_trainer.py 第1968行
batch.batch['token_level_rewards'] = batch.batch['token_level_scores']
# 或经过KL惩罚后
batch, kl_metrics = apply_kl_penalty(batch, ...)
```

**3 `advantages` - 优势函数**

**定义位置：**

- `verl/trainer/ppo/ray_trainer.py` 第1971-1977行

**形状：** `[batch_size, response_length]`

**用途：** PPO算法中的优势函数，用于策略更新

**创建流程：**

```python
# ray_trainer.py 第1971行
batch, filter_rate = compute_advantage(batch, ...)
# 在 compute_advantage 函数内部：
batch.batch['advantages'] = advantages
```

**4 `returns` - 回报值**

**定义位置：**

- `verl/trainer/ppo/ray_trainer.py` 第1971-1977行

**形状：** `[batch_size, response_length]`

**用途：** GAE（Generalized Advantage Estimation）计算的回报值

**创建流程：**

```python
# 在 compute_advantage 函数内部：
batch.batch['returns'] = returns
```

#### 6. 源码中的 GPRO 公式实现

> ```mermaid
> graph TD
>     A["ray_trainer.py 中最关键的一行代码 actor_output = self.actor_rollout.update_actor(batch)"] --> B["actor_rollout = all_wg[actor_rollout]"]
>     B --> T["all_wg=resource_pool_to_cls,构建资源和Ray组给每个role分配资源"]
>     T --> U["self.resource_pool_to_cls[resource_pool][actor_rollout]=actor_rollout_cls"]
>     B --> C["actor_rollout = actor_rollout_cls"]
>     U --> C
>     C --> D["self.actor_rollout_wg=actor_rollout_cls=RayClassWithInitArgs(
>     	cls = self.role_worker_mapping[ROLE.ActorRollout]),    config = self.config.actor_rollout_ref,
>     			role = actor_rollout)"]
>     D --> E["config 对应启动脚本的参数"]
>     D --> F["role_worker_mapping 从 Trainer 外层的 main_ppo.py 传入"]
>     F --> G["要找的actor_rollout_cls 就是 ActorRolloutRefWorker"]
>     H["ActorRolloutRefWorker 在 fsdp_workers.py 里面"] --> G
>     G --> I["该类中找到 update_actor 方法，最关键的是 metrics=self.actor.update_policy(data=data)"]
>     I --> J["self.actor 在 init_model() 中 self.actor=DataParellelPPOActor"]
>     J --> K["update_policy 方法 pg_loss=policy_loss
>     		negative_approx_KL = log_prob-old_log_prob
>     		通过这些计算损失函数"]
>     L["在verl/workers/actor/dp_actor.py文件"] --> K
> ```

VeRL中最核心的类就是`RayPPOTrainer`，训练的过程就是他的实例`trainer.fit()`调用`fit()`方法，他们从初始化`init_workers()`开始非常重要！！这也是非常关键的部分。

代码中最核心的一行，也是体现GRPO的`loss`的一行代码，这里就是根据损失函数更新 actor ，应该就是在`fit`中的：

```python
actor_output = self.actor_rollout_wg.update_actor(batch)
```

`update_actor`就是策略模型进行更新，应该对应于这个公式：
$$
\mathcal{L}_{\text{GRPO}}(\theta) = -\frac{1}{G} \sum_{i=1}^G \frac{1}{|\omega_i|} \sum_{t=1}^{|\omega_i|} \left[ \min \left( \frac{\pi_\theta(\omega_{i,t} | q, O_i, < t)}{\pi_{\theta_{\text{old}}}(\omega_{i,t} | q, O_i, < t)}, 1 - \epsilon, 1 + \epsilon \right) \hat{A}_{i,t} - \beta D_{\text{KL}}[\pi_\theta \parallel \pi_{\text{ref}}] \right]
$$
**`actor_rollout_wg` 是什么？**

很明显，这行代码的意思是调用了`actor_rollout_wg`的`update_actor`方法，首先要知道这个变量是什么。全局搜索可以在`init_workers`中找到：

```python
self.actor_rollout_wg = all_wg["actor_rollout"]
```

`all_wg`是什么？继续寻找可以发现：

```python
for resource_pool, class_dict in self.resource_pool_to_cls.items():
    worker_dict_cls = create_colocated_worker_cls(class_dict=class_dict)
    wg_dict = self.ray_worker_group_cls(
        resource_pool=resource_pool,
        ray_cls_with_init=worker_dict_cls,
    )
    spawn_wg = wg_dict.spawn(prefix_set=class_dict.keys())
    all_wg.update(spawn_wg)
```

这段代码就是在创建“资源池”，有一个`resource_pool_to_cls`，这里面存了`resource_pool`和`class_dict`，这样传很多的kv对都绑定在一起，理解起来也可以从代码逻辑上理解，就是把每一块“资源池”和每一个“组”用Ray绑定在一起，最后给各个`role`（例如actor, critic）分配资源，然后实例化放在`all_wg`（all workgroup）里面。那么这些“资源池”和对应的“组件”，一定在前面已经一对一对应了 `self.resource_pool_to_cls` 字典里面了吧。

继续往前找，可以发现：

```python
self.resource_pool_manager.create_resource_pool()
self.resource_pool_to_cls = {pool: {} for pool in self.resource_pool_manager.resource_pool_dict.values()}

# create actor and rollout
if self.hybrid_engine:
    resource_pool = self.resource_pool_manager.get_resource_pool(Role.ActorRollout)
    actor_rollout_cls = RayClassWithInitArgs(
        cls=self.role_worker_mapping[Role.ActorRollout],
        config=self.config.actor_rollout_ref,
        role="actor_rollout",
    )
    self.resource_pool_to_cls[resource_pool]["actor_rollout"] = actor_rollout_cls
else:
    raise NotImplementedError
    ......
```

以 `actor_rollout`，也就是 actor 为例，可以发现，前文提到的 `all_wg['actor_rollout']` 在这里出现了：

```python
self.resource_pool_to_cls[resource_pool]["actor_rollout"] = actor_rollout_cls
```

那么我们想知道的 `actor_rollout_wg` 其实就是这里的 `actor_rollout_cls`，然后他又被可恶的 `RayClassWithInitArgs` 封装了，不过问题不大，实际上不难看出 `self.config.actor_rollout_ref` 去初始化 `self.role_worker_mapping[Role.ActorRollout]`。

之前看到 `actor_rollout_ref`，很显然是在我们 `train_grpo.sh` 的脚本里面：

```bash
actor_rollout_ref.model.path=./models/sft/global_step_105/huggingface \
actor_rollout_ref.actor.optim.lr=1e-6 \
actor_rollout_ref.actor.use_remove_padding=True \
actor_rollout_ref.actor.ppo_mini_batch_size=16 \
actor_rollout_ref.actor.use_dynamic_bsz=True \
actor_rollout_ref.actor.ppo_max_token_len_per_gpu=5000 \
actor_rollout_ref.actor.use_kl_loss=False \
actor_rollout_ref.actor.kl_loss_coeff=0.0 \
actor_rollout_ref.actor.kl_loss_type=low_var_kl \
actor_rollout_ref.actor.entropy_coeff=0 \
actor_rollout_ref.model.enable_gradient_checkpointing=True \
actor_rollout_ref.actor.fsdp_config.param_offload=True \
actor_rollout_ref.actor.fsdp_config.optimizer_offload=True \
actor_rollout_ref.actor.tensor_model_parallel_size=1 \
actor_rollout_ref.rollout.name=vllm \
actor_rollout_ref.rollout.gpu_memory_utilization=0.7 \
actor_rollout_ref.rollout.n=8 \
actor_rollout_ref.rollout.param_offload=True
```

后来去 `role_worker_mapping` 这个字典里找到 actor 的 class，可以发现，它是从这个 `Trainer` 外层的 `main_ppo.py` 传入的。

```python
def add_actor_rollout_worker(self, config):
    """Add actor rollout worker based on the actor strategy."""
    from veril.single_controller.ray import RayWorkerGroup

    if config.actor_rollout_ref.actor.strategy in {"fsdp", "fsdp2"}:
        from veril.workers.fsdp_workers import ActorRolloutRefWorker, AsyncActorRolloutRefWorker

        actor_rollout_cls = (
            AsyncActorRolloutRefWorker
            if config.actor_rollout_ref.rollout.mode == "async"
            else ActorRolloutRefWorker
        )
        ray_worker_group_cls = RayWorkerGroup
        ......
    
    from veril.trainer.ppo.ray_trainer import Role
    self.role_worker_mapping[Role.ActorRollout] = ray.remote(actor_rollout_cls)

    return actor_rollout_cls, ray_worker_group_cls
```

不难发现，我们要找的就是 `actor_rollout_cls`，也就是 `ActorRolloutRefWorker` / `AsyncActorRolloutRefWorker`，我们以 `ActorRolloutRefWorker` 为例，可以发现它在 `fsdp_workers.py` 这个脚本中。

那么很显然，我们找到的 `actor_rollout_wg` 是什么？其实就是 `ActorRolloutRefWorker` 的实例。我们去 `fsdp_workers.py` 里面找到 `ActorRolloutRefWorker` 这个类，果然，我们想要的 `update_actor` 出现在了这个类中：

```python
@register(dispatch_mode_make_nd_compute_dataproto_dispatch_fn(mesh_name="actor"))
@DistProfiler.annotate(color="red", role="actor_update")
def update_actor(self, data: DataProto):
    assert self._is_actor
    if self.is_offload_param:
        load_fsdp_model_to_gpu(self.actor_module_fsdp)
    if self.is_offload_optimizer:
    load_fsdp_optimizer(optimizer=self.actor_optimizer, device_id=get_device_id())

    with self.ulysses_sharding_manager:
        data = data.to("cpu")  # data will to device with each micro batch on actor.update_policy

    # perform training
    with Timer(name="update_policy", logger=None) as timer:
        metrics = self.actor.update_policy(data=data)
        delta_time = timer.last
        global_num_tokens = data.meta_info["global_token_num"]
        estimated_flops, promised_flops = self.flops_counter.estimate_flops(global_num_tokens, delta_time)
        metrics["perf/mfu/actor"] = (
            estimated_flops * self.config.actor.ppo_epochs / promised_flops / self.world_size
        )
        metrics["perf/max_memory_allocated_gb"] = get_torch_device().max_memory_allocated() / (1024**3)
        metrics["perf/max_memory_reserved_gb"] = get_torch_device().max_memory_reserved() / (1024**3)
        metrics["perf/cpu_memory_used_gb"] = psutil.virtual_memory().used / (1024**3)

        lr = self.actor_lr_scheduler.get_last_lr()[0]
        metrics["actor/lr"] = lr
        self.actor_lr_scheduler.step()

        # TODO: here, we should return all metrics
        output = DataProto(meta_info={"metrics": metrics})

        output = output.to("cpu")

    if self.is_offload_param:
        offload_fsdp_model_to_cpu(self.actor_module_fsdp)
        log_gpu_memory_usage("After offload actor model during update_actor", logger=logger)
    if self.is_offload_optimizer:
        offload_fsdp_optimizer(optimizer=self.actor_optimizer)
        log_gpu_memory_usage("After offload actor optimizer during update_actor", logger=logger)

    return output
```

1. **update_actor 做了什么？**

在这个方法中，最关键的一行代码就是：

```python
metrics = self.actor.update_policy(data=data)
```

这里的 `self.actor` 可以在 `init_model()` 中找到：

```python
self.actor = DataParallelPPOActor(
    config=actor_cfg, actor_module=self.actor_module_fsdp, actor_optimizer=self.actor_optimizer
)
```

当然能在 `DataParallelPPOActor` 类中找到 `update_policy` 这个方法。由于代码太长，可以重点看一些核心代码：

```python
select_keys = [
    "responses",
    "response_mask",
    "input_ids",
    "attention_mask",
    "position_ids",
    "old_log_probs",
    "advantages",
]
if self.config.use_kl_loss:
    select_keys.append("ref_log_prob")
......
data = data.select(batch_keys=select_keys, non_tensor_batch_keys=non_tensor_select_keys)
```

首先可以看到这个 `list`，基本就是gpo公式中对应的一些变量。

```python
on_policy = len(mini_batches) == 1 and self.config.ppo_epochs == 1
```

以 `on_policy` 为例，当 `mini_batches` 数量和 `ppo_epochs` 为1时，将会优化成一个 `on_policy` 的更新，可以从paper里面的伪代码来看比较清楚：

![image-20251204092936615](assets/image-20251204092936615.png)

第10行的为时的关键，也就是说每个batch的数据更新一次的时候，off-policy的grpo会退化成on-policy，这也就是框架中默认的操作：（来自huggingface家trl的官方文档，好像早期的时候trl只支持μ=1的grpo）

$$
\mathcal{L}_{\text{GRPO}}(\theta) = -\frac{1}{G} \sum_{i=1}^G \frac{1}{|\omega_i|} \sum_{t=1}^{|\omega_i|} \left[ \min \left( \frac{\pi_\theta(\omega_{i,t} | q, O_i, < t)}{\pi_{\theta_{\text{old}}}(\omega_{i,t} | q, O_i, < t)}, 1 - \epsilon, 1 + \epsilon \right) \hat{A}_{i,t} - \beta D_{\text{KL}}[\pi_\theta \parallel \pi_{\text{ref}}] \right]
$$


观察代码可以发现，$\pi_{\theta_{\text{old}}}$ 是在batch内进行初始化的，也就是说，这个时候 $\pi_{\theta}$ 和 $\pi_{\theta_{\text{old}}}$ 其实是**完全相同的！**唯一的区别是π₀是我们的policy model，是可学习的，而π₀_old是不可学习的，在代码中的相关关键代码是：

```python
self.actor_module.train()  # 这就是π₀_{θ₃}
......
old_log_prob = log_prob.detach()  # 表示 π₀_{old}是不可梯度的
```

在 compute_policy_loss 中：

```python
@register_policy_loss("vanilla")
def compute_policy_loss_vanilla(
    old_log_prob: torch.Tensor, 
    log_prob: torch.Tensor, 
    advantages: torch.Tensor, 
    response_mask: torch.Tensor, 
    loss_agg_mode: str = "token-mean", 
    config: Optional[DictConfig | AlgoConfig] = None, 
    rollout_log_probs: torch.Tensor | None = None,
) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
    assert config is not None
    assert not isinstance(config, AlgoConfig)
    clip_ratio = config.clip_ratio  # Clipping parameter ε for standard PPO. See https://arxiv.org/abs/1707.06347.
    clip_ratio_low = config.clip_ratio_low if config.clip_ratio_low is not None else clip_ratio
    clip_ratio_high = config.clip_ratio_high if config.clip_ratio_high is not None else clip_ratio
    clip_ratio_c = config.get(  # Lower bound of the ratio for dual-clip PPO. See https://arxiv.org/pdf/1912.09729.
        "clip_ratio_c", 3.0
    )

    clipranger = clip_ratio
    clipranger_low = clip_ratio_low
    clipranger_high = clip_ratio_high

    assert clip_ratio_c > 1.0, (
        "The lower bound of the clip_ratio_c for dual-clip PPO should be greater than 1.0,"
        + f" but get the value: {clip_ratio_c}."
    )

    negative_approx_kl = log_prob - old_log_prob
    # Clamp negative_approx_kl for stability
    negative_approx_kl = torch.clamp(negative_approx_kl, min=-20.0, max=20.0)
    ppo_kl = verl.F.masked_mean(-negative_approx_kl, response_mask)

    pg_losses1 = -advantages * ratio
    if cliprange_low is None:
        cliprange_low = cliprange
    if cliprange_high is None:
        cliprange_high = cliprange

    pg_losses2 = -advantages * torch.clamp(
        ratio, 1 - cliprange_low, 1 + cliprange_high
    )  # - clip(ratio, 1-cliprange, 1+cliprange) * A
    clip_pg_losses1 = torch.maximum(pg_losses1, pg_losses2)
    # max(-ratio * A, -clip(ratio, 1-cliprange, 1+cliprange) * A)
    pg_clipfrac = verl.F.masked_mean(torch.gt(pg_losses2, pg_losses1).float(), response_mask)

    pg_losses3 = -advantages * clip_ratio_c
    clip_pg_losses2 = torch.min(pg_losses3, clip_pg_losses1)
    pg_clipfrac_lower = verl.F.masked_mean(
        torch.gt(clip_pg_losses1, pg_losses3) * (advantages < 0).float(), response_mask
    )

    pg_losses = torch.where(advantages < 0, clip_pg_losses2, clip_pg_losses1)

    if config.tis_imp_ratio_cap > 0 and rollout_log_probs is not None:
        # Apply truncated importance sampling -> https://fengyao.notion.site/off-policy-rl
        tis_imp_ratio = torch.exp(old_log_prob - rollout_log_probs)
        tis_imp_ratio = torch.clamp(tis_imp_ratio, max=config.tis_imp_ratio_cap)
        pg_losses = pg_losses * tis_imp_ratio

    pg_loss = agg_loss(loss_mat=pg_losses, loss_mask=response_mask, loss_agg_mode=loss_agg_mode)

    return pg_loss, pg_clipfrac, ppo_kl, pg_clipfrac_lower
```

可以观察到：`negative_approx_kl = log_prob - old_log_prob` 的结果其实永远是0，公式中表示为：
$$
\text{log\_prob} = \log \pi_{\theta}(o_i | q, o_{i,<t}) \\
\text{old\_log\_prob} = \log \pi_{\theta_{old}}(o_i | q, o_{i,<t})
$$
所以 `ratio = torch.exp(negative_approx_kl)` 得到的 `ratio` 永远是1，这里的 `ratio` 就是表达公式中的：
$$
\text{ratio} = \exp \left( \log \pi_{\theta}(o_i | q, o_{i,<t}) - \log \pi_{\theta_{old}}(o_i | q, o_{i,<t}) \right) = \frac{\pi_{\theta}(o_i | q, o_{i,<t})}{\pi_{\theta_{old}}(o_i | q, o_{i,<t})}
$$
随后代码中：

```
pg_losses1 = -advantages * ratio
```

这里的 `pg_losses1` 其实是等于 `-advantages`，`ratio` 虽然值为1, 但是梯度是存在的，这关系到我们π₀的更新。如果直接写 `pg_losses1 = -advantages` 就完全没有了，因为在grpo里面 `adv` 是来自`reward`，而reward对每个样本而言就是一个死的指标，没有东西可以引导，也不会对我们的策略模型产生影响，所以需要保留这个 `ratio`。

关于公式中 `min` 包含的第二个clip相关项，虽然此时clip没有用，因为比值为1，`clip(1, 1-e, 1+e)` 结果当然是1。

在 `trl` 的文档中写到：

![image-20251204124227539](assets/image-20251204124227539.png)

这就是简化后的loss。

当然在代码中，他每一部分都是老老实实写的，只是说 `on_policy` 会让公式得到非常简化：

```python
pg_losses1 = -advantages * ratio
if cliprange_low is None:
    cliprange_low = cliprange
if cliprange_high is None:
    cliprange_high = cliprange
pg_losses2 = -advantages * torch.clamp(
    ratio, 1 - cliprange_low, 1 + cliprange_high
)  # - clip(ratio, 1-cliprange, 1+cliprange) * A
clip_pg_losses1 = torch.maximum(
    pg_losses1, pg_losses2
)  # max(-ratio * A, -clip(ratio, 1-cliprange, 1+cliprange) * A)
```

`pg_losses1` 是公式中 `min` 的第一部分，`pg_losses2` 是第二部分的clip。

代码中还考虑了，如果 `adv` 是负数的情况，由于公式中 `min` 是用来限制上限的，所以当 `adv` 为负数的时候，会对下限进行clip，就是 `clip_pg_losses2`，其中 `clip_ratio_c` 是超参数：

```
pg_losses3 = -advantages * clip_ratio_c
clip_pg_losses2 = torch.min(pg_losses3, clip_pg_losses1)
```

这个是 在 *Mastering Complex Control in MOBA Games with Deep Reinforcement Learning* 这篇paper中提到的：

![image-20251204124400025](assets/image-20251204124400025.png)

grpo公式中 `adv` 顶端算了，还要计算梯度的loss。相关代码在外层，回到 `update_policy` 中：

```python
if self.config.use_kl_loss:
    ref_log_prob = model_inputs["ref_log_prob"]
    # compute kl loss
    kld = kl_penalty(
        logprob=log_prob, ref_logprob=ref_log_prob,
        kl_loss_type=self.config.kl.loss_type
    )
    kl_penalty = self.config.kl.loss_coef
    kl_loss = agg_loss(loss_mat=kld, loss_mask=response_mask, loss_agg_mode=loss_agg_mode)

    policy_loss = policy_loss + kl_loss * self.config.kl.loss_coef
    micro_batch_metrics["actor/kl_loss"] = kl_loss.detach().item() * loss_scale_factor
    micro_batch_metrics["actor/kl_coef"] = self.config.kl.loss_coef
```

其中 `policy_loss = policy_loss + kl_loss * self.config.kl.loss_coef` 就是对应公式中的：
$$
\beta D_{KL} \left[ \pi_{\theta} || \pi_{ref} \right]
$$
值得注意的是这里是“ref”，并不是“old”，可以看代码中，"old" 是每个batch用最新的policy model赋值，而"ref"是外面一层， 他的更新频率会低很多，目的是防止模型更新后再相差太大。

```python
# kl_penalty_forward
# J. Schulman. Approximating kl divergence, 2020.
# URL http://joschu.net/blog/kl-approx.html.
if self.config.kl.loss_type in ("low_var_kl", "k3"):
    kl = ref_logprob - logprob
    # For numerical stability
    kl = torch.clamp(kl, min=-20.0, max=20.0)
    ratio = torch.exp(kl)
    kld = (ratio - 1).contiguous()
    return torch.clamp(kld, min=-10, max=10)
```

可以发现，代码中的 `kld`，表示的公式就是：

这与trl文档中相一致：
$$
\frac{\pi_{ref}(o_{i,t} | q, o_{i,<t})}{\pi_{\theta}(o_{i,t} | q, o_{i,<t})} - \log \frac{\pi_{ref}(o_{i,t} | q, o_{i,<t})}{\pi_{\theta}(o_{i,t} | q, o_{i,<t})} - 1
$$


**Estimating the KL divergence**

KL divergence is estimated using the approximator introduced by Schulman et al. (2020). The approximator is defined as follows:

$$
D_{KL} \left[ \pi_{\theta} || \pi_{ref} \right] = \frac{\pi_{ref}(o_{i,t} | q, o_{i,<t})}{\pi_{\theta}(o_{i,t} | q, o_{i,<t})} - \log \frac{\pi_{ref}(o_{i,t} | q, o_{i,<t})}{\pi_{\theta}(o_{i,t} | q, o_{i,<t})} - 1
$$

计算后，就可以得到最终的loss：

```python
policy_loss = policy_loss + kl_loss * self.config.kl.loss_coef
```

#### 7. 框架

![image-20251207143750784](assets/image-20251207143750784.png)

#### 8. 使用SFT

**Data参数**

data里的参数都比较好理解，像一些我没用过的参数，比如multi_turn我就不写了，避免误导。

| 参数名称                      | 参数意义                                                     |
| ----------------------------- | ------------------------------------------------------------ |
| data.train_batch_size         | 一个batch的样本数                                            |
| data.micro_batch_size_per_gpu | 每次往单张卡上加载几个样本                                   |
| data.train_files              | parquet格式的训练数据集在本地路径                            |
| data.val_files                | parquet格式的验证数据集在本地路径                            |
| data.prompt_key               | 数据集中prompt的column name，一般直接用"prompt"              |
| data.response_key             | 数据集中response的column name，一般直接用"response"          |
| data.max_length               | 单个response的最大长度，注意不要让response超过这个长度，否则会报错。 |

**Data参数**

data里的参数都比较好理解，像一些我没用过的参数，比如multi_turn我就不写了，避免误导。

| 参数名称                      | 参数意义                                                     |
| ----------------------------- | ------------------------------------------------------------ |
| data.train_batch_size         | 一个batch的样本数                                            |
| data.micro_batch_size_per_gpu | 每次往单张卡上加载几个样本                                   |
| data.train_files              | parquet格式的训练数据集在本地路径                            |
| data.val_files                | parquet格式的验证数据集在本地路径                            |
| data.prompt_key               | 数据集中prompt的column name，一般直接用"prompt"              |
| data.response_key             | 数据集中response的column name，一般直接用"response"          |
| data.max_length               | 单个response的最大长度，注意不要让response超过这个长度，否则会报错。 |

**Model参数**

这里主要放的是和模型相关的一些参数，包括模型路径，并行策略和[Lora](https://zhida.zhihu.com/search?content_id=262206810&content_type=Article&match_order=1&q=Lora&zhida_source=entity)参数设置等

| 参数名称                            | 参数意义                                                     |
| ----------------------------------- | ------------------------------------------------------------ |
| model.partial_pretrain              | 模型权重的路径，例如：~/model/Qwen/Qwen3-4B                  |
| model.fsdp_config.model_dtype       | 训练时加载的精度，一般采用bf16防止梯度NaN导致不更新          |
| model.cpu_offload                   | 是否将梯度，optimizer的信息offload到内存上                   |
| model.offload_params                | 细节请看torch.distributed.fsdp.CPUOffload                    |
| model.enable_gradient_checkpointing | 同activation checkpointing，设置为True时，一些activation不会保存，在反向传播时重新计算 |
| model.lora_rank                     | 设置大于0的值默认开启lora，按需设置为8, 16, 32, 64...        |
| model.lora_alpha                    | LoRA训练时的缩放因子                                         |
| model.target_modules                | LoRA训练时的目标模块，一般用all-linear，也可以根据自己的需求，把具体的矩阵名字写成一个list传入 |
| model.use_liger                     | 启用 liger kernel 加速 LoRAd的矩阵乘法                       |
| model.strategy                      | DDP策略，可选fsdp/fsdp2                                      |

**Optim参数**

Optimizer的参数，官方默认**[AdamW](https://zhida.zhihu.com/search?content_id=262206810&content_type=Article&match_order=1&q=AdamW&zhida_source=entity)**，如果需要改成**Adam**，**Muon**等优化器，需要魔改一下官方的fsdp_sft_trainer.py。Optim的设置，可以参考相关的scaling law论文，看一下当前参数的模型如何设置。根据我看到的说法，一般full parameter finetuning使用1e-5 ~ 2e-5，LoRA则可以适当放到 1e-4~3e-4(Karpathy Constant ^^)。

| 参数名称           | 参数意义                                                     |
| ------------------ | ------------------------------------------------------------ |
| optim.lr           | 学习率                                                       |
| optim.betas        | AdamW的两个衰减系数                                          |
| optim.weight_decay | L2正则化系数                                                 |
| optim.clip_grad    | 梯度裁剪系数，grad_norm的范数大于clip_grad时，会将除以一个值缩放到clip_grad |
| optim.lr_scheduler | 学习率调整器，一般用cosine/linear等                          |

**Trainer参数**

| 参数名称                         | 参数意义                                                     |
| -------------------------------- | ------------------------------------------------------------ |
| trainer.default_local_dir        | 保存模型checkpoint的路径                                     |
| trainer.project_namede           | console/wandb/tensorboard等记录实验组的名称                  |
| trainer.experiment_name          | console/wandb/tensorboard等记录实验的名称                    |
| trainer.total_epochs             | train_set总共训练几遍                                        |
| trainer.logger                   | 在哪些位置记录实验数据，console一般全加，tensorboard/wandb按照习惯来填 |
| trainer.seed                     | 随即种子                                                     |
| trainer.save_freq                | 每过多少个step保存一次，根据数据集的大小来定夺               |
| trainer.test_freq                | 每过多少个step进行一次validation/test                        |
| trainer.nnodes                   | 多机训练中的机器节点数量，有几台机器填几                     |
| trainer.n_gpus_per_node          | 每个节点/每台机器里有几张GPU                                 |
| trainer.resume_mode              | 一般auto，如果没有检测到有checkpoint，如果当前实验在某一个训报废了（OOM/loss spike），可以找到checkpoint地址，从该checkpoint继续训练。 |
| trainer.resume_from_path         | 决定从哪个checkpoint/模型继续训练                            |
| trainer.checkpoint.save_contents | checkpoint需要保存的信息，model对应模型权重，optimizer对应模型优化器状态，extra暂时不知道是什么 |
| trainer.device                   | 使用的计算设备，常见的有cpu, cuda, npu, tpu之类的            |

这里主要放的是和模型相关的一些参数，包括模型路径，并行策略和[Lora](https://zhida.zhihu.com/search?content_id=262206810&content_type=Article&match_order=1&q=Lora&zhida_source=entity)参数设置等

| 参数名称                            | 参数意义                                                     |
| ----------------------------------- | ------------------------------------------------------------ |
| model.partial_pretrain              | 模型权重的路径，例如：~/model/Qwen/Qwen3-4B                  |
| model.fsdp_config.model_dtype       | 训练时加载的精度，一般采用bf16防止梯度NaN导致不更新          |
| model.cpu_offload                   | 是否将梯度，optimizer的信息offload到内存上                   |
| model.offload_params                | 细节请看torch.distributed.fsdp.CPUOffload                    |
| model.enable_gradient_checkpointing | 同activation checkpointing，设置为True时，一些activation不会保存，在反向传播时重新计算 |
| model.lora_rank                     | 设置大于0的值默认开启lora，按需设置为8, 16, 32, 64...        |
| model.lora_alpha                    | LoRA训练时的缩放因子                                         |
| model.target_modules                | LoRA训练时的目标模块，一般用all-linear，也可以根据自己的需求，把具体的矩阵名字写成一个list传入 |
| model.use_liger                     | 启用 liger kernel 加速 LoRAd的矩阵乘法                       |
| model.strategy                      | DDP策略，可选fsdp/fsdp2                                      |

**Optim参数**

Optimizer的参数，官方默认**[AdamW](https://zhida.zhihu.com/search?content_id=262206810&content_type=Article&match_order=1&q=AdamW&zhida_source=entity)**，如果需要改成**Adam**，**Muon**等优化器，需要魔改一下官方的fsdp_sft_trainer.py。Optim的设置，可以参考相关的scaling law论文，看一下当前参数的模型如何设置。根据我看到的说法，一般full parameter finetuning使用1e-5 ~ 2e-5，LoRA则可以适当放到 1e-4~3e-4(Karpathy Constant ^^)。

| 参数名称           | 参数意义                                                     |
| ------------------ | ------------------------------------------------------------ |
| optim.lr           | 学习率                                                       |
| optim.betas        | AdamW的两个衰减系数                                          |
| optim.weight_decay | L2正则化系数                                                 |
| optim.clip_grad    | 梯度裁剪系数，grad_norm的范数大于clip_grad时，会将除以一个值缩放到clip_grad |
| optim.lr_scheduler | 学习率调整器，一般用cosine/linear等                          |

**Trainer参数**

| 参数名称                         | 参数意义                                                     |
| -------------------------------- | ------------------------------------------------------------ |
| trainer.default_local_dir        | 保存模型checkpoint的路径                                     |
| trainer.project_namede           | console/wandb/tensorboard等记录实验组的名称                  |
| trainer.experiment_name          | console/wandb/tensorboard等记录实验的名称                    |
| trainer.total_epochs             | train_set总共训练几遍                                        |
| trainer.logger                   | 在哪些位置记录实验数据，console一般全加，tensorboard/wandb按照习惯来填 |
| trainer.seed                     | 随即种子                                                     |
| trainer.save_freq                | 每过多少个step保存一次，根据数据集的大小来定夺               |
| trainer.test_freq                | 每过多少个step进行一次validation/test                        |
| trainer.nnodes                   | 多机训练中的机器节点数量，有几台机器填几                     |
| trainer.n_gpus_per_node          | 每个节点/每台机器里有几张GPU                                 |
| trainer.resume_mode              | 一般auto，如果没有检测到有checkpoint，如果当前实验在某一个训报废了（OOM/loss spike），可以找到checkpoint地址，从该checkpoint继续训练。 |
| trainer.resume_from_path         | 决定从哪个checkpoint/模型继续训练                            |
| trainer.checkpoint.save_contents | checkpoint需要保存的信息，model对应模型权重，optimizer对应模型优化器状态，extra暂时不知道是什么 |
| trainer.device                   | 使用的计算设备，常见的有cpu, cuda, npu, tpu之类的            |

运行脚本，需要修改的地方如下
`data.train_files` 和 `data.val_files` 训练集合和验证集合，转换为parquet的方式见上

`data.prompt_key` 和 `data.response_key` 就是问题和答案的位置，它们都在 `extra_info` 字段，然后可以根据 `data.prompt_dict_keys` 获取对应的部分

`data.max_length` 是单个回复的最大 token

`--nnodes=1 --nproc_per_node=2` 表示 1 个节点，节点上有 2 块 gpu

`lora_rank` 一般是8或者16，`lora_alpha` 设为 rank 的两倍

`target_modules` 是目标模块

`default_local_dir`  表示检查点存储的位置

`total_epochs` 一般设为 1 2 3

`wandb` 只要在终端输入 `wandb login` 即可启动然后执行 `bash sft.sh` 即可。

如果要写系统提示词或者用户提示词的话直接放在prompt里即可，因为允许是 message 格式，即role和对应的content

```bash
torchrun --standalone --nnodes=1 --nproc_per_node=2 \
     -m verl.trainer.fsdp_sft_trainer \
    data.train_files=/mnt/public/wwj/zhaozq/cy/dataset/TeleQnA/parquet/train.parquet \
    data.val_files=/mnt/public/wwj/zhaozq/cy/dataset/TeleQnA/parquet/val.parquet \
    data.prompt_key=extra_info \
    data.response_key=extra_info \
    data.max_length=8192 \
    optim.lr=1e-4 \
    data.prompt_dict_keys=['question'] \
    +data.response_dict_keys=['answer'] \
    data.micro_batch_size_per_gpu=4 \
    model.partial_pretrain=/mnt/public/wwj/zhaozq/cy/model/Qwen/Qwen3-8B \
    trainer.default_local_dir=/mnt/public/wwj/zhaozq/cy/model/Qwen/Qwen3-8B/sft \
    trainer.project_name=teleqna-sft \
    trainer.experiment_name=teleqna-sft-qwen-3-8b \
    trainer.logger='["console","wandb"]' \
    trainer.total_epochs=1 \
    model.lora_rank=16 \
    model.lora_alpha=32 \
    model.target_modules=[q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj] $@ 
```

#### 9. 模型格式转换

运行结束后，格式是fsdp的，需要转换为 huggingface 格式，转换的脚本在 `verl/scripts/legacy_model_merger.py` ，使用方法如下

如果遇到无法导入包等问题，在系统路径中添加项目路径 `export PYTHONPATH="/path/to/your_project:$PYTHONPATH"`

```
python -m scripts.legacy_model_merger merge \
    --backend fsdp \
    --local_dir $model_path \
    --target_dir $new_hf_model_path
```

#### 10. 增加曲线记录

在 `verl/trainer/ppo/metric_utils` 下面的 compute_data_metrics 中加入 metrics 记录即可，也就是曲线都在 metrics 这个 map 中，例如我如果多步平均奖励的话，可以加入下面的代码

```python
# 计算8步移动平均reward（只在step是8的倍数时计算）
global _reward_history, _current_step
window_size = 8

# 增加步数计数器
_current_step += 1
_reward_history.append(reward_mean)

# 保持最近8步的历史记录（用于计算平均值）
if len(_reward_history) > window_size:
    _reward_history = _reward_history[-window_size:]

# 只在step是8的倍数时计算平均值并添加到metrics
if _current_step % window_size == 0 and len(_reward_history) >= window_size:
    # 计算最近8步的平均值（包括当前step），与critic/rewards/mean的计算方式保持一致
    moving_avg_reward = np.mean(_reward_history[-window_size:])
    metrics["critic/rewards/mean_8step_avg"] = moving_avg_reward
```

#### 11. 个人训练实战脚本

几个影响显存的变量，分别是数据的长度，batch_size 大小，采样次数，强化学习允许 batch_size 为 1

还有用于推理的部分 gpu 占用率，一般30%-50%即可

建议先增加 batch_size 然后逐步增加其他变量

```bash
data.max_prompt_length=2048 \
data.max_response_length=512 \
data.train_batch_size=1 \
actor_rollout_ref.rollout.n=4 \
actor_rollout_ref.rollout.gpu_memory_utilization=0.4 \
```



```bash
#!/usr/bin/env bash
set -xeuo pipefail

## !!!!!!!important!!!!!!
## set the following environment variables on all your nodes
# env_vars:
#   CUDA_DEVICE_MAX_CONNECTIONS: "1"
#   NCCL_NVLS_ENABLE: "0"
#   VLLM_USE_V1: 1
# install mbridge=0.1.13 on all your node with the following command:
# pip3 install git+https://github.com/ISEEKYAN/mbridge




enable_overlong_buffer=True
overlong_buffer_len=$((1024 * 1))
overlong_penalty_factor=1.0

loss_agg_mode="token-mean"

train_prompt_bsz=${TRAIN_BS:-32}
n_resp_per_prompt=8
train_prompt_mini_bsz=16

NNODES=${NNODES:-1}

# Algorithm
temperature=1.0
top_p=1.0
top_k=-1 # 0 for HF rollout, -1 for vLLM rollout
val_top_p=0.7
# Performance Related Parameter (3-GPU setup)
use_dynamic_bsz=False  # Disable dynamic batch size for stability
actor_ppo_max_token_len=$((2048))
infer_ppo_max_token_len=$((2048))
offload=True
OPTIM_OFFLOAD=${OPTIM_OFFLOAD:-False}
gen_tp=1  # 3-GPU setup: use 1 for rollout, 3 for training (FSDP)
train_tp=${TP:-1}  # FSDP doesn't need TP
train_pp=${PP:-1}  # No pipeline parallel

EP=${EP:-1}  # No expert parallelism
ETP=1
CP=1
optimizer_offload_fraction=${OFFLOAD_FRACTION:-0.5}
last_layer=${LAST_LAYER:-10}

project_name='verl-qwen3'
exp_name="qwen3-8b-rm-2gpu-${NNODES}"
CKPTS_DIR="/mnt/public/wwj/zhaozq/exp1/verl/huawei/checkpoint/Qwen3-8B-rm-Qwen3-8B-score"

# TODO: support cuda graph for rollout by setting the following config
    # actor_rollout_ref.rollout.cudagraph_capture_sizes=[1,2,4,8,16,32]
    # actor_rollout_ref.rollout.enforce_eager=False

python3 -m verl.trainer.main_ppo \
    data.train_files="/mnt/public/wwj/zhaozq/exp1/dataset/TeleQnA/parquet_rl/TeleQnA_train_grpo_format.parquet" \
    data.val_files="/mnt/public/wwj/zhaozq/exp1/dataset/TeleQnA/parquet_rl/TeleQnA_test_grpo_format_10percent.parquet" \
    custom_reward_function.path=/mnt/public/wwj/zhaozq/exp1/verl/huawei/experiments/code/TeleQnA_reward.py \
    custom_reward_function.name=compute_score \
    data.prompt_key=prompt \
    data.truncation='left' \
    data.max_prompt_length=2048 \
    data.max_response_length=512 \
    data.train_batch_size=1 \
    actor_rollout_ref.rollout.n=4 \
    actor_rollout_ref.rollout.name=vllm \
    actor_rollout_ref.rollout.enforce_eager=True \
    algorithm.adv_estimator=grpo \
    algorithm.use_kl_in_reward=False \
    algorithm.kl_ctrl.kl_coef=0.0 \
    actor_rollout_ref.model.use_fused_kernels=True \
    actor_rollout_ref.actor.use_kl_loss=True \
    actor_rollout_ref.actor.kl_loss_coef=0.001 \
    actor_rollout_ref.actor.clip_ratio_low=0.2 \
    actor_rollout_ref.actor.clip_ratio_high=0.28 \
    actor_rollout_ref.actor.clip_ratio_c=10.0 \
    actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=1 \
    actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=1 \
    actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=1 \
    actor_rollout_ref.actor.use_dynamic_bsz=False \
    actor_rollout_ref.ref.log_prob_use_dynamic_bsz=False \
    actor_rollout_ref.rollout.log_prob_use_dynamic_bsz=False \
    actor_rollout_ref.actor.ppo_max_token_len_per_gpu=2048 \
    actor_rollout_ref.ref.log_prob_max_token_len_per_gpu=2048 \
    actor_rollout_ref.rollout.log_prob_max_token_len_per_gpu=2048 \
    actor_rollout_ref.model.path="/mnt/public/wwj/zhaozq/exp1/LLaMA-Factory/saves/qwen3-8b/full/sft_rl" \
    actor_rollout_ref.actor.optim.lr=1e-6 \
    actor_rollout_ref.actor.optim.lr_warmup_steps=10 \
    actor_rollout_ref.actor.optim.weight_decay=0.1 \
    actor_rollout_ref.actor.fsdp_config.optimizer_offload=True \
    actor_rollout_ref.actor.fsdp_config.param_offload=False \
    actor_rollout_ref.ref.fsdp_config.param_offload=True \
    actor_rollout_ref.actor.ppo_mini_batch_size=1 \
    actor_rollout_ref.actor.entropy_coeff=0 \
    actor_rollout_ref.actor.optim.clip_grad=1.0 \
    actor_rollout_ref.actor.loss_agg_mode=${loss_agg_mode} \
    actor_rollout_ref.rollout.gpu_memory_utilization=0.4 \
    actor_rollout_ref.rollout.tensor_model_parallel_size=${gen_tp} \
    actor_rollout_ref.rollout.enable_chunked_prefill=False \
    actor_rollout_ref.rollout.max_num_batched_tokens=2048 \
    actor_rollout_ref.rollout.temperature=${temperature} \
    actor_rollout_ref.rollout.top_p=${top_p} \
    actor_rollout_ref.rollout.top_k=${top_k} \
    actor_rollout_ref.rollout.max_model_len=2048 \
    actor_rollout_ref.rollout.name=vllm \
    actor_rollout_ref.rollout.mode=sync \
    actor_rollout_ref.rollout.ignore_eos=False \
    actor_rollout_ref.rollout.dtype=bfloat16 \
    actor_rollout_ref.rollout.enforce_eager=true\
    actor_rollout_ref.nccl_timeout=1200 \
    actor_rollout_ref.rollout.val_kwargs.temperature=${temperature} \
    actor_rollout_ref.rollout.val_kwargs.top_p=${val_top_p} \
    actor_rollout_ref.rollout.val_kwargs.top_k=${top_k} \
    actor_rollout_ref.rollout.val_kwargs.do_sample=True \
    actor_rollout_ref.rollout.val_kwargs.n=1 \
    reward_model.reward_manager=naive \
    trainer.logger=['console','wandb'] \
    trainer.project_name="${project_name}" \
    trainer.experiment_name="${exp_name}" \
    trainer.n_gpus_per_node=2 \
    trainer.nnodes="${NNODES}" \
    trainer.val_before_train=False \
    trainer.test_freq=100 \
    trainer.save_freq=100 \
    trainer.total_epochs=10 \
    trainer.default_local_dir="${CKPTS_DIR}" \
    trainer.resume_mode=auto \
    trainer.log_val_generations=10 $@
```

### 七、vllm

#### 1. 介绍

`vLLM` 框架是一个高效的大语言模型**推理和部署服务系统**，具备以下特性：

- **高效的内存管理**：通过 `PagedAttention` 算法，`vLLM` 实现了对 `KV` 缓存的高效管理，减少了内存浪费，优化了模型的运行效率。
- **高吞吐量**：`vLLM` 支持异步处理和连续批处理请求，显著提高了模型推理的吞吐量，加速了文本生成和处理速度。
- **易用性**：`vLLM` 与 `HuggingFace` 模型无缝集成，支持多种流行的大型语言模型，简化了模型部署和推理的过程。兼容 `OpenAI` 的 `API` 服务器。
- **分布式推理**：框架支持在多 `GPU` 环境中进行分布式推理，通过模型并行策略和高效的数据通信，提升了处理大型模型的能力。
- **开源共享**：`vLLM` 由于其开源的属性，拥有活跃的社区支持，这也便于开发者贡献和改进，共同推动技术发展。

#### 2. 使用

首先从 `vLLM` 库中导入 `LLM` 和 `SamplingParams` 类。`LLM` 类是使用 `vLLM` 引擎运行离线推理的主要类。`SamplingParams` 类指定采样过程的参数，用于控制和调整生成文本的随机性和多样性。

`vLLM` 提供了非常方便的封装，我们直接传入模型名称或模型路径即可，不必手动初始化模型和分词器。

然后，通过使用分词器的 apply_chat_template 函数，将我们的 prompt（提示词）格式化为模型所需的输入格式。

默认情况下，Qwen3 启用了思考能力，类似于 QwQ-32B。这意味着该模型将利用其推理能力来提升生成回答的质量。例如，当在 tokenizer.apply_chat_template 中显式设置 enable_thinking=True 或保留其默认值时，模型将进入思考模式。

```python
from vllm import LLM, SamplingParams
from transformers import AutoTokenizer
import os
import json

# 自动下载模型时，指定使用modelscope; 否则，会从HuggingFace下载
os.environ['VLLM_USE_MODELSCOPE']='True'

def get_completion(prompts, model, tokenizer=None, temperature=0.6, top_p=0.95, top_k=20, min_p=0, max_tokens=4096, max_model_len=8192):
    stop_token_ids = [151645, 151643]
    # 创建采样参数。temperature 控制生成文本的多样性，top_p 控制核心采样的概率，top_k 通过限制候选词的数量来控制生成文本的质量和多样性, min_p 通过设置概率阈值来筛选候选词，从而在保证文本质量的同时增加多样性
    sampling_params = SamplingParams(temperature=temperature, top_p=top_p, top_k=top_k, min_p=min_p, max_tokens=max_tokens, stop_token_ids=stop_token_ids)  # max_tokens 用于限制模型在推理过程中生成的最大输出长度
    # 初始化 vLLM 推理引擎
    llm = LLM(model=model, tokenizer=tokenizer, max_model_len=max_model_len,trust_remote_code=True)  # max_model_len 用于限制模型在推理过程中可以处理的最大输入和输出长度之和。
    outputs = llm.generate(prompts, sampling_params)
    return outputs


if __name__ == "__main__":
    # 初始化 vLLM 推理引擎
    model='/root/autodl-tmp/Qwen/Qwen3-8B' # 指定模型路径
    tokenizer = AutoTokenizer.from_pretrained(model, use_fast=False) # 加载分词器

    prompt = "给我一个关于大模型的简短介绍。"
    messages = [
        {"role": "user", "content": prompt}
    ]
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
        enable_thinking=True  # 是否开启思考模式，默认为 True
    )

    outputs = get_completion(text, model, tokenizer=None, temperature=0.6, top_p = 0.95, top_k=20, min_p=0)  # 对于思考模式，官方建议使用以下参数：temperature = 0.6，TopP = 0.95，TopK = 20，MinP = 0。

    # 输出是一个包含 prompt、生成文本和其他信息的 RequestOutput 对象列表。
    # 打印输出。
    for output in outputs:
        prompt = output.prompt
        generated_text = output.outputs[0].text
        print(f"Prompt: {prompt!r}, \nResponse: {generated_text!r}")
```

#### 3. 提供fastapi接口

```python
from fastapi import FastAPI, Request
from vllm.engine.arg_utils import AsyncEngineArgs
from vllm.engine.async_llm_engine import AsyncLLMEngine
from vllm import SamplingParams
from transformers import AutoTokenizer
import uvicorn
import os
import uuid
import time
import asyncio

# === Configuration ===
# Use ModelScope
os.environ['VLLM_USE_MODELSCOPE'] = 'True'
MODEL_PATH = '/mnt/public/wwj/zhaozq/gosse/qwen3next/Qwen/Qwen3-Next-80B-A3B-Instruct'

app = FastAPI()

# Global variables
llm_engine = None
tokenizer = None

@app.on_event("startup")
async def startup_event():
    """
    Initialize the engine and tokenizer on app startup.
    This ensures everything is ready before the first request.
    """
    global llm_engine, tokenizer
    
    print("Loading Tokenizer...")
    # Tokenizer for manual template application
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, use_fast=False, trust_remote_code=True)

    print("Loading vLLM Async Engine...")
    # Define arguments for AsyncLLMEngine
    engine_args = AsyncEngineArgs(
        model=MODEL_PATH,
        tokenizer=MODEL_PATH,
        max_model_len=8192,
        trust_remote_code=True,
        tensor_parallel_size=2,  # Set based on your setup
        gpu_memory_utilization=0.7
    )
    
    # Create the Async Engine
    llm_engine = AsyncLLMEngine.from_engine_args(engine_args)
    print("Engine loaded successfully.")

@app.post("/v1/chat/completions")
async def chat_completions(request: Request):
    """
    OpenAI-compatible endpoint using vLLM AsyncBackend.
    """
    # 1. Parse request
    json_data = await request.json()
    messages = json_data.get("messages", [])
    temperature = json_data.get("temperature", 0.7)
    top_p = json_data.get("top_p", 0.8)
    
    # Generate a unique request ID (vLLM needs this to track the stream)
    request_id = str(uuid.uuid4())

    # === Logic: Apply Template & Disable Thinking ===
    prompt_text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
        enable_thinking=False
    )

    # === Logic: Sampling Parameters ===
    stop_token_ids = [151645, 151643]
    sampling_params = SamplingParams(
        temperature=temperature,
        top_p=top_p,
        top_k=20,
        min_p=0,
        max_tokens=4096,
        stop_token_ids=stop_token_ids
    )

    # 2. Generate (Async)
    # engine.generate returns an async iterator
    results_generator = llm_engine.generate(prompt_text, sampling_params, request_id)

    # Iterate to get the final result (Non-streaming mode)
    final_output = None
    async for request_output in results_generator:
        final_output = request_output
        # If the client disconnects, abort the request to free GPU resources
        if await request.is_disconnected():
            await llm_engine.abort(request_id)
            return {"error": "Client disconnected"}

    # Extract the generated text from the final output
    generated_text = final_output.outputs[0].text

    # 3. Format response
    return {
        "id": request_id,
        "object": "chat.completion",
        "model": "Qwen3-Next-80B-A3B-Instruct",
        "created": int(time.time()),
        "choices": [
            {
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": generated_text,
                },
                "finish_reason": final_output.outputs[0].finish_reason
            }
        ],
        "usage": {
            "prompt_tokens": len(final_output.prompt_token_ids),
            "completion_tokens": len(final_output.outputs[0].token_ids),
            "total_tokens": len(final_output.prompt_token_ids) + len(final_output.outputs[0].token_ids)
        }
    }

if __name__ == "__main__":
    print("Starting API Server on port 6006...")
    # Note: We don't initialize LLM here anymore, it's done in the @app.on_event("startup")
    uvicorn.run(app, host='0.0.0.0', port=6006, workers=1)
```

#### 4. 异步调用

```python
import json
import os
import asyncio
import aiohttp
from openai import AsyncOpenAI
from collections import defaultdict
import time
from typing import Dict, Any, List, Tuple

# ================= 配置区域 =================
# API 配置
API_KEY = "YOUR_API_KEY"
BASE_URL = "http://localhost:6006/v1/"
MODEL_NAME = "Qwen/Qwen3-Next-80B-A3B-Instruct"

# 文件路径
DATA_FILE = "TeleQnA_train.json"
OUTPUT_FILE = "TeleQnA_train_with_reasoning.json"

# 并发配置
# MAX_CONCURRENT_REQUESTS: 同时发送的请求数量
# 
# 当前vllm服务器配置（H200双卡）：
#   - GPU: 2x H200 (141GB显存/卡)
#   - 模型: Qwen3-Next-80B-A3B-Instruct (80B参数)
#   - tensor_parallel_size: 2
#   - gpu_memory_utilization: 0.7
#   - max_model_len: 8192
# 
# 针对H200双卡+80B模型的建议：
#   - 保守值: 15-20（稳定，适合长时间运行）
#   - 推荐值: 20-30（充分利用GPU资源）
#   - 激进值: 30-40（需要监控OOM和响应时间）
# 
# 调整策略：
#   1. 从20开始，观察GPU利用率和响应时间
#   2. 如果GPU利用率<80%，可以逐步增加到25-30
#   3. 如果出现OOM错误，降低到15-18
#   4. 如果响应时间>10秒/请求，考虑降低并发数
MAX_CONCURRENT_REQUESTS = 20  # H200双卡推荐起始值，可根据实际情况调整

# BATCH_SIZE: 每处理多少条数据显示一次进度更新
BATCH_SIZE = 50  # 每处理多少条显示一次进度

# 初始化异步客户端
client = AsyncOpenAI(api_key=API_KEY, base_url=BASE_URL)


def build_prompt(question: str, choices: Dict[str, str], option_key: str, option_text: str) -> Tuple[str, str]:
    """
    为单个选项构建提示词，生成思考轨迹
    假设当前选项就是正确答案，生成从问题到答案的思考轨迹
    返回: (system_prompt, user_prompt)
    """
    # 构建所有选项列表
    options_list = []
    for i in range(1, 6):
        key = f"option {i}"
        if key in choices:
            options_list.append(f"{i}. {choices[key]}")
    
    options_str = "\n".join(options_list)
    
    system_prompt = """You are an expert telecommunications engineer. You are given a question and a correct answer. Your task is to generate a reasoning chain that explains how someone would logically arrive at this answer from the question. The given answer is DEFINITELY CORRECT - you must treat it as such and only generate positive reasoning that supports it."""
    
    user_prompt = f"""Question: {question}

Options:
{options_str}

Correct Answer: {option_text}

Task: Generate a concise, logical reasoning chain that shows how to arrive at the given answer from the question. The answer "{option_text}" is CORRECT. Your reasoning should explain why this answer is the right choice.

IMPORTANT: 
- The given answer is CORRECT - do not question it, do not say it's wrong, do not say it contradicts standards
- Do NOT mention that other options are incorrect
- Do NOT compare the given answer with other options
- Only generate positive reasoning that explains why this answer is correct
- Act as if you genuinely believe this answer is correct and explain your reasoning

Requirements:
1. Start by understanding the question and identifying the key technical concepts
2. Explain how the given answer logically follows from the question and technical knowledge
3. Provide clear, step-by-step reasoning that leads to the conclusion that this answer is correct
4. Be precise and direct - every sentence must add value
5. Eliminate all redundancy, filler words, and unnecessary elaboration
6. Ensure logical flow: question understanding → concept identification → technical reasoning → answer conclusion

Write only the reasoning chain. Do not use phrases like "Let me think", "I need to", or "Let me start". Begin directly with the analysis of the question."""

    return system_prompt, user_prompt

async def generate_reasoning_for_option(
    question: str, 
    choices: Dict[str, str], 
    option_key: str, 
    option_text: str, 
    semaphore: asyncio.Semaphore
) -> Tuple[str, str]:
    """
    为单个选项生成思考轨迹
    假设该选项就是正确答案，生成从问题到答案的思考轨迹
    返回: (option_key, reasoning_text) 或 (option_key, None) 如果出错
    """
    async with semaphore:
        try:
            # 构建提示词
            system_prompt, user_prompt = build_prompt(
                question, choices, option_key, option_text
            )
            
            # 调用API
            response = await client.chat.completions.create(
                model=MODEL_NAME,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                temperature=0.2,  # 较低温度，确保逻辑性和一致性
                max_tokens=1200,  # 限制长度，避免冗余
            )
            
            reasoning_text = response.choices[0].message.content.strip()
            return option_key, reasoning_text
            
        except Exception as e:
            print(f"  Error generating reasoning for {option_key}: {e}")
            return option_key, None

async def generate_reasoning(item: Dict[str, Any], question_key: str, semaphore: asyncio.Semaphore) -> Tuple[str, Dict[str, str], Any]:
    """
    为单个问题的所有选项生成思考轨迹
    返回: (question_key, {option_key: reasoning_text, ...}, error_info)
    """
    try:
        # 提取问题和答案
        question = item.get('question', '')
        correct_answer_key = item.get('answer', '')
        
        if not correct_answer_key.startswith('option '):
            return question_key, {}, "Invalid answer format"
        
        # 提取所有选项
        choices = {}
        for i in range(1, 6):
            key = f"option {i}"
            if key in item:
                choices[key] = item[key]
        
        # 为每个选项生成思考轨迹（假设每个选项都是正确答案）
        reasoning_tasks = []
        for option_key, option_text in choices.items():
            task = generate_reasoning_for_option(
                question, choices, option_key, option_text, semaphore
            )
            reasoning_tasks.append(task)
        
        # 并发执行所有选项的推理生成
        results = await asyncio.gather(*reasoning_tasks, return_exceptions=True)
        
        # 收集结果
        reasoning_dict = {}
        for result in results:
            if isinstance(result, Exception):
                print(f"  Exception in reasoning generation: {result}")
                continue
            opt_key, reasoning = result
            if reasoning:
                reasoning_dict[opt_key] = reasoning
        
        return question_key, reasoning_dict, None
        
    except Exception as e:
        return question_key, {}, str(e)

async def process_batch(
    items: List[Tuple[str, Dict[str, Any]]], 
    semaphore: asyncio.Semaphore,
    start_idx: int
) -> List[Tuple[str, Dict[str, str], Any]]:
    """
    并发处理一批问题
    """
    tasks = [
        generate_reasoning(item, key, semaphore) 
        for key, item in items
    ]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # 处理异常
    processed_results = []
    for i, result in enumerate(results):
        if isinstance(result, Exception):
            key, _ = items[i]
            processed_results.append((key, {}, str(result)))
        else:
            processed_results.append(result)
    
    return processed_results


async def main():
    # 1. 加载数据
    dataset = load_data(DATA_FILE)
    
    # 2. 加载或创建输出文件
    if os.path.exists(OUTPUT_FILE):
        output_data = load_data(OUTPUT_FILE)
        items_to_process = [(key, item) for key, item in dataset.items() if key not in output_data]
    else:
        output_data = {}
        items_to_process = list(dataset.items())
    
    total_to_process = len(items_to_process)
    print(f"Questions to process: {total_to_process}")
    
    if total_to_process == 0:
        print("All questions have been processed!")
        return
    # 3. 创建信号量控制并发和文件写入锁
    semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)
    
    for batch_start in range(0, total_to_process, MAX_CONCURRENT_REQUESTS):
        batch_end = min(batch_start + MAX_CONCURRENT_REQUESTS, total_to_process)
        batch = items_to_process[batch_start:batch_end]       
        # 并发处理当前批次
        results = await process_batch(batch, semaphore, batch_start)
        
        

if __name__ == "__main__":
    asyncio.run(main())

```

### 八、机器学习的各种熵

https://lumingdong.cn/various-entropies-in-machine-learning.html

#### 1. 信息量

$I = -log\ p(x)$ 表示描述一个事件不确定程度，可以看出一个事件概率越大它越确定

#### 2. 信息熵

$H(x) = -\sum p(x)log \ p(x)$ 一件事的信息熵就是想要知道这个事情发生需要多少信息

信息量是某个具体的事情发生所带来的信息，而熵是随机变量不确定性的度量

熵越大，随机变量不确定性越大，系统越混乱（无序），选某个特定值的时候具有的信息量越大，所得信息价值越高。

其实 $H(x) = \sum p(x) log \frac{1}{p(x)} = \sum p(x) *I$ 

#### 3. 交叉熵

$H(p,q)=\sum p(x)log\frac{1}{q(x)}$ 

交叉熵用来衡量在给定的真实分布下，使用非真实分布所指定的策略消除系统的不确定性所需要付出成本的大小。

#### 4. KL 散度

描述两个随机变量的距离，也就是用 q 去衡量真实的分布 p 需要多花的信息量 

正向 KL 散度 $D_{KL}(p||q) = \sum p(x)log\frac{p(x)}{q(x)} = E_{p(x)}log\frac{p(x)}{q(x)} = H(p,q) - H(p)$ 

**正向 KL 散度被称为是 zero avoiding， 要求在 p 不为 0 的地方，q 也尽量不为 0**

反向 KL 散度 $D_{KL}(q||p) = \sum q(x)log\frac{q(x)}{p(x)}$ 

**反向 KL 散度被称为是 zero forcing， 要求在 p 为 0 的地方，q 也尽量为 0**

![image-20251222195135956](assets/image-20251222195135956.png)

从上图可知，当存在多峰的时候，正向 KL 散度会偏向拟合所有的峰；而反向 KL 散度会偏向拟合其中一个峰

在大模型中 反向 KL 散度用的更多

### 九、LLaMA-Factory

#### 1. 安装

```
git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
pip install -e ".[torch,metrics]"
```

执行 `llamafactory-cli version` 看看输出是不是 welcome，如果是安装成功

#### 2. 数据格式

数据放在 data 文件夹下面即可

```json
"alpaca_zh_demo.json"
{
  "instruction": "计算这些物品的总费用。 ",
  "input": "输入：汽车 - $3000，衣服 - $100，书 - $20。",
  "output": "汽车、衣服和书的总费用为 $3000 + $100 + $20 = $3120。"
},
```

#### 3. 训练脚本

以全参微调为例，小模型不要用 Lora

在examples 下面

model 写模型的位置就行

dataset 写文件名就行，要放在data下面

eval 就是测试集

个人训练经验 epoch 设置为 2

```yaml
### model
model_name_or_path: /mnt/public/wwj/zhaozq/exp1/model/Qwen/Qwen3-8B
trust_remote_code: true
use_v1_kernels: true

### method
stage: sft
do_train: true
finetuning_type: full
deepspeed: examples/deepspeed/ds_z3_config.json

### dataset
dataset: TeleQnA_train_analysis_sft
template: qwen3
cutoff_len: 2048
overwrite_cache: true
preprocessing_num_workers: 16
dataloader_num_workers: 4

### output
output_dir: saves/qwen3-8b/full/sft_rm_listwise
logging_steps: 1
save_steps: 5000
plot_loss: true
overwrite_output_dir: true
save_only_model: false
report_to: wandb  # choices: [none, wandb, tensorboard, swanlab, mlflow]
run_name: teleqna-sft-qwen-3-8b-rm-listwise

### train
per_device_train_batch_size: 2
gradient_accumulation_steps: 1
learning_rate: 1.0e-4
num_train_epochs: 2.0
lr_scheduler_type: cosine
warmup_ratio: 0.1
bf16: true
ddp_timeout: 180000000
resume_from_checkpoint: null

### eval
val_size: 0.1
per_device_eval_batch_size: 1
eval_strategy: steps
eval_steps: 500

```

执行命令

```bash
CUDA_VISIBLE_DEVICES=2 llamafactory-cli train examples/train_full/qwen3_full_sft_autotp.yaml
```

#### 4. 权重合并（仅针对 Lora 微调）

model_name_or_path 对应base模型，adapter_name_or_path对应生成的适应器

```yaml
### Note: DO NOT use quantized model or quantization_bit when merging lora adapters

### model
model_name_or_path: meta-llama/Meta-Llama-3-8B-Instruct
adapter_name_or_path: saves/llama3-8b/lora/sft
template: llama3
trust_remote_code: true

### export
export_dir: output/llama3_lora_sft
export_size: 5
export_device: cpu  # choices: [cpu, auto]
export_legacy_format: false
```

执行 `llamafactory-cli export merge_config.yaml`

### 十、lm-evaluation-harness评价框架

#### 1. 使用

在 lm_eval/tasks/teleqna 这里是可以用来自定义任务的

下面是 运行脚本

```bash
#!/bin/bash

# 设置你的模型路径
#MODEL_PATH="/mnt/public/wwj/zhaozq/exp1/model/Qwen/Qwen3-8B"
#MODEL_PATH="/mnt/public/wwj/zhaozq/exp1/LLaMA-Factory/saves/qwen3-8b/full/sft_rl"
#MODEL_PATH="/mnt/public/wwj/zhaozq/exp1/verl/huawei/model/Qwen3-8b-pointwise"
#MODEL_PATH="/mnt/public/wwj/zhaozq/exp1/verl/huawei/model/Qwen3-8b-listwise"
MODEL_PATH="/mnt/public/wwj/zhaozq/exp1/LLaMA-Factory/saves/qwen3-8b/full/sft_combined/checkpoint-1000"
# 设置显卡 ID
export CUDA_VISIBLE_DEVICES=2

echo "开始评估..."

# 运行 lm_eval
# --log_samples: 保存每个样本的详细预测结果
# --show_config: 显示任务配置
# --verbosity DEBUG: 显示详细日志
lm_eval --model hf \
    --model_args pretrained=${MODEL_PATH},trust_remote_code=True \
    --tasks teleqna \
    --include_path /mnt/public/wwj/zhaozq/exp1/lm-evaluation-harness/lm_eval/tasks \
    --batch_size auto \
    --output_path /mnt/public/wwj/zhaozq/exp1/verl/huawei/experiments/result/mcq \
    --log_samples \
    --show_config

echo "评估结束，结果已保存"
echo ""
echo "查看详细预测结果:"
echo "  cat /mnt/public/wwj/zhaozq/exp1/verl/huawei/experiments/result/mcq/samples_*.jsonl | head -20"
```

yaml 任务脚本编写

```yaml
#task: teleqna
#dataset_path: json
#dataset_kwargs:
#  data_files:
#    test: /mnt/public/wwj/zhaozq/exp1/lm-evaluation-harness/lm_eval/tasks/teleqna/teleqna_test.jsonl
#output_type: multiple_choice
#test_split: test
#doc_to_text: "Question: {{question}}\n\nAnswer:"
#doc_to_choice: choices
#doc_to_target: target
#metric_list:
#  - metric: acc
#    aggregation: mean
#    higher_is_better: true
#  - metric: acc_norm
#    aggregation: mean
#    higher_is_better: true
#metadata:
#  version: 1.0
#
#

task: teleqna
dataset_path: json
dataset_kwargs:
  data_files:
    test: /mnt/public/wwj/zhaozq/exp1/lm-evaluation-harness/lm_eval/tasks/teleqna/teleqna_test.jsonl

output_type: multiple_choice
test_split: test

doc_to_text: >
  You are an expert telecommunications engineer. Answer the following question accurately and concisely.
  
  Question: {{question}}

  Options:
  {% for c in choices %}
  option {{ loop.index }}: {{ c }}
  {% endfor %}

  The correct answer is option 
doc_to_target: "{{target + 1}}"

generation_kwargs:
  max_gen_toks: 200
  do_sample: false
  temperature: 0.0

# 使用正则表达式从生成的文本中提取答案
# 例如从 "2: To reach a given Home Environment." 中提取 "2"
process_results: !function utils.process_results_teleqna

# 使用自定义的 exact match 指标
metric_list:
  - metric: exact_match
    aggregation: mean
    higher_is_better: true

metadata:
  version: 2.0

```

















