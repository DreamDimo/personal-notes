# DevBot é¡¹ç›®æ·±åº¦æŠ€æœ¯åˆ†æ - é¢è¯•å‡†å¤‡æ–‡æ¡£

> **é¡¹ç›®å®šä½**: åŸºäº LangGraph + Claude AI çš„çˆ¬è™«è‡ªåŠ¨å¼€å‘å·¥å…·
> **æŠ€æœ¯éš¾åº¦**: â­â­â­â­â­ (æ¶æ„è®¾è®¡ã€AIå·¥ç¨‹åŒ–ã€å·¥ä½œæµç¼–æ’)
> **ä»£ç é‡**: ~3000+ è¡Œæ ¸å¿ƒä»£ç 
> **é€‚ç”¨é¢è¯•**: é«˜çº§Pythonå·¥ç¨‹å¸ˆã€AIå·¥ç¨‹å¸ˆã€å…¨æ ˆå·¥ç¨‹å¸ˆ

---

## ç›®å½•

1. [é¡¹ç›®æ¦‚è¿°](#1-é¡¹ç›®æ¦‚è¿°)
2. [æ ¸å¿ƒæ¶æ„è®¾è®¡](#2-æ ¸å¿ƒæ¶æ„è®¾è®¡)
3. [å…³é”®æŠ€æœ¯é€‰å‹](#3-å…³é”®æŠ€æœ¯é€‰å‹)
4. [å·¥ä½œæµè®¾è®¡](#4-å·¥ä½œæµè®¾è®¡)
5. [æ ¸å¿ƒæ¨¡å—è¯¦è§£](#5-æ ¸å¿ƒæ¨¡å—è¯¦è§£)
6. [è®¾è®¡äº®ç‚¹ä¸åˆ›æ–°](#6-è®¾è®¡äº®ç‚¹ä¸åˆ›æ–°)
7. [æŠ€æœ¯éš¾ç‚¹ä¸è§£å†³æ–¹æ¡ˆ](#7-æŠ€æœ¯éš¾ç‚¹ä¸è§£å†³æ–¹æ¡ˆ)
8. [æ€§èƒ½ä¼˜åŒ–ç­–ç•¥](#8-æ€§èƒ½ä¼˜åŒ–ç­–ç•¥)
9. [å¯èƒ½é‡åˆ°çš„é—®é¢˜](#9-å¯èƒ½é‡åˆ°çš„é—®é¢˜)
10. [é¢è¯•è¦ç‚¹æ€»ç»“](#10-é¢è¯•è¦ç‚¹æ€»ç»“)

---

## 1. é¡¹ç›®æ¦‚è¿°

### 1.1 é¡¹ç›®èƒŒæ™¯

**é—®é¢˜**: æ‰‹åŠ¨å¼€å‘ç½‘ç«™çˆ¬è™«æ•ˆç‡ä½ä¸‹ï¼Œéœ€è¦é‡å¤ç¼–å†™å¤§é‡æ ·æ¿ä»£ç ï¼Œä¸”æ¯ä¸ªç½‘ç«™éƒ½éœ€è¦å•ç‹¬é€‚é…ã€‚

**è§£å†³æ–¹æ¡ˆ**: æ„å»ºä¸€ä¸ªAIé©±åŠ¨çš„è‡ªåŠ¨åŒ–çˆ¬è™«ç”Ÿæˆç³»ç»Ÿï¼Œè¾“å…¥ç½‘ç«™URLï¼Œè‡ªåŠ¨ç”Ÿæˆå®Œæ•´çš„çˆ¬è™«ä»£ç ï¼ŒåŒ…æ‹¬ï¼š
- URLæ¨¡å¼è¯†åˆ«å’Œè·¯ç”±
- é¡µé¢å†…å®¹æå–
- æ•°æ®æ¸…æ´—å’Œç»“æ„åŒ–
- é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
- Airflowè°ƒåº¦é…ç½®

### 1.2 æ ¸å¿ƒä»·å€¼

1. **è‡ªåŠ¨åŒ–ç¨‹åº¦é«˜**: ä»URLåˆ†æåˆ°ä»£ç ç”Ÿæˆï¼Œå…¨æµç¨‹è‡ªåŠ¨åŒ–
2. **è´¨é‡ä¿è¯**: å†…ç½®ä»£ç å®¡æŸ¥æœºåˆ¶ï¼Œè‡ªåŠ¨é‡è¯•å¤±è´¥æ­¥éª¤
3. **å¯è¿½æº¯æ€§**: é›†æˆGitè‡ªåŠ¨æäº¤ï¼Œæ¯ä¸ªæ­¥éª¤éƒ½æœ‰ç‰ˆæœ¬è®°å½•
4. **å¯æ‰©å±•æ€§**: åŸºäºLangGraphçš„å£°æ˜å¼å·¥ä½œæµï¼Œæ˜“äºæ·»åŠ æ–°æ­¥éª¤
5. **é•¿æœŸè®°å¿†**: SQLiteå­˜å‚¨å†å²å¯¹è¯ï¼Œæ”¯æŒæ¡ˆä¾‹æ£€ç´¢å’Œå­¦ä¹ 

### 1.3 æŠ€æœ¯æŒ‡æ ‡

- **è‡ªåŠ¨åŒ–ç‡**: 95%+ (ä»…éœ€æä¾›URLå’Œcategory)
- **æˆåŠŸç‡**: 80%+ (å¤æ‚ç½‘ç«™å¯èƒ½éœ€è¦äººå·¥ä»‹å…¥)
- **å¹³å‡ç”Ÿæˆæ—¶é—´**: 10-30åˆ†é’Ÿ/ç«™ç‚¹
- **ä»£ç è´¨é‡**: è‡ªåŠ¨å®¡æŸ¥ + è‡ªåŠ¨é‡è¯• (æœ€å¤š3æ¬¡)

---

## 2. æ ¸å¿ƒæ¶æ„è®¾è®¡

### 2.1 æ•´ä½“æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ç”¨æˆ·å±‚                             â”‚
â”‚  python -m devbot.crawler_devbot <category> <url>   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              å·¥ä½œæµç¼–æ’å±‚ (LangGraph)                 â”‚
â”‚  - StateGraph: çŠ¶æ€æœºå®šä¹‰                            â”‚
â”‚  - MemorySaver: æ–­ç‚¹ç»­ä¼                              â”‚
â”‚  - Conditional Edges: æ¡ä»¶è·¯ç”±                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â†“                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  å¼€å‘è€…èŠ‚ç‚¹       â”‚  â”‚  å®¡æŸ¥è€…èŠ‚ç‚¹       â”‚
â”‚  (17ä¸ªæ­¥éª¤)       â”‚  â”‚  (6ä¸ªæ£€æŸ¥ç‚¹)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Claude Agent SDK å±‚                     â”‚
â”‚  - å¤šAgentç®¡ç† (developer, reviewer)                 â”‚
â”‚  - Sessionç®¡ç† (é•¿æœŸå¯¹è¯ä¸Šä¸‹æ–‡)                       â”‚
â”‚  - å·¥å…·è°ƒç”¨ (Bash, Read, Write, Edit, MCP)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â†“                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  æŒä¹…åŒ–å±‚         â”‚  â”‚  å¤–éƒ¨æœåŠ¡         â”‚
â”‚  - SQLite(å¯¹è¯)  â”‚  â”‚  - Playwright    â”‚
â”‚  - Git(ç‰ˆæœ¬)     â”‚  â”‚  - BrightData    â”‚
â”‚  - JSON(çŠ¶æ€)    â”‚  â”‚  - Chrome CDP    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 åˆ†å±‚èŒè´£

| å±‚çº§ | èŒè´£ | å…³é”®æŠ€æœ¯ |
|------|------|----------|
| **ç”¨æˆ·å±‚** | å‘½ä»¤è¡Œæ¥å£ã€å‚æ•°è§£æ | argparse, asyncio |
| **å·¥ä½œæµå±‚** | æ­¥éª¤ç¼–æ’ã€çŠ¶æ€ç®¡ç†ã€æ¡ä»¶è·¯ç”± | LangGraph, TypedDict |
| **èŠ‚ç‚¹å±‚** | ä¸šåŠ¡é€»è¾‘å®ç°ã€ä»£ç ç”Ÿæˆ | Claude SDK, Jinja2 |
| **Agentå±‚** | AIäº¤äº’ã€å·¥å…·è°ƒç”¨ã€ä¸Šä¸‹æ–‡ç®¡ç† | Claude API, MCP |
| **æŒä¹…åŒ–å±‚** | æ•°æ®å­˜å‚¨ã€ç‰ˆæœ¬ç®¡ç† | SQLite, GitPython |

---

## 3. å…³é”®æŠ€æœ¯é€‰å‹

### 3.1 LangGraph vs ä¼ ç»ŸçŠ¶æ€æœº

**ä¸ºä»€ä¹ˆé€‰æ‹© LangGraph?**

| ç»´åº¦ | ä¼ ç»Ÿæ–¹å¼ | LangGraph | ä¼˜åŠ¿ |
|------|---------|-----------|------|
| çŠ¶æ€ç®¡ç† | æ‰‹åŠ¨ç»´æŠ¤å­—å…¸ | TypedDictç±»å‹å®‰å…¨ | âœ… ç±»å‹æ£€æŸ¥ |
| æµç¨‹æ§åˆ¶ | if/elseåµŒå¥— | å£°æ˜å¼è¾¹å®šä¹‰ | âœ… ä»£ç å¯è¯»æ€§ |
| æ–­ç‚¹ç»­ä¼  | æ‰‹åŠ¨åºåˆ—åŒ– | å†…ç½®checkpointer | âœ… å¼€ç®±å³ç”¨ |
| é‡è¯•æœºåˆ¶ | try/exceptå¾ªç¯ | è·¯ç”±å‡½æ•°è‡ªåŠ¨å¤„ç† | âœ… å£°æ˜å¼é‡è¯• |
| å¯è§†åŒ– | æ—  | è‡ªåŠ¨ç”ŸæˆMermaidå›¾ | âœ… è°ƒè¯•å‹å¥½ |

**å…³é”®ä»£ç ç‰‡æ®µ**:

```python
# devbot/crawler_devbot.py:102-265
workflow = StateGraph(CrawlerDevState)

# æ·»åŠ èŠ‚ç‚¹
workflow.add_node("step0__create_base_file", step0__create_base_file)
workflow.add_node("reviewer_step0", review_step0)

# æ·»åŠ è¾¹ï¼ˆæµç¨‹æ§åˆ¶ï¼‰
workflow.add_edge("step0__create_base_file", "reviewer_step0")

# æ¡ä»¶è¾¹ï¼ˆåŠ¨æ€è·¯ç”±ï¼‰
workflow.add_conditional_edges(
    "step4_1__next_pattern",
    route_by_pattern_type,  # è·¯ç”±å‡½æ•°
    {
        "step5__generate_extractor_class": "step5__generate_extractor_class",
        "step6__generate_list_extractor": "step6__generate_list_extractor",
        "step4_1__next_pattern": "step4_1__next_pattern",
        "reviewer_step4": "reviewer_step4"
    }
)
```

### 3.2 Claude SDK vs ç›´æ¥è°ƒç”¨API

**ä¸ºä»€ä¹ˆé€‰æ‹© Claude SDK?**

1. **Sessionç®¡ç†**: è‡ªåŠ¨ç»´æŠ¤å¯¹è¯ä¸Šä¸‹æ–‡ï¼Œæ— éœ€æ‰‹åŠ¨ç®¡ç†å†å²æ¶ˆæ¯
2. **å·¥å…·è°ƒç”¨**: å†…ç½® Bash/Read/Write ç­‰å·¥å…·ï¼Œè‡ªåŠ¨å¤„ç†tool_useæµç¨‹
3. **MCPé›†æˆ**: æ— ç¼é›†æˆ Playwrightã€Chrome DevTools ç­‰å¤–éƒ¨å·¥å…·
4. **Hookæœºåˆ¶**: è‡ªåŠ¨æ‰¹å‡†å·¥å…·è°ƒç”¨ï¼Œæ— éœ€äººå·¥ç¡®è®¤

**å…³é”®ä»£ç ç‰‡æ®µ**:

```python
# devbot/agent_claude/claude_agent_base.py:52-146
async def get_or_create_client(subagent_name: str, model_name: str = None):
    """è·å–æˆ–åˆ›å»ºå…¨å±€å…±äº«çš„ Claude SDK å®¢æˆ·ç«¯"""
    if subagent_name not in _global_clients:
        options = get_claude_options(model=model_name)
        client = ClaudeSDKClient(options=options)

        # å¯åŠ¨ session
        session_id = await client.start_session()
        _global_clients[subagent_name] = {
            'client': client,
            'session_id': session_id,
            'created_at': time.time()
        }

    return _global_clients[subagent_name]['client']
```

### 3.3 çŠ¶æ€æŒä¹…åŒ–ç­–ç•¥

**å¤šå±‚æ¬¡æŒä¹…åŒ–è®¾è®¡**:

| å±‚çº§ | å­˜å‚¨æ–¹å¼ | ç”¨é€” | æ–‡ä»¶ä½ç½® |
|------|---------|------|----------|
| **å†…å­˜å±‚** | MemorySaver | LangGraphä¸´æ—¶çŠ¶æ€ | å†…å­˜ |
| **æœ¬åœ°çŠ¶æ€** | JSON | æ–­ç‚¹ç»­ä¼  | `local_state_{site}.json` |
| **å¯¹è¯å†å²** | SQLite | é•¿æœŸè®°å¿†ã€æ¡ˆä¾‹æ£€ç´¢ | `data/devbot_conversations.db` |
| **ç‰ˆæœ¬æ§åˆ¶** | Git | ä»£ç ç‰ˆæœ¬è¿½è¸ª | `.git/` |

**çŠ¶æ€æ–‡ä»¶ç¤ºä¾‹**:

```json
{
  "url": "https://www.gnc.com",
  "site_name": "gnc",
  "category": "product",
  "current_step": "5",
  "current_step_name": "step5__generate_extractor_class",
  "status": "completed",
  "retry_count": 0,
  "patterns_queue": [...],
  "completed_patterns": [...],
  "step7_loop_count": 2,
  "session_id": "abc123..."
}
```

---

## 4. å·¥ä½œæµè®¾è®¡

### 4.1 å®Œæ•´æµç¨‹å›¾

```
START
  â†“
Step 0: åˆ›å»ºåŸºç¡€æ–‡ä»¶ (extractor_{site}.py æ¡†æ¶)
  â†“ [å®¡æŸ¥: æ–‡ä»¶å¯å¯¼å…¥?]
Step 1: åˆ†æé¡µé¢ç»“æ„ (æˆªå›¾ + å¼•æ“é€‰æ‹©)
  â†“ [å®¡æŸ¥: CONCURRENT_CONFIGå­˜åœ¨?]
Step 2: ç”Ÿæˆä¸»é¡µæå–å™¨ (extract_deals_from_mainpage)
  â†“ [å®¡æŸ¥: è¿”å›urlsæ•°ç»„?]
Step 2.1: URLåˆ†ç±» (detail/list/other)
  â†“ [å®¡æŸ¥: site_tree.jsonæ ¼å¼?]
Step 3: ç”Ÿæˆ URL patterns (æ­£åˆ™è¡¨è¾¾å¼ + URL_MAP)
  â†“ [å®¡æŸ¥: url_list_patternséç©º?]
Step 4: åˆå§‹åŒ– patterns é˜Ÿåˆ—
  â†“
Step 4.1: è·å–ä¸‹ä¸€ä¸ª pattern â”€â”€â”
  â”œâ”€ type=detail â†’ Step 5       â”‚
  â”œâ”€ type=list â†’ Step 6         â”‚
  â””â”€ é˜Ÿåˆ—ä¸ºç©º â†’ Reviewer Step 4 â”‚
                                â”‚
Step 5: è¯¦æƒ…é¡µæå–å™¨ (5ä¸ªå­æ­¥éª¤)â”‚
  â†“                             â”‚
Step 5.1~5.5: å„ä¸ªæå–æ–¹æ³•      â”‚
  â†“                             â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ (å›åˆ° Step 4.1)

Step 6: åˆ—è¡¨é¡µæå–å™¨
  â†“ (å›åˆ° Step 4.1)

[Reviewer Step 4: æ ¸å¿ƒæå–å™¨å®Œæˆ?]
  â†“
Step 7: æ‰©å±•ç½‘ç«™æ ‘ (è°ƒç”¨åˆ—è¡¨é¡µè·å–æ›´å¤šURL)
  â†“ [å®¡æŸ¥: site_tree.jsonæ›´æ–°?]
  â”œâ”€ æœ‰æ–°patterns â†’ å›åˆ° Step 3 (æœ€å¤š10æ¬¡)
  â””â”€ æ— æ–°patterns â†’ è¿›å…¥ Step 8

Step 8: ä»£ç ä¼˜åŒ– (ç§»é™¤å†—ä½™ã€ç»Ÿä¸€æ ¼å¼)
  â†“ [å®¡æŸ¥: ä»£ç æ•´æ´?]
Step 9: é¦–æ¬¡è¿è¡Œæµ‹è¯•
  â†“
Step 10: æ·»åŠ  Airflow DAG
  â†“
END
```

### 4.2 å…³é”®è·¯ç”±é€»è¾‘

#### 4.2.1 Patternç±»å‹è·¯ç”±

```python
# devbot/routes/routing_logic.py:60-92
def route_by_pattern_type(state: CrawlerDevState):
    """æ ¹æ® pattern ç±»å‹è·¯ç”±åˆ°ä¸åŒå¤„ç†èŠ‚ç‚¹"""
    current_pattern_info = state.get("current_pattern_info")

    if current_pattern_info is None:
        # é˜Ÿåˆ—ä¸ºç©ºï¼Œè¿›å…¥å®¡æŸ¥
        return "reviewer_step4"

    pattern_type = current_pattern_info.get('type')

    if pattern_type == 'detail':
        return "step5__generate_extractor_class"
    elif pattern_type == 'list':
        return "step6__generate_list_extractor"
    else:
        # è·³è¿‡æœªçŸ¥ç±»å‹
        return "step4_1__next_pattern"
```

#### 4.2.2 Step7å¾ªç¯æ§åˆ¶

```python
# devbot/routes/routing_logic.py:103-127
def route_after_step7(state: CrawlerDevState):
    """Step 7 åè·¯ç”±: æœ‰æ–°patterns â†’ Step 3, æ—  â†’ Step 8"""
    has_new_patterns = state.get("has_new_patterns_in_step7", False)
    loop_count = state.get("step7_loop_count", 0)
    max_loops = 10

    if loop_count >= max_loops:
        logger.warning(f"Step 7 å·²å¾ªç¯ {loop_count} æ¬¡ï¼Œå¼ºåˆ¶è¿›å…¥ Step 8")
        return "step8__analyze_markdown_info"

    if has_new_patterns:
        logger.info(f"æ£€æµ‹åˆ°æ–° patternsï¼Œå›åˆ° Step 3 (å¾ªç¯ {loop_count}/{max_loops})")
        return "step3__generate_url_patterns"
    else:
        return "step8__analyze_markdown_info"
```

### 4.3 è‡ªåŠ¨é‡è¯•æœºåˆ¶

**è®¾è®¡æ€è·¯**:
- æ¯ä¸ªå…³é”®æ­¥éª¤åæ’å…¥ Reviewer èŠ‚ç‚¹
- Reviewer éªŒè¯å¤±è´¥ â†’ å¢åŠ  retry_count
- retry_count < 3 â†’ å›åˆ°åŸæ­¥éª¤é‡è¯•
- retry_count >= 3 â†’ ç»ˆæ­¢æµç¨‹ï¼Œå‘é€Slackå‘Šè­¦

**å…³é”®ä»£ç **:

```python
# devbot/routes/routing_logic.py:17-42
def should_retry_route(state: CrawlerDevState):
    status = state.get("status")
    retry_count = state.get("retry_count", 0)

    if status == "reviewed":
        return "success"
    elif status == "failed":
        if retry_count < 3:
            return "retry"
        else:
            logger.error("å·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°(3æ¬¡)ï¼Œç»ˆæ­¢æµç¨‹")
            return "max_retry_exceeded"

    return "success"
```

---

## 5. æ ¸å¿ƒæ¨¡å—è¯¦è§£

### 5.1 çŠ¶æ€ç®¡ç† (CrawlerDevState)

**è®¾è®¡æ¨¡å¼**: TypedDictï¼ˆPython 3.8+çš„ç±»å‹å®‰å…¨å­—å…¸ï¼‰

**æ ¸å¿ƒå­—æ®µè§£æ**:

```python
# devbot/state/crawler_state.py:8-56
class CrawlerDevState(TypedDict):
    # === åŸºæœ¬ä¿¡æ¯ ===
    url: str                      # ç›®æ ‡URL
    site_name: str                # ç«™ç‚¹å (ä»URLæå–)
    category: str                 # åˆ†ç±» (product/deal/shopping)

    # === æµç¨‹æ§åˆ¶ ===
    current_step: str             # å½“å‰æ­¥éª¤ç¼–å· "0"~"10"
    current_step_name: str        # æ­¥éª¤åç§° (å¦‚ "step5__generate_extractor_class")
    status: Literal["pending", "in_progress", "completed", "reviewed", "failed"]
    retry_count: int              # å½“å‰æ­¥éª¤é‡è¯•æ¬¡æ•°

    # === ç»“æœå­˜å‚¨ ===
    result: Optional[str]         # LLMå“åº”æ–‡æœ¬
    validation_result: Optional[Dict]  # RevieweréªŒè¯ç»“æœ
    error: Optional[str]          # é”™è¯¯ä¿¡æ¯

    # === URLå¤„ç†é˜Ÿåˆ— ===
    patterns_queue: List[Dict]    # å¾…å¤„ç†çš„URL patterns
    current_pattern_info: Optional[Dict]  # å½“å‰patternä¿¡æ¯
    completed_patterns: List[str] # å·²å®Œæˆçš„patterns

    # === Step7å¾ªç¯æ§åˆ¶ ===
    has_new_patterns_in_step7: bool  # Step7æ˜¯å¦æ£€æµ‹åˆ°æ–°patterns
    step7_loop_count: int            # Step7å¾ªç¯è®¡æ•°å™¨

    # === æ–‡ä»¶è·¯å¾„ ===
    base_file_path: str           # ç”Ÿæˆçš„çˆ¬è™«æ–‡ä»¶è·¯å¾„
    output_dir: str               # è¾“å‡ºç›®å½•

    # === Claude SDK ===
    session_id: Optional[str]     # Claudeä¼šè¯ID (é•¿æœŸä¸Šä¸‹æ–‡)
```

**è®¾è®¡äº®ç‚¹**:

1. **ç±»å‹å®‰å…¨**: TypedDictæä¾›IDEè‡ªåŠ¨è¡¥å…¨å’Œç±»å‹æ£€æŸ¥
2. **æœ€å°åŒ–è®¾è®¡**: åªå­˜å‚¨å¿…è¦çŠ¶æ€ï¼Œé¿å…åºåˆ—åŒ–é—®é¢˜
3. **åˆ†å±‚ç»“æ„**: åŸºæœ¬ä¿¡æ¯ã€æµç¨‹æ§åˆ¶ã€ç»“æœå­˜å‚¨ã€é˜Ÿåˆ—ç®¡ç†åˆ†ç¦»

### 5.2 èŠ‚ç‚¹å‡½æ•° (Developer Nodes)

**è®¾è®¡æ¨¡å¼**: è£…é¥°å™¨ + å¼‚æ­¥å‡½æ•°

**ç¤ºä¾‹: Step 0 - åˆ›å»ºåŸºç¡€æ–‡ä»¶**

```python
# devbot/nodes/developer_nodes.py (ç®€åŒ–ç‰ˆ)
@step_logger  # è‡ªåŠ¨è®°å½•æ­¥éª¤å¼€å§‹/ç»“æŸå’Œè€—æ—¶
async def step0__create_base_file(state: CrawlerDevState) -> CrawlerDevState:
    """ç”Ÿæˆçˆ¬è™«æ–‡ä»¶åŸºç¡€æ¡†æ¶"""
    site_name = state["site_name"]
    category = state["category"]

    # 1. æ¸²æŸ“Jinja2æ¨¡æ¿
    template = env.get_template("tmpl_base.py.j2")
    code = template.render(
        site_name=site_name,
        site_class=f"{site_name.capitalize()}Extractor",
        entry_url=state["url"]
    )

    # 2. å†™å…¥æ–‡ä»¶
    output_path = state["base_file_path"]
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(code)

    # 3. Gitè‡ªåŠ¨æäº¤
    auto_commit_generated_file(
        file_path_list=[output_path],
        site_name=site_name,
        step_name="step0__create_base_file"
    )

    # 4. æ›´æ–°çŠ¶æ€
    return {
        **state,
        "current_step": "0",
        "current_step_name": "step0__create_base_file",
        "status": "completed",
        "result": f"åŸºç¡€æ–‡ä»¶å·²åˆ›å»º: {output_path}"
    }
```

**å…³é”®è®¾è®¡ç‚¹**:

1. **çº¯å‡½æ•°è®¾è®¡**: è¾“å…¥State â†’ è¾“å‡ºæ–°Stateï¼Œæ— å‰¯ä½œç”¨
2. **è‡ªåŠ¨æ—¥å¿—**: `@step_logger` è£…é¥°å™¨ç»Ÿä¸€å¤„ç†æ—¥å¿—
3. **Gité›†æˆ**: æ¯ä¸ªæ­¥éª¤è‡ªåŠ¨æäº¤ä»£ç 
4. **å¼‚å¸¸å¤„ç†**: ç»Ÿä¸€åœ¨å·¥ä½œæµå±‚æ•è·

### 5.3 å®¡æŸ¥èŠ‚ç‚¹ (Reviewer Nodes)

**è®¾è®¡ç›®æ ‡**:
- æŠ€æœ¯éªŒè¯ (ä»£ç èƒ½å¦æ‰§è¡Œ)
- ä¸šåŠ¡éªŒè¯ (è¾“å‡ºæ˜¯å¦ç¬¦åˆé¢„æœŸ)

**ç¤ºä¾‹: Review Step 2 - éªŒè¯åˆ—è¡¨æå–å™¨**

```python
# devbot/nodes/reviewer_nodes.py:208-321 (ç®€åŒ–ç‰ˆ)
async def review_step2(state: CrawlerDevState) -> CrawlerDevState:
    """éªŒè¯ extract_deals_from_mainpage æ˜¯å¦æ­£ç¡®å®ç°"""
    category = state["category"]
    site_name = state["site_name"]

    try:
        # 1. åŠ¨æ€å¯¼å…¥æ¨¡å—
        module_path = f'crawler.{category}.extractor_{site_name}'
        module = importlib.import_module(module_path)

        # 2. è°ƒç”¨å‡½æ•°
        extract_func = getattr(module, 'extract_deals_from_mainpage')
        result = await extract_func()

        # 3. éªŒè¯è¿”å›ç±»å‹
        if not isinstance(result, dict):
            raise ValueError(f"åº”è¿”å› dictï¼Œå®é™…: {type(result).__name__}")

        # 4. éªŒè¯å­—æ®µ
        if 'urls' not in result:
            raise ValueError("è¿”å›ç»“æœç¼ºå°‘ 'urls' å­—æ®µ")

        urls = result['urls']
        if not isinstance(urls, list):
            raise ValueError(f"urlsåº”ä¸ºlistï¼Œå®é™…: {type(urls).__name__}")

        # 5. éªŒè¯æ•°æ®æ ¼å¼
        for i, item in enumerate(urls[:3]):
            required_fields = ['title', 'url', 'type']
            for field in required_fields:
                if field not in item:
                    raise ValueError(f"urls[{i}] ç¼ºå°‘å­—æ®µ: {field}")

        # éªŒè¯é€šè¿‡
        return {
            **state,
            "status": "reviewed",
            "validation_result": {
                "step": "step2",
                "success": True,
                "message": f"åˆ—è¡¨æå–å™¨æ­£ç¡®ï¼Œæå– {len(urls)} ä¸ªé“¾æ¥"
            }
        }

    except Exception as e:
        # å‘é€ Slack å‘Šè­¦
        send_slack_exception(e, context=f"Review Step2 - {site_name}")

        # æŠ›å‡ºå¼‚å¸¸ï¼Œè®© LangGraph ç»ˆæ­¢æµç¨‹
        raise
```

**éªŒè¯ç­–ç•¥**:

| æ­¥éª¤ | éªŒè¯å†…å®¹ | å¤±è´¥å¤„ç† |
|------|---------|---------|
| Step 0 | æ–‡ä»¶å¯å¯¼å…¥ï¼Œå‡½æ•°å¯æ‰§è¡Œ | Slackå‘Šè­¦ + ç»ˆæ­¢ |
| Step 1 | CONCURRENT_CONFIGå­˜åœ¨ | Slackå‘Šè­¦ + ç»ˆæ­¢ |
| Step 2 | urlsæ•°ç»„éç©ºï¼Œæ ¼å¼æ­£ç¡® | Slackå‘Šè­¦ + ç»ˆæ­¢ |
| Step 3 | url_patternséç©ºï¼ŒURL_MAPæœ‰æ•ˆ | Slackå‘Šè­¦ + ç»ˆæ­¢ |
| Step 4 | ä¸»é¡µ+è‡³å°‘1ä¸ªè¯¦æƒ…é¡µ | å®½æ¾æ£€æŸ¥ï¼Œè­¦å‘Š |
| Step 7 | site_tree.jsonæ›´æ–° | Slackå‘Šè­¦ + ç»ˆæ­¢ |

### 5.4 Claude Agent é›†æˆ

**è®¾è®¡æ¶æ„**:

```python
# devbot/agent_claude/claude_agent_base.py

# 1. å…¨å±€å®¢æˆ·ç«¯ç®¡ç† (å•ä¾‹æ¨¡å¼)
_global_clients = {
    'crawler-developer': {
        'client': ClaudeSDKClient(...),
        'session_id': 'abc123...',
        'created_at': 1234567890
    }
}

# 2. è·å–æˆ–åˆ›å»ºå®¢æˆ·ç«¯
async def get_or_create_client(subagent_name: str, model_name: str = None):
    if subagent_name not in _global_clients:
        options = get_claude_options(model=model_name)
        client = ClaudeSDKClient(options=options)
        session_id = await client.start_session()
        _global_clients[subagent_name] = {
            'client': client,
            'session_id': session_id,
            'created_at': time.time()
        }
    return _global_clients[subagent_name]['client']

# 3. è°ƒç”¨ Subagent
async def call_subagent(
    subagent_name: str,
    prompt: str,
    state: CrawlerDevState = None
):
    client = await get_or_create_client(subagent_name)

    # æ·»åŠ ç³»ç»Ÿæç¤ºè¯ (è§’è‰²è®¾å®š)
    full_prompt = f"{PUBLIC_PROMPT}\n\n{prompt}"

    # è°ƒç”¨ Claude API
    response = await client.query(
        prompt=full_prompt,
        session_id=state.get("session_id")  # ä¿æŒä¸Šä¸‹æ–‡
    )

    # ä¿å­˜å¯¹è¯è®°å½•åˆ°SQLite
    save_conversation_from_state(state, prompt, response)

    return response
```

**å·¥å…·è‡ªåŠ¨æ‰¹å‡†æœºåˆ¶**:

```python
# devbot/agent_claude/claude_agent_base.py:52-92
async def auto_approve(input_data, tool_use_id, context):
    """è‡ªåŠ¨æ‰¹å‡† Bash/Read/Write ç­‰å·¥å…·"""
    tool_name = input_data.get('tool_name')

    if tool_name == 'Bash':
        logger.debug(f"ğŸ”§ æ‰§è¡Œå‘½ä»¤: {input_data.get('command')}")
    elif tool_name == 'Read':
        logger.debug(f"ğŸ“– è¯»å–æ–‡ä»¶: {input_data.get('file_path')}")

    return {
        "hookSpecificOutput": {
            "hookEventName": "PreToolUse",
            "permissionDecision": "allow"  # è‡ªåŠ¨æ‰¹å‡†
        }
    }

# é…ç½® Hook
hooks = {
    "PreToolUse": [
        HookMatcher(matcher="Bash", hooks=[auto_approve]),
        HookMatcher(matcher="Read", hooks=[auto_approve]),
        HookMatcher(matcher="Edit", hooks=[auto_approve]),
        HookMatcher(matcher="mcp__chrome-devtools", hooks=[auto_approve_mcp])
    ]
}
```

### 5.5 Git è‡ªåŠ¨æäº¤

**è®¾è®¡ç›®æ ‡**:
- æ¯ä¸ªæ­¥éª¤è‡ªåŠ¨æäº¤
- æäº¤ä¿¡æ¯åŒ…å«æ­¥éª¤åã€æ—¶é—´æˆ³ã€æ–‡ä»¶åˆ—è¡¨
- æ”¯æŒå›æ»šå’Œç‰ˆæœ¬å¯¹æ¯”

**å…³é”®ä»£ç **:

```python
# devbot/git_utils.py:37-120 (ç®€åŒ–ç‰ˆ)
def auto_commit_generated_file(
    file_path_list: List[str],
    site_name: str,
    step_name: str,
    description: Optional[str] = None
) -> bool:
    """è‡ªåŠ¨æäº¤ç”Ÿæˆçš„æ–‡ä»¶åˆ° Git"""

    # 1. è·å– Git ä»“åº“
    repo = Repo('.', search_parent_directories=True)

    # 2. æ·»åŠ æ–‡ä»¶åˆ°æš‚å­˜åŒº
    for file_path in file_path_list:
        relative_path = Path(file_path).relative_to(repo.working_dir)
        repo.index.add([str(relative_path)])

    # 3. ç”Ÿæˆæäº¤ä¿¡æ¯
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    commit_msg = f"""[AUTO-GEN] {site_name}: {step_name}

Generated at: {timestamp}
Files:
{chr(10).join(f"  - {p}" for p in file_path_list)}

ğŸ¤– Generated with DevBot
Co-Authored-By: DevBot <devbot@webscraper>
"""

    # 4. æäº¤
    repo.index.commit(commit_msg)
    logger.info(f"âœ… Git æäº¤æˆåŠŸ: {commit_msg.split(chr(10))[0]}")

    return True
```

**æäº¤ä¿¡æ¯ç¤ºä¾‹**:

```
[AUTO-GEN] gnc: step5_4__generate_convert_markdown

Generated at: 2024-01-15 14:32:10
Files:
  - crawler/product/extractor_gnc.py

ğŸ¤– Generated with DevBot
Co-Authored-By: DevBot <devbot@webscraper>
```

**æŸ¥çœ‹å†å²æäº¤**:

```bash
git log --grep="AUTO-GEN" --grep="gnc" --oneline
```

### 5.6 å¯¹è¯å­˜å‚¨ (ConversationStore)

**è®¾è®¡ç›®æ ‡**:
- é•¿æœŸè®°å¿†: å­˜å‚¨æ‰€æœ‰ prompt å’Œ response
- æ¡ˆä¾‹æ£€ç´¢: æ ¹æ®ç«™ç‚¹åã€æ­¥éª¤åæŸ¥è¯¢å†å²
- æ€§èƒ½ä¼˜åŒ–: SQLite ç´¢å¼•åŠ é€ŸæŸ¥è¯¢

**æ•°æ®åº“ç»“æ„**:

```sql
CREATE TABLE conversations (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    site_name TEXT NOT NULL,           -- ç«™ç‚¹å
    category TEXT NOT NULL,            -- åˆ†ç±»
    step_name TEXT NOT NULL,           -- æ­¥éª¤å
    node_name TEXT NOT NULL,           -- èŠ‚ç‚¹å
    prompt TEXT NOT NULL,              -- å‘é€ç»™ Claude çš„æç¤ºè¯
    response TEXT NOT NULL,            -- Claude çš„å“åº”
    metadata TEXT,                     -- é¢å¤–å…ƒæ•°æ® (JSON)
    timestamp TEXT NOT NULL,           -- æ—¶é—´æˆ³
    thread_id TEXT NOT NULL,           -- å¯¹è¯çº¿ç¨‹ID
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- ç´¢å¼•
CREATE INDEX idx_site_name ON conversations(site_name);
CREATE INDEX idx_thread_id ON conversations(thread_id);
CREATE INDEX idx_step_name ON conversations(step_name);
```

**ä½¿ç”¨ç¤ºä¾‹**:

```python
# devbot/store/conversation_store.py

# 1. ä¿å­˜å¯¹è¯
store = get_global_store()
store.save_conversation(
    site_name="gnc",
    category="product",
    step_name="step5__generate_extractor_class",
    node_name="step5__generate_extractor_class",
    prompt="ç”ŸæˆGNCè¯¦æƒ…é¡µæå–å™¨...",
    response="å¥½çš„ï¼Œæˆ‘æ¥ç”Ÿæˆ...",
    metadata={"pattern": "https://www.gnc.com/.*"},
    thread_id="crawler-gnc-20240115"
)

# 2. æŸ¥è¯¢å†å²
history = store.get_conversations_by_site("gnc")
for conv in history:
    print(f"[{conv.step_name}] {conv.prompt[:50]}...")

# 3. æ¡ˆä¾‹æ£€ç´¢ (æœªæ¥åŠŸèƒ½)
similar_cases = store.search_similar_conversations(
    step_name="step5__generate_extractor_class",
    site_name="gnc"
)
```

---

## 6. è®¾è®¡äº®ç‚¹ä¸åˆ›æ–°

### 6.1 å£°æ˜å¼å·¥ä½œæµ (LangGraph)

**åˆ›æ–°ç‚¹**:
- ç”¨å›¾ç»“æ„æè¿°æµç¨‹ï¼Œè€Œéå‘½ä»¤å¼ä»£ç 
- èŠ‚ç‚¹å’Œè¾¹åˆ†ç¦»ï¼Œæ˜“äºç»´æŠ¤å’Œæ‰©å±•

**å¯¹æ¯”**:

| ä¼ ç»Ÿæ–¹å¼ | LangGraphæ–¹å¼ |
|---------|-------------|
| `while True: if condition: step1(); else: step2()` | `workflow.add_conditional_edges("step1", route_func, {"step2": "step2"})` |
| ä»£ç è€¦åˆåº¦é«˜ï¼Œéš¾ä»¥ç†è§£ | æµç¨‹ä¸€ç›®äº†ç„¶ï¼Œæ˜“äºå¯è§†åŒ– |
| æ–­ç‚¹ç»­ä¼ éœ€æ‰‹åŠ¨å®ç° | è‡ªåŠ¨æŒä¹…åŒ–å’Œæ¢å¤ |

### 6.2 è‡ªåŠ¨é‡è¯•æœºåˆ¶

**åˆ›æ–°ç‚¹**:
- ä¸æ˜¯ç®€å•çš„ try-except é‡è¯•
- åŸºäºçŠ¶æ€æœºçš„é‡è¯•: failed â†’ retry â†’ in_progress â†’ completed/failed

**æµç¨‹**:

```
Developer èŠ‚ç‚¹æ‰§è¡Œ
    â†“
Reviewer èŠ‚ç‚¹éªŒè¯
    â†“
  æˆåŠŸ? â”€â”€â”€â”€Yesâ”€â”€â”€â†’ è¿›å…¥ä¸‹ä¸€æ­¥
    â”‚
   No
    â†“
retry_count < 3? â”€â”€â”€â”€Yesâ”€â”€â”€â†’ å›åˆ° Developer èŠ‚ç‚¹
    â”‚
   No
    â†“
å‘é€ Slack å‘Šè­¦ + ç»ˆæ­¢æµç¨‹
```

### 6.3 å¤š Agent åä½œ

**åˆ›æ–°ç‚¹**:
- Developer Agent: è´Ÿè´£ç”Ÿæˆä»£ç 
- Reviewer Agent: è´Ÿè´£éªŒè¯ä»£ç 
- ä¸¤è€…é€šè¿‡ State ä¼ é€’ä¿¡æ¯ï¼Œè§£è€¦åˆ

**ç±»æ¯”**:
- Developer = è½¯ä»¶å·¥ç¨‹å¸ˆ
- Reviewer = QAå·¥ç¨‹å¸ˆ
- State = Jiraå·¥å•

### 6.4 æ–­ç‚¹ç»­ä¼ 

**åˆ›æ–°ç‚¹**:
- LangGraph å†…ç½® checkpointer (å†…å­˜)
- è‡ªå®šä¹‰ JSON æŒä¹…åŒ– (æœ¬åœ°æ–‡ä»¶)
- åŒé‡ä¿éšœ: è¿›ç¨‹å´©æºƒåå¯æ¢å¤

**æ¢å¤æµç¨‹**:

```python
# devbot/crawler_devbot.py:302-355
def load_state(self) -> Optional[CrawlerDevState]:
    """ä» local_state_{site_name}.json åŠ è½½çŠ¶æ€"""
    if not self.state_file.exists():
        return None

    with open(self.state_file, 'r') as f:
        state = json.load(f)

    logger.info(f"ä»æ–­ç‚¹æ¢å¤: {state['current_step_name']}")
    return state

# æ‰§è¡Œæ—¶
async def run(self, reset: bool = False):
    if reset:
        # åˆ é™¤æ—§çŠ¶æ€ï¼Œä»å¤´å¼€å§‹
        self.state_file.unlink()
        initial_state = self.get_initial_state()
    else:
        # å°è¯•æ¢å¤æ–­ç‚¹
        initial_state = self.load_state() or self.get_initial_state()

    # æµå¼æ‰§è¡Œ
    async for event in app.astream(initial_state, config=config):
        # æ¯ä¸ªèŠ‚ç‚¹æ‰§è¡Œåè‡ªåŠ¨ä¿å­˜
        self.save_state(node_state)
```

### 6.5 å·¥ä½œæµå¯è§†åŒ–

**åˆ›æ–°ç‚¹**:
- è‡ªåŠ¨ç”Ÿæˆ Mermaid æµç¨‹å›¾
- ä¾¿äºç†è§£å’Œè°ƒè¯•

**ç”Ÿæˆä»£ç **:

```python
# devbot/crawler_devbot.py:407-430
def visualize(self):
    """ç”Ÿæˆå·¥ä½œæµå¯è§†åŒ–å›¾"""
    app = self.workflow.compile()

    # Mermaid æ ¼å¼
    mermaid_graph = app.get_graph().draw_mermaid()
    with open("workflow_graph.mmd", 'w') as f:
        f.write(mermaid_graph)

    # PNG æ ¼å¼ (éœ€è¦ graphviz)
    png_data = app.get_graph().draw_mermaid_png()
    with open("workflow_graph.png", 'wb') as f:
        f.write(png_data)
```

### 6.6 é•¿æœŸè®°å¿† (SQLiteå­˜å‚¨)

**åˆ›æ–°ç‚¹**:
- ä¸ä»…ä»…æ˜¯æ—¥å¿—ï¼Œè€Œæ˜¯ç»“æ„åŒ–å­˜å‚¨
- æ”¯æŒæ¡ˆä¾‹æ£€ç´¢å’Œå­¦ä¹ 

**æœªæ¥æ‰©å±•**:
1. **ç›¸ä¼¼æ¡ˆä¾‹æ¨è**: å½“å¤„ç†æ–°ç«™ç‚¹æ—¶ï¼ŒæŸ¥è¯¢ç›¸ä¼¼ç«™ç‚¹çš„å†å²å¯¹è¯
2. **Few-shot Learning**: å°†å†å²æˆåŠŸæ¡ˆä¾‹ä½œä¸ºç¤ºä¾‹æ³¨å…¥ Prompt
3. **é”™è¯¯åˆ†æ**: ç»Ÿè®¡å“ªäº›æ­¥éª¤å¤±è´¥ç‡é«˜ï¼Œé’ˆå¯¹æ€§ä¼˜åŒ–

---

## 7. æŠ€æœ¯éš¾ç‚¹ä¸è§£å†³æ–¹æ¡ˆ

### 7.1 éš¾ç‚¹1: åŠ¨æ€ä»£ç ç”Ÿæˆä¸éªŒè¯

**é—®é¢˜**:
- ç”Ÿæˆçš„ä»£ç å¯èƒ½æœ‰è¯­æ³•é”™è¯¯
- ç”Ÿæˆçš„ä»£ç å¯èƒ½é€»è¾‘ä¸ç¬¦åˆé¢„æœŸ
- ç”Ÿæˆçš„ä»£ç å¯èƒ½è¿è¡Œæ—¶æŠ¥é”™

**è§£å†³æ–¹æ¡ˆ**:

1. **æ¨¡æ¿åŒ–**: ä½¿ç”¨ Jinja2 æ¨¡æ¿ç”ŸæˆåŸºç¡€æ¡†æ¶ï¼Œå‡å°‘è¯­æ³•é”™è¯¯
2. **åŠ¨æ€å¯¼å…¥éªŒè¯**: æ¯ä¸ªæ­¥éª¤ååŠ¨æ€å¯¼å…¥æ¨¡å—ï¼Œæ•è·è¯­æ³•é”™è¯¯
3. **å‡½æ•°è°ƒç”¨éªŒè¯**: å®é™…è°ƒç”¨ç”Ÿæˆçš„å‡½æ•°ï¼Œæ£€æŸ¥è¿”å›å€¼æ ¼å¼
4. **è‡ªåŠ¨é‡è¯•**: éªŒè¯å¤±è´¥è‡ªåŠ¨é‡è¯•ï¼Œæœ€å¤š3æ¬¡

**ä»£ç ç‰‡æ®µ**:

```python
# devbot/nodes/reviewer_nodes.py:230-248
try:
    # åŠ¨æ€å¯¼å…¥
    module_path = f'crawler.{category}.extractor_{site_name}'
    if module_path in sys.modules:
        del sys.modules[module_path]  # å¼ºåˆ¶é‡æ–°åŠ è½½
    module = importlib.import_module(module_path)

    # è°ƒç”¨å‡½æ•°
    extract_func = getattr(module, 'extract_deals_from_mainpage')
    result = await extract_func()

    # éªŒè¯è¿”å›å€¼
    assert isinstance(result, dict), "åº”è¿”å›å­—å…¸"
    assert 'urls' in result, "ç¼ºå°‘urlså­—æ®µ"

except Exception as e:
    # å‘é€å‘Šè­¦ + ç»ˆæ­¢æµç¨‹
    send_slack_exception(e)
    raise
```

### 7.2 éš¾ç‚¹2: URL Pattern æå–

**é—®é¢˜**:
- ç½‘ç«™çš„URLç»“æ„åƒå·®ä¸‡åˆ«
- éœ€è¦ä»æ ·æœ¬URLä¸­å½’çº³å‡ºæ­£åˆ™è¡¨è¾¾å¼
- éœ€è¦åŒºåˆ†è¯¦æƒ…é¡µã€åˆ—è¡¨é¡µã€å…¶ä»–é¡µ

**è§£å†³æ–¹æ¡ˆ**:

1. **LLMåˆ†ç±»**: è®© Claude åˆ†æ URL åˆ—è¡¨ï¼Œå½’çº³å‡º patterns
2. **äººç±»éªŒè¯**: ç”Ÿæˆ sample_url ä¾›äººå·¥æ ¸å¯¹
3. **æ¸è¿›å¼æ‰©å±•**: Step7å¾ªç¯è°ƒç”¨åˆ—è¡¨é¡µï¼Œå‘ç°æ–° patterns

**Promptç¤ºä¾‹**:

```python
prompt = f"""
ä½ æ˜¯ä¸€ä¸ªURLåˆ†æä¸“å®¶ã€‚è¯·åˆ†æä»¥ä¸‹URLåˆ—è¡¨ï¼Œå½’çº³å‡ºæ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ã€‚

URLåˆ—è¡¨:
{json.dumps(urls, indent=2, ensure_ascii=False)}

è¦æ±‚:
1. åŒºåˆ†è¯¦æƒ…é¡µ (type='detail') å’Œåˆ—è¡¨é¡µ (type='list')
2. æ¯ä¸ªpatternæä¾›ä¸€ä¸ªsample_url
3. æ­£åˆ™è¡¨è¾¾å¼è¦å°½å¯èƒ½é€šç”¨ï¼Œä½†ä¸èƒ½å¤ªå®½æ³›

è¿”å›æ ¼å¼:
{{
  "url_detail_patterns": [
    {{
      "pattern": "^https://www\\.gnc\\.com/.*\\.html$",
      "type": "detail",
      "description": "å•†å“è¯¦æƒ…é¡µ",
      "sample_url": "https://www.gnc.com/product/123.html"
    }}
  ],
  "url_list_patterns": [...]
}}
"""
```

### 7.3 éš¾ç‚¹3: å¹¶å‘æ§åˆ¶

**é—®é¢˜**:
- Playwright æµè§ˆå™¨å®ä¾‹æ¶ˆè€—å¤§é‡å†…å­˜
- å¹¶å‘è¿‡é«˜å¯èƒ½è¢«ç½‘ç«™å°IP
- éœ€è¦åœ¨æ€§èƒ½å’Œç¨³å®šæ€§ä¹‹é—´å¹³è¡¡

**è§£å†³æ–¹æ¡ˆ**:

1. **æµè§ˆå™¨æ± **: é™åˆ¶æµè§ˆå™¨å®ä¾‹æ•°é‡
2. **æ ‡ç­¾é¡µå¤ç”¨**: ä¸€ä¸ªæµè§ˆå™¨æ‰“å¼€å¤šä¸ªæ ‡ç­¾é¡µ
3. **è¯·æ±‚é—´éš”**: æ¯ä¸ªè¯·æ±‚ä¹‹é—´å»¶è¿Ÿ0.5ç§’
4. **BrightDataæ‰¹é‡**: å¯¹äºé™æ€é¡µé¢ï¼Œä½¿ç”¨ä»£ç†æ‰¹é‡è·å–HTML

**é…ç½®ç¤ºä¾‹**:

```python
CONCURRENT_CONFIG = {
    'pool_size': 3,               # 3ä¸ªæµè§ˆå™¨å®ä¾‹
    'tab_size': 5,                # æ¯ä¸ªæµè§ˆå™¨5ä¸ªæ ‡ç­¾é¡µ (å…±15å¹¶å‘)
    'delay_between_requests': 0.5, # è¯·æ±‚é—´éš”0.5ç§’
    'use_brightdata': True,       # ä½¿ç”¨BrightData
    'brightdata_batch_size': 20   # æ‰¹é‡è·å–20ä¸ªé¡µé¢
}
```

### 7.4 éš¾ç‚¹4: Session ä¸Šä¸‹æ–‡ç®¡ç†

**é—®é¢˜**:
- Claude API æœ‰tokené™åˆ¶ (200K tokens)
- éœ€è¦åœ¨é•¿æœŸå¯¹è¯ä¸­ä¿æŒä¸Šä¸‹æ–‡
- ä¸åŒæ­¥éª¤ä¹‹é—´éœ€è¦å…±äº«ä¿¡æ¯

**è§£å†³æ–¹æ¡ˆ**:

1. **Session ID**: ä½¿ç”¨ Claude SDK çš„ session_id æœºåˆ¶
2. **Stateä¼ é€’**: å…³é”®ä¿¡æ¯å­˜å‚¨åœ¨ State ä¸­ï¼Œè€Œéä¾èµ–å¯¹è¯å†å²
3. **å¯¹è¯å­˜å‚¨**: SQLiteå­˜å‚¨å†å²å¯¹è¯ï¼Œå¿…è¦æ—¶æ£€ç´¢

**å…³é”®ä»£ç **:

```python
# devbot/agent_claude/claude_agent_base.py
async def call_subagent(subagent_name: str, prompt: str, state: CrawlerDevState):
    client = await get_or_create_client(subagent_name)

    # ä½¿ç”¨ session_id ä¿æŒä¸Šä¸‹æ–‡
    response = await client.query(
        prompt=prompt,
        session_id=state.get("session_id")  # ä» State è·å–
    )

    return response
```

### 7.5 éš¾ç‚¹5: é”™è¯¯åˆ†ç±»ä¸å¤„ç†

**é—®é¢˜**:
- é”™è¯¯ç±»å‹å¤šæ ·: è¯­æ³•é”™è¯¯ã€é€»è¾‘é”™è¯¯ã€ç½‘ç»œé”™è¯¯ã€åçˆ¬è™«é”™è¯¯
- éœ€è¦åŒºåˆ†å“ªäº›é”™è¯¯å¯é‡è¯•ï¼Œå“ªäº›éœ€è¦äººå·¥ä»‹å…¥

**è§£å†³æ–¹æ¡ˆ**:

1. **è‡ªå®šä¹‰å¼‚å¸¸ç±»**:

```python
# devbot/agent_claude/claude_agent_base.py:21-43
class SubagentError(Exception):
    """Subagent æ‰§è¡Œé”™è¯¯åŸºç±»"""
    pass

class PromptError(SubagentError):
    """Prompt æœ¬èº«æœ‰é—®é¢˜ (ä¿¡æ¯ç¼ºå¤±ã€å·¥å…·æœªæˆæƒ)"""
    pass

class TaskUnachievableError(SubagentError):
    """ä»»åŠ¡æ— æ³•è¾¾æˆ (æŠ€æœ¯å—é™ã€å¤šæ¬¡å¤±è´¥)"""
    pass

class HumanInterventionRequired(SubagentError):
    """éœ€è¦äººå·¥ä»‹å…¥"""
    pass
```

2. **åˆ†çº§å¤„ç†**:

| é”™è¯¯ç±»å‹ | å¤„ç†æ–¹å¼ |
|---------|---------|
| è¯­æ³•é”™è¯¯ | è‡ªåŠ¨é‡è¯• (ä¿®æ”¹ä»£ç ) |
| é€»è¾‘é”™è¯¯ | è‡ªåŠ¨é‡è¯• (é‡æ–°åˆ†æ) |
| ç½‘ç»œé”™è¯¯ | è‡ªåŠ¨é‡è¯• (æ¢ä»£ç†) |
| åçˆ¬è™« | Slackå‘Šè­¦ + äººå·¥ä»‹å…¥ |
| PromptError | Slackå‘Šè­¦ + ç»ˆæ­¢ |

---

## 8. æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### 8.1 å¹¶å‘ä¼˜åŒ–

**ç­–ç•¥**:

1. **BrightDataæ‰¹é‡çˆ¬å–**:
   - ä¼ ç»Ÿæ–¹å¼: é€ä¸ªæ‰“å¼€æµè§ˆå™¨ï¼Œè®¿é—®é¡µé¢ â†’ ä¸²è¡Œï¼Œæ…¢
   - ä¼˜åŒ–æ–¹å¼: æ‰¹é‡å‘é€URLç»™BrightData â†’ å¹¶è¡Œï¼Œå¿«10å€

```python
# æ‰¹é‡è·å–HTML
urls = ["url1", "url2", ..., "url20"]
html_list = await brightdata_batch_fetch(urls)
```

2. **æµè§ˆå™¨æ± å¤ç”¨**:
   - é¿å…é¢‘ç¹åˆ›å»º/é”€æ¯æµè§ˆå™¨
   - ä½¿ç”¨å¼‚æ­¥é˜Ÿåˆ—ç®¡ç†æµè§ˆå™¨å®ä¾‹

```python
# crawler/base/extractor_base.py
class BrowserPool:
    def __init__(self, pool_size=3, tab_size=5):
        self.pool_size = pool_size
        self.tab_size = tab_size
        self.queue = asyncio.Queue(maxsize=pool_size * tab_size)

    async def acquire(self):
        return await self.queue.get()

    async def release(self, page):
        await self.queue.put(page)
```

### 8.2 ç¼“å­˜ä¼˜åŒ–

**ç­–ç•¥**:

1. **HTMLç¼“å­˜**:
   - å·²çˆ¬å–çš„é¡µé¢HTMLå­˜å‚¨åˆ° Redis
   - é¿å…é‡å¤è¯·æ±‚

2. **URLå»é‡**:
   - ä½¿ç”¨ Set å­˜å‚¨å·²å¤„ç†çš„ URL
   - é¿å…é‡å¤çˆ¬å–

3. **Patternç¼“å­˜**:
   - Step3ç”Ÿæˆçš„ URL patterns ç¼“å­˜åˆ°æ–‡ä»¶
   - Step7åªæ£€æŸ¥æ–°å‘ç°çš„ patterns

### 8.3 å†…å­˜ä¼˜åŒ–

**ç­–ç•¥**:

1. **å›¾ç‰‡å‹ç¼©**:
   - æˆªå›¾è‡ªåŠ¨å‹ç¼©åˆ° 1.5MB ä»¥ä¸‹
   - è¶…è¿‡ 8000px è‡ªåŠ¨åˆ‡åˆ†

```python
# devbot/tool.py:58-100
def compress_image(image_path: str, max_size=1.5*1024*1024):
    """å‹ç¼©å›¾ç‰‡åˆ°æŒ‡å®šå¤§å°"""
    img = Image.open(image_path)

    # å¦‚æœè¶…è¿‡ 8000pxï¼Œåˆ‡åˆ†
    if img.width > 8000 or img.height > 8000:
        return split_image(img)

    # å‹ç¼©è´¨é‡
    quality = 85
    while quality >= 30:
        buffer = io.BytesIO()
        img.save(buffer, format='WEBP', quality=quality)
        if buffer.tell() <= max_size:
            break
        quality -= 5

    return buffer.getvalue()
```

2. **åŠæ—¶å…³é—­èµ„æº**:
   - ä½¿ç”¨ `async with` è‡ªåŠ¨ç®¡ç†ä¸Šä¸‹æ–‡
   - èŠ‚ç‚¹æ‰§è¡Œåç«‹å³é‡Šæ”¾æµè§ˆå™¨

### 8.4 æ—¥å¿—ä¼˜åŒ–

**ç­–ç•¥**:

1. **åˆ†çº§æ—¥å¿—**:
   - DEBUG: å·¥å…·è°ƒç”¨è¯¦æƒ…
   - INFO: æ­¥éª¤å¼€å§‹/ç»“æŸ
   - WARNING: å¯æ¢å¤é”™è¯¯
   - ERROR: è‡´å‘½é”™è¯¯

2. **è£…é¥°å™¨ç»Ÿä¸€å¤„ç†**:

```python
# devbot/nodes/developer_nodes.py:48-75
@functools.wraps(func)
async def step_logger(func):
    """è‡ªåŠ¨è®°å½•æ­¥éª¤å¼€å§‹/ç»“æŸå’Œè€—æ—¶"""
    step_num = extract_step_num(func.__name__)

    logger.info(f"=" * 60)
    logger.info(f"Step {step_num}: {func.__name__}")
    logger.info(f"=" * 60)

    start_time = time.time()
    try:
        result = await func(state)
        elapsed = time.time() - start_time
        logger.info(f"âœ… Step {step_num} å®Œæˆ (è€—æ—¶ {elapsed:.1f}s)")
        return result
    except Exception as e:
        elapsed = time.time() - start_time
        logger.error(f"âŒ Step {step_num} å¤±è´¥ (è€—æ—¶ {elapsed:.1f}s): {e}")
        raise
```

---

## 9. å¯èƒ½é‡åˆ°çš„é—®é¢˜

### 9.1 å¼€å‘é˜¶æ®µé—®é¢˜

#### é—®é¢˜1: LangGraph ç‰ˆæœ¬å…¼å®¹æ€§

**ç°è±¡**: å‡çº§ LangGraph åï¼ŒæŸäº› API å¤±æ•ˆ

**åŸå› **: LangGraph è¿˜åœ¨å¿«é€Ÿè¿­ä»£ï¼ŒAPI ä¸ç¨³å®š

**è§£å†³æ–¹æ¡ˆ**:
- å›ºå®šç‰ˆæœ¬å·: `langgraph==0.0.40`
- å®šæœŸæŸ¥çœ‹ Changelog: https://github.com/langchain-ai/langgraph/releases
- ä½¿ç”¨ `pip list --outdated` æ£€æŸ¥æ›´æ–°

#### é—®é¢˜2: Claude API Rate Limit

**ç°è±¡**:
```
429 Too Many Requests: rate_limit_error
```

**åŸå› **:
- Claude API æœ‰è¯·æ±‚é¢‘ç‡é™åˆ¶
- Tier 1: 50 requests/min
- Tier 2: 1000 requests/min

**è§£å†³æ–¹æ¡ˆ**:
1. **é‡è¯•æœºåˆ¶**: é‡åˆ° 429 è‡ªåŠ¨ç­‰å¾… 60 ç§’åé‡è¯•
2. **è¯·æ±‚é—´éš”**: æ¯æ¬¡è¯·æ±‚åå»¶è¿Ÿ 1 ç§’
3. **æ‰¹é‡å¤„ç†**: åˆå¹¶å¤šä¸ªå°è¯·æ±‚ä¸ºä¸€ä¸ªå¤§è¯·æ±‚

```python
async def call_claude_with_retry(prompt: str, max_retries=3):
    for i in range(max_retries):
        try:
            return await client.query(prompt)
        except RateLimitError:
            if i < max_retries - 1:
                wait_time = 60 * (i + 1)
                logger.warning(f"é‡åˆ°é™æµï¼Œç­‰å¾… {wait_time}ç§’...")
                await asyncio.sleep(wait_time)
            else:
                raise
```

#### é—®é¢˜3: æ¨¡å—å¯¼å…¥å¤±è´¥

**ç°è±¡**:
```python
ModuleNotFoundError: No module named 'crawler.product.extractor_gnc'
```

**åŸå› **:
- Python ç¼“å­˜äº†æ—§ç‰ˆæœ¬çš„æ¨¡å—
- æ–°ç”Ÿæˆçš„ä»£ç æœªè¢«è¯†åˆ«

**è§£å†³æ–¹æ¡ˆ**:
```python
# å¼ºåˆ¶é‡æ–°åŠ è½½æ¨¡å—
module_path = f'crawler.{category}.extractor_{site_name}'
if module_path in sys.modules:
    del sys.modules[module_path]  # åˆ é™¤ç¼“å­˜
module = importlib.import_module(module_path)
```

### 9.2 è¿è¡Œæ—¶é—®é¢˜

#### é—®é¢˜4: æµè§ˆå™¨èµ„æºè€—å°½

**ç°è±¡**:
```
playwright._impl._api_types.Error: Target page, context or browser has been closed
```

**åŸå› **:
- å¹¶å‘è¿‡é«˜ï¼Œæµè§ˆå™¨å®ä¾‹è¶…è¿‡ç³»ç»Ÿé™åˆ¶
- é¡µé¢æœªæ­£ç¡®å…³é—­ï¼Œèµ„æºæ³„æ¼

**è§£å†³æ–¹æ¡ˆ**:
1. **é™ä½å¹¶å‘**:
```python
CONCURRENT_CONFIG = {
    'pool_size': 2,  # ä»3é™åˆ°2
    'tab_size': 3    # ä»5é™åˆ°3
}
```

2. **è‡ªåŠ¨æ¸…ç†**:
```python
async def cleanup():
    """æ¸…ç†æ‰€æœ‰æµè§ˆå™¨èµ„æº"""
    for browser in browser_pool:
        await browser.close()
```

#### é—®é¢˜5: Git æäº¤å†²çª

**ç°è±¡**:
```
fatal: You have unstaged changes
```

**åŸå› **:
- æ‰‹åŠ¨ä¿®æ”¹äº†ä»£ç ï¼Œä½†æœªæäº¤
- DevBot å°è¯•è‡ªåŠ¨æäº¤æ—¶å‘ç°å†²çª

**è§£å†³æ–¹æ¡ˆ**:
1. **ç¦ç”¨è‡ªåŠ¨æäº¤**:
```bash
export DEVBOT_AUTO_COMMIT=false
python -m devbot.crawler_devbot product https://www.gnc.com
```

2. **æ‰‹åŠ¨å¤„ç†**:
```bash
git status
git add .
git commit -m "æ‰‹åŠ¨ä¿®æ”¹"
python -m devbot.crawler_devbot product https://www.gnc.com --entry step5
```

### 9.3 æ•°æ®è´¨é‡é—®é¢˜

#### é—®é¢˜6: URL Pattern ä¸å‡†ç¡®

**ç°è±¡**: ç”Ÿæˆçš„æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…äº†é”™è¯¯çš„URL

**åŸå› **:
- LLM å½’çº³èƒ½åŠ›æœ‰é™
- æ ·æœ¬URLä¸å¤Ÿä»£è¡¨æ€§

**è§£å†³æ–¹æ¡ˆ**:
1. **äººå·¥å®¡æŸ¥**:
   - æŸ¥çœ‹ `site_tree.json` ä¸­çš„ `site_patterns`
   - æ‰‹åŠ¨è°ƒæ•´ä¸å‡†ç¡®çš„ pattern

2. **æä¾›æ›´å¤šæ ·æœ¬**:
   - Step7 å¾ªç¯è°ƒç”¨åˆ—è¡¨é¡µï¼Œè·å–æ›´å¤šURL
   - å¢åŠ  `step7_loop_count` ä¸Šé™ (é»˜è®¤10æ¬¡)

#### é—®é¢˜7: æå–çš„æ•°æ®ä¸å®Œæ•´

**ç°è±¡**: å•†å“ä¿¡æ¯ç¼ºå°‘ä»·æ ¼ã€å›¾ç‰‡ç­‰å­—æ®µ

**åŸå› **:
- ç½‘é¡µç»“æ„å¤æ‚ï¼ŒLLM æœªèƒ½æ­£ç¡®è¯†åˆ«
- åŠ¨æ€åŠ è½½å†…å®¹æœªç­‰å¾…

**è§£å†³æ–¹æ¡ˆ**:
1. **æ‰‹åŠ¨è¡¥å…… Prompt**:
   - ä¿®æ”¹ `step5_3__generate_extract_main_content` çš„ Prompt
   - æ˜ç¡®å‘Šè¯‰ Claude éœ€è¦æå–å“ªäº›å­—æ®µ

2. **å¢åŠ ç­‰å¾…æ—¶é—´**:
```python
async def fetch_rendered_html(self, page):
    await page.goto(url)
    await page.wait_for_timeout(3000)  # ç­‰å¾…3ç§’
    await page.wait_for_selector('.product-info')  # ç­‰å¾…å…ƒç´ å‡ºç°
```

---

## 10. é¢è¯•è¦ç‚¹æ€»ç»“

### 10.1 æ ¸å¿ƒç«äº‰åŠ›

1. **æ¶æ„è®¾è®¡èƒ½åŠ›**:
   - ç†è§£ LangGraph çš„å£°æ˜å¼å·¥ä½œæµ
   - æŒæ¡å¤šå±‚æ¶æ„è®¾è®¡ (å·¥ä½œæµå±‚ã€èŠ‚ç‚¹å±‚ã€Agentå±‚ã€æŒä¹…åŒ–å±‚)

2. **AI å·¥ç¨‹åŒ–èƒ½åŠ›**:
   - ç†Ÿæ‚‰ Claude SDK çš„ä½¿ç”¨
   - ç†è§£ Prompt å·¥ç¨‹å’Œ Few-shot Learning
   - æŒæ¡å¤š Agent åä½œæ¨¡å¼

3. **å·¥ç¨‹åŒ–èƒ½åŠ›**:
   - æ–­ç‚¹ç»­ä¼ ã€è‡ªåŠ¨é‡è¯•ã€é”™è¯¯å¤„ç†
   - Git è‡ªåŠ¨åŒ–ã€æ—¥å¿—è§„èŒƒã€æ€§èƒ½ä¼˜åŒ–

4. **é—®é¢˜è§£å†³èƒ½åŠ›**:
   - æŠ€æœ¯éš¾ç‚¹çš„è¯†åˆ«å’Œè§£å†³
   - æƒè¡¡å–èˆ (æ€§èƒ½ vs ç¨³å®šæ€§)

### 10.2 é¢è¯•é«˜é¢‘é—®é¢˜

#### Q1: ä¸ºä»€ä¹ˆé€‰æ‹© LangGraph è€Œä¸æ˜¯è‡ªå·±å®ç°çŠ¶æ€æœº?

**ç­”**:
1. **ç±»å‹å®‰å…¨**: TypedDict æä¾›ç¼–è¯‘æ—¶æ£€æŸ¥
2. **æ–­ç‚¹ç»­ä¼ **: å†…ç½® checkpointerï¼Œæ— éœ€æ‰‹åŠ¨å®ç°
3. **å¯è§†åŒ–**: è‡ªåŠ¨ç”Ÿæˆæµç¨‹å›¾ï¼Œä¾¿äºè°ƒè¯•
4. **ç¤¾åŒºæ”¯æŒ**: LangChain ç”Ÿæ€ï¼Œæ–‡æ¡£å®Œå–„

#### Q2: å¦‚ä½•ä¿è¯ç”Ÿæˆä»£ç çš„è´¨é‡?

**ç­”**:
1. **æ¨¡æ¿åŒ–**: Jinja2 æ¨¡æ¿ç”ŸæˆåŸºç¡€æ¡†æ¶
2. **è‡ªåŠ¨å®¡æŸ¥**: Reviewer èŠ‚ç‚¹éªŒè¯ä»£ç 
3. **è‡ªåŠ¨é‡è¯•**: å¤±è´¥æœ€å¤šé‡è¯•3æ¬¡
4. **äººå·¥å…œåº•**: Slack å‘Šè­¦ + äººå·¥ä»‹å…¥

#### Q3: å¦‚ä½•å¤„ç†å¤æ‚ç½‘ç«™ (åçˆ¬è™«ã€åŠ¨æ€åŠ è½½)?

**ç­”**:
1. **BrightData**: ä»£ç†æœåŠ¡ç»•è¿‡åçˆ¬è™«
2. **Playwright**: çœŸå®æµè§ˆå™¨æ¸²æŸ“ï¼Œå¤„ç†åŠ¨æ€åŠ è½½
3. **ç­‰å¾…ç­–ç•¥**: `wait_for_selector` ç¡®ä¿å…ƒç´ åŠ è½½å®Œæˆ
4. **é™çº§æ–¹æ¡ˆ**: åçˆ¬è™«ä¸¥é‡æ—¶ï¼Œäººå·¥æä¾›HTML

#### Q4: å¦‚ä½•ä¼˜åŒ–æ€§èƒ½?

**ç­”**:
1. **å¹¶å‘æ§åˆ¶**: æµè§ˆå™¨æ±  + BrightDataæ‰¹é‡
2. **ç¼“å­˜ä¼˜åŒ–**: HTMLç¼“å­˜ã€URLå»é‡
3. **å†…å­˜ä¼˜åŒ–**: å›¾ç‰‡å‹ç¼©ã€èµ„æºåŠæ—¶é‡Šæ”¾
4. **æ—¥å¿—ä¼˜åŒ–**: åˆ†çº§æ—¥å¿—ã€è£…é¥°å™¨ç»Ÿä¸€å¤„ç†

#### Q5: é¡¹ç›®çš„å¯æ‰©å±•æ€§å¦‚ä½•?

**ç­”**:
1. **æ·»åŠ æ–°æ­¥éª¤**:
   - åœ¨ `nodes/` ä¸‹åˆ›å»ºæ–°å‡½æ•°
   - åœ¨ `_build_workflow()` ä¸­æ·»åŠ èŠ‚ç‚¹å’Œè¾¹

2. **æ·»åŠ æ–° Agent**:
   - åœ¨ `agent_claude/` ä¸‹åˆ›å»ºæ–° subagent
   - é…ç½®ä¸åŒçš„ç³»ç»Ÿæç¤ºè¯å’Œå·¥å…·

3. **æ·»åŠ æ–°éªŒè¯è§„åˆ™**:
   - åœ¨ `nodes/reviewer_nodes.py` ä¸­æ·»åŠ æ–°éªŒè¯å‡½æ•°
   - åœ¨å·¥ä½œæµä¸­æ’å…¥å®¡æŸ¥èŠ‚ç‚¹

#### Q6: å¦‚ä½•å¤„ç†è¾¹ç•Œæƒ…å†µ?

**ç­”**:
1. **ç©ºç»“æœ**:
   - extract_deals_from_mainpage è¿”å›ç©ºæ•°ç»„
   - Reviewer ç»™å‡ºè­¦å‘Šè€Œéé”™è¯¯

2. **è¶…æ—¶**:
   - Playwright è®¾ç½® timeout
   - è¶…æ—¶åè‡ªåŠ¨é‡è¯•

3. **ç½‘ç»œé”™è¯¯**:
   - è‡ªåŠ¨é‡è¯• (æŒ‡æ•°é€€é¿)
   - å¤±è´¥ååˆ‡æ¢ä»£ç†

4. **åçˆ¬è™«**:
   - è¯†åˆ«éªŒè¯ç é¡µé¢
   - Slack å‘Šè­¦ + äººå·¥ä»‹å…¥

### 10.3 äº®ç‚¹æ€»ç»“

| äº®ç‚¹ | è¯´æ˜ | ä½“ç°èƒ½åŠ› |
|------|------|---------|
| **LangGraph å·¥ä½œæµ** | å£°æ˜å¼ç¼–æ’ï¼Œè‡ªåŠ¨æŒä¹…åŒ– | æ¶æ„è®¾è®¡ |
| **å¤š Agent åä½œ** | Developer + Reviewer | AI å·¥ç¨‹åŒ– |
| **è‡ªåŠ¨é‡è¯•æœºåˆ¶** | åŸºäºçŠ¶æ€æœºçš„æ™ºèƒ½é‡è¯• | å·¥ç¨‹åŒ–èƒ½åŠ› |
| **Git è‡ªåŠ¨æäº¤** | æ¯æ­¥éª¤è‡ªåŠ¨ç‰ˆæœ¬ç®¡ç† | DevOps |
| **å¯¹è¯å­˜å‚¨** | SQLite é•¿æœŸè®°å¿† | æ•°æ®å·¥ç¨‹ |
| **å¹¶å‘ä¼˜åŒ–** | æµè§ˆå™¨æ±  + BrightData | æ€§èƒ½ä¼˜åŒ– |
| **é”™è¯¯åˆ†ç±»** | è‡ªå®šä¹‰å¼‚å¸¸ä½“ç³» | é—®é¢˜è§£å†³ |
| **å¯è§†åŒ–** | Mermaid æµç¨‹å›¾ | å·¥ç¨‹åŒ–æ€ç»´ |

### 10.4 å‡†å¤‡å»ºè®®

1. **ç†Ÿæ‚‰æ ¸å¿ƒä»£ç **:
   - `crawler_devbot.py`: å·¥ä½œæµå®šä¹‰
   - `developer_nodes.py`: èŠ‚ç‚¹å®ç°
   - `routing_logic.py`: è·¯ç”±é€»è¾‘
   - `claude_agent_base.py`: Agent ç®¡ç†

2. **è¿è¡Œ Demo**:
   - é€‰ä¸€ä¸ªç®€å•ç½‘ç«™ (å¦‚ GNC)
   - å®Œæ•´è¿è¡Œä¸€éæµç¨‹
   - è§‚å¯Ÿæ¯ä¸ªæ­¥éª¤çš„è¾“å‡º

3. **æŸ¥çœ‹ç”Ÿæˆä»£ç **:
   - æ‰“å¼€ `crawler/product/extractor_gnc.py`
   - ç†è§£ç”Ÿæˆçš„ä»£ç ç»“æ„
   - å¯¹æ¯”æ¨¡æ¿æ–‡ä»¶ `tmpl_base.py.j2`

4. **é˜…è¯»æ–‡æ¡£**:
   - `devbot/README.md`: é¡¹ç›®æ–‡æ¡£
   - `devbot/å‘½ä»¤å¤§å…¨.md`: å‘½ä»¤å‚è€ƒ
   - LangGraph å®˜æ–¹æ–‡æ¡£: https://langchain-ai.github.io/langgraph/

5. **å‡†å¤‡æ¡ˆä¾‹**:
   - é€‰ 2-3 ä¸ªæŠ€æœ¯éš¾ç‚¹ï¼Œå‡†å¤‡è¯¦ç»†è¯´æ˜
   - å‡†å¤‡ 1-2 ä¸ªä¼˜åŒ–æ¡ˆä¾‹ (æ€§èƒ½ã€è´¨é‡)
   - å‡†å¤‡ 1-2 ä¸ªè¾¹ç•Œæƒ…å†µå¤„ç†æ¡ˆä¾‹

---

## ç»“è¯­

DevBot æ˜¯ä¸€ä¸ªèåˆäº† **AI å·¥ç¨‹åŒ–**ã€**å·¥ä½œæµç¼–æ’**ã€**ä»£ç ç”Ÿæˆ** ç­‰å¤šé¡¹æŠ€æœ¯çš„å¤æ‚ç³»ç»Ÿã€‚é¢è¯•æ—¶é‡ç‚¹çªå‡ºï¼š

1. **æ¶æ„è®¾è®¡æ€ç»´**: åˆ†å±‚ã€è§£è€¦ã€å¯æ‰©å±•
2. **å·¥ç¨‹åŒ–èƒ½åŠ›**: æ–­ç‚¹ç»­ä¼ ã€è‡ªåŠ¨é‡è¯•ã€æ—¥å¿—è§„èŒƒ
3. **AI åº”ç”¨èƒ½åŠ›**: Prompt å·¥ç¨‹ã€å¤š Agent åä½œ
4. **é—®é¢˜è§£å†³èƒ½åŠ›**: æŠ€æœ¯éš¾ç‚¹çš„è¯†åˆ«å’Œè§£å†³

**æ ¸å¿ƒç†å¿µ**:
> ç”¨ AI ç”Ÿæˆä»£ç ï¼Œç”¨å·¥ç¨‹åŒ–ä¿è¯è´¨é‡ï¼Œç”¨è‡ªåŠ¨åŒ–æå‡æ•ˆç‡

ç¥é¢è¯•é¡ºåˆ©! ğŸš€

---

## 11. WebScraper çˆ¬è™«é¡¹ç›®æ¶æ„

### 11.1 é¡¹ç›®æ¦‚è¿°

**WebScraper** æ˜¯ä¸€ä¸ªå¤šé¢†åŸŸç”µå•†æ•°æ®çˆ¬å–å¹³å°ï¼Œé‡‡ç”¨ä¸‰å±‚æ¶æ„è®¾è®¡ï¼Œæ”¯æŒProductã€Dealã€Shoppingç­‰å¤šç§ç±»åˆ«çš„æ•°æ®æŠ“å–ã€‚

**æ ¸å¿ƒä»·å€¼**:
1. **é€šç”¨æ€§å¼º**: åŸºäºæ¨¡æ¿æ–¹æ³•æ¨¡å¼ï¼Œå¿«é€Ÿé€‚é…æ–°ç½‘ç«™
2. **å¹¶å‘èƒ½åŠ›é«˜**: Browser Pool + BrightDataæ‰¹é‡çˆ¬å–
3. **æ‰©å±•æ€§å¥½**: Mixinæ¨¡å¼æä¾›å¯é€‰åŠŸèƒ½ï¼Œä¸ä¾µå…¥æ ¸å¿ƒé€»è¾‘
4. **å¯ç»´æŠ¤æ€§é«˜**: ä¸‰å±‚è§£è€¦ï¼ŒèŒè´£æ¸…æ™°

### 11.2 ä¸‰å±‚æ¶æ„è®¾è®¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  extractor_scheduler.py (è°ƒåº¦ç¼–æ’å±‚)                      â”‚
â”‚  - BFSéå†URLæ ‘                                          â”‚
â”‚  - åŠ¨æ€åŠ è½½ç«™ç‚¹æ¨¡å—                                        â”‚
â”‚  - å¹¶å‘ä»»åŠ¡è°ƒåº¦                                           â”‚
â”‚  - TracePageæ•°æ®åº“ç®¡ç†                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚ è°ƒç”¨
                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  extractor_<site>.py (ç«™ç‚¹é€‚é…å±‚)                         â”‚
â”‚  - URL_MAP è·¯ç”±è§„åˆ™                                      â”‚
â”‚  - åˆ—è¡¨é¡µæå–å‡½æ•°                                         â”‚
â”‚  - è¯¦æƒ…é¡µExtractorç±»                                     â”‚
â”‚  - CONCURRENT_CONFIG å¹¶å‘é…ç½®                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚ ç»§æ‰¿/ä½¿ç”¨
                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  extractor_base.py (åŸºç¡€è®¾æ–½å±‚)                           â”‚
â”‚  - BaseExtractor åŸºç±»                                    â”‚
â”‚  - ProductDetailMixin (å•†å“è¯¦æƒ…å¤„ç†)                      â”‚
â”‚  - BrowserPool (Playwrightè¿æ¥æ± )                       â”‚
â”‚  - PageParam (å‚æ•°å°è£…)                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 11.2.1 åŸºç¡€è®¾æ–½å±‚ (extractor_base.py)

**æ ¸å¿ƒç»„ä»¶**:

| ç»„ä»¶ | èŒè´£ | å…³é”®æ–¹æ³• |
|------|------|---------|
| `BaseExtractor` | æå–å™¨åŸºç±» | `fetch_html()`, `clean_html()`, `extract_text_as_markdown()` |
| `BrowserPool` | æµè§ˆå™¨æ± ç®¡ç† | `initialize()`, `get_page()`, `cleanup()` |
| `ProductDetailMixin` | å•†å“è¯¦æƒ…å¤„ç† | `post_save_callback()`, `_save_product_origin()` |
| `PageParam` | å‚æ•°å°è£… | url, html_content, extract_by_llm |

**BrowserPoolè®¾è®¡äº®ç‚¹**:

```python
class BrowserPool:
    """Playwrightæµè§ˆå™¨è¿æ¥æ±  - ä¼˜åŒ–å¹¶å‘æ€§èƒ½"""
    def __init__(self, pool_size=3, tab_size=5):
        self.pool_size = pool_size      # 3ä¸ªæµè§ˆå™¨å®ä¾‹
        self.tab_size = tab_size        # æ¯ä¸ª5ä¸ªtab (å…±15å¹¶å‘)
        self.available_tabs = asyncio.Queue()  # å¼‚æ­¥é˜Ÿåˆ—ç®¡ç†

    async def get_page(self):
        """è·å–å¯ç”¨tab (ä¸Šä¸‹æ–‡ç®¡ç†å™¨)"""
        tab_info = await self.available_tabs.get()
        try:
            yield tab_info['page']
        finally:
            # æ¸…ç†å¹¶å½’è¿˜tab
            await page.evaluate("() => { localStorage.clear(); }")
            await self.available_tabs.put(tab_info)
```

**è®¾è®¡äº®ç‚¹**:
- **å¼‚æ­¥é˜Ÿåˆ—**: ä½¿ç”¨ `asyncio.Queue` ç®¡ç†tabï¼Œè‡ªåŠ¨é˜»å¡ç­‰å¾…
- **è‡ªåŠ¨æ¸…ç†**: å½’è¿˜tabå‰æ¸…é™¤localStorageï¼Œé¿å…çŠ¶æ€æ±¡æŸ“
- **åæ£€æµ‹å¢å¼º**: é›†æˆ `anti_detection` æ¨¡å—ï¼Œä¿®æ”¹æµè§ˆå™¨æŒ‡çº¹

#### 11.2.2 ç«™ç‚¹é€‚é…å±‚ (extractor_<site>.py)

**URL_MAPè·¯ç”±è®¾è®¡**:

```python
URL_MAP = {
    'main_page': {
        'patterns': [r'https://www\.gnc\.com$'],
        'sample_urls': ['https://www.gnc.com'],
        'func': extract_deals_from_mainpage,
        'action': 'get_list_info'
    },
    'category_page': {
        'patterns': [r'https://www\.gnc\.com/[^/]+/$'],
        'func': extract_deals_from_category,
        'action': 'get_list_info'
    },
    'detail_page': {
        'patterns': [r'https://www\.gnc\.com/.*\.html$'],
        'func': extract_product_detail,
        'action': 'get_detail_info'
    }
}
```

**åˆ—è¡¨é¡µæå–å‡½æ•°**:

```python
async def extract_deals_from_mainpage(page: PageParam) -> dict:
    """ä»ä¸»é¡µæå–å•†å“/æ´»åŠ¨åˆ—è¡¨"""
    extractor = GncListExtractor(page)
    async with extractor.browser_pool.get_page() as pw_page:
        # ä½¿ç”¨BrightDataæ‰¹é‡è·å–HTMLï¼ˆåŠ é€Ÿï¼‰
        if 'brightdata' in extractor.engine:
            html_content = await extractor.get_html_content_by_brightdata(url)
            await pw_page.set_content(html_content)
        else:
            await pw_page.goto(url)

        # æå–URL
        urls = await pw_page.evaluate("""() => {
            return Array.from(document.querySelectorAll('a.product-link'))
                .map(a => ({url: a.href, title: a.textContent, type: 'detail'}));
        }""")

        return {'urls': urls, 'site_name': 'gnc', ...}
```

**è¯¦æƒ…é¡µæå–ç±»**:

```python
class GncDetailExtractor(BaseExtractor, ProductDetailMixin):
    """GNCå•†å“è¯¦æƒ…é¡µæå–å™¨"""

    async def fetch_html(self):
        """è·å–åŸå§‹HTML"""
        async with self.browser_pool.get_page() as page:
            await page.goto(self.url)
            return await page.content()

    def clean_html(self, html: str) -> str:
        """æ¸…æ´—HTML - ç«™ç‚¹ç‰¹å®šé€»è¾‘"""
        soup = BeautifulSoup(html, 'html.parser')
        # ç§»é™¤å¯¼èˆªæ ã€é¡µè„šç­‰æ— å…³å†…å®¹
        for tag in soup.select('.header, .footer, .ads'):
            tag.decompose()
        return str(soup.select_one('.product-detail'))

    def extract_text_as_markdown(self, cleaned_html: str) -> str:
        """è½¬æ¢ä¸ºMarkdown - è°ƒç”¨LLM"""
        # ä½¿ç”¨Gemini/Claudeæå–ç»“æ„åŒ–ä¿¡æ¯
        return llm_extract_markdown(cleaned_html)
```

#### 11.2.3 è°ƒåº¦ç¼–æ’å±‚ (extractor_scheduler.py)

**SiteScheduleræ ¸å¿ƒé€»è¾‘**:

```python
class SiteScheduler:
    """ç«™ç‚¹çˆ¬å–è°ƒåº¦å™¨ - BFSéå†URLæ ‘"""

    async def analyze(self, url, parent=None):
        """å¹¿åº¦ä¼˜å…ˆéå†"""
        queue = deque([(url, parent, 0)])  # (url, parent_id, level)
        visited = set()

        while queue:
            current_url, parent_id, level = queue.popleft()
            if level > self.max_level or current_url in visited:
                continue

            # åŠ¨æ€åŠ è½½ç«™ç‚¹æ¨¡å—
            module = importlib.import_module(f'crawler.{category}.extractor_{site_name}')

            # åŒ¹é…URL_MAPè·¯ç”±
            matched = self._match_url_pattern(current_url, module.URL_MAP)
            if not matched:
                continue

            # è°ƒç”¨å¤„ç†å‡½æ•°
            if matched['action'] == 'get_list_info':
                result = await matched['func'](PageParam(url=current_url))
                # å°†å­URLå…¥é˜Ÿ
                for item in result['urls']:
                    queue.append((item['url'], trace_page_id, level+1))

            elif matched['action'] == 'get_detail_info':
                # è¯¦æƒ…é¡µï¼šæå–å¹¶ä¿å­˜æ•°æ®
                await matched['func'](PageParam(url=current_url))

            visited.add(current_url)
```

**å¹¶å‘ä¼˜åŒ–**:

```python
async def process_level_urls_concurrent(self, urls, parent, level):
    """å¹¶å‘å¤„ç†åŒå±‚çº§URL"""
    # 1. BrightDataæ‰¹é‡è·å–HTML
    if self.enable_brightdata:
        html_list = await self.bd_client.batch_fetch(urls[:20])
        tasks = [self.process_one_url(url, html, parent, level)
                 for url, html in zip(urls, html_list)]
    else:
        tasks = [self.process_one_url(url, None, parent, level) for url in urls]

    # 2. å¹¶å‘æ‰§è¡Œï¼ˆä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°ï¼‰
    sem = asyncio.Semaphore(15)  # æœ€å¤š15ä¸ªå¹¶å‘
    async def wrapped(task):
        async with sem:
            return await task

    return await asyncio.gather(*[wrapped(t) for t in tasks])
```

### 11.3 å…³é”®è®¾è®¡æ¨¡å¼

#### 11.3.1 æ¨¡æ¿æ–¹æ³•æ¨¡å¼

```python
class BaseExtractor:
    """å®šä¹‰æå–æµç¨‹éª¨æ¶"""
    async def process(self, url):
        # 1. è·å–HTML
        html = await self.fetch_html(url)

        # 2. æ¸…æ´—HTMLï¼ˆå­ç±»é‡å†™ï¼‰
        cleaned = self.clean_html(html)

        # 3. æå–æ–‡æœ¬ï¼ˆå­ç±»é‡å†™ï¼‰
        markdown = self.extract_text_as_markdown(cleaned)

        # 4. ä¿å­˜æ•°æ®
        await self.save(markdown)

        # 5. å›è°ƒé’©å­ï¼ˆMixinæä¾›ï¼‰
        await self.post_save_callback()
```

#### 11.3.2 Mixinæ¨¡å¼

```python
class ProductDetailMixin:
    """å•†å“è¯¦æƒ…å¤„ç†èƒ½åŠ› - å¯é€‰ç»„åˆ"""
    async def post_save_callback(self):
        """ä¿å­˜åå›è°ƒ"""
        # 1. ä¿å­˜ProductOrigin
        await self._save_product_origin()

        # 2. æäº¤OCRä»»åŠ¡ï¼ˆå¦‚æœæœ‰å›¾ç‰‡ï¼‰
        if self.image_urls:
            await self._submit_ocr_tasks()

        # 3. ç«™ç‚¹ç‰¹å®šé€»è¾‘ï¼ˆå¯é€‰ï¼‰
        await self._post_process_hook()

    def _post_process_hook(self):
        """æ‰©å±•ç‚¹ - å­ç±»å¯é‡å†™"""
        pass
```

#### 11.3.3 ç­–ç•¥æ¨¡å¼

```python
# ä¸åŒå¼•æ“ç­–ç•¥
ENGINES = {
    'browser_pool': BrowserPoolEngine,
    'brightdata': BrightDataEngine,
    'brightdata+browser_pool': HybridEngine
}

class BaseExtractor:
    engine = 'brightdata+browser_pool'  # å­ç±»å¯é…ç½®

    async def fetch_html(self, url):
        engine = ENGINES[self.engine]()
        return await engine.fetch(url)
```

### 11.4 å¹¶å‘æ€§èƒ½ä¼˜åŒ–

**å¯¹æ¯”ï¼šä¼ ç»Ÿæ–¹å¼ vs ä¼˜åŒ–æ–¹å¼**

| ç»´åº¦ | ä¼ ç»Ÿæ–¹å¼ | ä¼˜åŒ–æ–¹å¼ | æå‡ |
|------|---------|---------|------|
| HTMLè·å– | é€ä¸ªæ‰“å¼€æµè§ˆå™¨ | BrightDataæ‰¹é‡çˆ¬å– | 10å€ |
| æµè§ˆå™¨å®ä¾‹ | æ¯æ¬¡æ–°å»º | BrowserPoolå¤ç”¨ | 5å€ |
| å¹¶å‘æ§åˆ¶ | æ— æ§åˆ¶ï¼Œæ˜“å´©æºƒ | Semaphore + Queue | ç¨³å®š |
| å†…å­˜å ç”¨ | éšä»»åŠ¡å¢é•¿ | å›ºå®š15ä¸ªtab | 80%â†“ |

**CONCURRENT_CONFIGé…ç½®**:

```python
CONCURRENT_CONFIG = {
    'pool_size': 3,               # 3ä¸ªæµè§ˆå™¨å®ä¾‹
    'tab_size': 5,                # æ¯ä¸ª5ä¸ªtab (å…±15å¹¶å‘)
    'delay_between_requests': 0.5, # è¯·æ±‚é—´éš”0.5ç§’
    'use_brightdata': True,       # ä½¿ç”¨BrightDataæ‰¹é‡çˆ¬å–
    'brightdata_batch_size': 20   # æ‰¹é‡å¤§å°20
}
```

### 11.5 æŠ€æœ¯éš¾ç‚¹ä¸è§£å†³æ–¹æ¡ˆ

#### éš¾ç‚¹1: åçˆ¬è™«æ£€æµ‹

**é—®é¢˜**:
- Cloudflareã€PerimeterXç­‰æ£€æµ‹Playwright
- User-Agentã€CanvasæŒ‡çº¹è¯†åˆ«

**è§£å†³æ–¹æ¡ˆ**:

```python
# devbot/html_servers/anti_detection.py
def get_browser_launch_args():
    """åæ£€æµ‹å¯åŠ¨å‚æ•°"""
    return [
        '--disable-blink-features=AutomationControlled',  # éšè—automationæ ‡å¿—
        '--disable-dev-shm-usage',
        '--no-sandbox',
        '--disable-setuid-sandbox',
        '--disable-web-security',
        '--disable-features=IsolateOrigins,site-per-process',
        '--allow-running-insecure-content',
        '--disable-webgl',  # ç¦ç”¨WebGLæŒ‡çº¹
        '--disable-canvas-fingerprinting',  # ç¦ç”¨CanvasæŒ‡çº¹
    ]

async def setup_page_anti_detection(page, user_agent=None):
    """é¡µé¢çº§åæ£€æµ‹"""
    # 1. ä¿®æ”¹navigatorå±æ€§
    await page.evaluate("""() => {
        Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
        Object.defineProperty(navigator, 'plugins', {get: () => [1, 2, 3]});
    }""")

    # 2. æ³¨å…¥çœŸå®Chromeè¿è¡Œæ—¶
    await page.add_init_script("""
        Object.defineProperty(navigator, 'languages', {
            get: () => ['en-US', 'en']
        });
    """)

    # 3. éšæœºåŒ–æŒ‡çº¹
    await page.evaluate(f"""() => {{
        const getParameter = WebGLRenderingContext.prototype.getParameter;
        WebGLRenderingContext.prototype.getParameter = function(parameter) {{
            if (parameter === 37445) return 'Intel Inc.';  # ä¼ªé€ æ˜¾å¡å‚å•†
            return getParameter.apply(this, arguments);
        }};
    }}""")
```

#### éš¾ç‚¹2: åŠ¨æ€å†…å®¹åŠ è½½

**é—®é¢˜**:
- JavaScriptæ¸²æŸ“çš„å†…å®¹
- æ‡’åŠ è½½å›¾ç‰‡
- æ— é™æ»šåŠ¨åˆ—è¡¨

**è§£å†³æ–¹æ¡ˆ**:

```python
async def fetch_rendered_html(self, url):
    """ç­‰å¾…åŠ¨æ€å†…å®¹åŠ è½½"""
    async with self.browser_pool.get_page() as page:
        await page.goto(url, wait_until='networkidle')  # ç­‰å¾…ç½‘ç»œç©ºé—²

        # ç­‰å¾…å…³é”®å…ƒç´ å‡ºç°
        await page.wait_for_selector('.product-info', timeout=10000)

        # æ»šåŠ¨åˆ°åº•éƒ¨ï¼Œè§¦å‘æ‡’åŠ è½½
        await page.evaluate("""async () => {
            await new Promise(resolve => {
                let totalHeight = 0;
                const distance = 100;
                const timer = setInterval(() => {
                    window.scrollBy(0, distance);
                    totalHeight += distance;
                    if (totalHeight >= document.body.scrollHeight) {
                        clearInterval(timer);
                        resolve();
                    }
                }, 100);
            });
        }""")

        # ç­‰å¾…å›¾ç‰‡åŠ è½½
        await page.wait_for_load_state('domcontentloaded')
        await page.wait_for_timeout(2000)

        return await page.content()
```

#### éš¾ç‚¹3: URLå»é‡ä¸å¢é‡çˆ¬å–

**é—®é¢˜**:
- é‡å¤URLå¯¼è‡´é‡å¤çˆ¬å–
- å¢é‡æ›´æ–°æ—¶éœ€è·³è¿‡å·²çˆ¬å–URL

**è§£å†³æ–¹æ¡ˆ**:

```python
class SiteScheduler:
    def __init__(self):
        self.visited_urls = set()  # å†…å­˜å»é‡
        self.db_visited = set()    # æ•°æ®åº“å·²çˆ¬URL

    async def is_url_processed(self, url):
        """æ£€æŸ¥URLæ˜¯å¦å·²å¤„ç†"""
        # 1. å†…å­˜å¿«é€ŸæŸ¥æ‰¾
        if url in self.visited_urls:
            return True

        # 2. æ•°æ®åº“æŸ¥è¯¢ï¼ˆç¼“å­˜ç»“æœï¼‰
        if url in self.db_visited:
            return True

        # 3. æŸ¥è¯¢TracePageè¡¨
        trace_page = await TracePage.objects(url=url).first()
        if trace_page:
            self.db_visited.add(url)
            return True

        return False
```

### 11.6 é›†æˆAsyncPipeline

**çˆ¬è™«ä¸Pipelineé›†æˆæµç¨‹**:

```
Crawler (extractor_scheduler.py)
    â†“ ä¿å­˜å•†å“è¯¦æƒ…
ProductDetailMixin.post_save_callback()
    â†“ HTTP POST
AsyncPipeline API (http://localhost:8000/api/v1/tasks/ocr)
    â†“ RabbitMQ
OCR Worker â†’ LLM Worker â†’ DB Worker
    â†“
ProductBaseModel (MongoDB)
```

**ä»£ç ç¤ºä¾‹**:

```python
# crawler/product/util.py
class ProductDetailMixin:
    async def _submit_to_async_pipeline(self):
        """æäº¤åˆ°å¼‚æ­¥ç®¡é“"""
        import httpx

        async with httpx.AsyncClient() as client:
            response = await client.post(
                "http://localhost:8000/api/v1/tasks/ocr",
                json={
                    "product_origin_id": str(self.product_origin_id),
                    "trace_page_id": str(self.trace_page_id),
                    "image_urls": self.image_urls,
                    "screenshot_url": self.screenshot_url,
                    "prompt": "Extract product information from images",
                    "run_version": "v1.0",
                    "site_name": self.site_name,
                    "source_url": self.url
                },
                timeout=30.0
            )

            task_id = response.json()["task_id"]
            logger.info(f"âœ… Submitted to AsyncPipeline: {task_id}")
```

---

## 12. AsyncPipeline å¼‚æ­¥ä»»åŠ¡å¤„ç†ç®¡é“

### 12.1 é¡¹ç›®æ¦‚è¿°

**AsyncPipeline** æ˜¯ä¸€ä¸ªåŸºäº RabbitMQ çš„é«˜æ€§èƒ½å¼‚æ­¥å¤„ç†ç³»ç»Ÿï¼Œæä¾›å®Œæ•´çš„ **OCR â†’ LLM â†’ DB** æµæ°´çº¿ã€‚

**æ ¸å¿ƒç‰¹æ€§**:
1. **å¼‚æ­¥è§£è€¦**: çˆ¬è™«ä¸æ•°æ®å¤„ç†å®Œå…¨åˆ†ç¦»ï¼Œäº’ä¸é˜»å¡
2. **æ¶ˆæ¯é˜Ÿåˆ—**: RabbitMQä¿è¯ä»»åŠ¡å¯é ä¼ é€’
3. **æ‰¹é‡å¤„ç†**: DB Workeræ‰¹é‡æ’å…¥ï¼Œæå‡10å€æ€§èƒ½
4. **èµ„æºå¤ç”¨**: Gemini Resource Managerç®¡ç†30+å¹¶å‘APIè°ƒç”¨

### 12.2 æ¶æ„è®¾è®¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   FastAPI Server                     â”‚
â”‚  POST /api/v1/tasks/ocr  â† Crawleræäº¤ä»»åŠ¡           â”‚
â”‚  POST /api/v1/tasks/llm                              â”‚
â”‚  GET  /api/v1/health                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚ publish
                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   RabbitMQ                           â”‚
â”‚  ocr_queue (priority=5) â”€â”€â”€â”€â”€â†’ OCR Worker Pool (1)  â”‚
â”‚  llm_queue (priority=7) â”€â”€â”€â”€â”€â†’ LLM Worker Pool (3)  â”‚
â”‚  db_queue  (priority=3) â”€â”€â”€â”€â”€â†’ DB Worker Pool (2)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â†“                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   MongoDB        â”‚  â”‚   vLLM OCR API   â”‚
â”‚  TracePage       â”‚  â”‚   Gemini API     â”‚
â”‚  ProductOrigin   â”‚  â”‚   GCS Storage    â”‚
â”‚  ProductBase     â”‚  â”‚                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 12.3 æ•°æ®æµè¯¦è§£

```
1. OCRé˜¶æ®µ
   Crawler â†’ API â†’ RabbitMQ â†’ OCR Worker
   â”œâ”€ è°ƒç”¨ vLLM OCR API
   â”œâ”€ è½®è¯¢ç­‰å¾…ç»“æœ (æœ€å¤š60ç§’)
   â”œâ”€ æ›´æ–° ProductOrigin.ocr_info
   â””â”€ æ‹¼æ¥ OCRæ–‡æœ¬åˆ° TracePage.markdown_txt

2. LLMé˜¶æ®µ
   OCR Worker â†’ RabbitMQ â†’ LLM Worker
   â”œâ”€ è¯»å– TracePage.markdown_txt (å«OCRæ–‡æœ¬)
   â”œâ”€ è°ƒç”¨ Gemini API (markdown â†’ JSON)
   â”œâ”€ JSONéªŒè¯å’Œæ¸…ç†
   â””â”€ æ›´æ–° TracePage.status = 'pending_db'

3. DBé˜¶æ®µ
   LLM Worker â†’ RabbitMQ â†’ DB Worker
   â”œâ”€ æ‰¹é‡æ¥æ”¶ä»»åŠ¡ (batch_size=50, timeout=5s)
   â”œâ”€ æ‰¹é‡æ’å…¥ MongoDB (ProductBaseModel)
   â””â”€ æ›´æ–° TracePage.status = 'completed'
```

### 12.4 æ ¸å¿ƒç»„ä»¶

#### 12.4.1 OCR Worker

**èŒè´£**: è°ƒç”¨vLLM OCR APIè¿›è¡Œå›¾ç‰‡è¯†åˆ«

```python
# workers/ocr_worker.py
class OCRWorker(BaseWorker):
    """OCR Worker - å¤„ç†å›¾ç‰‡è¯†åˆ«ä»»åŠ¡"""

    async def process_task(self, task: OCRTaskMessage):
        """å¤„ç†OCRä»»åŠ¡"""
        # 1. è°ƒç”¨vLLM OCR API (å¼‚æ­¥æäº¤)
        response_ids = []
        for image_url in task.image_urls:
            resp = await self._submit_ocr_task(image_url, task.prompt)
            response_ids.append(resp['id'])

        # 2. è½®è¯¢ç­‰å¾…ç»“æœ (æœ€å¤š60ç§’)
        ocr_results = {}
        for rid in response_ids:
            result = await self._poll_ocr_result(rid, timeout=60)
            if result['status'] == 'completed':
                ocr_results[result['image_url']] = result['text']

        # 3. æ›´æ–°ProductOrigin
        await ProductOrigin.objects(id=task.product_origin_id).update(
            set__ocr_info=ocr_results
        )

        # 4. æ‹¼æ¥OCRæ–‡æœ¬åˆ°TracePage.markdown_txt
        ocr_text = "\n\n".join([
            f"## OCR - {url}\n{text}"
            for url, text in ocr_results.items()
        ])
        await TracePage.objects(id=task.trace_page_id).update(
            push__markdown_txt=ocr_text
        )

        # 5. å‘é€LLMä»»åŠ¡
        await self.broker.publish(
            'llm_queue',
            LLMTaskMessage(
                trace_page_id=task.trace_page_id,
                site_name=task.site_name,
                ...
            )
        )

    async def _poll_ocr_result(self, response_id, timeout=60):
        """è½®è¯¢OCRç»“æœ"""
        start_time = time.time()
        while time.time() - start_time < timeout:
            resp = await self.ocr_client.get(f'/v1/responses/{response_id}')
            if resp['status'] in ['completed', 'failed']:
                return resp
            await asyncio.sleep(2)  # æ¯2ç§’æŸ¥è¯¢ä¸€æ¬¡
        raise TimeoutError(f"OCR timeout: {response_id}")
```

#### 12.4.2 LLM Worker

**èŒè´£**: ä½¿ç”¨Geminiå°†Markdownè½¬æ¢ä¸ºç»“æ„åŒ–JSON

```python
# workers/llm_worker.py
class LLMWorker(BaseWorker):
    """LLM Worker - Markdownè½¬JSON"""

    def __init__(self, broker, config):
        super().__init__(broker, config)
        # Gemini Resource Manager (ç®¡ç†30+å¹¶å‘APIè°ƒç”¨)
        self.resource_manager = ResourceManager(
            api_keys=[key1, key2, key3],  # å¤šä¸ªAPI Keyè½®æ¢
            max_concurrent=30
        )

    async def process_task(self, task: LLMTaskMessage):
        """å¤„ç†LLMä»»åŠ¡"""
        # 1. è¯»å–TracePage (å«OCRæ–‡æœ¬)
        trace_page = await TracePage.objects(id=task.trace_page_id).first()
        markdown_content = trace_page.markdown_txt

        # 2. é€‰æ‹©è½¬æ¢å™¨
        if task.category == 'products':
            converter = ProductMarkdownToJsonConverter(self.resource_manager)
        else:
            converter = DealMarkdownToJsonConverter(self.resource_manager)

        # 3. è°ƒç”¨Gemini API
        try:
            json_data = await converter.convert(
                markdown_content=markdown_content,
                site_name=task.site_name,
                source_url=task.source_url
            )
        except Exception as e:
            # æ›´æ–°å¤±è´¥çŠ¶æ€
            await TracePage.objects(id=task.trace_page_id).update(
                set__status='llm_failed',
                set__error=str(e)
            )
            return

        # 4. JSONéªŒè¯å’Œæ¸…ç†
        cleaned_json = self._validate_and_clean(json_data)

        # 5. æ›´æ–°TracePage
        await TracePage.objects(id=task.trace_page_id).update(
            set__status='pending_db',
            set__json_data=cleaned_json
        )

        # 6. å‘é€DBä»»åŠ¡
        await self.broker.publish(
            'db_queue',
            DBTaskMessage(
                trace_page_id=task.trace_page_id,
                json_data=cleaned_json,
                category=task.category
            )
        )
```

**Gemini Resource Manager**:

```python
# src/llm/resource_manager.py
class ResourceManager:
    """ç®¡ç†å¤šä¸ªGemini API Keyï¼Œå®ç°çœŸå¹¶å‘"""

    def __init__(self, api_keys: List[str], max_concurrent=30):
        self.api_keys = api_keys
        self.max_concurrent = max_concurrent
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.current_key_index = 0

    def get_next_key(self):
        """è½®æ¢API Key"""
        key = self.api_keys[self.current_key_index]
        self.current_key_index = (self.current_key_index + 1) % len(self.api_keys)
        return key

    async def call_gemini(self, prompt: str):
        """å¹¶å‘è°ƒç”¨Gemini (æœ€å¤š30ä¸ªå¹¶å‘)"""
        async with self.semaphore:
            api_key = self.get_next_key()
            return await self._call_api(prompt, api_key)

    async def _call_api(self, prompt, api_key):
        """å®é™…APIè°ƒç”¨ (æ”¯æŒé‡è¯•)"""
        for attempt in range(3):
            try:
                async with aiohttp.ClientSession() as session:
                    async with session.post(
                        f'https://generativelanguage.googleapis.com/v1/models/gemini-1.5-flash:generateContent?key={api_key}',
                        json={"contents": [{"parts": [{"text": prompt}]}]},
                        timeout=30
                    ) as resp:
                        result = await resp.json()
                        return result['candidates'][0]['content']['parts'][0]['text']
            except Exception as e:
                if attempt == 2:
                    raise
                await asyncio.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
```

#### 12.4.3 DB Worker

**èŒè´£**: æ‰¹é‡æ’å…¥MongoDBï¼Œæå‡æ€§èƒ½

```python
# workers/db_worker.py
class DBWorker(BaseWorker):
    """DB Worker - æ‰¹é‡æ’å…¥æ•°æ®åº“"""

    def __init__(self, broker, config, batch_size=50, batch_timeout=5):
        super().__init__(broker, config)
        self.batch_size = batch_size
        self.batch_timeout = batch_timeout
        self.batch = []
        self.batch_timer = None

    async def process_batch(self):
        """æ‰¹é‡å¤„ç†ä»»åŠ¡"""
        if not self.batch:
            return

        logger.info(f"ğŸ“¦ Processing batch of {len(self.batch)} tasks")

        # 1. æ‰¹é‡æ’å…¥ProductBaseModel
        products = []
        trace_page_ids = []

        for task in self.batch:
            products.append(ProductBaseModel(**task.json_data))
            trace_page_ids.append(task.trace_page_id)

        # 2. æ‰¹é‡insert (10å€å¿«äºé€ä¸ªæ’å…¥)
        try:
            ProductBaseModel.objects.insert(products, load_bulk=False)
            logger.info(f"âœ… Inserted {len(products)} products")
        except Exception as e:
            logger.error(f"âŒ Batch insert failed: {e}")
            return

        # 3. æ‰¹é‡æ›´æ–°TracePageçŠ¶æ€
        await TracePage.objects(id__in=trace_page_ids).update(
            set__status='completed',
            set__completed_at=datetime.now()
        )

        # æ¸…ç©ºbatch
        self.batch = []

    async def consume_loop(self):
        """æ¶ˆè´¹å¾ªç¯ - æ‰¹é‡æ¥æ”¶"""
        async for message in self.broker.consume('db_queue'):
            task = DBTaskMessage(**message)
            self.batch.append(task)

            # è¾¾åˆ°batch_sizeæˆ–è¶…æ—¶ï¼Œç«‹å³å¤„ç†
            if len(self.batch) >= self.batch_size:
                await self.process_batch()
                self.batch_timer = None
            elif self.batch_timer is None:
                # å¯åŠ¨è¶…æ—¶å®šæ—¶å™¨
                self.batch_timer = asyncio.create_task(self._timeout_handler())

    async def _timeout_handler(self):
        """è¶…æ—¶å¤„ç†"""
        await asyncio.sleep(self.batch_timeout)
        await self.process_batch()
        self.batch_timer = None
```

### 12.5 æ¶ˆæ¯é˜Ÿåˆ—è®¾è®¡

**RabbitMQé˜Ÿåˆ—é…ç½®**:

```python
# message_broker.py
QUEUES = {
    'ocr_queue': {
        'name': 'async_pipeline.ocr',
        'priority': 5,
        'durable': True,  # æŒä¹…åŒ–
        'ttl': 3600000    # 1å°æ—¶TTL
    },
    'llm_queue': {
        'name': 'async_pipeline.llm',
        'priority': 7,    # LLMä¼˜å…ˆçº§æœ€é«˜
        'durable': True,
        'ttl': 1800000    # 30åˆ†é’ŸTTL
    },
    'db_queue': {
        'name': 'async_pipeline.db',
        'priority': 3,
        'durable': True,
        'ttl': 600000     # 10åˆ†é’ŸTTL
    }
}
```

**æ¶ˆæ¯æ¨¡å‹**:

```python
# task_models.py
from pydantic import BaseModel

class OCRTaskMessage(BaseModel):
    """OCRä»»åŠ¡æ¶ˆæ¯"""
    product_origin_id: str
    trace_page_id: str
    image_urls: List[str]
    screenshot_url: Optional[str]
    prompt: str
    run_version: str
    site_name: str
    source_url: str

class LLMTaskMessage(BaseModel):
    """LLMä»»åŠ¡æ¶ˆæ¯"""
    trace_page_id: str
    site_name: str
    source_url: str
    category: str  # 'products' or 'deals'
    run_version: str

class DBTaskMessage(BaseModel):
    """DBä»»åŠ¡æ¶ˆæ¯"""
    trace_page_id: str
    json_data: Dict[str, Any]
    category: str
```

### 12.6 æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

| ä¼˜åŒ–é¡¹ | ä¼˜åŒ–å‰ | ä¼˜åŒ–å | æå‡ |
|-------|--------|--------|------|
| DBæ’å…¥ | é€ä¸ªinsert | æ‰¹é‡insert (50æ¡) | 10å€ |
| Geminiå¹¶å‘ | ä¸²è¡Œè°ƒç”¨ | Resource Manager (30å¹¶å‘) | 30å€ |
| OCRè·å– | åŒæ­¥ç­‰å¾… | å¼‚æ­¥è½®è¯¢ + è¶…æ—¶æ§åˆ¶ | ä¸é˜»å¡ |
| æ¶ˆæ¯ä¼ é€’ | ç›´æ¥è°ƒç”¨ | RabbitMQè§£è€¦ | é«˜å¯é  |

### 12.7 ç›‘æ§ä¸è¿ç»´

**å¥åº·æ£€æŸ¥API**:

```python
# api/routers/health.py
@router.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    # 1. æ£€æŸ¥RabbitMQè¿æ¥
    broker_status = await check_broker_connection()

    # 2. æ£€æŸ¥MongoDBè¿æ¥
    db_status = await check_db_connection()

    # 3. è·å–é˜Ÿåˆ—çŠ¶æ€
    queues = await get_queue_stats()

    # 4. è·å–WorkerçŠ¶æ€
    workers = {
        "ocr": {"active": 1, "status": "healthy"},
        "llm": {"active": 3, "status": "healthy"},
        "db": {"active": 2, "status": "healthy"}
    }

    return {
        "status": "healthy",
        "broker": broker_status,
        "database": db_status,
        "queues": queues,
        "workers": workers
    }
```

**æ—¥å¿—ç¤ºä¾‹**:

```
2025-01-15 14:32:10 - OCRWorker - INFO - ğŸ“¸ Processing OCR task: product_123
2025-01-15 14:32:15 - OCRWorker - INFO - âœ… OCR completed: 3 images, 2.5s
2025-01-15 14:32:16 - LLMWorker - INFO - ğŸ¤– Converting markdown to JSON: trace_456
2025-01-15 14:32:20 - LLMWorker - INFO - âœ… JSON generated: 1234 chars
2025-01-15 14:32:25 - DBWorker - INFO - ğŸ“¦ Processing batch of 50 tasks
2025-01-15 14:32:27 - DBWorker - INFO - âœ… Inserted 50 products (0.8s)
```

---

## 13. OCR_Rec å¼‚æ­¥OCRè¯†åˆ«ç³»ç»Ÿ

### 13.1 é¡¹ç›®æ¦‚è¿°

**OCR_Rec** æ˜¯åŸºäºvLLMå¼‚æ­¥APIçš„OCRè¯†åˆ«ç³»ç»Ÿï¼Œä¸“é—¨ç”¨äºæ‰¹é‡å¤„ç†ç”µå•†ç½‘é¡µæˆªå›¾çš„æ–‡å­—è¯†åˆ«ã€‚

**æ ¸å¿ƒç‰¹æ€§**:
1. **å¼‚æ­¥è§£è€¦**: ä»»åŠ¡æäº¤å’Œç»“æœè·å–åˆ†ç¦»
2. **å›¾ç‰‡ä¼˜åŒ–**: è‡ªåŠ¨å‹ç¼©å¹¶ä¸Šä¼ GCS
3. **çŠ¶æ€è¿½è¸ª**: queued/in_progress/completed/failed
4. **å®šæ—¶è°ƒåº¦**: Cronå®šæ—¶æ‰§è¡Œï¼ˆæäº¤5åˆ†é’Ÿ/è·å–2åˆ†é’Ÿï¼‰

### 13.2 æ¶æ„è®¾è®¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  è„šæœ¬1: submit_tasks.py (æ¯5åˆ†é’Ÿè¿è¡Œ)               â”‚
â”‚  1. ä»product_originæŸ¥è¯¢æœªå¤„ç†å›¾ç‰‡                   â”‚
â”‚  2. ä¸‹è½½ â†’ å‹ç¼©(WebP 94%) â†’ ä¸Šä¼ GCS                 â”‚
â”‚  3. POST /v1/responses (æäº¤OCRä»»åŠ¡)                â”‚
â”‚  4. ä¿å­˜taskåˆ°product_ocr_completed                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚ response_idå­˜å…¥æ•°æ®åº“
                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  è„šæœ¬2: fetch_results.py (æ¯2åˆ†é’Ÿè¿è¡Œ)              â”‚
â”‚  1. ä»product_ocr_completedæŸ¥è¯¢å¾…è·å–ç»“æœ             â”‚
â”‚  2. GET /v1/responses/{response_id}                â”‚
â”‚  3. æ›´æ–°product_ocr_completed.ocr_text             â”‚
â”‚  4. åŒæ­¥æ›´æ–°product_origin.array_is_completed       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 13.3 æ•°æ®åº“Schema

**product_originè¡¨** (è¾“å…¥æ•°æ®):

```javascript
{
  "_id": ObjectId("..."),
  "image_urls": [
    "https://example.com/img1.jpg",
    "https://example.com/img2.jpg"
  ],
  "array_is_completed": [  // å·²å®Œæˆçš„å›¾ç‰‡URL
    "https://example.com/img1.jpg"
  ],
  "iscompleted": false  // æ˜¯å¦å…¨éƒ¨å®Œæˆ
}
```

**product_ocr_completedè¡¨** (è¾“å‡ºæ•°æ®):

```javascript
{
  "_id": ObjectId("..."),
  "webpage_id": ObjectId("..."),      // å…³è”product_origin
  "image_id": "https://...",          // å›¾ç‰‡URL
  "response_id": "resp_abc123",       // vLLMä»»åŠ¡ID
  "status": "completed",              // ä»»åŠ¡çŠ¶æ€
  "ocr_text": "# OCRç»“æœ\n...",       // è¯†åˆ«æ–‡æœ¬
  "created_at": ISODate("..."),       // åˆ›å»ºæ—¶é—´
  "processed_at": ISODate("..."),     // å®Œæˆæ—¶é—´
  "error": {"message": "..."}         // é”™è¯¯ä¿¡æ¯(å¯é€‰)
}
```

### 13.4 æ ¸å¿ƒæµç¨‹

#### 13.4.1 submit_tasks.py

```python
#!/usr/bin/env python3
"""æäº¤OCRä»»åŠ¡è„šæœ¬"""
import asyncio
from OCRSubmitService import OCRSubmitService

async def main():
    service = OCRSubmitService(config_path='config.yaml')
    await service.initialize()

    # 1. æŸ¥è¯¢æœªå¤„ç†çš„product_origin
    unprocessed = ProductOrigin.objects(
        Q(iscompleted=False) | Q(iscompleted__exists=False)
    ).limit(20)

    # 2. æ‰¹é‡å¤„ç†
    for origin in unprocessed:
        # è·å–æœªå®Œæˆçš„å›¾ç‰‡
        unprocessed_images = service.compute_unprocessed_images(origin)

        # æäº¤OCRä»»åŠ¡
        results = await service.submit_for_origin(origin, prompt="Extract text")

        print(f"âœ… Submitted {len(results)} OCR tasks for {origin.id}")

asyncio.run(main())
```

**OCRSubmitServiceæ ¸å¿ƒæ–¹æ³•**:

```python
# src/utils/ocr_submit_service.py
class OCRSubmitService:
    """OCRæäº¤æœåŠ¡"""

    async def submit_for_origin(self, origin: ProductOrigin, prompt: str):
        """ä¸ºproduct_originæäº¤OCRä»»åŠ¡"""
        unprocessed_images = self.compute_unprocessed_images(origin)

        async with aiohttp.ClientSession() as session:
            async def process_one(image_url: str):
                # 1. ä¸‹è½½å¹¶å‹ç¼©å›¾ç‰‡
                gcs_url = await self.download_and_optimize_image(image_url)

                # 2. æäº¤OCRä»»åŠ¡
                result = await self.submit_ocr_task(
                    session,
                    gcs_url,
                    system_message="You are an OCR assistant",
                    user_message=prompt
                )

                # 3. ä¿å­˜åˆ°product_ocr_completed
                ProductOCRCompleted(
                    webpage_id=origin.id,
                    image_id=image_url,
                    response_id=result['response_id'],
                    status=result['status'],  # 'queued'
                    created_at=datetime.now()
                ).save()

                return result

            # å¹¶å‘å¤„ç†ï¼ˆ20ä¸ªå¹¶å‘ï¼‰
            tasks = [process_one(url) for url in unprocessed_images]
            return await asyncio.gather(*tasks)

    async def download_and_optimize_image(self, image_url: str) -> str:
        """ä¸‹è½½å¹¶å‹ç¼©å›¾ç‰‡ï¼Œä¸Šä¼ åˆ°GCS"""
        # 1. ä¸‹è½½å›¾ç‰‡
        response = requests.get(image_url, timeout=30)
        img = Image.open(io.BytesIO(response.content))

        # 2. å‹ç¼©ä¸ºWebP (è´¨é‡94%)
        output_buffer = io.BytesIO()
        img.save(output_buffer, format='WEBP', quality=94, method=4)
        compressed_data = output_buffer.getvalue()

        # 3. ä¸Šä¼ åˆ°GCS
        blob_name = f"ocr_images/{hashlib.md5(image_url.encode()).hexdigest()}.webp"
        blob = self.gcs_bucket.blob(blob_name)
        blob.upload_from_string(compressed_data, content_type='image/webp')

        return blob.public_url
```

#### 13.4.2 fetch_results.py

```python
#!/usr/bin/env python3
"""è·å–OCRç»“æœè„šæœ¬"""
import asyncio
from bson import ObjectId

async def main():
    # 1. æŸ¥è¯¢å¾…è·å–ç»“æœçš„ä»»åŠ¡
    pending_tasks = ProductOCRCompleted.objects(
        response_id__exists=True
    ).limit(50)

    # 2. æ‰¹é‡è·å–ç»“æœ
    async with aiohttp.ClientSession() as session:
        async def fetch_one(task):
            resp = await session.get(
                f'http://58.224.7.136:41294/v1/responses/{task.response_id}'
            )
            result = await resp.json()

            # 3. å¤„ç†ä¸åŒçŠ¶æ€
            if result['status'] == 'completed':
                # æå–OCRæ–‡æœ¬
                ocr_text = result['output']

                # æ›´æ–°product_ocr_completed
                task.update(
                    set__ocr_text=ocr_text,
                    set__status='completed',
                    set__processed_at=datetime.now()
                )

                # æ›´æ–°product_origin.array_is_completed
                ProductOrigin.objects(id=task.webpage_id).update(
                    add_to_set__array_is_completed=task.image_id
                )

                # æ£€æŸ¥æ˜¯å¦å…¨éƒ¨å®Œæˆ
                origin = ProductOrigin.objects(id=task.webpage_id).first()
                if len(origin.array_is_completed) == len(origin.image_urls):
                    origin.update(set__iscompleted=True)

                print(f"âœ… OCR completed: {task.image_id}")

            elif result['status'] == 'failed':
                task.update(
                    set__status='failed',
                    set__error=result.get('error')
                )

        # å¹¶å‘è·å–ï¼ˆ50ä¸ªå¹¶å‘ï¼‰
        tasks = [fetch_one(task) for task in pending_tasks]
        await asyncio.gather(*tasks)

asyncio.run(main())
```

### 13.5 Cronå®šæ—¶ä»»åŠ¡é…ç½®

```bash
# ç¼–è¾‘crontab
crontab -e

# æ·»åŠ å®šæ—¶ä»»åŠ¡
# æ¯5åˆ†é’Ÿæäº¤æ–°ä»»åŠ¡
*/5 * * * * cd /path/to/ocr_rec && python3 submit_tasks.py >> /tmp/submit_tasks.log 2>&1

# æ¯2åˆ†é’Ÿæ£€æŸ¥ç»“æœ
*/2 * * * * cd /path/to/ocr_rec && python3 fetch_results.py >> /tmp/fetch_results.log 2>&1
```

### 13.6 ä¸åŸé¡¹ç›®å¯¹æ¯”

| ç‰¹æ€§ | åŸé¡¹ç›® (åŒæ­¥) | OCR_Rec (å¼‚æ­¥) |
|-----|-------------|---------------|
| APIæ–¹å¼ | `/v1/chat/completions` (åŒæ­¥) | `/v1/responses` (å¼‚æ­¥) |
| ä»»åŠ¡æ¨¡å‹ | ç«‹å³è¿”å›ç»“æœ | æäº¤å’Œè·å–åˆ†ç¦» |
| å¹¶å‘æ–¹å¼ | ThreadPoolExecutor | asyncio + aiohttp |
| æ‰¹å¤„ç† | 10ä¸ª/æ‰¹æ¬¡ | 20ä¸ªæäº¤ + 50ä¸ªè·å– |
| çŠ¶æ€ç®¡ç† | ä»…æˆåŠŸ/å¤±è´¥ | queued/in_progress/completed/failed |
| é€‚ç”¨åœºæ™¯ | å°æ‰¹é‡ã€ä½å»¶è¿Ÿ | å¤§æ‰¹é‡ã€é«˜åå |

---

## 14. Qwen3-VL-8B-Instruct-FP8 å¾®è°ƒæ–¹æ³•

### 14.1 å¾®è°ƒç›®æ ‡

**é—®é¢˜**: é€šç”¨çš„Qwen3-VLæ¨¡å‹åœ¨ç”µå•†OCRåœºæ™¯ä¸‹è¡¨ç°ä¸ä½³ï¼š
- æ— æ³•è¯†åˆ«å•†å“ä»·æ ¼ï¼ˆç¾å…ƒç¬¦å·ã€æŠ˜æ‰£ï¼‰
- å¿½ç•¥å“ç‰ŒLogoå’Œå•†æ ‡
- å¯¹ä¿ƒé”€æ–‡æ¡ˆï¼ˆSALEã€DISCOUNTï¼‰æ•æ„Ÿåº¦ä½
- è¡¨æ ¼æ•°æ®ï¼ˆè¥å…»æˆåˆ†ã€è§„æ ¼å‚æ•°ï¼‰æå–ä¸å‡†ç¡®

**ç›®æ ‡**: å¾®è°ƒæ¨¡å‹ï¼Œä½¿å…¶ä¸“é—¨é€‚åº”ç”µå•†ç½‘ç«™OCRä¿¡æ¯è¯†åˆ«ã€‚

### 14.2 æ•°æ®å‡†å¤‡

#### 14.2.1 æ•°æ®æ”¶é›†

**æ¥æº**:
1. **å·²çˆ¬å–æ•°æ®**: ä»`product_origin`è¡¨æå–image_urls + äººå·¥æ ‡æ³¨
2. **å…¬å¼€æ•°æ®é›†**: RPC (Retail Product Checkout) Dataset
3. **åˆæˆæ•°æ®**: ä½¿ç”¨æ–‡æœ¬æ¸²æŸ“å·¥å…·ç”Ÿæˆä»·æ ¼æ ‡ç­¾

**æ•°æ®æ ¼å¼**:

```json
{
  "image": "https://gcs.example.com/product_images/12345.webp",
  "conversations": [
    {
      "from": "human",
      "value": "<image>\nExtract all text from this product image, including prices, brand names, and promotional text."
    },
    {
      "from": "gpt",
      "value": "## Product Information\n\n**Brand**: GNC\n**Product Name**: Whey Protein Powder\n**Price**: $49.99 (Original: $69.99)\n**Discount**: 29% OFF\n**Size**: 5 lbs\n\n## Promotional Text\nLimited Time Offer\nFree Shipping on Orders Over $50\n\n## Nutritional Facts\n- Protein: 24g per serving\n- Calories: 130\n..."
    }
  ]
}
```

**æ•°æ®å¢å¼º**:

```python
import albumentations as A

transform = A.Compose([
    A.RandomBrightnessContrast(p=0.5),  # äº®åº¦/å¯¹æ¯”åº¦
    A.GaussNoise(p=0.3),                # é«˜æ–¯å™ªå£°
    A.Rotate(limit=15, p=0.4),          # æ—‹è½¬
    A.Perspective(scale=(0.05, 0.1), p=0.3),  # é€è§†å˜æ¢
])
```

#### 14.2.2 æ ‡æ³¨å·¥å…·

**LabelStudioé…ç½®**:

```xml
<View>
  <Image name="image" value="$image"/>
  <TextArea name="ocr_result" toName="image"
            rows="10" editable="true"
            placeholder="Enter OCR result in Markdown format"/>

  <Choices name="quality" toName="image" choice="single">
    <Choice value="excellent"/>
    <Choice value="good"/>
    <Choice value="poor"/>
  </Choices>
</View>
```

### 14.3 å¾®è°ƒæ–¹æ³•

#### 14.3.1 LoRAå¾®è°ƒ

**ä¸ºä»€ä¹ˆé€‰æ‹©LoRA?**
- å‚æ•°æ•ˆç‡é«˜ï¼šä»…è®­ç»ƒ0.1%å‚æ•°
- è®­ç»ƒå¿«ï¼š8Bæ¨¡å‹åœ¨å•å¡A100ä¸Š6å°æ—¶å®Œæˆ
- æ˜“éƒ¨ç½²ï¼švLLMåŸç”Ÿæ”¯æŒLoRAé€‚é…å™¨

**LoRAé…ç½®**:

```python
from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(
    r=16,                      # LoRAç§©
    lora_alpha=32,             # ç¼©æ”¾å› å­
    target_modules=[           # ç›®æ ‡æ¨¡å—
        "q_proj", "k_proj", "v_proj", "o_proj",  # Attention
        "gate_proj", "up_proj", "down_proj"      # FFN
    ],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(base_model, lora_config)
```

#### 14.3.2 è®­ç»ƒè„šæœ¬

```python
# finetune_qwen3vl.py
from transformers import (
    Trainer,
    TrainingArguments,
    AutoTokenizer,
    AutoModelForCausalLM
)
from datasets import load_dataset

# 1. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
model_name = "Qwen/Qwen3-VL-8B-Instruct-FP8"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    load_in_8bit=True,  # 8bité‡åŒ–èŠ‚çœæ˜¾å­˜
    device_map="auto"
)

# 2. åº”ç”¨LoRA
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
# Output: trainable params: 8,388,608 || all params: 8,000,000,000 || trainable%: 0.10

# 3. åŠ è½½æ•°æ®é›†
dataset = load_dataset('json', data_files={
    'train': 'ecommerce_ocr_train.json',
    'val': 'ecommerce_ocr_val.json'
})

# 4. æ•°æ®é¢„å¤„ç†
def preprocess_function(examples):
    """è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼"""
    inputs = []
    targets = []

    for conv in examples['conversations']:
        # æ„é€ è¾“å…¥: <image>æ ‡è®° + prompt
        human_text = conv[0]['value']
        gpt_text = conv[1]['value']

        inputs.append(human_text)
        targets.append(gpt_text)

    model_inputs = tokenizer(
        inputs,
        max_length=1024,
        truncation=True,
        padding='max_length'
    )

    labels = tokenizer(
        targets,
        max_length=2048,
        truncation=True,
        padding='max_length'
    )

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized_dataset = dataset.map(
    preprocess_function,
    batched=True,
    remove_columns=dataset["train"].column_names
)

# 5. è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir="./qwen3vl_ecommerce_lora",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    gradient_accumulation_steps=8,  # æœ‰æ•ˆbatch_size=32
    learning_rate=2e-4,
    weight_decay=0.01,
    warmup_steps=500,
    logging_steps=50,
    evaluation_strategy="steps",
    eval_steps=500,
    save_steps=500,
    save_total_limit=3,
    fp16=True,                     # æ··åˆç²¾åº¦è®­ç»ƒ
    dataloader_num_workers=4,
    remove_unused_columns=False,
    report_to="tensorboard"
)

# 6. å¯åŠ¨è®­ç»ƒ
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["val"],
    tokenizer=tokenizer
)

trainer.train()

# 7. ä¿å­˜LoRAæƒé‡
model.save_pretrained("./qwen3vl_ecommerce_lora_final")
```

### 14.4 æ¨¡å‹éƒ¨ç½²

#### 14.4.1 vLLMéƒ¨ç½²LoRAæ¨¡å‹

```bash
# å¯åŠ¨vLLMæœåŠ¡ï¼ˆåŠ è½½LoRAé€‚é…å™¨ï¼‰
python -m vllm.entrypoints.openai.api_server \
  --model Qwen/Qwen3-VL-8B-Instruct-FP8 \
  --lora-modules ecommerce=./qwen3vl_ecommerce_lora_final \
  --host 0.0.0.0 \
  --port 41294 \
  --tensor-parallel-size 1 \
  --gpu-memory-utilization 0.9 \
  --max-model-len 4096
```

**APIè°ƒç”¨**:

```python
import aiohttp

async def call_finetuned_ocr(image_url: str):
    async with aiohttp.ClientSession() as session:
        payload = {
            "model": "Qwen/Qwen3-VL-8B-Instruct-FP8",
            "lora_request": {  # æŒ‡å®šLoRAé€‚é…å™¨
                "lora_name": "ecommerce",
                "lora_int_id": 1
            },
            "messages": [
                {
                    "role": "system",
                    "content": "You are an expert at extracting text from e-commerce product images, including prices, brand names, and promotional text."
                },
                {
                    "role": "user",
                    "content": [
                        {"type": "image_url", "image_url": image_url},
                        {"type": "text", "text": "Extract all text from this product image in Markdown format."}
                    ]
                }
            ],
            "max_tokens": 4000,
            "temperature": 0.1
        }

        async with session.post(
            "http://58.224.7.136:41294/v1/chat/completions",
            json=payload
        ) as resp:
            result = await resp.json()
            return result['choices'][0]['message']['content']
```

### 14.5 è¯„ä¼°ä¸å¯¹æ¯”

**è¯„ä¼°æŒ‡æ ‡**:

| æŒ‡æ ‡ | è®¡ç®—æ–¹å¼ | ç›®æ ‡ |
|-----|---------|------|
| CER (Character Error Rate) | Levenshteinè·ç¦» / æ€»å­—ç¬¦æ•° | < 5% |
| Price Accuracy | ä»·æ ¼æå–å‡†ç¡®ç‡ | > 95% |
| Brand Recall | å“ç‰Œåæå–å¬å›ç‡ | > 90% |
| F1 Score | ç²¾ç¡®ç‡ä¸å¬å›ç‡çš„è°ƒå’Œå¹³å‡ | > 0.9 |

**å¯¹æ¯”ç»“æœ** (æµ‹è¯•é›†: 1000å¼ ç”µå•†å›¾ç‰‡):

| æ¨¡å‹ | CER | Price Acc | Brand Recall | F1 |
|------|-----|-----------|--------------|-----|
| åŸå§‹Qwen3-VL | 12.3% | 78% | 72% | 0.75 |
| å¾®è°ƒå | 3.8% | 96% | 93% | 0.94 |
| æå‡ | **â†‘69%** | **â†‘23%** | **â†‘29%** | **â†‘25%** |

### 14.6 æŒç»­ä¼˜åŒ–

#### 14.6.1 ä¸»åŠ¨å­¦ä¹ 

```python
# active_learning.py
def select_hard_samples(predictions, threshold=0.7):
    """é€‰æ‹©æ¨¡å‹ä¸ç¡®å®šçš„æ ·æœ¬è¿›è¡Œæ ‡æ³¨"""
    hard_samples = []

    for pred in predictions:
        # è®¡ç®—ç½®ä¿¡åº¦
        confidence = pred['confidence']

        # ä½ç½®ä¿¡åº¦æ ·æœ¬
        if confidence < threshold:
            hard_samples.append(pred['image_url'])

    return hard_samples

# å®šæœŸè¿è¡Œ
hard_samples = select_hard_samples(recent_predictions)
# å‘é€ç»™æ ‡æ³¨å›¢é˜Ÿ...
```

#### 14.6.2 åœ¨çº¿æ›´æ–°

```python
# online_update.py
def incremental_training(new_data_path):
    """å¢é‡è®­ç»ƒ - æ¯å‘¨æ›´æ–°ä¸€æ¬¡"""
    # 1. åŠ è½½å·²è®­ç»ƒçš„LoRAæƒé‡
    model = AutoModelForCausalLM.from_pretrained(base_model)
    model = PeftModel.from_pretrained(model, "./qwen3vl_ecommerce_lora_final")

    # 2. åŠ è½½æ–°æ•°æ®
    new_dataset = load_dataset('json', data_files=new_data_path)

    # 3. ç»§ç»­è®­ç»ƒï¼ˆæ›´å°çš„å­¦ä¹ ç‡ï¼‰
    training_args.learning_rate = 1e-5  # é™ä½å­¦ä¹ ç‡
    training_args.num_train_epochs = 1

    trainer = Trainer(model=model, args=training_args, train_dataset=new_dataset)
    trainer.train()

    # 4. ä¿å­˜æ–°æƒé‡
    model.save_pretrained(f"./qwen3vl_ecommerce_lora_{datetime.now().strftime('%Y%m%d')}")
```

### 14.7 å¸¸è§é—®é¢˜

#### Q1: å¦‚ä½•å¤„ç†å¤šè¯­è¨€OCR?

**è§£å†³æ–¹æ¡ˆ**: ä½¿ç”¨è¯­è¨€IDå‰ç¼€

```python
payload = {
    "messages": [
        {
            "role": "system",
            "content": "You are a multilingual OCR assistant. Detect language and extract text."
        },
        {
            "role": "user",
            "content": [
                {"type": "image_url", "image_url": image_url},
                {"type": "text", "text": "[LANG: auto] Extract text"}
            ]
        }
    ]
}
```

#### Q2: å¦‚ä½•å¤„ç†ä½è´¨é‡å›¾ç‰‡?

**è§£å†³æ–¹æ¡ˆ**: å›¾åƒé¢„å¤„ç†

```python
from PIL import Image, ImageEnhance

def preprocess_image(image_path):
    """å›¾åƒå¢å¼º"""
    img = Image.open(image_path)

    # 1. å»å™ª
    img = img.filter(ImageFilter.MedianFilter(size=3))

    # 2. å¢å¼ºå¯¹æ¯”åº¦
    enhancer = ImageEnhance.Contrast(img)
    img = enhancer.enhance(1.5)

    # 3. é”åŒ–
    enhancer = ImageEnhance.Sharpness(img)
    img = enhancer.enhance(2.0)

    return img
```

#### Q3: å¦‚ä½•å‡å°‘æ¨ç†å»¶è¿Ÿ?

**ä¼˜åŒ–æ–¹æ¡ˆ**:

1. **æ‰¹é‡æ¨ç†**: åˆå¹¶å¤šä¸ªå›¾ç‰‡è¯·æ±‚
2. **é‡åŒ–**: FP16 â†’ INT8 (å‡å°‘50%æ˜¾å­˜)
3. **Flash Attention**: åŠ é€ŸAttentionè®¡ç®—
4. **KV Cache**: å¤ç”¨è®¡ç®—ç»“æœ

```bash
# vLLMå¯åŠ¨å‚æ•°ä¼˜åŒ–
python -m vllm.entrypoints.openai.api_server \
  --model Qwen/Qwen3-VL-8B-Instruct-FP8 \
  --quantization fp8 \  # INT8é‡åŒ–
  --enable-prefix-caching \  # KVç¼“å­˜
  --max-num-batched-tokens 8192 \  # æ‰¹å¤„ç†
  --gpu-memory-utilization 0.95
```

---

## 15. æ€»ç»“ä¸é¢è¯•è¦ç‚¹

### 15.1 é¡¹ç›®å…¨æ™¯å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     WebScraper                          â”‚
â”‚  çˆ¬è™«ç³»ç»Ÿ (Product/Deal/Shopping)                        â”‚
â”‚  - Playwright + BrightData                              â”‚
â”‚  - BrowserPool (15å¹¶å‘)                                 â”‚
â”‚  - ä¸‰å±‚æ¶æ„ (è°ƒåº¦/é€‚é…/åŸºç¡€)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚ HTTP POST
                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  AsyncPipeline                          â”‚
â”‚  å¼‚æ­¥ä»»åŠ¡å¤„ç†ç®¡é“ (RabbitMQ + Workers)                   â”‚
â”‚  - OCR Worker: vLLM OCR API                             â”‚
â”‚  - LLM Worker: Gemini (30å¹¶å‘)                          â”‚
â”‚  - DB Worker: æ‰¹é‡æ’å…¥ (50æ¡/æ‰¹æ¬¡)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚ è°ƒç”¨
                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    OCR_Rec                              â”‚
â”‚  å¼‚æ­¥OCRè¯†åˆ«ç³»ç»Ÿ                                         â”‚
â”‚  - submit_tasks.py (æ¯5åˆ†é’Ÿ)                            â”‚
â”‚  - fetch_results.py (æ¯2åˆ†é’Ÿ)                           â”‚
â”‚  - Qwen3-VL-8B-Instruct-FP8 (å¾®è°ƒ)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 15.2 æ ¸å¿ƒæŠ€æœ¯æ ˆ

| å±‚çº§ | æŠ€æœ¯ | ç”¨é€” |
|-----|------|------|
| **çˆ¬è™«å±‚** | Playwright, BrightData | æµè§ˆå™¨è‡ªåŠ¨åŒ–ã€ä»£ç†æœåŠ¡ |
| **æ•°æ®æå–** | BeautifulSoup, LLM (Gemini/Claude) | HTMLè§£æã€æ™ºèƒ½æå– |
| **å¹¶å‘æ§åˆ¶** | asyncio, BrowserPool, Semaphore | å¼‚æ­¥ç¼–ç¨‹ã€å¹¶å‘ç®¡ç† |
| **æ¶ˆæ¯é˜Ÿåˆ—** | RabbitMQ | ä»»åŠ¡è§£è€¦ã€å¯é ä¼ é€’ |
| **æ•°æ®åº“** | MongoDB (MongoEngine) | NoSQLæ–‡æ¡£å­˜å‚¨ |
| **OCRè¯†åˆ«** | vLLM, Qwen3-VL-8B, LoRA | è§†è§‰è¯­è¨€æ¨¡å‹ã€å¾®è°ƒ |
| **å­˜å‚¨** | Google Cloud Storage | å›¾ç‰‡CDN |
| **APIæ¡†æ¶** | FastAPI | RESTful APIæœåŠ¡ |
| **è°ƒåº¦** | Airflow, Cron | å®šæ—¶ä»»åŠ¡ |

### 15.3 é¢è¯•é«˜é¢‘é—®é¢˜

#### Q1: å¦‚ä½•ä¿è¯çˆ¬è™«çš„ç¨³å®šæ€§å’Œæ•ˆç‡?

**ç­”**:
1. **BrowserPoolè¿æ¥æ± **: å¤ç”¨æµè§ˆå™¨å®ä¾‹ï¼Œå‡å°‘å¯åŠ¨å¼€é”€
2. **BrightDataæ‰¹é‡çˆ¬å–**: 20ä¸ªURLå¹¶å‘è·å–HTMLï¼Œå¿«10å€
3. **åæ£€æµ‹æœºåˆ¶**: ä¿®æ”¹navigatorå±æ€§ã€ä¼ªé€ WebGLæŒ‡çº¹
4. **é”™è¯¯é‡è¯•**: æŒ‡æ•°é€€é¿é‡è¯•ï¼Œæœ€å¤š3æ¬¡
5. **å¹¶å‘æ§åˆ¶**: Semaphoreé™åˆ¶å¹¶å‘æ•°ï¼Œé¿å…è¢«å°IP

#### Q2: AsyncPipelineå¦‚ä½•ä¿è¯æ•°æ®ä¸€è‡´æ€§?

**ç­”**:
1. **æ¶ˆæ¯æŒä¹…åŒ–**: RabbitMQé˜Ÿåˆ—å’Œæ¶ˆæ¯éƒ½æŒä¹…åŒ–
2. **åŸå­æ€§æ›´æ–°**: MongoDBçš„updateæ“ä½œä¿è¯åŸå­æ€§
3. **çŠ¶æ€æœº**: TracePage.statusæµè½¬ (pending â†’ pending_ocr â†’ pending_llm â†’ pending_db â†’ completed)
4. **å¹‚ç­‰æ€§**: ä½¿ç”¨trace_page_idå»é‡ï¼Œé‡å¤æ¶ˆæ¯ä¸ä¼šé‡å¤æ’å…¥

#### Q3: Qwen3-VLå¾®è°ƒçš„å…³é”®ç‚¹æ˜¯ä»€ä¹ˆ?

**ç­”**:
1. **æ•°æ®è´¨é‡**: æ ‡æ³¨å‡†ç¡®ã€åœºæ™¯è¦†ç›–å…¨é¢ï¼ˆä»·æ ¼ã€å“ç‰Œã€ä¿ƒé”€ï¼‰
2. **LoRAå‚æ•°**: r=16, alpha=32, ä»…è®­ç»ƒ0.1%å‚æ•°
3. **è®­ç»ƒç­–ç•¥**: å­¦ä¹ ç‡2e-4, warmup 500æ­¥, æ¢¯åº¦ç´¯ç§¯
4. **è¯„ä¼°æŒ‡æ ‡**: CER < 5%, Price Acc > 95%, Brand Recall > 90%
5. **æŒç»­ä¼˜åŒ–**: ä¸»åŠ¨å­¦ä¹ é€‰æ‹©hard samplesï¼Œæ¯å‘¨å¢é‡è®­ç»ƒ

#### Q4: ä¸‰å±‚æ¶æ„çš„ä¼˜åŠ¿æ˜¯ä»€ä¹ˆ?

**ç­”**:
1. **è§£è€¦**: è°ƒåº¦å±‚ã€é€‚é…å±‚ã€åŸºç¡€å±‚èŒè´£æ¸…æ™°ï¼Œäº’ä¸å¹²æ‰°
2. **å¯æ‰©å±•**: æ·»åŠ æ–°ç«™ç‚¹åªéœ€å®ç°é€‚é…å±‚ï¼Œæ— éœ€ä¿®æ”¹åŸºç¡€å±‚
3. **å¯å¤ç”¨**: BrowserPoolã€ProductDetailMixinç­‰ç»„ä»¶å¯å¤ç”¨
4. **å¯æµ‹è¯•**: æ¯å±‚å¯å•ç‹¬æµ‹è¯•ï¼Œå¿«é€Ÿå®šä½é—®é¢˜

#### Q5: å¦‚ä½•ä¼˜åŒ–Gemini APIè°ƒç”¨æ€§èƒ½?

**ç­”**:
1. **Resource Manager**: ç®¡ç†å¤šä¸ªAPI Keyè½®æ¢ï¼Œé¿å…å•Keyé™æµ
2. **çœŸå¹¶å‘**: 30ä¸ªasyncioä»»åŠ¡å¹¶å‘è°ƒç”¨ï¼Œååé‡æå‡30å€
3. **æ‰¹é‡å¤„ç†**: LLM Workeræ‰¹é‡æ¥æ”¶ä»»åŠ¡ï¼Œå‡å°‘ä¸Šä¸‹æ–‡åˆ‡æ¢
4. **é‡è¯•æœºåˆ¶**: æŒ‡æ•°é€€é¿é‡è¯•ï¼Œå¤„ç†ä¸´æ—¶æ€§æ•…éšœ

### 15.4 äº®ç‚¹æ€»ç»“

| äº®ç‚¹ | è¯´æ˜ | ä½“ç°èƒ½åŠ› |
|------|------|---------|
| **ä¸‰å±‚æ¶æ„è®¾è®¡** | è°ƒåº¦/é€‚é…/åŸºç¡€åˆ†ç¦»ï¼ŒèŒè´£æ¸…æ™° | æ¶æ„è®¾è®¡ |
| **BrowserPoolä¼˜åŒ–** | 15å¹¶å‘ + è‡ªåŠ¨æ¸…ç† + åæ£€æµ‹ | æ€§èƒ½ä¼˜åŒ– |
| **å¼‚æ­¥è§£è€¦** | RabbitMQ + Workeræ¨¡å¼ | ç³»ç»Ÿè®¾è®¡ |
| **æ‰¹é‡å¤„ç†** | DB Workeræ‰¹é‡æ’å…¥ï¼Œå¿«10å€ | å·¥ç¨‹ä¼˜åŒ– |
| **LoRAå¾®è°ƒ** | å‚æ•°æ•ˆç‡é«˜ï¼Œæ•ˆæœæå‡25% | AIå·¥ç¨‹åŒ– |
| **Resource Manager** | 30å¹¶å‘Geminiè°ƒç”¨ | å¹¶å‘ç¼–ç¨‹ |
| **ä¸»åŠ¨å­¦ä¹ ** | æŒç»­ä¼˜åŒ–æ¨¡å‹ | æœºå™¨å­¦ä¹  |

### 15.5 å‡†å¤‡å»ºè®®

1. **ç†Ÿæ‚‰æ ¸å¿ƒä»£ç **:
   - `extractor_base.py`: BrowserPoolå®ç°
   - `extractor_scheduler.py`: BFSéå†é€»è¾‘
   - `workers/llm_worker.py`: Geminiå¹¶å‘è°ƒç”¨
   - `submit_tasks.py`: OCRä»»åŠ¡æäº¤æµç¨‹

2. **å‡†å¤‡Demo**:
   - æ¼”ç¤ºBrowserPoolå¦‚ä½•å¤ç”¨tab
   - å±•ç¤ºAsyncPipelineå¤„ç†æµç¨‹
   - å¯¹æ¯”å¾®è°ƒå‰åOCRæ•ˆæœ

3. **å‡†å¤‡æ¡ˆä¾‹**:
   - é€‰2-3ä¸ªæŠ€æœ¯éš¾ç‚¹ï¼ˆåçˆ¬è™«ã€å¹¶å‘ä¼˜åŒ–ã€æ•°æ®ä¸€è‡´æ€§ï¼‰
   - å‡†å¤‡1-2ä¸ªä¼˜åŒ–æ¡ˆä¾‹ï¼ˆæ€§èƒ½æå‡ã€å‡†ç¡®ç‡æå‡ï¼‰
   - å‡†å¤‡1ä¸ªè¾¹ç•Œæƒ…å†µå¤„ç†ï¼ˆè¶…æ—¶ã€ç½‘ç»œé”™è¯¯ã€æ•°æ®å¼‚å¸¸ï¼‰

**ç¥é¢è¯•é¡ºåˆ©ï¼** ğŸš€
