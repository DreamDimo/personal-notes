# Webscraper é¡¹ç›®æ·±åº¦æŠ€æœ¯åˆ†æ - é¢è¯•å‡†å¤‡æ–‡æ¡£

> **é¡¹ç›®å®šä½**: åŸºäº LangGraph + Claude AI çš„çˆ¬è™«è‡ªåŠ¨å¼€å‘å·¥å…·
> **æŠ€æœ¯éš¾åº¦**: â­â­â­â­â­ (æ¶æ„è®¾è®¡ã€AIå·¥ç¨‹åŒ–ã€å·¥ä½œæµç¼–æ’)
> **ä»£ç é‡**: ~3000+ è¡Œæ ¸å¿ƒä»£ç 
> **é€‚ç”¨é¢è¯•**: é«˜çº§Pythonå·¥ç¨‹å¸ˆã€AIå·¥ç¨‹å¸ˆã€å…¨æ ˆå·¥ç¨‹å¸ˆ

---

## 0. ä¸ªäººæ€»ç»“

å› ä¸ºç¾å›½è´­ç‰©ç½‘ç«™éå¸¸å¤šï¼Œæœ‰å‡ ç™¾ä¸ªï¼Œä½†æ˜¯æƒ³è¦å¼€å‘ä¸€ä¸ªé€šç”¨çš„çˆ¬è™«ç¨‹åºæ¥åº”å¯¹ä¸åŒçš„ç½‘ç«™è¾ƒä¸ºå›°éš¾ï¼Œå› ä¸ºä¸åŒçš„ç½‘ç«™å¯èƒ½å…ƒç´ ä¼šå¯¹åº”ä¸åŒçš„é€‰æ‹©å™¨ã€‚

æ‰€ä»¥æˆ‘ä»¬æ˜¯å…ˆå¯¹å¤šä¸ªç½‘ç«™æ‰‹åŠ¨å¼€å‘ï¼Œè¯•å›¾å¯»æ‰¾å®ƒä»¬çš„å…±æ€§ã€‚æˆ‘ä»¬å‘ç°ä¸€ä¸ªç½‘ç«™çš„çˆ¬è™«å¤§æ¦‚æ˜¯è¦åˆ†æˆå‡ æ­¥ï¼Œå¯¹äºæ‰‹åŠ¨å¼€å‘ï¼š

* é¦–å…ˆæˆ‘ä»¬éœ€è¦å»æ‰“å¼€ä¸€ä¸ªç½‘é¡µï¼Œç¬¬ä¸€ä¸ªé¡µé¢ä¸€å®šæ˜¯ä¸»é¡µï¼Œæˆ‘ä»¬éœ€è¦æå–å®ƒé‡Œé¢çš„urlé“¾æ¥ï¼Œä¸€èˆ¬æ˜¯é€šè¿‡å¼€å‘è€…å·¥å…·ç¡®å®šå‡ ä¸ªå…ƒç´ ï¼Œç„¶åæ ¹æ®å…ƒç´ æå–urlã€‚
* ä¸‹ä¸€æ­¥å°±æ˜¯åˆ†æè¿™äº›urlï¼Œå¯¹è¿™äº›urlè¿›è¡Œåˆ†ç±»ï¼Œå®ƒä»¬å¯èƒ½å¯¹åº”è¯¦æƒ…é¡µï¼Œåˆ—è¡¨é¡µå’Œå…¶ä»–é¡µé¢ï¼Œæˆ‘ä»¬éœ€è¦å¯¹å®ƒä»¬è¿›è¡Œåˆ†ç±»ã€‚
* ä¹‹åæ ¹æ®åˆ†ç±»ï¼Œå¯¹äºåˆ—è¡¨é¡µï¼Œæˆ‘ä»¬éœ€è¦æ¨¡ä»¿ä¸»é¡µæå–è§„åˆ™ç»§ç»­æå–æœ‰æ•ˆçš„urlé“¾æ¥ï¼›
* å¯¹äºè¯¦æƒ…é¡µï¼Œæˆ‘ä»¬éœ€è¦é¦–å…ˆè·å–åˆ° $html$ ï¼Œé€šå¸¸éœ€è¦æ»šåŠ¨é¡µé¢æ¥è§¦å‘æ¸²æŸ“ã€‚å¯¹äºè¯„è®ºæˆ–è€…å†…å®¹å­—æ®µæœ‰äº›ä¸ºåŠ¨æ€åŠ è½½ï¼Œéœ€è¦é€šè¿‡ajaxæŠ€æœ¯è·å–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¯é€šè¿‡å¼€å‘è€…å·¥å…·å¾—åˆ°è¯·æ±‚ APIï¼Œæ ¹æ® api å»æ‹¦æˆªè¯·æ±‚å¾—åˆ°å…¨éƒ¨çš„è¯„è®ºæˆ–è€…è¯¦æƒ…ä¿¡æ¯ã€‚è·å–åæˆ‘ä»¬éœ€è¦é€šè¿‡ $Beautifulsoup$ å¯¹ html è¿›è¡Œæ ¼å¼è½¬æ¢ã€‚ç„¶åå¼€å§‹å¯¹ html åšä¸€è½®æ¸…ç†ï¼Œæ¸…ç†æ‰ç½‘ç«™å¤´å°¾ï¼Œç„¶åç§»é™¤éå•†å“ç›¸å…³å…ƒç´ ï¼ˆåŒ…æ‹¬æ¨èå•†å“ï¼Œå¹¿å‘Šï¼Œç›¸å…³é“¾æ¥ç­‰ï¼‰ï¼Œç§»é™¤æ‰æ‰€æœ‰çš„ $script$ ä»£ç ï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»é€šè¿‡è¯·æ±‚è·å–äº†ï¼Œä¿ç•™ä¸»è¦å†…å®¹ã€‚ä¹‹ååˆ©ç”¨å…ƒç´ æ ‡ç­¾æå–å¯¹åº”çš„å…ƒç´ å°†å…¶è½¬æ¢ä¸º markdown ã€‚
* å¯¹äºæå–åˆ°çš„å•†å“å›¾ç‰‡ï¼Œæœ‰äº›å›¾ç‰‡çš„æ–‡å­—å¯¹æˆ‘ä»¬æ˜¯æœ‰ç”¨çš„ï¼Œæ¯”å¦‚åŒ–å¦†å“ã€é£Ÿå“ã€ä¿å¥å“çš„é…æ–™è¡¨ä¿¡æ¯ï¼Œæˆ‘ä»¬éœ€è¦åˆ©ç”¨ $OCR$ æŠ€æœ¯å»è·å–è¿™äº›ä¿¡æ¯ï¼Œæˆ‘ä»¬é‡‡ç”¨çš„æ˜¯å¯¹ $Qwen3-8b-VL$ è¿›è¡Œå¾®è°ƒã€‚å› ä¸ºæˆ‘ä»¬å‘ç°è¯¥æ¨¡å‹åœ¨å¤„ç†å°å­—çš„æ—¶å€™ç»å¸¸ä¼šå‡ºç°å¹»è§‰ï¼Œä¸”è¾“å‡ºå†…å®¹è´¨é‡è¾ƒä½ï¼Œè¾“å‡ºæ ¼å¼éœ€è¦å¤„ç†è½¬æ¢ï¼Œå› æ­¤æˆ‘ä»¬å¯¹å…¶è¿›è¡Œå¾®è°ƒï¼Œæˆ‘ä»¬é‡‡é›†ä¸€äº›ç½‘ç«™çš„å›¾ç‰‡ä¸»è¦æ˜¯é…æ–™è¡¨ä¿¡æ¯ï¼Œç„¶åæ ¹æ®äººå·¥æ ‡æ³¨å½¢æˆä¸€å¥—æ•°æ®é›†ï¼Œç„¶ååˆ©ç”¨è¿™ä¸ªå¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶æé«˜è¾“å‡ºå‡†ç¡®ç‡ï¼Œå¹¶èƒ½å¤ŸçŸ¥é“æˆ‘ä»¬éœ€è¦çš„ä¸»è¦ä¿¡æ¯ï¼Œä¹‹åæ¨¡å‹çš„è¾“å‡ºå‡†ç¡®ç‡éƒ½æœ‰äº†ä¸€å®šçš„æé«˜ã€‚
* ç„¶åå°†å›¾ç‰‡è§£æçš„OCRä¿¡æ¯å’Œä¹‹å‰çˆ¬è™«è·å–çš„markdownæ‹¼æ¥èµ·æ¥ï¼Œé€å…¥ $gemini$ è¿›è¡Œåˆ†æï¼Œæ ¹æ®è®¾è®¡çš„äº§å“æ¨¡å‹è¡¨ä¿¡æ¯ï¼Œåšå…·ä½“çš„è½¬æ¢ï¼Œè¿™é‡Œä¸»è¦æ˜¯æ ¹æ®äº§å“æ¨¡å‹ç¼–å†™æç¤ºè¯ï¼Œè®©å¤§æ¨¡å‹èƒ½å¤ŸçŸ¥é“å“ªäº›éœ€è¦è½¬æ¢ä»¥åŠæ€ä¹ˆè½¬æ¢ã€‚å…¶ä¸­è¿™é‡Œæœ‰ä¸€æ­¥ä¸­é—´æ­¥éª¤ï¼Œå°±æ˜¯æˆ‘ä»¬éœ€è¦æ ¹æ®æ¸…æ´—åçš„markdownæ¥ç¡®å®šå…·ä½“çš„äº§å“åˆ†ç±»ï¼Œå®ƒæ˜¯å±äºé£Ÿå“è¿˜æ˜¯è¡£ç‰©è¿˜æ˜¯ä¿å¥å“ç­‰ç­‰ï¼Œå› ä¸ºä¸åŒç±»åˆ«å¯¹åº”çš„äº§å“æ¨¡å‹ä¸åŒã€‚**TODO** 

æˆ‘ä»¬å¯ä»¥çœ‹å‡ºæ­¥éª¤ä¸ŠåŸºæœ¬æ˜¯ä¸€è‡´çš„ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨ $code \ agent$ åšå¼€å‘ï¼Œæˆ‘ä»¬å…ˆç”¨ $jinja2$ ç»™å‡ºä¸€ä¸ªå¤§è‡´çš„æ¨¡ç‰ˆï¼Œå³åŒ…æ‹¬ä¸€ä¸ªæå–å™¨ç±»ç”¨æ¥æå–ï¼Œ å¯¹äº $ai$ è¾…åŠ©å¼€å‘ï¼Œæœ‰å‡ ä¸ªè¦ç‚¹éœ€è¦æ€è€ƒåˆ†æï¼š

* æˆ‘ä»¬æ‰“å¼€ç½‘é¡µå¹¶å¯¹ç½‘é¡µè¿›è¡Œåˆ†æï¼Œè¿™ä¸ªå¦‚ä½•è®© $agent$ æ¥æ“ä½œï¼Ÿæˆ‘ä»¬çš„æ€è·¯æ˜¯åˆ©ç”¨è°·æ­Œå¼€å‘è€… $mcp$ å·¥å…·å³ $chrome\_devtools\_mcp$ å»åˆ†æé¡µé¢ï¼Œå› ä¸ºå®ƒå¯ä»¥å¿«é€ŸæŸ¥æ‰¾æŒ‡å®šå…ƒç´ çš„ $html$ å…ƒç´ ï¼Œå¯¹é¡µé¢è¿›è¡Œæˆªå›¾æ¯”å¯¹åˆ†æã€‚åŒæ—¶è¦ç¡®è®¤æ˜¯å¦ä¼šé­é‡åçˆ¬ï¼Œå¦‚æœé­é‡åçˆ¬åˆ™å¯ç”¨ $brightdata$ æå– $html$ï¼Œå³è¦ç¡®è®¤å¼•æ“æ˜¯åªä½¿ç”¨ $playwright$ è¿˜æ˜¯è¦ç»“åˆä½¿ç”¨ $brightdata$ ã€‚è¿˜éœ€è¦ç¡®å®šé¡µé¢çš„æ¸²æŸ“æ–¹å¼ï¼Œæ˜¯å¦æœ‰å¾ˆå¤š $js$ ä»£ç 
* ä¸‹é¢æ˜¯ç”Ÿæˆåˆ—è¡¨æç¤ºå™¨ï¼Œæˆ‘ä»¬æä¾›ä¸€ä¸ªåŸºæœ¬çš„æ¨¡ç‰ˆï¼Œç„¶åç»™ $claude \ code$ ä¸€äº› $mcp$ æˆ–è€… $skills$ï¼Œè®©å®ƒèƒ½å¤Ÿç”Ÿæˆå¯¹åº”çš„åˆ—è¡¨æå–å™¨ï¼Œæˆ‘ä»¬ç»™çš„æç¤ºè¯æ˜¯ç”¨ `chrome_devtools` åˆ†æé¡µé¢ï¼Œç„¶åä»£ç ä¸­ä½¿ç”¨ `playwright` ï¼Œå»ç½‘é¡µæ³¨å…¥ä¸€ä¸ª `js` è„šæœ¬å»æå–æ‰€æœ‰çš„é“¾æ¥ï¼Œç„¶åå°† url å­˜å…¥ state é‡Œé¢çš„ `site_tree` å­—æ®µã€‚
* æˆ‘ä»¬éœ€è¦å¯¹æå–åˆ°çš„ `url` è¿›è¡Œè¿›ä¸€æ­¥çš„åˆ†ç±»ï¼Œæˆ‘ä»¬éœ€è¦æŠŠå®ƒä»¬åˆ†ä¸ºåˆ—è¡¨é¡µã€è¯¦æƒ…é¡µã€ä¸ç¡®å®šé¡µã€å…¶ä»–é¡µé¢ï¼ˆé€šè¿‡è®¾è®¡æç¤ºè¯å‘Šè¯‰æ¯ç§é¡µé¢çš„ä¸€äº›è§„åˆ™ï¼Œåˆ†ç±»è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨æ›´ä¾¿å®œçš„ gemini æä¾›çš„ mcp å»å¤„ç†ï¼‰ã€‚ç„¶åæˆ‘ä»¬å¯¹æ¯ä¸€ä¸ªåˆ†ç±»çš„ `url` é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼è®¾è®¡ä¸€äº›åŒ¹é…è§„åˆ™ï¼Œå³è¯†åˆ«å‡ºä¸€äº› `pattern` æ¨¡å¼ï¼Œå¹¶ä¸”åˆ©ç”¨æˆ‘ä»¬è‡ªå®šä¹‰çš„å·¥å…·å»æ£€æŸ¥è¿™äº›æ¨¡å¼æ˜¯å¦æ­£ç¡®ï¼ˆå³æ£€æŸ¥æœ‰æ²¡æœ‰urlå’Œå…¶åŒ¹é…ï¼‰ï¼Œæˆ‘ä»¬å°†è¯†åˆ«å‡ºçš„ `pattern` æ”¾å…¥ state ä¸­çš„ `site_pattern` å­—æ®µï¼Œç„¶åå†™å…¥åˆ°ä»£ç é‡Œé¢ï¼ŒæŒ‰ç…§ä¸Šé¢å››ä¸ªåˆ†ç±»æŠŠå®ƒä»¬åˆ†æˆä¸åŒçš„ `patterns` ã€‚è¿™ä¸€æ­¥å¯èƒ½éœ€è¦å¤šè½®ï¼Œç¬¬ä¸€è½®ç»“æŸåè¦æ£€æŸ¥æœ‰æ²¡æœ‰ `url` æ²¡æœ‰è¢«åŒ¹é…åˆ°ï¼Œç„¶åé‡æ–°åŒ¹é…è¿™äº› urlã€‚ å¯¹äºä¸ç¡®å®šçš„é¡µé¢æˆ‘ä»¬é€šè¿‡è®¾è®¡æç¤ºè¯å¢åŠ ä¸€è½®æ¥è¿›ä¸€æ­¥åˆ†æå…¶å±äºçš„èŒƒå›´ã€‚ç„¶åæ›´æ–° `site_tree` å­—æ®µã€‚è¿™é‡Œæˆ‘ä»¬ä¹Ÿè®¾è®¡äº†å·¥å…·ï¼Œå»éšæœºæŠ½å–ä¸€äº› url æ£€æŸ¥ url åˆ†ç±»æ˜¯å¦å‡†ç¡®ï¼Œåˆ¤æ–­æ–¹æ³•æ˜¯ç”¨è°·æ­Œå¼€å‘è€…å·¥å…·å»æ‰“å¼€ç½‘é¡µç„¶ååˆ¤æ–­
* ä¹‹åæˆ‘ä»¬ä»æ¯ä¸ª patterns é‡Œé¢æ‰¾ä¸€äº› `sample_url` ï¼Œç„¶åæ ¹æ®åˆ†ç±»è¿›å…¥ä¸åŒçš„è¯¦æƒ…é¡µæå–å’Œåˆ—è¡¨é¡µæå–åˆ†æã€‚å¯¹äºè¯¦æƒ…é¡µï¼Œæˆ‘ä»¬ä¹Ÿæ˜¯å…ˆç”¨ `chrome-devtools` å¯¹æ ·ä¾‹é¡µè¿›è¡Œåˆ†æï¼Œé¦–å…ˆé€šè¿‡è°·æ­Œå¼€å‘è€…å·¥å…·å°†å…¶æ»‘åŠ¨åˆ°åº•éƒ¨ç„¶åç­‰å¾…çœ‹æ˜¯å¦ä¼šæœ‰åçˆ¬ï¼Œå¦‚æœæœ‰åçˆ¬æˆ‘ä»¬ä½¿ç”¨ brigthdataå…ˆè·å–é¡µé¢ï¼Œç„¶åç”¨è°·æ­Œå¼€å‘è€…å·¥å…·è¯»å–è¿™ä¸ªä¸´æ—¶çš„ html æ–‡ä»¶ï¼Œç„¶åæˆ‘ä»¬éœ€è¦å¯¹é¡µé¢æ»šåŠ¨ç„¶åè¿›è¡Œæˆªå›¾ï¼Œæˆªå›¾éœ€è¦æˆ‘ä»¬è‡ªå®šä¹‰çš„å·¥å…·è¿›è¡Œå‹ç¼©ã€‚æˆªå›¾çš„ç›®çš„æ˜¯è®© ai æ¯”è¾ƒé€šè¿‡ html åˆ†æå¾—åˆ°çš„ç»“æœå’Œæˆªå›¾è¿›è¡Œæ¯”è¾ƒåˆ†æï¼Œå¹¶ç»™å‡ºä¸€äº›é—®é¢˜è®© ai è¿›è¡Œåˆ†æï¼ˆé¡µé¢çš„å¸ƒå±€ï¼Œå¤´æ åº•æ åœ¨å“ªï¼Œæœ‰æ— å…¬å‘Šï¼›å†…å®¹å®šä½ï¼Œä¸»å•†å“å›¾ç‰‡ï¼Œå•†å“ä»‹ç»ï¼Œæ˜¯å¦é€šè¿‡jsç­‰ï¼‰ã€‚åˆ†æå®Œæˆåè¾“å‡ºåˆ°æ³¨é‡Šé‡Œé¢ï¼Œç„¶åæ ¹æ®æ³¨é‡Šå»åˆ¶ä½œä¸€ä¸ªè¯¦æƒ…é¡µæå–ç±»ï¼ŒåŒ…æ‹¬å¤šä¸ªæ­¥éª¤ï¼Œå³è·å– htmlï¼Œæ¸…ç†å¹¶è½¬æ¢ï¼Œç„¶åå¦‚æœæœ‰çš„æ˜¯é€šè¿‡ `ajax` å»æœåŠ¡å™¨è¯»å–çš„ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å‘é€ `get_ajax_comment` æ¥æ ¹æ® api è¯·æ±‚è·å–åˆ°æƒ³è¦çš„ä¿¡æ¯ã€‚
* ç„¶åå°±æ˜¯æˆ‘ä»¬éœ€è¦å¢åŠ æ ‘çš„æ·±åº¦ï¼Œå³éœ€è¦ä¸æ–­æ‰©å±• urlï¼Œå› ä¸ºæˆ‘ä»¬æå–åˆ°çš„åˆ—è¡¨é¡µå¯èƒ½åŒ…å«å…¶ä»–åˆ—è¡¨é¡µï¼Œæ‰€ä»¥éœ€è¦ä¸æ–­æ‰©å±•ï¼Œè¿™é‡Œå°±æ˜¯é‡å¤æ‰§è¡Œä¸Šè¿°çš„æ­¥éª¤ã€‚æ‰§è¡Œå®Œåæˆ‘ä»¬è®©æ¨¡å‹å¯¹æ‰€æœ‰æ–¹æ³•è¿›è¡Œæ£€æŸ¥ï¼Œæ£€æŸ¥ä¸€ä¸‹å‡ ä¸ªå…³é”®åœ°æ–¹æ˜¯å¦ç”Ÿæˆã€‚ç„¶åè®©æ¨¡å‹æµ‹è¯•ï¼Œå¾—åˆ°æ—¶é•¿ï¼Œå³è®°å½•ä¸€ä¸‹éœ€è¦å¤šä¹…æ‰èƒ½æ‰§è¡Œå®Œè¿™æ–¹æ³•ï¼Œç„¶åé€šè¿‡è¿™ä¸ªå»å¯¹è€—æ—¶é•¿çš„æ–¹æ³•å»ä¼˜åŒ–ã€‚
* åŒæ—¶ä¸ºäº†ä¿è¯ä»£ç çš„è´¨é‡ï¼Œæˆ‘ä»¬åœ¨æ¯ä¸€æ­¥åéƒ½ä¼šåŠ å…¥ä¸€ä¸ª `review nodes` è¿™ä¸ªæ˜¯å¦ä¸€ä¸ª `subagent` ï¼Œä¼šå»æ£€æŸ¥æ¯ä¸€æ­¥ç”Ÿæˆæ˜¯å¦æ»¡è¶³è¦æ±‚ï¼Œè¿™ä¸ªä¸»è¦æ˜¯æˆ‘ä»¬è¿›è¡Œä¸€äº›æ­£åˆ™è¡¨è¾¾å¼æ£€æŸ¥ï¼ˆéœ€è¦ç”Ÿæˆçš„æ–¹æ³•æˆ–è€…ç±»æˆ–è€…å±æ€§æ˜¯å¦å­˜åœ¨ï¼‰ï¼Œç„¶åå»æ‰§è¡Œä»£ç å¯¹è¿”å›ç»“æœè¿›è¡Œåˆ†æï¼Œæ˜¯å¦æœ‰æŠ¥é”™ï¼Œæ˜¯å¦æœ‰è¿”å›ç»“æœç­‰ã€‚åœ¨å‰æœŸæˆ‘ä»¬ä¼šåŠ å…¥ `human-in-loop` å»æ£€æŸ¥ä¸€äº›ä¸­é—´æ–‡ä»¶ï¼Œæ¯”å¦‚åˆ—è¡¨é¡µæå–åˆ°çš„åˆ—è¡¨æ–‡ä»¶ï¼Œè¯¦æƒ…é¡µçš„htmlæ–‡ä»¶ç­‰ï¼Œåé¢å¾…æç¤ºè¯ä¿®å¤å®Œå–„ï¼Œæµ‹è¯•å‘½ä»¤å®Œå–„åæˆ‘ä»¬åˆ é™¤äº†è¿™ä¸€éƒ¨åˆ†ï¼Œæœ€å¤§å¯èƒ½çš„ä¿è¯ç³»ç»Ÿå…¨éƒ¨è‡ªåŠ¨åŒ–è¿è¡Œã€‚åŒæ—¶æˆ‘ä»¬æ¥å…¥äº† `slack boot` ï¼Œå½“å¼€å‘å‡ºç°é—®é¢˜æ—¶èƒ½å¤Ÿè‡ªåŠ¨å‘æˆ‘ä»¬çš„å·¥ä½œåŒºå‘å¸ƒè­¦å‘Šï¼Œç„¶åäººæ¥å®¡æ ¸ä¿®å¤ã€‚

è¿™é‡Œæˆ‘ä»¬ç”¨çš„å°±æ˜¯ $LangGraph$ è¿›è¡Œå·¥ä½œæµçš„è®¾è®¡ï¼Œç„¶åç”¨å…¶ä¸­çš„ `checkpoint` æœºåˆ¶å»ä¿å­˜æ¯ä¸€æ­¥çš„ä¿¡æ¯ï¼Œç„¶åç”¨ `git` å»å®ç°è‡ªåŠ¨æäº¤ä»£ç ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥å®ç°ä»æŸä¸€æ­¥æ¢å¤é‡åšç­‰æ–¹æ³•ã€‚æˆ‘ä»¬åˆ©ç”¨çš„ `code agent` å°±æ˜¯ `claude code` ï¼Œå› ä¸ºå®ƒå…·æœ‰ `mcp` ã€`skills` ä»¥åŠè®°å¿†ç­‰èƒ½åŠ›ï¼ŒåŒæ—¶æˆ‘ä»¬ç”¨å…¨å±€çš„ client é˜²æ­¢é‡å¤åˆ›å»ºï¼Œç„¶åæˆ‘ä»¬é€šè¿‡ hook äº‹ä»¶æ‰¹å‡†ä¸€äº›å·¥å…·çš„ä½¿ç”¨ï¼Œç”¨session_idä¿æŒä¸€è½®ä¼šè¯çš„ä¸€è‡´æ€§ï¼Œç„¶åå…·ä½“çš„ä½¿ç”¨æ–¹æ³•å¯ä»¥è¯¦è§å¤§æ¨¡å‹å¼€å‘ä¸­çš„ `claude code` ã€‚

## 1. é¡¹ç›®æ¦‚è¿°

### 1.1 é¡¹ç›®å®šä½

**DevBot** æ˜¯ä¸€ä¸ªåŸºäº LangGraph + Claude Agent SDK çš„ AI è¾…åŠ©çˆ¬è™«å¼€å‘ç³»ç»Ÿï¼Œèƒ½å¤Ÿè‡ªåŠ¨åŒ–ç”Ÿæˆå’Œä¼˜åŒ–ç½‘ç«™çˆ¬è™«ä»£ç ã€‚

**æ ¸å¿ƒä»·å€¼**:

- **è‡ªåŠ¨åŒ–å¼€å‘**: ä» URL åˆ°å¯è¿è¡Œçˆ¬è™«ä»£ç çš„å…¨æµç¨‹è‡ªåŠ¨åŒ–
- **æ™ºèƒ½å†³ç­–**: é€šè¿‡ LLM åˆ†æé¡µé¢ç»“æ„ã€é€‰æ‹©åˆé€‚çš„çˆ¬è™«å¼•æ“å’Œå‚æ•°
- **è´¨é‡ä¿è¯**: Developer + Reviewer åŒé‡èŠ‚ç‚¹ï¼Œç¡®ä¿ä»£ç è´¨é‡
- **æŒç»­ä¼˜åŒ–**: æ”¯æŒæ–­ç‚¹æ¢å¤ã€æ­¥éª¤é‡è¯•ã€ä»£ç ä¼˜åŒ–ç­‰é«˜çº§åŠŸèƒ½

### 1.2 æŠ€æœ¯æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      DevBot ç³»ç»Ÿæ¶æ„                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚  LangGraph  â”‚  â”‚ Claude Agent â”‚  â”‚  MCP Tools   â”‚       â”‚
â”‚  â”‚  StateGraph â”‚â”€â”€â”‚  SDK Client  â”‚â”€â”€â”‚ chrome-devto â”‚       â”‚
â”‚  â”‚  Workflow   â”‚  â”‚  (Session)   â”‚  â”‚ gemini-cli   â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚         â”‚                 â”‚                   â”‚              â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                           â”‚                                  â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚         â”‚                                     â”‚              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Developer    â”‚                  â”‚    Reviewer     â”‚    â”‚
â”‚  â”‚  Nodes (23)   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚   Nodes (6)     â”‚    â”‚
â”‚  â”‚  Step 0-31    â”‚   Validate       â”‚   review_step   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚         â”‚                                     â”‚              â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                           â”‚                                  â”‚
â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚                  â”‚  CrawlerDevState  â”‚                       â”‚
â”‚                  â”‚   TypedDict (27+) â”‚                       â”‚
â”‚                  â”‚  - url, site_name â”‚                       â”‚
â”‚                  â”‚  - current_step   â”‚                       â”‚
â”‚                  â”‚  - status, retry  â”‚                       â”‚
â”‚                  â”‚  - patterns_queue â”‚                       â”‚
â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                           â”‚                                  â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚         â”‚                                     â”‚              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  SQLite Store â”‚                  â”‚   JSON State    â”‚    â”‚
â”‚  â”‚  Conversation â”‚                  â”‚   Checkpoint    â”‚    â”‚
â”‚  â”‚  History      â”‚                  â”‚   MemorySaver   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.3 æ ¸å¿ƒæµç¨‹æ¦‚è§ˆ

```
ç”¨æˆ·è¾“å…¥ URL
    â†“
Step 0: åˆ›å»ºåŸºç¡€æ–‡ä»¶æ¡†æ¶ (Jinja2æ¨¡æ¿)
    â†“
Step 1: åˆ†æé¡µé¢ç»“æ„ (Chrome DevTools + LLM)
    â†“ â†’ Reviewer: éªŒè¯å¼•æ“é…ç½®
    â†“
Step 2: ç”Ÿæˆåˆ—è¡¨æå–å™¨ (æå–å•†å“/ä¼˜æƒ åˆ—è¡¨)
    â†“ â†’ Reviewer: éªŒè¯è¿”å›æ ¼å¼
    â†“
Step 2.1: URL åˆ†ç±» (detail/list/other/unclear)
    â†“ â†’ Reviewer: éªŒè¯ site_patterns
    â†“
Step 3: ç”Ÿæˆ URL patterns (æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…)
    â†“ â†’ Reviewer: éªŒè¯ URL_MAP
    â†“
Step 4-6: å¾ªç¯ç”Ÿæˆå„ pattern çš„æå–å™¨ (detail/list)
    â†“ â†’ Reviewer: éªŒè¯æ ¸å¿ƒæå–å™¨
    â†“
Step 7: ç½‘ç«™æ ‘æ‰©å±• (BFSéå†å­é¡µé¢)
    â†“ â†’ Reviewer: éªŒè¯æ‰©å±•åŠŸèƒ½
    â†“
Step 8: Markdown ä¿¡æ¯åˆ†æ (å ä½æ­¥éª¤)
    â†“ â†’ Reviewer: éªŒè¯çŠ¶æ€
    â†“
Step 9: é¦–æ¬¡è¿è¡Œæµ‹è¯•
    â†“
Step 10: ç”Ÿæˆ Airflow DAG
    â†“
Step 20-22: ä»£ç æ£€æŸ¥ä¸ä¼˜åŒ– (æ€§èƒ½ã€æ–¹æ³•ç­¾åã€æ•°æ®å®Œæ•´æ€§)
    â†“
è¾“å‡º: å¯è¿è¡Œçš„çˆ¬è™«ä»£ç 
```

---

## 2. State çŠ¶æ€ç®¡ç†æ·±åº¦è§£æ

### 2.1 ä¸ºä»€ä¹ˆé€‰æ‹© TypedDictï¼Ÿ

**TypedDict** æ˜¯ Python 3.8+ å¼•å…¥çš„ç±»å‹æç¤ºå·¥å…·ï¼Œç”¨äºå®šä¹‰å­—å…¸çš„ç»“æ„ã€‚

**ä¼˜åŠ¿**:

1. **ç±»å‹å®‰å…¨**: IDE è‡ªåŠ¨è¡¥å…¨ã€é™æ€ç±»å‹æ£€æŸ¥
2. **æ–‡æ¡£ä½œç”¨**: å­—æ®µå®šä¹‰å³æ–‡æ¡£ï¼Œæ¸…æ™°æ˜äº†
3. **æ— è¿è¡Œæ—¶å¼€é”€**: ä»…ç”¨äºç±»å‹æç¤ºï¼Œä¸å½±å“æ€§èƒ½
4. **LangGraph å…¼å®¹**: LangGraph åŸç”Ÿæ”¯æŒ TypedDict ä½œä¸ºçŠ¶æ€ç±»å‹

**ç¤ºä¾‹å¯¹æ¯”**:

```python
# âŒ æ™®é€šå­—å…¸ï¼šæ— ç±»å‹æç¤ºï¼Œæ˜“å‡ºé”™
state = {"url": "https://example.com", "step": 1}
print(state["urll"])  # æ‹¼å†™é”™è¯¯ï¼Œè¿è¡Œæ—¶æ‰æŠ¥é”™

# âœ… TypedDictï¼šIDE è‡ªåŠ¨æ£€æŸ¥
class State(TypedDict):
    url: str
    step: int

state: State = {"url": "https://example.com", "step": 1}
print(state["urll"])  # IDE ç«‹å³æ ‡çº¢æç¤ºé”™è¯¯
```

### 2.2 CrawlerDevState å®Œæ•´å­—æ®µè§£æ

DevBot çš„çŠ¶æ€å®šä¹‰åœ¨ `devbot/state/crawler_state.py:8-56`ï¼Œå…± **27+ ä¸ªå­—æ®µ**ï¼Œåˆ†ä¸º 7 å¤§ç±»ï¼š

#### 2.2.1 çˆ¬è™«åŸºæœ¬ä¿¡æ¯ (4 å­—æ®µ)

```python
class CrawlerDevState(TypedDict):
    # ============= çˆ¬è™«åŸºæœ¬ä¿¡æ¯ =============
    url: str                             # ç›®æ ‡ç½‘ç«™ URL
    site_name: str                       # ç½‘ç«™åç§° (ä» URL æå–)
    proxy: Optional[str]                 # ä»£ç†æœåŠ¡å™¨åœ°å€
    category: str                        # çˆ¬è™«åˆ†ç±» (product/shopping/dealç­‰)
```

**å­—æ®µè¯¦è§£**:

| å­—æ®µ        | ç±»å‹            | è¯´æ˜                     | ç¤ºä¾‹                        | ä½¿ç”¨åœºæ™¯               |
| ----------- | --------------- | ------------------------ | --------------------------- | ---------------------- |
| `url`       | `str`           | ç›®æ ‡ç½‘ç«™å…¥å£ URL         | `https://www.gnc.com`       | æ‰€æœ‰æ­¥éª¤ï¼Œç”¨äºè®¿é—®ç½‘ç«™ |
| `site_name` | `str`           | ç½‘ç«™æ ‡è¯†ç¬¦ï¼ˆä»åŸŸåæå–ï¼‰ | `gnc`                       | æ–‡ä»¶å‘½åã€æ¨¡å—å¯¼å…¥     |
| `proxy`     | `Optional[str]` | ä»£ç†åœ°å€ï¼ˆåçˆ¬æ—¶ä½¿ç”¨ï¼‰   | `http://proxy:8080`         | Step 1 é¡µé¢è®¿é—®        |
| `category`  | `str`           | çˆ¬è™«ç±»åˆ«                 | `product`/`shopping`/`deal` | å†³å®šè¾“å‡ºç›®å½•ç»“æ„       |

**æå– site_name çš„é€»è¾‘**:

```python
# crawler_devbot.py:155-158
from urllib.parse import urlparse
domain = urlparse(url).netloc
site_name = domain.split('.')[-2]  # www.gnc.com â†’ gnc
```

**æ½œåœ¨é—®é¢˜**:

- **å­åŸŸåæ··æ·†**: `shop.example.com` â†’ `example`ï¼ˆæ­£ç¡®ï¼‰
- **å¤šçº§åŸŸå**: `example.co.uk` â†’ `co`ï¼ˆé”™è¯¯ï¼‰
  - **è§£å†³**: æ‰‹åŠ¨æŒ‡å®š `site_name` å‚æ•°

#### 2.2.2 å½“å‰æ­¥éª¤ä¿¡æ¯ (3 å­—æ®µ)

```python
# ============= å½“å‰æ­¥éª¤ä¿¡æ¯ =============
current_step: str                    # å½“å‰æ­¥éª¤ç¼–å· ("0", "1", "2", ...)
current_step_name: str               # å½“å‰æ­¥éª¤åç§° (å¦‚ "create_base_file")
current_depth: int                   # å½“å‰æ·±åº¦ï¼ˆç”¨äºåµŒå¥—æ­¥éª¤ï¼‰
```

**å­—æ®µè¯¦è§£**:

| å­—æ®µ                | ç±»å‹  | è¯´æ˜                           | ç¤ºä¾‹                               | æ›´æ–°æ—¶æœº                |
| ------------------- | ----- | ------------------------------ | ---------------------------------- | ----------------------- |
| `current_step`      | `str` | æ­¥éª¤ç¼–å·ï¼ˆå­—ç¬¦ä¸²ï¼Œæ”¯æŒå­æ­¥éª¤ï¼‰ | `"0"`, `"2.1"`, `"5.3"`            | æ¯ä¸ª Developer èŠ‚ç‚¹ç»“æŸ |
| `current_step_name` | `str` | æ­¥éª¤è¯­ä¹‰åç§°                   | `create_base_file`, `analyze_page` | æ¯ä¸ª Developer èŠ‚ç‚¹ç»“æŸ |
| `current_depth`     | `int` | é€’å½’æ·±åº¦ï¼ˆStep 7 ä¸“ç”¨ï¼‰        | `0`, `1`, `2`                      | Step 7 ç½‘ç«™æ ‘æ‰©å±•       |

**æ­¥éª¤ç¼–å·è§„åˆ™**:

- **ä¸»æ­¥éª¤**: `"0"`, `"1"`, `"2"`, ... , `"10"`
- **å­æ­¥éª¤**: `"2.1"`, `"5.1"`, `"5.2"`, ... , `"5.6"`
- **ç‰¹æ®Šæ­¥éª¤**: `"20"`, `"21"`, `"22"` (ä»£ç æ£€æŸ¥ä¸ä¼˜åŒ–)

**ç¤ºä¾‹**:

```python
# Step 2 å®Œæˆå
state = {
    "current_step": "2",
    "current_step_name": "generate_list_extractor",
    ...
}

# Step 2.1 å®Œæˆå
state = {
    "current_step": "2.1",
    "current_step_name": "classify_urls",
    ...
}
```

#### 2.2.3 æ­¥éª¤çŠ¶æ€ (2 å­—æ®µ)

```python
# ============= æ­¥éª¤çŠ¶æ€ =============
status: Literal["pending", "in_progress", "completed", "reviewed", "failed"]
retry_count: int                     # é‡è¯•æ¬¡æ•°è®¡æ•°å™¨
```

**å­—æ®µè¯¦è§£**:

| å­—æ®µ          | ç±»å‹      | è¯´æ˜             | å¯é€‰å€¼                                                      | çŠ¶æ€è½¬æ¢                         |
| ------------- | --------- | ---------------- | ----------------------------------------------------------- | -------------------------------- |
| `status`      | `Literal` | å½“å‰æ­¥éª¤çŠ¶æ€     | `pending`, `in_progress`, `completed`, `reviewed`, `failed` | Developer â†’ Reviewer â†’ Next Step |
| `retry_count` | `int`     | å½“å‰æ­¥éª¤é‡è¯•æ¬¡æ•° | `0`, `1`, `2`, ...                                          | éªŒè¯å¤±è´¥æ—¶ +1                    |

**çŠ¶æ€æœºè½¬æ¢**:

```
pending
   â”‚
   â†“ (Developer å¼€å§‹)
in_progress
   â”‚
   â”œâ”€â†’ completed (Developer æˆåŠŸ) â”€â”€â”€â”€â†’ reviewed (Reviewer é€šè¿‡) â”€â”€â†’ Next Step
   â”‚                                         â†“ (Reviewer å¤±è´¥)
   â””â”€â†’ failed â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ pending (retry_count++)
```

**é‡è¯•é€»è¾‘**:

```python
# routing_logic.py:120-130 (ç®€åŒ–ç‰ˆ)
def should_retry(state: CrawlerDevState) -> bool:
    max_retries = 3
    if state["status"] == "failed" and state["retry_count"] < max_retries:
        return True
    elif state["retry_count"] >= max_retries:
        raise Exception(f"Step {state['current_step']} å¤±è´¥æ¬¡æ•°è¿‡å¤š")
    return False
```

**æ½œåœ¨é—®é¢˜**:

- **æ— é™é‡è¯•**: å¦‚æœ Reviewer é€»è¾‘æœ‰è¯¯ï¼Œå¯èƒ½å¯¼è‡´æ­»å¾ªç¯
  - **è§£å†³**: `max_retries = 3` ç¡¬ç¼–ç é™åˆ¶
- **çŠ¶æ€ä¸ä¸€è‡´**: å¦‚æœ Developer æŠ›å¼‚å¸¸ä½†æœªæ›´æ–°çŠ¶æ€
  - **è§£å†³**: LangGraph çš„å¼‚å¸¸å¤„ç†æœºåˆ¶ä¼šæ•è·å¹¶æ ‡è®°ä¸º `failed`

#### 2.2.4 æ­¥éª¤ç»“æœ (3 å­—æ®µ)

```python
# ============= æ­¥éª¤ç»“æœ =============
result: Optional[str]                # å½“å‰æ­¥éª¤çš„ LLM å“åº”æ–‡æœ¬
validation_result: Optional[Dict[str, Any]]  # Reviewer çš„éªŒè¯ç»“æœ
error: Optional[str]                 # é”™è¯¯ä¿¡æ¯
```

**å­—æ®µè¯¦è§£**:

| å­—æ®µ                | ç±»å‹             | è¯´æ˜              | ç¤ºä¾‹                                  | ä½¿ç”¨åœºæ™¯           |
| ------------------- | ---------------- | ----------------- | ------------------------------------- | ------------------ |
| `result`            | `Optional[str]`  | LLM çš„å®Œæ•´å“åº”    | `"å·²å®Œæˆé¡µé¢åˆ†æ..."`                 | Developer èŠ‚ç‚¹è¾“å‡º |
| `validation_result` | `Optional[Dict]` | Reviewer éªŒè¯ç»“æœ | `{"valid": True, "message": "..."}`   | Reviewer èŠ‚ç‚¹è¾“å‡º  |
| `error`             | `Optional[str]`  | é”™è¯¯è¯¦æƒ…          | `"å¯¼å…¥æ¨¡å—å¤±è´¥: ModuleNotFoundError"` | å¼‚å¸¸å¤„ç†           |

**ç¤ºä¾‹**:

```python
# Step 1 Developer å®Œæˆå
state["result"] = """
å·²å®Œæˆé¡µé¢åˆ†æ:
- é¡µé¢å¤æ‚åº¦: medium
- æ¨èå¼•æ“: browser_pool
- å¹¶å‘é…ç½®: pool_size=3, tab_size=5
"""

# Step 1 Reviewer å®Œæˆå
state["validation_result"] = {
    "step": "step1",
    "success": True,
    "message": "é¡µé¢åˆ†æå®Œæˆ,ä»£ç å¯æ­£å¸¸æ‰§è¡Œ"
}

# å¦‚æœå¤±è´¥
state["error"] = "ImportError: No module named 'crawler.product.extractor_gnc'"
state["status"] = "failed"
```

#### 2.2.5 URL æ¨¡å¼å¤„ç† (6 å­—æ®µ)

è¿™æ˜¯ Step 4-6 å¾ªç¯å¤„ç† URL patterns çš„æ ¸å¿ƒå­—æ®µã€‚

```python
# ============= URL æ¨¡å¼å¤„ç† (Step 4-7 ä¸“ç”¨) =============
current_url_pattern: Optional[str]   # å½“å‰æ­£åœ¨å¤„ç†çš„ URL pattern
current_sample_url: Optional[str]    # å½“å‰ URL pattern çš„æ ·ä¾‹ URL
current_sample_url_md5: str          # æ ·ä¾‹ URL çš„ MD5 (ç”¨äºç¼“å­˜æ–‡ä»¶å)
completed_patterns: List[str]        # å·²å®Œæˆçš„ URL patterns åˆ—è¡¨

# LangGraph å¾ªç¯æ§åˆ¶
patterns_queue: List[Dict[str, Any]]  # å¾…å¤„ç†çš„ patterns é˜Ÿåˆ—
current_pattern_info: Optional[Dict[str, Any]]  # å½“å‰ pattern ä¿¡æ¯
```

**å­—æ®µè¯¦è§£**:

| å­—æ®µ                     | ç±»å‹             | è¯´æ˜                        | ç¤ºä¾‹                                                         | ä½•æ—¶ä½¿ç”¨            |
| ------------------------ | ---------------- | --------------------------- | ------------------------------------------------------------ | ------------------- |
| `current_url_pattern`    | `Optional[str]`  | å½“å‰å¤„ç†çš„æ­£åˆ™è¡¨è¾¾å¼        | `r'https://www\.gnc\.com/[\w-]+/\d+\.html'`                  | Step 4.1, 5.x       |
| `current_sample_url`     | `Optional[str]`  | ç”¨äºæµ‹è¯• pattern çš„çœŸå® URL | `https://www.gnc.com/vitamins/123.html`                      | Step 5.x (è®¿é—®é¡µé¢) |
| `current_sample_url_md5` | `str`            | MD5 å“ˆå¸Œå€¼                  | `a1b2c3d4...`                                                | ä¸´æ—¶æ–‡ä»¶å‘½å        |
| `completed_patterns`     | `List[str]`      | å·²å¤„ç†å®Œçš„ patterns         | `["pattern1", "pattern2"]`                                   | åˆ¤æ–­æ˜¯å¦ç»§ç»­å¾ªç¯    |
| `patterns_queue`         | `List[Dict]`     | å¾…å¤„ç†é˜Ÿåˆ—                  | `[{"pattern": "...", "type": "detail", "sample_url": "..."}]` | Step 4-6 å¾ªç¯       |
| `current_pattern_info`   | `Optional[Dict]` | å½“å‰ pattern çš„å®Œæ•´ä¿¡æ¯     | `{"pattern": "...", "type": "detail", ...}`                  | Step 5.x å„å­æ­¥éª¤   |

**å·¥ä½œæµç¨‹ç¤ºä¾‹**:

```python
# Step 4: åˆå§‹åŒ–é˜Ÿåˆ—
state["patterns_queue"] = [
    {"pattern": "r'https://www\.gnc\.com/[\w-]+/\d+\.html'", "type": "detail", "sample_url": "https://www.gnc.com/vitamins/123.html"},
    {"pattern": "r'https://www\.gnc\.com/category/[\w-]+'", "type": "list", "sample_url": "https://www.gnc.com/category/vitamins"}
]
state["completed_patterns"] = []

# Step 4.1: å–å‡ºç¬¬ä¸€ä¸ª pattern
state["current_pattern_info"] = state["patterns_queue"].pop(0)
state["current_url_pattern"] = state["current_pattern_info"]["pattern"]
state["current_sample_url"] = state["current_pattern_info"]["sample_url"]
state["current_sample_url_md5"] = hashlib.md5(state["current_sample_url"].encode()).hexdigest()

# Step 5.1-5.6: å¤„ç†å½“å‰ pattern (ç”Ÿæˆæå–å™¨çš„å„ä¸ªæ–¹æ³•)
# ...

# Step 6: å°† pattern æ ‡è®°ä¸ºå®Œæˆ
state["completed_patterns"].append(state["current_url_pattern"])

# è·¯ç”±åˆ¤æ–­: å¦‚æœé˜Ÿåˆ—éç©ºï¼Œè¿”å› Step 4.1 (ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ª pattern)
if state["patterns_queue"]:
    return "step4_1__next_pattern"
else:
    return "step7__site_tree_expand"
```

**æ½œåœ¨é—®é¢˜**:

- **é˜Ÿåˆ—ä¸ºç©ºä½†æœªå¤„ç†**: å¦‚æœ URL patterns ä¸ºç©ºï¼Œä¼šè·³è¿‡ Step 5-6
  - **è§£å†³**: Step 3 Reviewer éªŒè¯ `url_detail_patterns` éç©º
- **MD5 å†²çª**: ç†è®ºä¸Šå¯èƒ½ï¼ˆæ¦‚ç‡æä½ï¼‰
  - **è§£å†³**: å®é™…ä¸­æœªé‡åˆ°ï¼Œæš‚ä¸å¤„ç†

#### 2.2.6 ç¼“å­˜æ•°æ® (1 å­—æ®µ)

```python
# ============= ç¼“å­˜æ•°æ® =============
cache_data: Dict[str, Any]           # ç¼“å­˜æ•°æ® (urls ç­‰)
```

**ç”¨é€”**: å­˜å‚¨ `extract_deals_from_mainpage` è¿”å›çš„ URLs åˆ—è¡¨ï¼Œé¿å…é‡å¤æ‰§è¡Œã€‚

**ç¤ºä¾‹**:

```python
# Step 2 å®Œæˆå
state["cache_data"] = {
    "site_tree": [
        {"title": "Product 1", "url": "https://...", "type": "detail"},
        {"title": "Category", "url": "https://...", "type": "list"},
        ...
    ]
}
```

#### 2.2.7 æ§åˆ¶å­—æ®µ (8 å­—æ®µ)

```python
# ============= Step 5 æ§åˆ¶ =============
last_file_id: Optional[str]         # å½“å‰ä¿å­˜æ–‡ä»¶çš„idæ ‡è¯†

# ============= Step 7 å¾ªç¯æ§åˆ¶ =============
has_new_patterns_in_step7: bool      # Step 7 æ˜¯å¦æ£€æµ‹åˆ°æ–° patterns
step7_loop_count: int                # Step 7 å¾ªç¯è®¡æ•°å™¨

# ============= è¾“å‡ºè·¯å¾„ =============
base_file_path: str                  # ç”Ÿæˆçš„çˆ¬è™«æ–‡ä»¶è·¯å¾„ (extractor_xxx.py)

# ============= Session ç®¡ç† =============
session_id: Optional[str]            # Claude SDK session_id

# ============= æµç¨‹æ§åˆ¶ =============
next_action: Optional[str]           # ä¸‹ä¸€æ­¥åŠ¨ä½œ
regenerate_step: Optional[str]       # éœ€è¦é‡æ–°ç”Ÿæˆçš„æ­¥éª¤
regenerate_from: Optional[str]       # step22__fix_code ä¿®å¤åè·³å›çš„æ­¥éª¤
```

**å­—æ®µè¯¦è§£**:

| å­—æ®µ                        | ç±»å‹            | è¯´æ˜                       | ç¤ºä¾‹                        | ä½•æ—¶ä½¿ç”¨                |
| --------------------------- | --------------- | -------------------------- | --------------------------- | ----------------------- |
| `last_file_id`              | `Optional[str]` | å½“å‰æ–‡ä»¶æ ‡è¯†               | `"detail_page_v2"`          | Step 5.x å¤šæ–‡ä»¶ç®¡ç†     |
| `has_new_patterns_in_step7` | `bool`          | Step 7 æ˜¯å¦å‘ç°æ–° patterns | `True`/`False`              | Step 7 å†³å®šæ˜¯å¦ç»§ç»­æ‰©å±• |
| `step7_loop_count`          | `int`           | Step 7 å¾ªç¯æ¬¡æ•°            | `0`, `1`, `2`               | é˜²æ­¢æ— é™å¾ªç¯ (max=3)    |
| `base_file_path`            | `str`           | çˆ¬è™«ä»£ç æ–‡ä»¶è·¯å¾„           | `/path/to/extractor_gnc.py` | æ‰€æœ‰ä»£ç ç”Ÿæˆæ­¥éª¤        |
| `session_id`                | `Optional[str]` | Claude ä¼šè¯ ID             | `"session_abc123"`          | ä¿æŒå¯¹è¯ä¸Šä¸‹æ–‡          |
| `next_action`               | `Optional[str]` | ä¸‹ä¸€æ­¥åŠ¨ä½œ                 | `"continue"`, `"retry"`     | ç”¨æˆ·äº¤äº’                |
| `regenerate_step`           | `Optional[str]` | éœ€é‡åšçš„æ­¥éª¤               | `"step5"`                   | ç”¨æˆ·æ‰‹åŠ¨è§¦å‘            |
| `regenerate_from`           | `Optional[str]` | ä¿®å¤åçš„è¿”å›ç‚¹             | `"step9"`                   | Step 22 ä¿®å¤ä»£ç åè·³è½¬  |

**Step 7 å¾ªç¯æ§åˆ¶ç¤ºä¾‹**:

```python
# Step 7 ç¬¬ä¸€æ¬¡æ‰§è¡Œ
state["step7_loop_count"] = 0
state["has_new_patterns_in_step7"] = False

# å¦‚æœå‘ç°æ–° patterns
if new_patterns_found:
    state["has_new_patterns_in_step7"] = True
    state["step7_loop_count"] += 1

    # è·¯ç”±åˆ¤æ–­
    if state["step7_loop_count"] < 3:
        return "step3__generate_url_patterns"  # é‡æ–°ç”Ÿæˆ patterns
    else:
        logger.warning("Step 7 å¾ªç¯æ¬¡æ•°è¾¾åˆ°ä¸Šé™,åœæ­¢æ‰©å±•")
        return "step8__analyze_markdown_info"
```

### 2.3 State çš„ç”Ÿå‘½å‘¨æœŸ

```
1. åˆå§‹åŒ– (crawler_devbot.py:155-180)
   â”œâ”€ ä»å‘½ä»¤è¡Œå‚æ•°æå–: url, site_name, category
   â”œâ”€ è®¾ç½®é»˜è®¤å€¼: status="pending", retry_count=0
   â””â”€ è®¡ç®—è·¯å¾„: base_file_path

2. Developer èŠ‚ç‚¹æ›´æ–° (developer_nodes.py:å„æ­¥éª¤)
   â”œâ”€ æ›´æ–° current_step, current_step_name
   â”œâ”€ æ›´æ–° status="completed"
   â”œâ”€ ä¿å­˜ result (LLM å“åº”)
   â””â”€ è¿”å›æ›´æ–°åçš„ state

3. Reviewer èŠ‚ç‚¹éªŒè¯ (reviewer_nodes.py)
   â”œâ”€ è¯»å– current_step å†³å®šéªŒè¯é€»è¾‘
   â”œâ”€ æ‰§è¡ŒéªŒè¯ (å¯¼å…¥æ¨¡å—ã€è°ƒç”¨å‡½æ•°ç­‰)
   â”œâ”€ æ›´æ–° status="reviewed" æˆ– "failed"
   â”œâ”€ ä¿å­˜ validation_result æˆ– error
   â””â”€ æ›´æ–° retry_count (å¤±è´¥æ—¶ +1)

4. è·¯ç”±å†³ç­– (routing_logic.py)
   â”œâ”€ è¯»å– status, current_step, retry_count
   â”œâ”€ åˆ¤æ–­ä¸‹ä¸€æ­¥: next_developer, retry, end
   â””â”€ è¿”å›èŠ‚ç‚¹åç§°

5. æŒä¹…åŒ– (MemorySaver + JSON)
   â”œâ”€ LangGraph MemorySaver: è‡ªåŠ¨ä¿å­˜åˆ° local_state_<site>.json
   â””â”€ ConversationStore: å¯¹è¯å†å²å­˜å…¥ SQLite
```

---

## 3. Tool å·¥å…·ç³»ç»Ÿæ·±åº¦è§£æ

### 3.1 å·¥å…·åˆ†ç±»

DevBot ä½¿ç”¨çš„å·¥å…·åˆ†ä¸ºä¸‰å¤§ç±»ï¼š

```
Tools
â”œâ”€â”€ MCP å·¥å…· (Model Context Protocol)
â”‚   â”œâ”€â”€ chrome-devtools (ç½‘é¡µæ“ä½œ)
â”‚   â”œâ”€â”€ gemini-cli (LLM è°ƒç”¨)
â”‚   â”œâ”€â”€ playwright (å¤‡ç”¨æµè§ˆå™¨å·¥å…·)
â”‚   â”œâ”€â”€ mongodb (æ•°æ®åº“)
â”‚   â”œâ”€â”€ notion (ç¬”è®°)
â”‚   â””â”€â”€ git (ç‰ˆæœ¬æ§åˆ¶)
â”‚
â”œâ”€â”€ Claude SDK å†…ç½®å·¥å…·
â”‚   â”œâ”€â”€ Read (è¯»å–æ–‡ä»¶)
â”‚   â”œâ”€â”€ Write (å†™å…¥æ–‡ä»¶)
â”‚   â”œâ”€â”€ Edit (ç¼–è¾‘æ–‡ä»¶)
â”‚   â”œâ”€â”€ Bash (æ‰§è¡Œå‘½ä»¤)
â”‚   â”œâ”€â”€ Glob (æŸ¥æ‰¾æ–‡ä»¶)
â”‚   â””â”€â”€ Grep (æœç´¢å†…å®¹)
â”‚
â””â”€â”€ è‡ªå®šä¹‰ Python å·¥å…· (devbot/tool.py)
    â”œâ”€â”€ compress_image (å›¾ç‰‡å‹ç¼©)
    â”œâ”€â”€ save_tmp_page_by_brightdata (BrightData è·å–é¡µé¢)
    â”œâ”€â”€ classify_urls (URL åˆ†ç±»)
    â””â”€â”€ check_url_patterns (æ£€æŸ¥ URL patterns)
```

### 3.2 MCP (Model Context Protocol) è¯¦è§£

**MCP** æ˜¯ Anthropic æå‡ºçš„ä¸€ç§å·¥å…·åè®®ï¼Œå…è®¸ LLM é€šè¿‡æ ‡å‡†åŒ–æ¥å£è°ƒç”¨å¤–éƒ¨å·¥å…·ã€‚

#### 3.2.1 chrome-devtools MCP

**ä½œç”¨**: æ§åˆ¶ Chrome æµè§ˆå™¨è¿›è¡Œç½‘é¡µæ“ä½œã€‚

**æ ¸å¿ƒåŠŸèƒ½**:

| å‡½æ•°                    | è¯´æ˜               | å‚æ•°                                              | è¿”å›å€¼    | ä½¿ç”¨åœºæ™¯        |
| ----------------------- | ------------------ | ------------------------------------------------- | --------- | --------------- |
| `new_page`              | æ‰“å¼€æ–°é¡µé¢         | `url: str`                                        | é¡µé¢ç´¢å¼•  | Step 1 æ‰“å¼€ç½‘ç«™ |
| `navigate_page`         | å¯¼èˆªåˆ° URL         | `url: str, type: "url"/"back"/"forward"/"reload"` | æ—         | é¡µé¢è·³è½¬        |
| `take_screenshot`       | æˆªå›¾               | `fullPage: bool, format: str, filePath: str`      | å›¾ç‰‡è·¯å¾„  | Step 1 é¡µé¢æˆªå›¾ |
| `take_snapshot`         | é¡µé¢å¿«ç…§ (a11y æ ‘) | `filePath: str`                                   | å¿«ç…§æ–‡æœ¬  | è·å–é¡µé¢ç»“æ„    |
| `click`                 | ç‚¹å‡»å…ƒç´            | `uid: str`                                        | æ—         | äº¤äº’æ“ä½œ        |
| `fill`                  | å¡«å†™è¡¨å•           | `uid: str, value: str`                            | æ—         | å¡«å†™è¾“å…¥æ¡†      |
| `evaluate_script`       | æ‰§è¡Œ JS            | `function: str, args: List`                       | JS è¿”å›å€¼ | æå–æ•°æ®        |
| `list_network_requests` | ç½‘ç»œè¯·æ±‚åˆ—è¡¨       | `pageIdx: int`                                    | è¯·æ±‚æ•°ç»„  | åˆ†æ AJAX       |

**ç¤ºä¾‹**:

```python
# Step 1 prompt ä¸­çš„ä½¿ç”¨
```

ç”¨ chrome-devtools æ‰“å¼€ç½‘ç«™: {{ url }}

1. ä½¿ç”¨ new_page æ‰“å¼€
2. ä½¿ç”¨ take_screenshot æˆªé•¿å›¾ (fullPage=true, format="jpeg")
3. ä½¿ç”¨ list_network_requests æŸ¥çœ‹ AJAX è¯·æ±‚

**æ½œåœ¨é—®é¢˜**:

- **æˆªå›¾è¿‡å¤§**: æŸäº›ç½‘ç«™æˆªå›¾è¶…è¿‡ 8000pxï¼ŒClaude API æ‹’ç»
  - **è§£å†³**: Step 1 prompt ä¸­è¦æ±‚ä½¿ç”¨ `compress_image` å·¥å…·
- **åçˆ¬é˜»æ­¢**: chrome-devtools ç›´æ¥è®¿é—®è¢« Cloudflare æ‹¦æˆª
  - **è§£å†³**: å…ˆç”¨ `save_tmp_page_by_brightdata` è·å– HTMLï¼Œå†é€šè¿‡ `file://` è®¿é—®

#### 3.2.2 gemini-cli MCP

**ä½œç”¨**: è°ƒç”¨ Google Gemini APIï¼ˆå¤‡ç”¨ LLMï¼‰ã€‚

**ä½¿ç”¨åœºæ™¯**: å½“éœ€è¦å¤§æ‰¹é‡å¹¶å‘è°ƒç”¨æ—¶ï¼ŒGemini æˆæœ¬æ›´ä½ã€‚

### 3.3 Claude SDK å†…ç½®å·¥å…·

Claude Agent SDK æä¾›äº†ä¸€å¥—æ–‡ä»¶å’Œç³»ç»Ÿæ“ä½œå·¥å…·ã€‚

#### 3.3.1 æ–‡ä»¶æ“ä½œå·¥å…·

| å·¥å…·     | è¯´æ˜     | å‚æ•°                                               | ä½¿ç”¨åœºæ™¯           |
| -------- | -------- | -------------------------------------------------- | ------------------ |
| `Read`   | è¯»å–æ–‡ä»¶ | `file_path: str`                                   | æŸ¥çœ‹ä»£ç ã€é…ç½®æ–‡ä»¶ |
| `Write`  | å†™å…¥æ–‡ä»¶ | `file_path: str, content: str`                     | åˆ›å»ºæ–°æ–‡ä»¶         |
| `Edit`   | ç¼–è¾‘æ–‡ä»¶ | `file_path: str, old_string: str, new_string: str` | ä¿®æ”¹ä»£ç            |
| `Move`   | ç§»åŠ¨æ–‡ä»¶ | `source: str, destination: str`                    | é‡å‘½å/ç§»åŠ¨        |
| `Delete` | åˆ é™¤æ–‡ä»¶ | `file_path: str`                                   | æ¸…ç†ä¸´æ—¶æ–‡ä»¶       |

**è‡ªåŠ¨æ‰¹å‡†æœºåˆ¶**:

DevBot ä¸­æ‰€æœ‰è¿™äº›å·¥å…·éƒ½é…ç½®äº†è‡ªåŠ¨æ‰¹å‡† hookï¼Œæ— éœ€æ‰‹åŠ¨ç¡®è®¤ã€‚

```python
# claude_agent_base.py:116-137
COMMANDS = [
    "Read", "Write", "Edit", "TodoWrite", "Move", "Delete",
    "ListDir", "MakeDir", "Grep", "Glob", "Search",
    "GetDefinition", "GetReferences", "Bash",
]

# ä¸ºæ¯ä¸ªå·¥å…·é…ç½® auto_approve hook
hooks = {
    "PreToolUse": [
        HookMatcher(matcher="Bash", hooks=[auto_approve]),
        HookMatcher(matcher="Read", hooks=[auto_approve]),
        HookMatcher(matcher="Edit", hooks=[auto_approve]),
        # ... å…¶ä»–å·¥å…·
    ]
}
```

#### 3.3.2 Bash å·¥å…·

**ç‰¹æ®Šæ€§**: Bash å·¥å…·å¯ä»¥æ‰§è¡Œä»»æ„ shell å‘½ä»¤ï¼Œéœ€è¦è°¨æ…ä½¿ç”¨ã€‚

**è‡ªåŠ¨æ‰¹å‡†ç­–ç•¥**:

```python
# claude_agent_base.py:53-77
async def auto_approve(input_data, tool_use_id, context):
    """è‡ªåŠ¨æ‰¹å‡† Bash/Edit/Read ç­‰åŸºç¡€å·¥å…·"""
    tool_name = input_data.get('tool_name', 'unknown')
    tool_input = input_data.get('tool_input', {})

    if tool_name == 'Bash':
        logger.debug(f"ğŸ”§ Hook è¢«è§¦å‘: {tool_name}ï¼Œç›®æ ‡: {tool_input.pop('description', '')}")
        logger.debug(f"ğŸ“‹ å‘½ä»¤: {tool_input.get('command')}")

    return {
        "hookSpecificOutput": {
            "hookEventName": "PreToolUse",
            "permissionDecision": "allow",  # è‡ªåŠ¨æ‰¹å‡†
        }
    }
```

**å®‰å…¨è€ƒè™‘**:

- DevBot è¿è¡Œåœ¨å—æ§ç¯å¢ƒï¼ˆæœ¬åœ°å¼€å‘æœºå™¨ï¼‰
- ä»…ç”¨äºæµ‹è¯•çˆ¬è™«ä»£ç ï¼Œä¸æ¶‰åŠæ•æ„Ÿæ“ä½œ
- æ‰€æœ‰å‘½ä»¤éƒ½æœ‰æ—¥å¿—è®°å½•

### 3.4 è‡ªå®šä¹‰ Python å·¥å…· (devbot/tool.py)

#### 3.4.1 compress_image (å›¾ç‰‡å‹ç¼©å·¥å…·)

**ä½œç”¨**: å‹ç¼©å›¾ç‰‡åˆ° Claude API å¯æ¥å—çš„å¤§å°å’Œå°ºå¯¸ã€‚

**Claude API é™åˆ¶**:

- **æ–‡ä»¶å¤§å°**: 5 MB (base64 ç¼–ç å)
- **åƒç´ å°ºå¯¸**: å•è¾¹ä¸è¶…è¿‡ 8000px

**å®ç°åŸç†**:

```python
# tool.py:compress_image ä¼ªä»£ç 
async def compress_image(file_path: str, output_path: str = None):
    img = Image.open(file_path)
    width, height = img.size

    # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é™åˆ¶
    if max(width, height) > 8000:
        # åˆ‡åˆ†å›¾ç‰‡
        num_slices = math.ceil(height / 8000)
        slices = []
        for i in range(num_slices):
            slice_img = img.crop((0, i*8000, width, min((i+1)*8000, height)))
            slice_path = output_path.replace('.webp', f'_{i}.webp')
            slice_img.save(slice_path, format='WEBP', quality=85)
            slices.append(slice_path)
        return slices

    # æ£€æŸ¥æ–‡ä»¶å¤§å°
    if os.path.getsize(file_path) > 2 * 1024 * 1024:  # 2 MB
        # å‹ç¼©ä¸º WebP
        img.save(output_path, format='WEBP', quality=85, optimize=True)

    return [output_path]
```

**ä½¿ç”¨åœºæ™¯**:

```python
# Step 1 prompt ä¸­è¦æ±‚
# å¦‚æœå›¾ç‰‡è¶…è¿‡äº† 2 MB æˆ–å•è¾¹åƒç´ è¶…è¿‡äº†8000ï¼Œæ‰§è¡Œä»¥ä¸‹å‘½ä»¤å‹ç¼©å›¾ç‰‡:
python -m devbot.tool compress_image {{ output_part_dir }}/mainpage_long_screenshot.jpg {{ output_part_dir }}/mainpage_long_screenshot.webp
```

**æ½œåœ¨é—®é¢˜**:

- **DecompressionBombError**: å›¾ç‰‡è¿‡å¤§ï¼ˆè¶…è¿‡ 1 äº¿åƒç´ ï¼‰
  - **è§£å†³**: Step 1 prompt è¦æ±‚é‡è¯•æµç¨‹ï¼ˆå…³é—­é¡µé¢ â†’ ç­‰å¾… â†’ é‡æ–°æˆªå›¾ï¼Œæœ€å¤š 5 æ¬¡ï¼‰

#### 3.4.2 save_tmp_page_by_brightdata (åçˆ¬å¤„ç†)

**ä½œç”¨**: ä½¿ç”¨ BrightData ä»£ç†è·å–é¡µé¢ HTMLï¼Œç»•è¿‡åçˆ¬é™åˆ¶ã€‚

**å®ç°**:

```python
# tool.py:save_tmp_page_by_brightdata
async def save_tmp_page_by_brightdata(url: str) -> str:
    """
    ä½¿ç”¨ BrightData è·å–é¡µé¢ HTML å¹¶ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶

    Returns:
        str: ä¸´æ—¶ HTML æ–‡ä»¶è·¯å¾„
    """
    bd_client = BDClient()
    html = await bd_client.get_page_html(url)

    tmp_file = f"/tmp/scraper/brightdata_{hashlib.md5(url.encode()).hexdigest()}.html"
    with open(tmp_file, 'w', encoding='utf-8') as f:
        f.write(html)

    logger.info(f"âœ… BrightData é¡µé¢å·²ä¿å­˜: {tmp_file}")
    return tmp_file
```

**ä½¿ç”¨åœºæ™¯**:

```python
# Step 1 prompt ä¸­çš„åçˆ¬å¤„ç†
# å¦‚æœåçˆ¬å¯¼è‡´ chrome-devtools æ— æ³•ç›´æ¥è®¿é—®é¡µé¢ï¼Œåˆ™å…ˆä½¿ç”¨ brightdata ä¿å­˜ä¸´æ—¶é¡µé¢ï¼Œ
# å†ç”¨ chrome-devtools é€šè¿‡æ–‡ä»¶åœ°å€è®¿é—®è¿™ä¸ªä¸´æ—¶é¡µé¢ã€‚
python -m devbot.tool save_tmp_page_by_brightdata <url>
```

#### 3.4.3 classify_urls / check_url_patterns

**ä½œç”¨**: è¾…åŠ© LLM è¿›è¡Œ URL åˆ†ç±»å’Œ pattern éªŒè¯ã€‚

**å®ç°**:

```python
# tool.py:classify_urls
async def classify_urls(category: str, site_name: str):
    """
    è¯»å– site_tree.jsonï¼Œç”¨ LLM åˆ†ç±»æ‰€æœ‰ URLs

    Returns:
        dict: {'detail': [...], 'list': [...], 'other': [...]}
    """
    site_tree_file = f"crawler/{category}/output/{site_name}/site_tree.json"
    with open(site_tree_file, 'r') as f:
        data = json.load(f)

    urls = data['site_tree']

    # è°ƒç”¨ Gemini API (æˆæœ¬æ›´ä½)
    result = await call_gemini_classify(urls)
    return result
```

---

## 4. Claude Agent SDK é›†æˆè¯¦è§£

### 4.1 ClaudeSDKClient æ¶æ„

**ClaudeSDKClient** æ˜¯ Anthropic å®˜æ–¹æä¾›çš„ Python SDKï¼Œç”¨äºä¸ Claude API äº¤äº’ã€‚

#### 4.1.1 å…¨å±€å®¢æˆ·ç«¯ç®¡ç†å™¨

DevBot ä½¿ç”¨å…¨å±€å­—å…¸ç®¡ç†å¤šä¸ª subagent çš„å®¢æˆ·ç«¯ï¼š

```python
# claude_agent_base.py:49-50
_global_clients = {}  # {subagent_name: ClaudeSDKClient}
```

**ä¼˜åŠ¿**:

- **Session å¤ç”¨**: åŒä¸€ subagent çš„å¤šæ¬¡è°ƒç”¨å…±äº«ä¸Šä¸‹æ–‡
- **èµ„æºä¼˜åŒ–**: é¿å…é‡å¤åˆ›å»ºå®¢æˆ·ç«¯

#### 4.1.2 get_or_create_client å‡½æ•°

**ä½œç”¨**: è·å–æˆ–åˆ›å»º subagent çš„å®¢æˆ·ç«¯ã€‚

**å®ç°** (ç®€åŒ–ç‰ˆ):

```python
# claude_agent_base.py (ä¼ªä»£ç )
async def get_or_create_client(subagent_name: str, model_name: str = None):
    """è·å–æˆ–åˆ›å»º subagent å®¢æˆ·ç«¯"""

    # æ£€æŸ¥ç¼“å­˜
    if subagent_name in _global_clients:
        logger.debug(f"â™»ï¸ å¤ç”¨å·²æœ‰å®¢æˆ·ç«¯: {subagent_name}")
        return _global_clients[subagent_name]

    # è¯»å– agent å®šä¹‰æ–‡ä»¶
    agent_file = PROJ_PATH / '.claude' / 'agents' / f'{subagent_name}.md'

    if not agent_file.exists():
        raise FileNotFoundError(f"Agent å®šä¹‰æ–‡ä»¶ä¸å­˜åœ¨: {agent_file}")

    # è§£æ frontmatter
    with open(agent_file, 'r') as f:
        content = f.read()

    # æå– YAML frontmatter
    match = re.match(r'^---\n(.*?)\n---\n(.*)', content, re.DOTALL)
    frontmatter_yaml = match.group(1)
    prompt = match.group(2)

    agent_config = yaml.safe_load(frontmatter_yaml)

    # åˆ›å»º AgentDefinition
    agent_def = AgentDefinition(
        description=agent_config.get('description'),
        prompt=prompt,
        tools=agent_config.get('tools')  # ['Read', 'Write', 'Bash', ...]
    )

    # åˆ›å»º ClaudeSDKClient
    client = ClaudeSDKClient(
        agent_definition=agent_def,
        options=get_claude_options(
            model=model_name,
            sys_prompt=prompt,
            allowed_tools_add=agent_config.get('tools')
        )
    )

    # ç¼“å­˜å®¢æˆ·ç«¯
    _global_clients[subagent_name] = client
    logger.info(f"âœ… åˆ›å»ºæ–°å®¢æˆ·ç«¯: {subagent_name}")

    return client
```

### 4.2 Agent Definition (ä»£ç†å®šä¹‰)

#### 4.2.1 Agent å®šä¹‰æ–‡ä»¶æ ¼å¼

Agent å®šä¹‰å­˜å‚¨åœ¨ `.claude/agents/<name>.md`ï¼Œä½¿ç”¨ YAML frontmatter + Markdown æ ¼å¼ã€‚

**ç¤ºä¾‹**: `.claude/agents/crawler-developer.md`

```markdown
---
name: crawler-developer
description: çˆ¬è™«å¼€å‘ä¸“å®¶ï¼Œä¸“æ³¨äºä½¿ç”¨ Playwright å’Œ Chrome DevTools å¼€å‘é«˜è´¨é‡çš„ç½‘é¡µçˆ¬è™«
tools:
  - Read
  - Write
  - Edit
  - Bash
  - Glob
  - Grep
  - mcp__chrome-devtools
  - mcp__gemini-cli
---

# çˆ¬è™«å¼€å‘ä¸“å®¶

ä½ æ˜¯èµ„æ·±ç½‘é¡µçˆ¬è™«å·¥ç¨‹å¸ˆï¼Œç²¾é€š Pythonã€Playwrightã€BeautifulSoupã€æ­£åˆ™è¡¨è¾¾å¼ã€‚

## æ ¸å¿ƒèƒ½åŠ›
- ä½¿ç”¨ chrome-devtools åˆ†æé¡µé¢ç»“æ„
- ç¼–å†™å¥å£®çš„æå–é€»è¾‘
- å¤„ç†åŠ¨æ€åŠ è½½å†…å®¹
- ç»•è¿‡åçˆ¬è™«æœºåˆ¶

## å·¥ä½œæµç¨‹
1. åˆ†æé¡µé¢ç»“æ„ (chrome-devtools)
2. è®¾è®¡æå–ç­–ç•¥
3. ç¼–å†™ä»£ç å®ç°
4. æµ‹è¯•å¹¶ä¼˜åŒ–

## ç¼–ç è§„èŒƒ
- éµå¾ª PEP 8
- æ·»åŠ è¯¦ç»†æ³¨é‡Š
- é”™è¯¯å¤„ç†å®Œå–„
- æ€§èƒ½ä¼˜åŒ–ä¼˜å…ˆ
```

#### 4.2.2 Agent Tools é…ç½®

**tools** å­—æ®µæŒ‡å®š agent å¯ä»¥ä½¿ç”¨çš„å·¥å…·åˆ—è¡¨ã€‚

**ç¤ºä¾‹å¯¹æ¯”**:

```yaml
# crawler-developer (å¼€å‘ä¸“å®¶)
tools:
  - Read
  - Write
  - Edit
  - Bash
  - mcp__chrome-devtools  # éœ€è¦è®¿é—®ç½‘é¡µ
  - mcp__gemini-cli       # éœ€è¦è°ƒç”¨ LLM

# project-reviewer (å®¡æŸ¥ä¸“å®¶)
tools:
  - Read   # åªéœ€è¯»å–ä»£ç 
  - Bash   # æ‰§è¡Œæµ‹è¯•å‘½ä»¤
```

**å·¥å…·æƒé™æ§åˆ¶**:

```python
# claude_agent_base.py:get_claude_options
def get_claude_options(allowed_tools_add=None):
    # åŸºç¡€å·¥å…· (æ‰€æœ‰ agent éƒ½æœ‰)
    base_tools = ["Read", "Bash"]

    # é¢å¤–å·¥å…· (agent å®šä¹‰ä¸­çš„ tools)
    all_tools = base_tools + (allowed_tools_add or [])

    # è‡ªåŠ¨æ‰¹å‡†è§„åˆ™
    hooks = {
        "PreToolUse": [
            HookMatcher(matcher=tool, hooks=[auto_approve])
            for tool in all_tools
        ]
    }

    return ClaudeAgentOptions(
        max_buffer_size=100 * 1024 * 1024,
        hooks=hooks
    )
```

### 4.3 Session Management (ä¼šè¯ç®¡ç†)

#### 4.3.1 Session ID çš„ä½œç”¨

**Session ID** ç”¨äºä¿æŒå¯¹è¯ä¸Šä¸‹æ–‡ï¼Œå®ç°å¤šè½®å¯¹è¯ã€‚

**å·¥ä½œåŸç†**:

```
Step 1: åˆ†æé¡µé¢ç»“æ„
    â†“
è°ƒç”¨ call_subagent('crawler-developer', prompt1)
    â† è¿”å› session_id_1
    â†“
Step 2: ç”Ÿæˆåˆ—è¡¨æå–å™¨
    â†“
è°ƒç”¨ call_subagent('crawler-developer', prompt2, session_id=session_id_1)
    â† å¤ç”¨ä¸Šä¸‹æ–‡ï¼ŒçŸ¥é“ Step 1 çš„åˆ†æç»“æœ
```

**ä»£ç ç¤ºä¾‹**:

```python
# developer_nodes.py:step1__analyze_page
async def step1__analyze_page(state: CrawlerDevState) -> CrawlerDevState:
    prompt = get_step_prompt('step1', state)

    # ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼Œæ—  session_id
    result = await call_subagent('crawler-developer', prompt, session_id=None)

    new_session_id = result.get("session_id")  # è·å–æ–° session_id

    return {
        **state,
        "session_id": new_session_id,  # ä¿å­˜åˆ° state
        ...
    }

# developer_nodes.py:step2__generate_list_extractor
async def step2__generate_list_extractor(state: CrawlerDevState) -> CrawlerDevState:
    prompt = get_step_prompt('step2', state)

    # å¤ç”¨ session_id
    result = await call_subagent('crawler-developer', prompt, session_id=state.get("session_id"))

    new_session_id = result.get("session_id")

    return {
        **state,
        "session_id": new_session_id,  # æ›´æ–° session_id
        ...
    }
```

#### 4.3.2 Session çš„ç”Ÿå‘½å‘¨æœŸ

```
åˆå§‹åŒ–
    â†“
Step 1: session_id = None â†’ åˆ›å»ºæ–° session â†’ session_abc123
    â†“
Step 2: session_id = session_abc123 â†’ å¤ç”¨ session â†’ session_abc123 (å¯èƒ½æ›´æ–°)
    â†“
Step 3: session_id = session_abc123 â†’ ç»§ç»­å¤ç”¨
    â†“
...
    â†“
Step 10: å®Œæˆ â†’ session è‡ªåŠ¨æ¸…ç†
```

**æ½œåœ¨é—®é¢˜**:

- **Session è¿‡æœŸ**: Claude SDK ä¼šè‡ªåŠ¨å¤„ç†ï¼Œæ— éœ€æ‹…å¿ƒ
- **ä¸Šä¸‹æ–‡è¿‡é•¿**: è¶…è¿‡ 200k tokens æ—¶ï¼Œæ—©æœŸå¯¹è¯å¯èƒ½è¢«æˆªæ–­
  - **è§£å†³**: DevBot çš„æ¯ä¸ªæ­¥éª¤ prompt éƒ½æ˜¯è‡ªåŒ…å«çš„ï¼Œä¸ä¾èµ–æ—©æœŸä¸Šä¸‹æ–‡

### 4.4 Hook Mechanism (é’©å­æœºåˆ¶)

#### 4.4.1 Hook ç±»å‹

Claude SDK æ”¯æŒä¸‰ç§ hook äº‹ä»¶ï¼š

| Hook äº‹ä»¶     | è§¦å‘æ—¶æœº   | ç”¨é€”               |
| ------------- | ---------- | ------------------ |
| `PreToolUse`  | å·¥å…·è°ƒç”¨å‰ | æƒé™æ§åˆ¶ã€å‚æ•°éªŒè¯ |
| `PostToolUse` | å·¥å…·è°ƒç”¨å | ç»“æœå¤„ç†ã€æ—¥å¿—è®°å½• |
| `PreMessage`  | å‘é€æ¶ˆæ¯å‰ | æ¶ˆæ¯æ‹¦æˆªã€å†…å®¹è¿‡æ»¤ |

**DevBot ä¸­ä»…ä½¿ç”¨ `PreToolUse`** (è‡ªåŠ¨æ‰¹å‡†)ã€‚

#### 4.4.2 Auto Approve å®ç°

**åŸç†**: æ‹¦æˆªå·¥å…·è°ƒç”¨è¯·æ±‚ï¼Œç›´æ¥è¿”å› `allow` å†³ç­–ã€‚

**ä»£ç **:

```python
# claude_agent_base.py:53-77
async def auto_approve(input_data, tool_use_id, context):
    """è‡ªåŠ¨æ‰¹å‡† Bash/Edit/Read ç­‰åŸºç¡€å·¥å…·"""
    tool_name = input_data.get('tool_name', 'unknown')
    tool_input = input_data.get('tool_input', {})

    # æ—¥å¿—è®°å½• (ç”¨äºè°ƒè¯•)
    if tool_name == 'Bash':
        logger.debug(f"ğŸ”§ Hook è¢«è§¦å‘: {tool_name}")
        logger.debug(f"ğŸ“‹ å‘½ä»¤: {tool_input.get('command')}")
    elif tool_name == 'Read':
        logger.debug(f"ğŸ“– Hook è¢«è§¦å‘: {tool_name}, è¯»å–æ–‡ä»¶: {tool_input.get('file_path')}")
    else:
        logger.debug(f"âœï¸ Hook è¢«è§¦å‘: {tool_name}")

    # è¿”å›æ‰¹å‡†å†³ç­–
    return {
        "hookSpecificOutput": {
            "hookEventName": "PreToolUse",
            "permissionDecision": "allow",  # âœ… è‡ªåŠ¨æ‰¹å‡†
        }
    }
```

**Hook æ³¨å†Œ**:

```python
# claude_agent_base.py:116-137
COMMANDS = ["Read", "Write", "Edit", "Bash", ...]

hooks = {
    "PreToolUse": [
        HookMatcher(matcher="Bash", hooks=[auto_approve]),
        HookMatcher(matcher="Read", hooks=[auto_approve]),
        HookMatcher(matcher="Edit", hooks=[auto_approve]),
        # ... å…¶ä»–å·¥å…·
    ]
}

options = ClaudeAgentOptions(hooks=hooks)
client = ClaudeSDKClient(agent_definition=agent_def, options=options)
```

#### 4.4.3 MCP å·¥å…·çš„è‡ªåŠ¨æ‰¹å‡†

**MCP å·¥å…·** ä½¿ç”¨å•ç‹¬çš„ hook å‡½æ•°ï¼š

```python
# claude_agent_base.py:80-92
async def auto_approve_mcp(input_data, tool_use_id, context):
    """è‡ªåŠ¨æ‰¹å‡†æ‰€æœ‰ MCP å·¥å…·"""
    tool_name = input_data.get('tool_name', 'unknown')
    tool_input = input_data.get('tool_input', {})
    logger.debug(f"ğŸ“‹ å·¥å…·: {tool_name}")
    logger.debug(f"ğŸ“‹ å‚æ•°: {tool_input}")

    return {
        "hookSpecificOutput": {
            "hookEventName": "PreToolUse",
            "permissionDecision": "allow",  # è‡ªåŠ¨æ‰¹å‡†
        }
    }

# æ³¨å†Œ MCP hooks
ALLOW_MCP_TOOLS = ["mcp__chrome-devtools", "mcp__playwright", "mcp__brightdata"]
ALLOW_MCP_HOOKS = [
    HookMatcher(matcher=m, hooks=[auto_approve_mcp])
    for m in ALLOW_MCP_TOOLS
]
```

**ä¸ºä»€ä¹ˆåˆ†å¼€**:

- **æ—¥å¿—åŒºåˆ†**: MCP å·¥å…·çš„å‚æ•°ç»“æ„ä¸åŒï¼Œéœ€è¦å•ç‹¬å¤„ç†
- **æƒé™åˆ†ç¦»**: æœªæ¥å¯ä»¥å¯¹ MCP å·¥å…·å®æ–½æ›´ä¸¥æ ¼çš„æ§åˆ¶

---

## 5. Developer æ­¥éª¤å®Œæ•´è§£æ

DevBot å…±æœ‰ **28 ä¸ª Developer èŠ‚ç‚¹** (åŒ…æ‹¬å­æ­¥éª¤)ï¼Œåˆ†ä¸º 4 å¤§é˜¶æ®µï¼š

```
é˜¶æ®µ 1: åŸºç¡€æ¡†æ¶ (Step 0-3)
    Step 0: åˆ›å»ºåŸºç¡€æ–‡ä»¶
    Step 1: åˆ†æé¡µé¢ç»“æ„
    Step 2: ç”Ÿæˆåˆ—è¡¨æå–å™¨
    Step 2.1: URL åˆ†ç±»
    Step 3: ç”Ÿæˆ URL patterns
    Step 3.1: å¤„ç†ä¸ç¡®å®š patterns

é˜¶æ®µ 2: æå–å™¨å¾ªç¯ (Step 4-6)
    Step 4: åˆå§‹åŒ– patterns é˜Ÿåˆ—
    Step 4.1: å–å‡ºä¸‹ä¸€ä¸ª pattern
    Step 5: ç”Ÿæˆè¯¦æƒ…é¡µæå–å™¨ç±»
    Step 5.1-5.6: ç”Ÿæˆå„ä¸ªæ–¹æ³•
    Step 6: ç”Ÿæˆåˆ—è¡¨é¡µæå–å™¨

é˜¶æ®µ 3: ç½‘ç«™æ ‘æ‰©å±• (Step 7-10)
    Step 7: ç½‘ç«™æ ‘æ‰©å±•ä¸€å±‚
    Step 8: Markdown ä¿¡æ¯åˆ†æ (å ä½)
    Step 9: é¦–æ¬¡è¿è¡Œæµ‹è¯•
    Step 10: ç”Ÿæˆ Airflow DAG

é˜¶æ®µ 4: ä»£ç ä¼˜åŒ– (Step 20-22)
    Step 20: ä»£ç æ£€æŸ¥
    Step 20.1-20.4: æ€§èƒ½ã€ç­¾åã€æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
    Step 21: æ€§èƒ½ä¼˜åŒ–
    Step 22: ä¿®å¤ä»£ç 
```

æœ¬æ–‡æ¡£è¯¦ç»†åˆ†æ DevBot çˆ¬è™«å¼€å‘ç³»ç»Ÿä¸­æ¯ä¸ªå¼€å‘èŠ‚ç‚¹(developer_nodes)çš„è®¾è®¡æ€è·¯å’Œå®ç°é€»è¾‘ã€‚

### 5.1 æ€»ä½“æ¶æ„

DevBot é‡‡ç”¨ LangGraph æ„å»ºçš„å£°æ˜å¼å·¥ä½œæµï¼Œé€šè¿‡ StateGraph ç®¡ç†çŠ¶æ€æµè½¬ã€‚æ•´ä¸ªç³»ç»Ÿåˆ†ä¸ºä¸»çº¿å¼€å‘æµç¨‹(Step 0-10)å’Œä»£ç è´¨é‡æ£€æµ‹æµç¨‹(Step 20-22)ã€‚

### 5.2 æ ¸å¿ƒè®¾è®¡ç†å¿µ

1. **æ¸è¿›å¼æ„å»º**: ä»åŸºç¡€æ¡†æ¶åˆ°å®Œæ•´åŠŸèƒ½ï¼Œé€æ­¥ç”Ÿæˆçˆ¬è™«ä»£ç 
2. **AIé©±åŠ¨**: æ¯ä¸ªæ­¥éª¤è°ƒç”¨ Claude AI ç”Ÿæˆç‰¹å®šä»£ç ç‰‡æ®µ
3. **æ–­ç‚¹ç»­ä¼ **: é€šè¿‡çŠ¶æ€æŒä¹…åŒ–æ”¯æŒä»ä»»æ„èŠ‚ç‚¹æ¢å¤
4. **è‡ªåŠ¨æäº¤**: æ¯ä¸ªæ­¥éª¤å®Œæˆåè‡ªåŠ¨ Git æäº¤ï¼Œä¾¿äºè¿½è¸ª
5. **å¾ªç¯å¤„ç†**: æ”¯æŒå¤„ç†å¤šä¸ª URL patterns çš„å¾ªç¯
6. **è´¨é‡ä¿è¯**: å†…ç½®ä»£ç æ£€æŸ¥å’Œæ€§èƒ½ä¼˜åŒ–æµç¨‹

---

### 5.3 ä¸»çº¿å¼€å‘æµç¨‹ (Step 0-10)

#### Step 0: åˆ›å»ºåŸºç¡€æ–‡ä»¶ (`step0__create_base_file`)

**è®¾è®¡æ€è·¯**:

- ä½¿ç”¨ Jinja2 æ¨¡æ¿(`tmpl_base.py.j2`)ç”Ÿæˆçˆ¬è™«æ–‡ä»¶éª¨æ¶
- åˆ›å»ºå¿…è¦çš„ç›®å½•ç»“æ„
- æä¾›åŸºç¡€çš„ç±»å®šä¹‰å’Œé…ç½®æ¡†æ¶

**å…³é”®å®ç°**:

```python
- è¯»å–æ¨¡æ¿æ–‡ä»¶ tmpl_base.py.j2
- æ¸²æŸ“æ¨¡æ¿ï¼Œå¡«å……ç«™ç‚¹åã€URLã€åˆ†ç±»ç­‰å‚æ•°
- ç”Ÿæˆ extractor_{site_name}.py æ–‡ä»¶
- åˆ›å»º output_dirã€output_part_dirã€tmp_folder ç­‰ç›®å½•
- è‡ªåŠ¨ Git æäº¤
```

**è¾“å‡ºäº§ç‰©**:

- `crawler/{category}/extractor_{site_name}.py` - åŸºç¡€ä»£ç æ¡†æ¶
- ç›¸å…³ç›®å½•ç»“æ„

---

#### Step 1: åˆ†æé¡µé¢ç»“æ„ (`step1__analyze_page`)

**è®¾è®¡æ€è·¯**:

- ä½¿ç”¨ chrome-devtools è®¿é—®ç›®æ ‡ç½‘ç«™
- åˆ†æé¡µé¢å¤æ‚åº¦(é™æ€/JSæ¸²æŸ“/åŠ¨æ€åŠ è½½)
- é€‰æ‹©åˆé€‚çš„çˆ¬è™«å¼•æ“å’Œå¹¶å‘å‚æ•°
- æˆªå–é¦–é¡µé•¿å›¾å¹¶å‹ç¼©
- åˆ†æåŠ¨æ€æ•°æ®åŠ è½½æƒ…å†µ(AJAXã€è¯„è®ºåŒºã€å•†å“å‚æ•°ç­‰)

**æ ¸å¿ƒä»»åŠ¡**:

1. **åçˆ¬å¤„ç†**: å¦‚é‡åçˆ¬ï¼Œå…ˆç”¨ BrightData ä¿å­˜ä¸´æ—¶é¡µé¢
2. **é¡µé¢åˆ†æ**:
   - è¯»å–å•†å“åˆ—è¡¨é¡µ URL ä¿å­˜åˆ° `first_analyze.json`
   - æˆªå–é¦–é¡µé•¿å›¾(fullPage=true)
   - å‹ç¼©å›¾ç‰‡åˆ° 2MB ä»¥å†…(é¿å…è¶…è¿‡ API é™åˆ¶)
3. **åŠ¨æ€æ•°æ®åˆ†æ**:
   - è¯„è®ºåŒºåŠ è½½æ–¹å¼(é™æ€/AJAX/æ— é™æ»šåŠ¨)
   - å•†å“å‚æ•°è·å–æ–¹å¼
   - å…¶ä»– AJAX è¯·æ±‚
4. **å¼•æ“é€‰æ‹©**:
   - `browser_pool`: æ— åçˆ¬æˆ–ç®€å•åçˆ¬
   - `brightdata+browser_pool`: å¤æ‚åçˆ¬

**Prompt å…³é”®ç‚¹**:

```markdown
- æˆªå›¾è¦æ±‚: format="jpeg", fullPage=true
- å‹ç¼©é™åˆ¶: 2MB ä»¥å†…ï¼Œå•è¾¹åƒç´ ä¸è¶…è¿‡ 8000
- é‡è¯•æœºåˆ¶: DecompressionBombError æœ€å¤šé‡è¯• 5 æ¬¡
```

**è¾“å‡ºäº§ç‰©**:

- ä»£ç ä¸­çš„"é¦–é¡µåˆ†æ"æ³¨é‡Š
- CONCURRENT_CONFIG å¹¶å‘é…ç½®
- Base{Site}Extractor çš„ engine è®¾ç½®
- é¦–é¡µé•¿å›¾æˆªå›¾æ–‡ä»¶

---

#### Step 2: ç”Ÿæˆåˆ—è¡¨æå–å™¨ (`step2__generate_list_extractor`)

**è®¾è®¡æ€è·¯**:

- å®ç° `extract_deals_from_mainpage` æ–¹æ³•
- ä»é¦–é¡µæå–å•†å“/æ´»åŠ¨/ä¼˜æƒ åˆ—è¡¨
- è¿”å›æ ‡å‡†åŒ–çš„æ•°æ®ç»“æ„

**å…³é”®è¦æ±‚**:

1. **è¿”å›æ•°æ®ç»“æ„**:

```python
{
    'site_name': str,
    'title': str,
    'content_length': int,
    'detail_count': int,  # è¯¦æƒ…é¡µæ•°é‡
    'list_count': int,    # åˆ—è¡¨é¡µæ•°é‡
    'urls': [
        {
            'type': 'detail' | 'list' | 'other' | 'unclear',
            'title': str,
            'url': str,  # å®Œæ•´URL (httpå¼€å¤´)
            'image_url': str,  # å¯é€‰ï¼Œä»…detailç±»å‹
            'price': str       # å¯é€‰ï¼Œä»…detailç±»å‹
        }
    ]
}
```

2. **URLç±»å‹å®šä¹‰**:
   - **detail**: è¯¦æƒ…é¡µ - URLå«å”¯ä¸€æ ‡è¯†ç¬¦(ID/SKU/slug)
   - **list**: åˆ—è¡¨é¡µ - å±•ç¤ºå¤šä¸ªæ¡ç›®ï¼Œæœ‰åˆ†é¡µ
   - **other**: åŠŸèƒ½é¡µ - ç™»å½•ã€å¸®åŠ©ã€è´­ç‰©è½¦ç­‰
   - **unclear**: ä¸ç¡®å®š - å¦‚é¦–é¡µã€æ··åˆé¡µé¢

3. **æµ‹è¯•éªŒè¯**:

```bash
python -m crawler.{category}.extractor_{site_name} extract_deals_from_mainpage
```

**è¾“å‡ºäº§ç‰©**:

- `extract_deals_from_mainpage` æ–¹æ³•å®ç°
- `site_tree.json` - ç«™ç‚¹URLæ ‘ç»“æ„

---

#### Step 2.1: URL åˆ†ç±» (`step2_1__classify_urls`)

**è®¾è®¡æ€è·¯**:

- åŸºäº `site_tree.json` ä¸­çš„ URLs
- ä½¿ç”¨ Claude AI è¯†åˆ« URL patterns
- è‡ªåŠ¨å½’ç±»å¹¶éªŒè¯ patterns

**æ ¸å¿ƒæµç¨‹**:

1. **Pattern è¯†åˆ«**:
   - æœ€å¤šè¯†åˆ« 30 ä¸ª URL patterns
   - ä¸ºæ¯ä¸ª pattern ç”Ÿæˆæ­£åˆ™è¡¨è¾¾å¼
   - åˆ†ç±»ä¸º detail/list/other/unclear

2. **éªŒè¯æœºåˆ¶**:

```bash
# æ¯ä¸ª pattern å¿…é¡»é€šè¿‡éªŒè¯
python -m devbot.tool check_pattern_match "<pattern>" "{output_file_dir}/site_tree.json"
```

   - è¿”å› True: pattern æœ‰æ•ˆ
   - è¿”å› False: é‡æ–°è¯†åˆ«

3. **ä¸¤è½®å¤„ç†**:
   - ç¬¬ä¸€è½®: å¤„ç†å…¨éƒ¨æˆ–éšæœºé‡‡æ · 200 ä¸ª URL
   - ç¬¬äºŒè½®: å¤„ç†å‰©ä½™æœªåŒ¹é…çš„ URL(æœ€å¤š100ä¸ª)
   - å»é‡åˆå¹¶ç»“æœ

**è¾“å‡ºäº§ç‰©**:

- `{tmp_folder}/{site_name}/temp_site_patterns.json` - ä¸´æ—¶ patterns
- `site_tree.json` æ›´æ–° - æ·»åŠ  `site_patterns` å’Œ `matched_pattern` å­—æ®µ

---

#### Step 3: ç”Ÿæˆ URL Patterns (`step3__generate_url_patterns`)

**è®¾è®¡æ€è·¯**:

- è¯»å– step2.1 ç”Ÿæˆçš„ patterns
- æŒ‰ç±»å‹åˆ†ç±»æ³¨å†Œåˆ°ä»£ç 
- è®¾ç½® `url_list_patterns`ã€`url_detail_patterns`ã€`unclear_patterns`

**æ ¸å¿ƒé€»è¾‘**:

```python
# ä» site_tree.json è¯»å– patterns
patterns_data = {
    "list_patterns": [p for p in patterns if p['type'] == 'list'],
    "detail_patterns": [p for p in patterns if p['type'] == 'detail'],
    "unclear_patterns": [p for p in patterns if p['type'] not in ('detail', 'list')]
}

# åœ¨ä»£ç ä¸­è®¾ç½®è¿™ä¸‰ä¸ªæ•°ç»„
url_list_patterns = [...]
url_detail_patterns = [...]
unclear_patterns = [...]
```

**ç¤ºä¾‹ä»£ç **:

```python
url_list_patterns = [
    r'^https?://(?:www\.)?newsite\.com/?$',  # ä¸»é¡µ
    r'^https?://(?:www\.)?newsite\.com/shop\?.*',  # æ–°å“åˆ—è¡¨é¡µ
]
url_detail_patterns = [
    r'^https?://(?:www\.)?newsite\.com/view-deal/\d+/?',  # è¯¦æƒ…é¡µ
]
```

---

#### Step 3.1: å¤„ç† unclear patterns (`step3_1__handle_unclear_pattern`)

**è®¾è®¡æ€è·¯**:

- é’ˆå¯¹ unclear ç±»å‹çš„ URL
- ä½¿ç”¨ chrome-devtools å®é™…è®¿é—®é¡µé¢
- äººå·¥/AI åˆ¤æ–­æ˜¯åˆ—è¡¨é¡µè¿˜æ˜¯è¯¦æƒ…é¡µ

**åˆ¤æ–­æ ‡å‡†**:

**åˆ—è¡¨é¡µç‰¹å¾**:

- å±•ç¤ºå¤šä¸ªæ¡ç›®
- æœ‰åˆ†é¡µ/åŠ è½½æ›´å¤š
- åŒ…å«å¤šä¸ªé“¾æ¥æŒ‡å‘è¯¦æƒ…é¡µ

**è¯¦æƒ…é¡µç‰¹å¾**:

- å±•ç¤ºå•ä¸ªæ¡ç›®å®Œæ•´ä¿¡æ¯
- URL å«å”¯ä¸€æ ‡è¯†ç¬¦
- æœ‰ä»·æ ¼ã€è´­ä¹°æŒ‰é’®ç­‰

**æ“ä½œæ­¥éª¤**:

1. `mcp__chrome-devtools__new_page` æ‰“å¼€ URL
2. `mcp__chrome-devtools__take_snapshot` è·å–å¿«ç…§
3. åˆ†æé¡µé¢ç»“æ„
4. æ›´æ–° pattern ç±»å‹

---

#### Step 4: åˆå§‹åŒ– Patterns é˜Ÿåˆ— (`step4__init_patterns_queue`)

**è®¾è®¡æ€è·¯**:

- å‡†å¤‡å¤„ç†æ‰€æœ‰ URL patterns
- åˆ›å»º patterns é˜Ÿåˆ—
- è·Ÿè¸ªå·²å®Œæˆçš„ patterns

**æ ¸å¿ƒé€»è¾‘**:

```python
# ä»ä»£ç æ¨¡å—è¯»å– patterns
url_list_patterns = getattr(module, 'url_list_patterns', [])
url_detail_patterns = getattr(module, 'url_detail_patterns', [])

# æ„å»ºé˜Ÿåˆ—
patterns_queue = [
    {
        'pattern': pattern,
        'type': 'list',
        'sample_url': get_sample_url(pattern)
    }
    for pattern in url_list_patterns
] + [
    {
        'pattern': pattern,
        'type': 'detail',
        'sample_url': get_sample_url(pattern)
    }
    for pattern in url_detail_patterns
]

# è¿‡æ»¤å·²å®Œæˆçš„ patterns
completed_patterns = state.get("completed_patterns", [])
patterns_queue = [p for p in patterns_queue if p['pattern'] not in completed_patterns]
```

---

#### Step 4.1: è·å–ä¸‹ä¸€ä¸ª Pattern (`step4_1__next_pattern`)

**è®¾è®¡æ€è·¯**:

- ä»é˜Ÿåˆ—ä¸­å–å‡ºä¸‹ä¸€ä¸ª pattern
- è®¾ç½®å½“å‰å¤„ç†çš„ pattern ä¿¡æ¯
- è·¯ç”±åˆ°å¯¹åº”çš„å¤„ç†èŠ‚ç‚¹

**è·¯ç”±é€»è¾‘**:

```python
if not patterns_queue:
    return "reviewer_step4"  # é˜Ÿåˆ—ä¸ºç©ºï¼Œè¿›å…¥å®¡æŸ¥
elif current_pattern['type'] == 'detail':
    return "step5__generate_extractor_class"  # è¯¦æƒ…é¡µå¤„ç†
elif current_pattern['type'] == 'list':
    return "step6__generate_list_extractor"  # åˆ—è¡¨é¡µå¤„ç†
```

---

#### Step 5: ç”Ÿæˆè¯¦æƒ…é¡µæå–å™¨ç±» (`step5__generate_extractor_class`)

**è®¾è®¡æ€è·¯**:

- ä¸ºæ¯ä¸ªè¯¦æƒ…é¡µ pattern åˆ›å»ºæå–å™¨ç±»
- å®ç° `do()` æ–¹æ³•æ¡†æ¶
- æ·»åŠ è¯¦æƒ…é¡µåˆ†ææ³¨é‡Š

**å…³é”®ä»»åŠ¡**:

1. **é¡µé¢åˆ†æ**:
   - æ‰“å¼€æ ·ä¾‹ URL
   - æˆªå–é•¿å›¾
   - åˆ†æé¡µé¢ç»“æ„(å¤´æ ã€è¯„è®ºåŒºã€å•†å“å‚æ•°ä½ç½®)

2. **ç±»å‘½åè§„åˆ™**:

```python
# ç¤ºä¾‹: gnc ç«™ç‚¹çš„ product ç±»åˆ«
class GncProductExtractor(BaseExtractor, ProductDetailMixin):
    pass
```

3. **æ–¹æ³•æ¡†æ¶**:

```python
async def extract_product_detail(page: PageParam) -> dict:
    """æå–å•†å“è¯¦æƒ…"""
    return await GncProductExtractor(page).do()
```

4. **URL_MAP æ³¨å†Œ**:

```python
URL_MAP = {
    'product_detail': {
        'patterns': [r'https://www\.gnc\.com/.+/\d+\.html'],
        'func': extract_product_detail,
        'action': 'get_detail_info'
    }
}
```

**è·³è¿‡æœºåˆ¶**:

- å¦‚æœæ£€æµ‹åˆ°"è¯¥é¡µé¢ä¸æ˜¯ç›®æ ‡è¯¦æƒ…é¡µ"ï¼Œè·³è¿‡åç»­æ­¥éª¤
- æ ‡è®° pattern ä¸ºå·²å®Œæˆï¼Œç»§ç»­ä¸‹ä¸€ä¸ª

---

#### Step 5.1: å®ç° fetch_rendered_html (`step5_1__generate_fetch_rendered_html`)

**è®¾è®¡æ€è·¯**:

- è·å–æ¸²æŸ“åçš„çº¯ HTML(æ—  JS)
- è§¦å‘é¡µé¢åŠ¨æ€å†…å®¹åŠ è½½
- ä¿å­˜åŸå§‹å’Œæ¸²æŸ“åçš„ HTML

**å®ç°æ­¥éª¤**:

```python
async def fetch_rendered_html(self, page) -> tuple:
    """åœ¨å·²åŠ è½½çš„é¡µé¢ä¸Šæ»šåŠ¨è§¦å‘æ¸²æŸ“ï¼Œä¿å­˜HTMLæ–‡ä»¶

    Args:
        page: playwrighté¡µé¢å¯¹è±¡(å·²ç”±doæ–¹æ³•åŠ è½½å¥½å†…å®¹)

    Returns:
        tuple: (page, æ¸²æŸ“åHTML(æ— JS), site_infoå­—å…¸)
    """
    # 1. æ»šåŠ¨åˆ°è¯„è®ºåŒºè§¦å‘åŠ è½½(å¦‚æœ‰)
    await page.evaluate("window.scrollTo(0, document.body.scrollHeight * 0.6)")
    await page.wait_for_timeout(3000)  # ç­‰å¾…è¯„è®ºåŠ è½½

    # 2. æ»šåŠ¨åˆ°åº•éƒ¨ç¡®ä¿æ‰€æœ‰å†…å®¹åŠ è½½
    await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
    await page.wait_for_timeout(2000)

    # 3. è·å–å®Œæ•´HTMLå¹¶ä¿å­˜
    html = await page.content()
    self.save_origin_html_file(html)  # step0_origin_xxx.html
    rendered_html = self.save_rendered_html(html)  # step1_rendered_xxx.html (ç§»é™¤JS)

    # 4. è¿”å›ä¸‰å…ƒç»„
    site_info = {'title': await page.title()}
    return (page, rendered_html, site_info)
```

**éªŒè¯æ£€æŸ¥**:

- step0_origin_xxx.html - åŸå§‹HTML(å«JS)
- step1_rendered_xxx.html - æ¸²æŸ“åHTML(å·²ç§»é™¤JS)
- ç¡®è®¤è¯„è®ºã€å‚æ•°ç­‰JSåŠ è½½å†…å®¹å·²æ˜¾ç¤º

---

#### Step 5.2: å®ç° remove_site_chrome (`step5_2__generate_remove_site_chrome`)

**è®¾è®¡æ€è·¯**:

- ç§»é™¤ç½‘ç«™è£…é¥°å…ƒç´ (å¤´æ ã€åº•æ ã€å¯¼èˆªç­‰)
- ä¿ç•™å•†å“æ ¸å¿ƒå†…å®¹
- "site chrome"æ˜¯ç½‘é¡µè¡Œä¸šæœ¯è¯­ï¼ŒæŒ‡å…¨ç«™æ¡†æ¶å…ƒç´ 

**æ¸…ç†ç›®æ ‡**:

1. **å¤´éƒ¨å¯¼èˆª** - header, nav
2. **åº•éƒ¨åŒºåŸŸ** - footer
3. **ä¾§è¾¹æ ** - sidebar
4. **æµ®åŠ¨å…ƒç´ ** - å›ºå®šå®šä½çš„å¹¿å‘Šã€å®¢æœçª—å£

**æ–¹æ³•ç­¾å**:

```python
def remove_site_chrome(self, html: str) -> str:
    """ç§»é™¤ç½‘ç«™é€šç”¨å¤´æ /åº•æ /å…¬å‘Šç­‰ chrome UI

    Args:
        html: æ¸²æŸ“åçš„HTML(æ¥è‡ªstep1)

    Returns:
        str: ç§»é™¤è£…é¥°åçš„HTML
    """
    soup = BeautifulSoup(html, 'html.parser')

    # ç§»é™¤å¤´éƒ¨
    for header in soup.find_all(['header', 'nav']):
        header.decompose()

    # ç§»é™¤åº•éƒ¨
    for footer in soup.find_all('footer'):
        footer.decompose()

    return str(soup)
```

**éªŒè¯æ­¥éª¤**:

- å¯¹æ¯” step1 å’Œ step2 æ–‡ä»¶å­—ç¬¦æ•°
- ç”¨ chrome-devtools æ‰“å¼€ step2 æ–‡ä»¶
- ç¡®è®¤æ—  header/nav/footer æ ‡ç­¾
- å•†å“å†…å®¹å®Œæ•´ä¿ç•™

---

#### Step 5.3: å®ç° extract_main_content (`step5_3__generate_extract_main_content`)

**è®¾è®¡æ€è·¯**:

- ç§»é™¤éå•†å“ç›¸å…³çš„å¹²æ‰°å…ƒç´ 
- ä¿ç•™å•†å“ä¸»ä½“å†…å®¹
- åŒºåˆ«äº step5.2: è¿™é‡Œç§»é™¤çš„æ˜¯é¡µé¢å†…å®¹å±‚é¢çš„å¹²æ‰°

**æ¸…ç†ç›®æ ‡**:

1. **æ¨èå•†å“** - "ä½ å¯èƒ½å–œæ¬¢"ã€"ç›¸å…³å•†å“"
2. **å¹¿å‘Šæ¨¡å—** - å„ç±»å¹¿å‘Šä½
3. **ä¾§è¾¹æ ** - éä¸»è¦å†…å®¹
4. **å¼¹çª—æç¤º** - Cookieæç¤ºã€è®¢é˜…å¼¹çª—
5. **å¯¼èˆªé¢åŒ…å±‘** - å¯é€‰ä¿ç•™

**ä¿ç•™å†…å®¹**:

- âœ… å•†å“æ ‡é¢˜ã€ä»·æ ¼
- âœ… å•†å“å›¾ç‰‡
- âœ… è§„æ ¼å‚æ•°
- âœ… è¯¦ç»†æè¿°
- âœ… ç”¨æˆ·è¯„è®º
- âœ… Q&Aé—®ç­”

**æ–¹æ³•ç­¾å**:

```python
def extract_main_content(self, html: str) -> str:
    """ç§»é™¤æ¨èå•†å“ã€å¹¿å‘Šã€ä¾§æ ç­‰éä¸»å•†å“å†…å®¹

    Args:
        html: ç§»é™¤ç«™ç‚¹è£…é¥°åçš„HTML(æ¥è‡ªstep2)

    Returns:
        str: æå–ä¸»è¦å†…å®¹åçš„HTML
    """
    soup = BeautifulSoup(html, 'html.parser')

    # ç§»é™¤æ¨èå•†å“
    for rec in soup.find_all(class_=re.compile(r'recommend|related|similar')):
        rec.decompose()

    # ç§»é™¤å¹¿å‘Š
    for ad in soup.find_all(class_=re.compile(r'ad|banner|promo')):
        ad.decompose()

    return str(soup)
```

**éªŒè¯è¦æ±‚**:

- æˆªé•¿å›¾å¯¹æ¯” step2 å’Œ step3
- å­—æ•°åº”å‡å°‘(åªä¿ç•™ä¸»è¦å†…å®¹)
- å•†å“æ ¸å¿ƒä¿¡æ¯å®Œæ•´

---

#### Step 5.4: å®ç° convert_html_to_markdown (`step5_4__generate_convert_markdown`)

**è®¾è®¡æ€è·¯**:

- å°†æ¸…ç†åçš„ HTML è½¬æ¢ä¸º Markdown
- ä¿æŒå†…å®¹ç»“æ„å’Œå±‚çº§
- ä¾¿äºåç»­ LLM æå–

**è½¬æ¢è¦æ±‚**:

**éœ€è¦æå–çš„å†…å®¹**:

1. **æ–‡æœ¬å†…å®¹** - æ‰€æœ‰å¯è§æ–‡æœ¬ï¼Œä¿æŒå±‚çº§
2. **å›¾ç‰‡** - è½¬ä¸º Markdown å›¾ç‰‡è¯­æ³• `![alt](url)`
3. **é“¾æ¥** - è½¬ä¸º `[text](url)`
4. **è¡¨æ ¼** - è½¬ä¸º Markdown è¡¨æ ¼
5. **åˆ—è¡¨** - è½¬ä¸º Markdown åˆ—è¡¨

**æ–¹æ³•ç­¾å**:

```python
def convert_html_to_markdown(self, html: str) -> str:
    """å°† HTML è½¬æ¢ä¸º Markdown

    Args:
        html: æ¸…ç†åçš„HTML(æ¥è‡ªstep3)

    Returns:
        str: Markdown æ ¼å¼æ–‡æœ¬
    """
    # ä½¿ç”¨ html2text æˆ–è‡ªå®šä¹‰è½¬æ¢é€»è¾‘
    import html2text
    h = html2text.HTML2Text()
    h.ignore_links = False
    h.ignore_images = False
    return h.handle(html)
```

**è¾“å‡ºäº§ç‰©**:

- step4_markdown_xxx.md - æœ€ç»ˆçš„ Markdown æ–‡ä»¶

---

#### Step 5.5: æ”¶é›†å…¶å®ƒä¿¡æ¯ (`step5_5__collect_other_info`)

**è®¾è®¡æ€è·¯**:

- ä»é¡µé¢æå–é¢å¤–çš„å…ƒæ•°æ®
- è¡¥å…… `do()` æ–¹æ³•çš„è¿”å›å­—å…¸
- ä¸ºåç»­ LLM æå–æä¾›ä¸Šä¸‹æ–‡

**å¯èƒ½æ”¶é›†çš„ä¿¡æ¯**:

- é¡µé¢ meta æ ‡ç­¾
- ç»“æ„åŒ–æ•°æ®(JSON-LD)
- Open Graph æ ‡ç­¾
- ä»·æ ¼ã€åº“å­˜ç­‰å…³é”®ä¿¡æ¯

---

#### Step 6: ç”Ÿæˆåˆ—è¡¨é¡µæå–å™¨ (`step6__generate_list_extractor`)

**è®¾è®¡æ€è·¯**:

- ä¸ºæ¯ä¸ªåˆ—è¡¨é¡µ pattern å®ç°æå–æ–¹æ³•
- ä»åˆ—è¡¨é¡µæå–å­é¡µé¢ URL
- æ”¯æŒåˆ†é¡µå¤„ç†

**å…³é”®ä»»åŠ¡**:

1. **æ–¹æ³•å‘½å**: `extract_list_from_{pattern_name}`
2. **è¿”å›æ ¼å¼**:

```python
{
    'site_name': str,
    'title': str,
    'urls': [
        {'url': str, 'title': str, 'type': 'detail'|'list'}
    ]
}
```

3. **URL_MAP æ³¨å†Œ**:

```python
URL_MAP = {
    'category_list': {
        'patterns': [r'https://www\.site\.com/category/.*'],
        'func': extract_list_from_category,
        'action': 'get_list_info'
    }
}
```

**å¼‚å¸¸å¤„ç†**:

- æ•è· SubagentError å¹¶è®°å½•
- å‘é€ Slack å‘Šè­¦
- æ ‡è®° pattern ä¸ºå·²å®Œæˆ(è·³è¿‡)ï¼Œç»§ç»­ä¸‹ä¸€ä¸ª

---

#### Step 7: æ‰©å±• Site Tree æ·±åº¦ (`step7__site_tree_expand_level`)

**è®¾è®¡æ€è·¯**:

- ä»å·²æœ‰çš„åˆ—è¡¨é¡µç»§ç»­æŠ“å–ä¸‹ä¸€å±‚ URL
- å‘ç°æ–°çš„ URL patterns
- å¾ªç¯å¤„ç†ç›´åˆ°æ— æ–° patterns

**æ ¸å¿ƒæµç¨‹**:

```python
# 1. åŠ¨æ€å¯¼å…¥ Extractor ç±»
extractor_class = getattr(module, f'Base{site_name.capitalize()}Extractor')

# 2. æ‰©å±•ä¸€å±‚
expansion_result = await extractor_class.expand_one_level_from_list_pages(urls)

# 3. æ›´æ–° site_tree.json
content['site_tree'] = urls + expansion_result['new_urls']

# 4. AI åˆ†ææ–° URLsï¼Œç”Ÿæˆæ–° patterns
patterns = UrlHandler.classify_urls(category, site_name)

# 5. åˆ¤æ–­æ˜¯å¦æœ‰æ–° patterns
has_new_patterns = len(patterns) > len(site_patterns)
```

**å¾ªç¯æ§åˆ¶**:

- å¦‚æœæœ‰æ–° patterns: å›åˆ° Step 3
- å¦‚æœæ— æ–° patterns: è¿›å…¥ Step 8
- `step7_loop_count`: è®°å½•å¾ªç¯æ¬¡æ•°

**è¾“å‡ºä¿¡æ¯**:

- å±‚çº§å˜åŒ–: 1 â†’ 2
- æ–°å¢ URL æ•°é‡
- å¤„ç†çš„ patterns åˆ—è¡¨

---

#### Step 8: åˆ†æ Markdown ä¿¡æ¯ (`step8__analyze_markdown_info`)

**è®¾è®¡æ€è·¯**:

- åˆ†æç”Ÿæˆçš„ Markdown å†…å®¹
- ä¸º LLM æå–å‡†å¤‡æç¤ºè¯
- (å½“å‰ç‰ˆæœ¬ä¸ºå ä½å®ç°)

**æœªæ¥æ‰©å±•**:

- è¯†åˆ« Markdown ä¸­çš„ç»“æ„åŒ–ä¿¡æ¯
- ç”Ÿæˆé’ˆå¯¹æ€§çš„æå– prompts
- ä¼˜åŒ– LLM æå–æ•ˆæœ

---

#### Step 9: ç¬¬ä¸€æ¬¡å®Œæ•´è¿è¡Œ (`step9__first_run`)

**è®¾è®¡æ€è·¯**:

- æä¾›å®Œæ•´è¿è¡Œå‘½ä»¤
- éªŒè¯æ•´ä¸ªçˆ¬è™«ç³»ç»Ÿ
- ç”±äºè€—æ—¶é•¿ï¼Œä¸è‡ªåŠ¨æ‰§è¡Œ

**æ‰§è¡Œå‘½ä»¤**:

```bash
python -m crawler.{category}.extractor_{site_name} main
```

**éªŒè¯å†…å®¹**:

- åˆ—è¡¨é¡µæå–æ˜¯å¦æ­£å¸¸
- è¯¦æƒ…é¡µæå–æ˜¯å¦æ­£å¸¸
- æ•°æ®ä¿å­˜æ˜¯å¦æ­£ç¡®
- æ€§èƒ½æ˜¯å¦å¯æ¥å—

---

#### Step 10: æ·»åŠ  Airflow DAG (`step10__add_airflow_dag`)

**è®¾è®¡æ€è·¯**:

- ç”Ÿæˆ Airflow è°ƒåº¦é…ç½®
- éšæœºåˆ†é…è¿è¡Œæ—¶é—´(é¿å…åŒæ—¶è¿è¡Œ)
- åˆ›å»º DAG æ–‡ä»¶

**é…ç½®ç”Ÿæˆ**:

```python
# éšæœºè°ƒåº¦æ—¶é—´
hour = random.randint(0, 23)
minute = random.randint(0, 59)
schedule = f"{minute} {hour} * * *"  # Cron æ ¼å¼

# ç”Ÿæˆé…ç½®
config = generate_dag_config(
    site_name=site_name,
    site_url=site_url,
    category=category,
    schedule=schedule,
    retries=2,
    retry_delay_minutes=7
)

# ç”Ÿæˆ DAG æ–‡ä»¶
output_path = generate_dag_file(config)
# è¾“å‡º: dags/{category}_{site_name}_dag.py
```

**DAG ç¤ºä¾‹**:

```python
# dags/product_gnc_dag.py
from airflow import DAG
from datetime import datetime, timedelta

default_args = {
    'owner': 'crawler',
    'retries': 2,
    'retry_delay': timedelta(minutes=7)
}

dag = DAG(
    'product_gnc',
    default_args=default_args,
    schedule_interval='30 14 * * *',  # æ¯å¤© 14:30
    start_date=datetime(2024, 1, 1),
    catchup=False
)
```

---

### 5.4 ä»£ç è´¨é‡æ£€æµ‹æµç¨‹ (Step 20-22)

#### Step 20: æ£€æŸ¥ä»£ç å®Œæˆåº¦ (`step20__check_code`)

**è®¾è®¡æ€è·¯**:

- æ£€æµ‹ä»£ç å„éƒ¨åˆ†æ˜¯å¦å®Œæ•´
- å†³å®šä¸‹ä¸€æ­¥éª¤æˆ–è¿›å…¥æ€§èƒ½æ£€æµ‹
- æä¾›è¡¥å…¨è·¯å¾„

**æ£€æŸ¥é¡¹ç›®**:

```python
def _check_extractor_code(state):
    # 1. æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not file_path.exists():
        return {'next_step': 'step0', 'reason': 'æ–‡ä»¶ä¸å­˜åœ¨'}

    # 2. é¦–é¡µåˆ†ææ³¨é‡Šæ˜¯å¦å®Œæˆ(è‡³å°‘50å­—)
    if not page_analysis_match or len(match.group(1)) < 50:
        return {'next_step': 'step1', 'reason': 'é¦–é¡µåˆ†ææ³¨é‡Šæœªå®Œæˆ'}

    # 3. extract_deals_from_mainpage æ–¹æ³•æ˜¯å¦å®Œæ•´(è‡³å°‘15è¡Œ)
    if len(method_lines) < 15:
        return {'next_step': 'step2', 'reason': 'æ–¹æ³•ä¸è¶³15è¡Œ'}

    # 4. site_tree.json æ˜¯å¦å­˜åœ¨
    if not site_tree_file.exists():
        return {'next_step': 'step2_1', 'reason': 'site_tree.jsonä¸å­˜åœ¨'}

    # 5. url_list_patterns æ˜¯å¦æœ‰å€¼
    if not url_list_patterns:
        return {'next_step': 'step3', 'reason': 'url_list_patternsæ— å€¼'}

    # 6. url_detail_patterns æ˜¯å¦æœ‰å€¼
    if not url_detail_patterns:
        return {'next_step': 'step7', 'reason': 'url_detail_patternsæ— å€¼'}

    # 7. URL_MAP æ˜¯å¦å¤„ç†äº†è¶³å¤Ÿçš„ patterns(è‡³å°‘30%)
    if handled_in_map < len(url_list_patterns) * 0.3:
        return {'next_step': 'step4', 'reason': 'URL_MAPå¤„ç†ä¸è¶³'}

    # 8. URL_MAP æ˜¯å¦æœ‰ get_detail_info
    if not has_detail_info:
        return {'next_step': 'step4', 'reason': 'ç¼ºå°‘get_detail_info'}

    # 9. DAG æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not dag_file.exists():
        return {'next_step': 'step10', 'reason': 'DAGæ–‡ä»¶ä¸å­˜åœ¨'}

    # é€šè¿‡æ‰€æœ‰æ£€æŸ¥
    return {'next_step': 'step20_3', 'reason': 'ä»£ç æ£€æŸ¥é€šè¿‡'}
```

**è·¯ç”±é€»è¾‘**:

- ä»£ç ä¸å®Œæ•´: è¿”å›å¯¹åº”çš„è¡¥å…¨æ­¥éª¤
- ä»£ç å®Œæ•´: è¿›å…¥ step20_3 æ–¹æ³•ç­¾åæ£€æŸ¥

---

#### Step 20.1: æ€§èƒ½æ£€æµ‹ (`step20_1__check_performance`)

**è®¾è®¡æ€è·¯**:

- æµ‹è¯• URL_MAP ä¸­æ‰€æœ‰æ–¹æ³•çš„æ‰§è¡Œæ€§èƒ½
- è¯†åˆ«è€—æ—¶è¶…è¿‡é˜ˆå€¼çš„æ–¹æ³•
- ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š

**æ£€æµ‹æµç¨‹**:

```python
# 1. è¯»å– URL_MAP
for map_key, map_config in url_map.items():
    func = map_config['func']
    patterns = map_config['patterns']

    # 2. ä» site_tree æˆ– patterns æ‰¾æµ‹è¯• URL
    test_url = find_test_url(patterns, site_tree)

    # 3. æ‰§è¡Œæ–¹æ³•å¹¶è®¡æ—¶
    start_time = time.time()
    result = await func(test_url)
    duration = time.time() - start_time

    # 4. è®°å½•ç»“æœ
    performance_results['methods'].append({
        'method': func.__name__,
        'test_url': test_url,
        'duration_seconds': duration,
        'status': 'success',
        'result_count': len(result.get('urls', []))
    })

    # 5. æ ‡è®°æ…¢æ–¹æ³•(è¶…è¿‡30ç§’)
    if duration > 30:
        slow_methods.append(func.__name__)
```

**è¾“å‡ºæ ¼å¼**:

```json
{
  "site_name": "hsn",
  "category": "product",
  "methods": [
    {
      "method": "extract_deals_from_mainpage",
      "action": "get_list_info",
      "test_url": "https://www.hsn.com",
      "duration_seconds": 14.43,
      "status": "success",
      "result_count": 501
    }
  ],
  "summary": {
    "total_methods": 5,
    "slow_methods": ["extract_category_page"]
  }
}
```

**ä¿å­˜è·¯å¾„**:

- `crawler/{category}/output/{site_name}/performance.json`

---

#### Step 20.2: ä¼˜åŒ–å‡½æ•° (`step20_2__optimize_funcs`)

**è®¾è®¡æ€è·¯**:

- é’ˆå¯¹æ€§èƒ½æŠ¥å‘Šä¸­çš„æ…¢æ–¹æ³•
- è°ƒç”¨ AI ç”Ÿæˆä¼˜åŒ–å»ºè®®
- é‡å†™è€—æ—¶å‡½æ•°

**ä¼˜åŒ–æ–¹å‘**:

1. **å¹¶å‘å¤„ç†** - ä½¿ç”¨ asyncio.gather
2. **ç¼“å­˜æœºåˆ¶** - é¿å…é‡å¤è¯·æ±‚
3. **å‡å°‘ç­‰å¾…** - ä¼˜åŒ– wait_for_timeout
4. **é€‰æ‹©å™¨ä¼˜åŒ–** - æ›´ç²¾ç¡®çš„ CSS/XPath
5. **åˆ†é¡µç­–ç•¥** - é™åˆ¶çˆ¬å–æ·±åº¦

**Prompt ç¤ºä¾‹**:

```markdown
ä»¥ä¸‹æ–¹æ³•æ‰§è¡Œè€—æ—¶ {duration} ç§’ï¼Œè¶…è¿‡é˜ˆå€¼ 30 ç§’:

æ–¹æ³•å: {func_name}
æµ‹è¯•URL: {test_url}

è¯·åˆ†æä»£ç å¹¶æä¾›ä¼˜åŒ–å»ºè®®ï¼Œé‡ç‚¹è€ƒè™‘:
1. æ˜¯å¦å¯ä»¥ä½¿ç”¨å¹¶å‘å¤„ç†
2. æ˜¯å¦æœ‰ä¸å¿…è¦çš„ç­‰å¾…
3. é€‰æ‹©å™¨æ˜¯å¦å¯ä»¥ä¼˜åŒ–
```

---

#### Step 20.3: æ£€æŸ¥æ–¹æ³•ç­¾å (`step20_3__check_method_sign`)

**è®¾è®¡æ€è·¯**:

- éªŒè¯æ‰€æœ‰æå–æ–¹æ³•çš„ç­¾åæ˜¯å¦ç¬¦åˆè§„èŒƒ
- æ£€æŸ¥å‚æ•°ç±»å‹ã€è¿”å›ç±»å‹
- ç¡®ä¿æ–¹æ³•å¯è¢«è°ƒåº¦å™¨æ­£ç¡®è°ƒç”¨

**æ£€æŸ¥é¡¹**:

1. **å‚æ•°è§„èŒƒ**:

```python
# åˆ—è¡¨é¡µæ–¹æ³•
async def extract_list_from_xxx(page: Optional[Union[str, PageParam]] = None) -> dict

# è¯¦æƒ…é¡µæ–¹æ³•
async def extract_detail_from_xxx(page: PageParam) -> dict
```

2. **è¿”å›å€¼è§„èŒƒ**:

```python
# å¿…é¡»åŒ…å«çš„å­—æ®µ
{
    'site_name': str,
    'title': str,
    'urls': list  # åˆ—è¡¨é¡µ
    # æˆ–è¯¦æƒ…é¡µç‰¹å®šå­—æ®µ
}
```

**ä¿®å¤æµç¨‹**:

- å‘ç°ä¸ç¬¦åˆè§„èŒƒçš„æ–¹æ³•
- è°ƒç”¨ step22 ä¿®å¤ä»£ç 
- é‡æ–°æ£€æŸ¥

---

#### Step 20.4: æ£€æŸ¥ç¼ºå¤±æ•°æ® (`step20_4__check_missing_data`)

**è®¾è®¡æ€è·¯**:

- æ‰§è¡Œä¸€æ¬¡å°è§„æ¨¡çˆ¬å–
- æ£€æŸ¥æå–çš„æ•°æ®å®Œæ•´æ€§
- è¯†åˆ«ç¼ºå¤±çš„å…³é”®å­—æ®µ

**æ£€æŸ¥å†…å®¹**:

1. **è¯¦æƒ…é¡µæ•°æ®**:
   - æ˜¯å¦æœ‰æ ‡é¢˜
   - æ˜¯å¦æœ‰ä»·æ ¼
   - æ˜¯å¦æœ‰å›¾ç‰‡
   - æ˜¯å¦æœ‰æè¿°

2. **åˆ—è¡¨é¡µæ•°æ®**:
   - URL æ˜¯å¦å®Œæ•´
   - ç±»å‹æ˜¯å¦æ­£ç¡®
   - æ•°é‡æ˜¯å¦åˆç†

**è¾“å‡º**:

```python
missing_fields = {
    'extract_product_detail': ['price', 'description'],
    'extract_list_from_category': []
}
```

**ä¿®å¤æµç¨‹**:

- æœ‰ç¼ºå¤±æ•°æ®: è°ƒç”¨ step22 ä¿®å¤
- æ— ç¼ºå¤±: è¿›å…¥ step20_1 æ€§èƒ½æ£€æµ‹

---

#### Step 21: ä¼˜åŒ–æ€§èƒ½ (`step21__optimize_performance`)

**è®¾è®¡æ€è·¯**:

- åŸºäº step20_1 çš„æ€§èƒ½æŠ¥å‘Š
- ç»¼åˆä¼˜åŒ–æ•´ä½“æ€§èƒ½
- (å½“å‰ç‰ˆæœ¬ä¸ºå ä½å®ç°)

**ä¼˜åŒ–ç­–ç•¥**:

- è°ƒæ•´å¹¶å‘å‚æ•°
- ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢
- å‡å°‘ç½‘ç»œè¯·æ±‚
- ä½¿ç”¨ç¼“å­˜æœºåˆ¶

---

#### Step 22: ä¿®å¤ä»£ç  (`step22__fix_code`)

**è®¾è®¡æ€è·¯**:

- ä½œä¸ºä¿®å¤çš„ç»Ÿä¸€å…¥å£
- æ ¹æ®æ£€æµ‹ç»“æœè°ƒç”¨ AI ä¿®å¤
- ä¿®å¤åè¿”å›å¯¹åº”çš„æ£€æŸ¥æ­¥éª¤

**ä¿®å¤æµç¨‹**:

```python
# 1. è¯»å–é”™è¯¯ä¿¡æ¯
regenerate_from = state.get('regenerate_from')  # step20_3 æˆ– step20_4

# 2. æ„å»ºä¿®å¤ prompt
prompt = f"""
æ£€æµ‹åˆ°ä»¥ä¸‹é—®é¢˜:
{error_description}

è¯·ä¿®å¤ä»£ç :
{code_snippet}
"""

# 3. è°ƒç”¨ AI ç”Ÿæˆä¿®å¤ä»£ç 
result = await call_subagent('crawler-developer', prompt)

# 4. è¿”å›æ£€æŸ¥æ­¥éª¤é‡æ–°éªŒè¯
return {
    'regenerate_from': regenerate_from  # å›åˆ° step20_3 æˆ– step20_4
}
```

**å¾ªç¯æ§åˆ¶**:

- æœ€å¤šé‡è¯• 3 æ¬¡
- è¶…è¿‡é™åˆ¶åˆ™æŠ¥è­¦å¹¶ç»ˆæ­¢

---

### 5.5 è¾…åŠ©è®¾è®¡æ¨¡å¼

#### 1. æ­¥éª¤æ—¥å¿—è£…é¥°å™¨ (`@step_logger`)

**è®¾è®¡æ€è·¯**:

- ç»Ÿä¸€è®°å½•æ­¥éª¤å¼€å§‹/ç»“æŸ
- è‡ªåŠ¨è®¡ç®—æ‰§è¡Œè€—æ—¶
- å¼‚å¸¸è‡ªåŠ¨æ•è·å’Œè®°å½•

**å®ç°**:

```python
def step_logger(func):
    @functools.wraps(func)
    async def wrapper(state, *args, **kwargs):
        # ä»å‡½æ•°åæå–æ­¥éª¤ä¿¡æ¯: step5_1__generate_fetch_rendered_html
        match = re.match(r'step(\d+)(?:_(\d+))?__(.+)', func.__name__)
        step_num = f"{match.group(1)}.{match.group(2)}" if match.group(2) else match.group(1)
        step_name = match.group(3).replace('_', ' ').title()

        logger.info(f"Step {step_num}: {step_name}")
        start_time = time.time()

        result = await func(state, *args, **kwargs)

        elapsed = time.time() - start_time
        logger.info(f"âœ… Step {step_num} å®Œæˆ (è€—æ—¶ {elapsed:.1f}s)")

        return result
    return wrapper
```

---

#### 2. Prompt æ¸²æŸ“ç³»ç»Ÿ

**è®¾è®¡æ€è·¯**:

- ä½¿ç”¨ Jinja2 æ¨¡æ¿ç®¡ç† prompts
- æ”¯æŒå˜é‡æ›¿æ¢å’Œé€»è¾‘æ§åˆ¶
- ç»Ÿä¸€ prompt é£æ ¼

**æ¨¡æ¿å˜é‡**:

```python
context = {
    'public_prompt': PUBLIC_PROMPT,  # å…¬å…±æç¤ºè¯
    'proj_root_dir': PROJ_ROOT_DIR,
    'site_name': state['site_name'],
    'category': state['category'],
    'base_file_path': state['base_file_path'],
    'output_dir': output_dir,
    'output_part_dir': output_part_dir,
    'tmp_folder': tmp_folder,
    'attention': attention,  # é¢å¤–æ³¨æ„äº‹é¡¹
    **kwargs  # æ­¥éª¤ç‰¹å®šå‚æ•°
}
```

**ä½¿ç”¨æ–¹å¼**:

```python
# 1. å®šä¹‰æ¨¡æ¿
_step1_tmpl = r'''{{ public_prompt }}

ç”¨ chrome-devtools è®¿é—®ä»¥ä¸‹ç½‘ç«™:
{{ url }}

ä¸­é—´æ–‡ä»¶å­˜æ”¾ä½ç½®:
{{ tmp_folder }}/
{{ attention }}

ä»»åŠ¡æ­¥éª¤:
1. ...
'''

# 2. å®šä¹‰å‚æ•°å‡½æ•°
def _step1_kwargs(state):
    return {
        'site_capitalize': state['site_name'].capitalize()
    }

# 3. æ¸²æŸ“ prompt
prompt = get_step_prompt('step1', state)
```

---

#### 3. è‡ªåŠ¨æäº¤æœºåˆ¶

**è®¾è®¡æ€è·¯**:

- æ¯ä¸ªæ­¥éª¤å®Œæˆåè‡ªåŠ¨ Git æäº¤
- ä¾¿äºç‰ˆæœ¬è¿½è¸ªå’Œå›æ»š
- æäº¤ä¿¡æ¯åŒ…å«æ­¥éª¤åå’Œæè¿°

**å®ç°**:

```python
def auto_commit_if_enabled(state, step_name, description, extra_files=None):
    if not conf.devbot.get('auto_commit', True):
        return

    file_path_list = [
        state["base_file_path"],  # ä»£ç æ–‡ä»¶
        str(output_file_dir),     # è¾“å‡ºç›®å½•
        str(attention_file)       # æ³¨æ„äº‹é¡¹æ–‡ä»¶
    ]

    if extra_files:
        file_path_list.extend(extra_files)

    auto_commit_generated_file(
        file_path_list=file_path_list,
        site_name=state['site_name'],
        step_name=step_name,
        description=description
    )
```

**æäº¤æ ¼å¼**:

```
[AUTO-GEN] {site_name} - {step_name}: {description}

ä¾‹å¦‚:
[AUTO-GEN] gnc - step2__generate_list_extractor: ç”Ÿæˆåˆ—è¡¨æå–å™¨ä»£ç 
```

---

#### 4. å¯¹è¯è®°å½•ä¿å­˜

**è®¾è®¡æ€è·¯**:

- ä¿å­˜æ¯æ¬¡ AI å¯¹è¯çš„ prompt å’Œ response
- ç”¨äºè°ƒè¯•å’Œå®¡è®¡
- æ”¯æŒé•¿æœŸè®°å¿†(Store)

**å®ç°**:

```python
save_conversation_from_state(
    state=state,
    prompt=prompt,
    response=response_text,
    node_name="step2_generate_list_extractor",
    metadata={"agent": "crawler-developer"}
)
```

**å­˜å‚¨ç»“æ„**:

```python
{
    'session_id': str,
    'node_name': str,
    'prompt': str,
    'response': str,
    'timestamp': datetime,
    'metadata': dict
}
```

---

#### 5. å¾ªç¯å¤„ç†æœºåˆ¶

**è®¾è®¡æ€è·¯**:

- æ”¯æŒå¤„ç†å¤šä¸ª URL patterns
- é˜Ÿåˆ—ç®¡ç†å’ŒçŠ¶æ€è·Ÿè¸ª
- æ¡ä»¶è·¯ç”±

**é˜Ÿåˆ—ç®¡ç†**:

```python
# åˆå§‹åŒ–é˜Ÿåˆ—
patterns_queue = build_patterns_queue(state)

# å¾ªç¯å¤„ç†
while patterns_queue:
    current_pattern = patterns_queue.pop(0)

    if current_pattern['type'] == 'detail':
        await process_detail_pattern(current_pattern)
    elif current_pattern['type'] == 'list':
        await process_list_pattern(current_pattern)

    completed_patterns.append(current_pattern['pattern'])
```

**LangGraph å®ç°**:

```python
# æ¡ä»¶è·¯ç”±å‡½æ•°
def route_by_pattern_type(state):
    patterns_queue = state.get("patterns_queue", [])

    if not patterns_queue:
        return "reviewer_step4"  # é˜Ÿåˆ—ä¸ºç©ºï¼Œè¿›å…¥å®¡æŸ¥

    current_pattern = patterns_queue[0]

    if current_pattern['type'] == 'detail':
        return "step5__generate_extractor_class"
    elif current_pattern['type'] == 'list':
        return "step6__generate_list_extractor"
```

---

### 5.6 é”™è¯¯å¤„ç†å’Œé‡è¯•

#### 1. SubagentError å¤„ç†

**è®¾è®¡æ€è·¯**:

- AI è°ƒç”¨å¯èƒ½å¤±è´¥(è¶…æ—¶ã€APIé”™è¯¯ç­‰)
- æ•è·å¼‚å¸¸å¹¶å‘é€å‘Šè­¦
- æ ‡è®°ä¸ºå·²å®Œæˆ(è·³è¿‡)ï¼Œç»§ç»­ä¸‹ä¸€ä¸ª

**å®ç°**:

```python
try:
    result = await call_subagent('crawler-developer', prompt)
except SubagentError as e:
    logger.error(f"âŒ Subagent å¼‚å¸¸: {e}")
    send_slack_message(
        f"Step6 æ‰§è¡Œå¤±è´¥\nç«™ç‚¹: {state['site_name']}\né”™è¯¯: {e}",
        title="DevBot Step6 å‘Šè­¦",
        level="error"
    )
    # æ ‡è®°ä¸ºå·²å®Œæˆ(è·³è¿‡)
    completed_patterns.append(current_pattern)
```

---

#### 2. é‡è¯•æœºåˆ¶

**è®¾è®¡æ€è·¯**:

- æŸäº›æ­¥éª¤å…è®¸é‡è¯•(å¦‚æˆªå›¾å¤±è´¥)
- æœ€å¤šé‡è¯•å›ºå®šæ¬¡æ•°
- é‡è¯•é—´éš”é€’å¢

**ç¤ºä¾‹**:

```python
max_retries = 5
retry_count = 0

while retry_count < max_retries:
    try:
        screenshot = await page.screenshot(full_page=True)
        break
    except Exception as e:
        retry_count += 1
        if 'DecompressionBombError' in str(e):
            logger.warning(f"æˆªå›¾å¼‚å¸¸ï¼Œé‡è¯• {retry_count}/{max_retries}")
            await page.close()
            await asyncio.sleep(5)
            page = await browser.new_page()
        else:
            raise
```

---

#### 3. éªŒè¯å’Œä¿®å¤å¾ªç¯

**è®¾è®¡æ€è·¯**:

- æ£€æµ‹ â†’ ä¿®å¤ â†’ é‡æ–°æ£€æµ‹
- æœ€å¤šå¾ªç¯å›ºå®šæ¬¡æ•°
- é˜²æ­¢æ— é™å¾ªç¯

**æµç¨‹**:

```
step20_3 æ£€æŸ¥æ–¹æ³•ç­¾å
    â†“ ä¸é€šè¿‡
step22 ä¿®å¤ä»£ç 
    â†“
step20_3 é‡æ–°æ£€æŸ¥
    â†“ é€šè¿‡
step20_4 æ£€æŸ¥ç¼ºå¤±æ•°æ®
```

---

### 5.7 çŠ¶æ€ç®¡ç†

#### CrawlerDevState ç»“æ„

**è®¾è®¡æ€è·¯**:

- TypedDict æä¾›ç±»å‹å®‰å…¨
- åŒ…å«æ‰€æœ‰æ­¥éª¤éœ€è¦çš„çŠ¶æ€ä¿¡æ¯
- æ”¯æŒåºåˆ—åŒ–åˆ° JSON

**æ ¸å¿ƒå­—æ®µ**:

```python
{
    # åŸºæœ¬ä¿¡æ¯
    'url': str,                    # ç›®æ ‡ç½‘ç«™ URL
    'site_name': str,              # ç«™ç‚¹å(å¦‚ gnc)
    'category': str,               # åˆ†ç±»(product/deal/shopping)
    'proxy': str,                  # ä»£ç†åœ°å€

    # æ­¥éª¤æ§åˆ¶
    'current_step': str,           # å½“å‰æ­¥éª¤å·(å¦‚ "5.1")
    'current_step_name': str,      # æ­¥éª¤å(å¦‚ "generate_fetch_rendered_html")
    'status': str,                 # çŠ¶æ€(pending/in_progress/completed/failed)
    'retry_count': int,            # é‡è¯•æ¬¡æ•°

    # ç»“æœå’Œé”™è¯¯
    'result': Any,                 # æ­¥éª¤ç»“æœ
    'validation_result': Any,      # éªŒè¯ç»“æœ
    'error': str,                  # é”™è¯¯ä¿¡æ¯

    # Pattern å¤„ç†
    'current_url_pattern': str,    # å½“å‰å¤„ç†çš„ pattern
    'current_sample_url': str,     # å½“å‰æ ·ä¾‹ URL
    'completed_patterns': list,    # å·²å®Œæˆçš„ patterns
    'patterns_queue': list,        # å¾…å¤„ç†çš„ patterns é˜Ÿåˆ—
    'current_pattern_info': dict,  # å½“å‰ pattern è¯¦ç»†ä¿¡æ¯

    # ç¼“å­˜å’Œä¸´æ—¶æ•°æ®
    'cache_data': dict,            # ç¼“å­˜æ•°æ®(å¦‚ site_tree)

    # Step 7 ä¸“ç”¨
    'has_new_patterns_in_step7': bool,  # æ˜¯å¦æœ‰æ–° patterns
    'step7_loop_count': int,            # Step 7 å¾ªç¯æ¬¡æ•°

    # æ–‡ä»¶è·¯å¾„
    'base_file_path': str,         # ä»£ç æ–‡ä»¶è·¯å¾„
    'output_dir': str,             # è¾“å‡ºç›®å½•
    'output_part_dir': str,        # éƒ¨åˆ†è¾“å‡ºç›®å½•

    # ä¼šè¯ç®¡ç†
    'session_id': str              # Claude AI ä¼šè¯ ID
}
```

---

#### çŠ¶æ€æŒä¹…åŒ–

**è®¾è®¡æ€è·¯**:

- æ¯ä¸ªèŠ‚ç‚¹æ‰§è¡Œåè‡ªåŠ¨ä¿å­˜çŠ¶æ€
- æ”¯æŒæ–­ç‚¹ç»­ä¼ 
- çŠ¶æ€æ–‡ä»¶ä¾¿äºäººå·¥æŸ¥çœ‹å’Œä¿®æ”¹

**ä¿å­˜ä½ç½®**:

- `crawler/{category}/local_state_{site_name}.json`

**æ–­ç‚¹ç»­ä¼ **:

```python
# åŠ è½½çŠ¶æ€
state = load_state()

if state is None:
    # ä»å¤´å¼€å§‹
    state = get_initial_state()
else:
    # ä»æ–­ç‚¹æ¢å¤
    logger.info(f"ä» {state['current_step_name']} æ¢å¤")

# æµå¼æ‰§è¡Œ
async for event in app.astream(state, config):
    # å¤„ç†èŠ‚ç‚¹
    save_state(event)
```

---

### 5.8 å·¥ä½œæµå¯è§†åŒ–

**è®¾è®¡æ€è·¯**:

- ç”Ÿæˆ Mermaid æµç¨‹å›¾
- å¯è§†åŒ–èŠ‚ç‚¹å’Œè¾¹
- ä¾¿äºç†è§£å·¥ä½œæµç»“æ„

**ç”Ÿæˆæ–¹å¼**:

```bash
python -m devbot.crawler_devbot --visualize product https://www.gnc.com
```

**è¾“å‡º**:

- `workflow_graph.mmd` - Mermaid æºç 
- `workflow_graph.png` - PNG å›¾ç‰‡(éœ€è¦ graphviz)

**æµç¨‹å›¾ç¤ºä¾‹**:

```mermaid
graph TD
    step0[Step 0: Create Base File] --> step1[Step 1: Analyze Page]
    step1 --> step2[Step 2: Generate List Extractor]
    step2 --> step2_1[Step 2.1: Classify URLs]
    step2_1 --> step3[Step 3: Generate URL Patterns]
    ...
```

---

### 5.9 æœ€ä½³å®è·µæ€»ç»“

#### 1. æç¤ºè¯å·¥ç¨‹

- **ç»“æ„åŒ–**: ä½¿ç”¨æ˜ç¡®çš„ç« èŠ‚æ ‡é¢˜å’Œç¼–å·
- **ç¤ºä¾‹é©±åŠ¨**: æä¾›å……åˆ†çš„ä»£ç ç¤ºä¾‹
- **éªŒè¯è¦æ±‚**: æ˜ç¡®æµ‹è¯•å‘½ä»¤å’ŒæœŸæœ›ç»“æœ
- **æ³¨æ„äº‹é¡¹**: çªå‡ºå…³é”®è¦æ±‚(å¦‚æ–‡ä»¶å¤§å°é™åˆ¶)

#### 2. æ¸è¿›å¼ç”Ÿæˆ

- **å°æ­¥å¿«è·‘**: æ¯ä¸ªæ­¥éª¤åªå®Œæˆä¸€ä¸ªå°ç›®æ ‡
- **åŠæ—¶éªŒè¯**: æ¯æ­¥éƒ½æœ‰æµ‹è¯•å’ŒéªŒè¯
- **å¿«é€Ÿåé¦ˆ**: å‘ç°é—®é¢˜ç«‹å³ä¿®å¤

#### 3. å¯è§‚æµ‹æ€§

- **è¯¦ç»†æ—¥å¿—**: è®°å½•æ¯ä¸ªæ­¥éª¤çš„è¾“å…¥è¾“å‡º
- **çŠ¶æ€æŒä¹…åŒ–**: éšæ—¶å¯æŸ¥çœ‹å½“å‰è¿›åº¦
- **å¯è§†åŒ–**: æµç¨‹å›¾ç›´è§‚å±•ç¤ºå·¥ä½œæµ
- **å‘Šè­¦æœºåˆ¶**: å…³é”®é”™è¯¯å‘é€ Slack é€šçŸ¥

#### 4. å®¹é”™è®¾è®¡

- **å¼‚å¸¸æ•è·**: æ¯ä¸ªæ­¥éª¤éƒ½æœ‰å¼‚å¸¸å¤„ç†
- **ä¼˜é›…é™çº§**: å¤±è´¥æ—¶è·³è¿‡è€Œä¸æ˜¯å´©æºƒ
- **é‡è¯•æœºåˆ¶**: ä¸´æ—¶æ€§é”™è¯¯è‡ªåŠ¨é‡è¯•
- **äººå·¥å¹²é¢„**: çŠ¶æ€æ–‡ä»¶å¯æ‰‹åŠ¨ä¿®æ”¹

#### 5. æ¨¡å—åŒ–è®¾è®¡

- **å•ä¸€èŒè´£**: æ¯ä¸ªæ­¥éª¤åªåšä¸€ä»¶äº‹
- **è§£è€¦åˆ**: æ­¥éª¤ä¹‹é—´é€šè¿‡çŠ¶æ€ä¼ é€’æ•°æ®
- **å¯å¤ç”¨**: å…¬å…±é€»è¾‘æå–ä¸ºå‡½æ•°æˆ–è£…é¥°å™¨
- **å¯æ‰©å±•**: æ–°å¢æ­¥éª¤ä¸å½±å“ç°æœ‰æµç¨‹

---

### 5.10 æ€»ç»“

DevBot Developer Nodes é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„å¤šæ­¥éª¤æµç¨‹ï¼Œå®ç°äº†çˆ¬è™«ä»£ç çš„è‡ªåŠ¨åŒ–ç”Ÿæˆï¼š

1. **åŸºç¡€æ­å»º** (Step 0-1): åˆ›å»ºæ¡†æ¶å’Œåˆ†æé¡µé¢
2. **URL å¤„ç†** (Step 2-3): æå–å’Œåˆ†ç±» URL patterns
3. **å¾ªç¯ç”Ÿæˆ** (Step 4-6): ä¸ºæ¯ä¸ª pattern ç”Ÿæˆæå–ä»£ç 
4. **æ·±åº¦æ‰©å±•** (Step 7): æ‰©å±• URL æ ‘å‘ç°æ–° patterns
5. **ç³»ç»Ÿé›†æˆ** (Step 8-10): å®Œå–„åŠŸèƒ½å’Œè°ƒåº¦é…ç½®
6. **è´¨é‡ä¿è¯** (Step 20-22): æ£€æŸ¥å’Œä¼˜åŒ–ä»£ç 

æ•´ä¸ªç³»ç»Ÿå……åˆ†åˆ©ç”¨äº† LangGraph çš„çŠ¶æ€ç®¡ç†å’Œæ¡ä»¶è·¯ç”±èƒ½åŠ›ï¼Œç»“åˆ Claude AI çš„ä»£ç ç”Ÿæˆèƒ½åŠ›ï¼Œå®ç°äº†é«˜åº¦è‡ªåŠ¨åŒ–çš„çˆ¬è™«å¼€å‘æµç¨‹ã€‚

## 6. Reviewer éªŒè¯æœºåˆ¶

### 6.1 Reviewer èŠ‚ç‚¹åˆ—è¡¨

DevBotå…±æœ‰**6ä¸ªReviewerèŠ‚ç‚¹**:

| Reviewer       | å¯¹åº”æ­¥éª¤ | éªŒè¯å†…å®¹               | éªŒè¯æ–¹å¼                                 |
| -------------- | -------- | ---------------------- | ---------------------------------------- |
| review_step0   | Step 0   | åŸºç¡€æ–‡ä»¶å¯å¯¼å…¥         | importlib.import_module                  |
| review_step1   | Step 1   | å¼•æ“å’Œå¹¶å‘é…ç½®å·²è®¾ç½®   | æ£€æŸ¥ä»£ç å†…å®¹                             |
| review_step2   | Step 2   | åˆ—è¡¨æå–å™¨è¿”å›æ ¼å¼æ­£ç¡® | æ‰§è¡Œæ–¹æ³•å¹¶éªŒè¯è¿”å›å€¼                     |
| review_step2_1 | Step 2.1 | site_patternsæ ¼å¼æ­£ç¡®  | è¯»å–JSONå¹¶éªŒè¯å­—æ®µ                       |
| review_step3   | Step 3   | URL_MAPæ ¼å¼æ­£ç¡®        | å¯¼å…¥æ¨¡å—å¹¶éªŒè¯ç»“æ„                       |
| review_step4   | Step 4   | æ ¸å¿ƒæå–å™¨å·²å®ç°       | æ£€æŸ¥URL_MAPä¸­çš„funcå­—æ®µ                  |
| review_step7   | Step 7   | ç½‘ç«™æ ‘æ‰©å±•åŠŸèƒ½å­˜åœ¨     | æ£€æŸ¥expand_one_level_from_list_pagesæ–¹æ³• |
| review_step8   | Step 8   | çŠ¶æ€æ­£ç¡®               | æ£€æŸ¥current_stepå’Œstatus                 |

### 6.2 Reviewer å·¥ä½œæµç¨‹

```
Developer èŠ‚ç‚¹å®Œæˆ
    â†“ status="completed"
    â†“
Reviewer èŠ‚ç‚¹éªŒè¯
    â”œâ”€ å¯¼å…¥æ¨¡å—
    â”œâ”€ æ‰§è¡Œæµ‹è¯•å‡½æ•°
    â”œâ”€ éªŒè¯è¿”å›å€¼/æ–‡ä»¶æ ¼å¼
    â””â”€ åˆ¤æ–­æ˜¯å¦é€šè¿‡
         â”œâ”€ é€šè¿‡ â†’ status="reviewed"
         â””â”€ å¤±è´¥ â†’ status="failed", retry_count++
```

### 6.3 éªŒè¯ç¤ºä¾‹ï¼šreview_step2

```python
# reviewer_nodes.py:208-321
async def review_step2(state: CrawlerDevState) -> CrawlerDevState:
    """éªŒè¯ Step 2: æ£€æŸ¥åˆ—è¡¨æå–å™¨æ˜¯å¦æ­£ç¡®å®ç°"""

    category = state["category"]
    site_name = state["site_name"]

    try:
        # 1. å¯¼å…¥æ¨¡å—
        module_path = f'crawler.{category}.extractor_{site_name}'
        if module_path in sys.modules:
            del sys.modules[module_path]

        module = importlib.import_module(module_path)

        # 2. è°ƒç”¨ extract_deals_from_mainpage
        extract_func = getattr(module, 'extract_deals_from_mainpage')
        result = await extract_func()

        # 3. éªŒè¯è¿”å›ç±»å‹
        if not isinstance(result, dict):
            raise ValueError(f"åº”è¿”å› dictï¼Œå®é™…è¿”å›: {type(result).__name__}")

        # 4. éªŒè¯æ˜¯å¦åŒ…å« urls å­—æ®µ
        if 'urls' not in result:
            raise ValueError("è¿”å›ç»“æœç¼ºå°‘ 'urls' å­—æ®µ")

        # 5. éªŒè¯ urls æ˜¯æ•°ç»„
        urls = result['urls']
        if not isinstance(urls, list):
            raise ValueError(f"urls åº”ä¸º listï¼Œå®é™…ä¸º: {type(urls).__name__}")

        # 6. éªŒè¯ urls æ•°æ®æ ¼å¼
        for i, item in enumerate(urls[:3]):  # åªæ£€æŸ¥å‰3ä¸ª
            if not isinstance(item, dict):
                raise ValueError(f"urls[{i}] åº”ä¸º dict")

            # æ£€æŸ¥å¿…éœ€å­—æ®µ
            required_fields = ['title', 'url', 'type']
            for field in required_fields:
                if field not in item:
                    raise ValueError(f"urls[{i}] ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")

            # æ£€æŸ¥ type å­—æ®µå€¼
            if item['type'] not in ['detail', 'list', 'other', 'unclear']:
                raise ValueError(f"urls[{i}] type å­—æ®µå€¼é”™è¯¯: {item['type']}")

        logger.info("âœ… Step2 éªŒè¯é€šè¿‡")

    except Exception as e:
        # å‘é€ Slack å‘Šè­¦
        send_slack_exception(e, context=f"Review Step2 - {site_name}")
        # æŠ›å‡ºå¼‚å¸¸ï¼Œè®© LangGraph ç»ˆæ­¢æµç¨‹
        raise

    # éªŒè¯æˆåŠŸ
    return {
        **state,
        "status": "reviewed",
        "validation_result": {
            "step": "step2",
            "success": True,
            "message": f"åˆ—è¡¨æå–å™¨å®ç°æ­£ç¡®ï¼ŒæˆåŠŸæå– {len(urls)} ä¸ªé“¾æ¥"
        }
    }
```

### 6.4 å¼‚å¸¸å¤„ç†ç­–ç•¥

**æŠ€æœ¯å¼‚å¸¸** (ä»£ç é”™è¯¯ã€å¯¼å…¥å¤±è´¥):

- å‘é€ Slack å‘Šè­¦
- æŠ›å‡ºå¼‚å¸¸ï¼Œç»ˆæ­¢æµç¨‹
- éœ€è¦äººå·¥ä»‹å…¥ä¿®å¤

**ä¸šåŠ¡å¼‚å¸¸** (è¿”å›å€¼æ ¼å¼é”™è¯¯ã€æ•°æ®ä¸å®Œæ•´):

- è®°å½•é”™è¯¯åˆ° state["error"]
- æ ‡è®° status="failed"
- è§¦å‘é‡è¯•æœºåˆ¶

---

## 7. å·¥ä½œæµè·¯ç”±ä¸æ§åˆ¶

### 7.1 è·¯ç”±å†³ç­–å‡½æ•°

**routing_logic.py** å®šä¹‰äº†æ‰€æœ‰çš„è·¯ç”±è§„åˆ™ï¼š

```python
# ä¸»è·¯ç”±å‡½æ•°
def route_after_developer(state: CrawlerDevState) -> str:
    """DeveloperèŠ‚ç‚¹å®Œæˆåçš„è·¯ç”±å†³ç­–"""
    current_step = state["current_step"]
    status = state["status"]

    # å¦‚æœå¤±è´¥æˆ–éœ€è¦reviewï¼Œè¿›å…¥reviewer
    if status in ["completed", "failed"]:
        # æ£€æŸ¥æ˜¯å¦æœ‰å¯¹åº”çš„reviewer
        if has_reviewer_for_step(current_step):
            return f"review_step{current_step.replace('.', '_')}"

    # å¦åˆ™ç›´æ¥è¿›å…¥ä¸‹ä¸€ä¸ªdeveloper
    return next_developer_step(current_step)

def route_after_reviewer(state: CrawlerDevState) -> str:
    """ReviewerèŠ‚ç‚¹å®Œæˆåçš„è·¯ç”±å†³ç­–"""
    status = state["status"]
    retry_count = state["retry_count"]

    if status == "reviewed":
        # é€šè¿‡éªŒè¯ï¼Œè¿›å…¥ä¸‹ä¸€æ­¥
        return next_developer_step(state["current_step"])
    elif status == "failed":
        # å¤±è´¥ï¼Œæ£€æŸ¥é‡è¯•æ¬¡æ•°
        if retry_count < 3:
            # é‡è¯•ï¼šè¿”å›å½“å‰developer
            return f"step{state['current_step'].replace('.', '_')}"
        else:
            # è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œç»ˆæ­¢
            raise Exception(f"Step {state['current_step']} å¤±è´¥æ¬¡æ•°è¿‡å¤š")
    else:
        # å…¶ä»–çŠ¶æ€ï¼Œç»ˆæ­¢
        raise Exception(f"æœªçŸ¥çŠ¶æ€: {status}")
```

### 7.2 æ¡ä»¶è¾¹ (Conditional Edges)

LangGraphä½¿ç”¨**æ¡ä»¶è¾¹**å®ç°è·¯ç”±ï¼š

```python
# crawler_devbot.py: æ„å»ºworkflow
workflow = StateGraph(CrawlerDevState)

# æ·»åŠ èŠ‚ç‚¹
workflow.add_node("step0", step0__create_base_file)
workflow.add_node("review_step0", review_step0)
workflow.add_node("step1", step1__analyze_page)
# ... æ›´å¤šèŠ‚ç‚¹

# æ·»åŠ æ¡ä»¶è¾¹
workflow.add_conditional_edges(
    "step0",  # æºèŠ‚ç‚¹
    route_after_developer,  # è·¯ç”±å‡½æ•°
    {
        "review_step0": "review_step0",  # å¦‚æœè¿”å›"review_step0"ï¼Œè·³è½¬åˆ°review_step0èŠ‚ç‚¹
        "step1": "step1"  # å¦‚æœè¿”å›"step1"ï¼Œè·³è½¬åˆ°step1èŠ‚ç‚¹
    }
)

workflow.add_conditional_edges(
    "review_step0",
    route_after_reviewer,
    {
        "step0": "step0",  # é‡è¯•
        "step1": "step1"  # ä¸‹ä¸€æ­¥
    }
)

# ... æ›´å¤šæ¡ä»¶è¾¹
```

### 7.3 å¾ªç¯æ£€æµ‹ä¸ç»ˆæ­¢

**é—®é¢˜**: å¦‚ä½•é˜²æ­¢æ— é™å¾ªç¯ï¼Ÿ

**è§£å†³æ–¹æ¡ˆ**:

1. **é‡è¯•æ¬¡æ•°é™åˆ¶**: retry_count < 3
2. **Step 7å¾ªç¯é™åˆ¶**: step7_loop_count < 3
3. **Patterné˜Ÿåˆ—ç©ºæ£€æµ‹**: len(patterns_queue) == 0
4. **æ‰‹åŠ¨ç»ˆæ­¢**: ç”¨æˆ·å¯ä»¥éšæ—¶ä¸­æ–­

---

## 8. é”™è¯¯å¤„ç†ä¸é‡è¯•æœºåˆ¶

### 8.1 å¼‚å¸¸ç±»å‹

DevBotå®šä¹‰äº†4ç§è‡ªå®šä¹‰å¼‚å¸¸ï¼š

```python
# claude_agent_base.py:21-43
class SubagentError(Exception):
    """Subagent æ‰§è¡Œé”™è¯¯åŸºç±»"""
    pass

class PromptError(SubagentError):
    """Prompt æœ¬èº«æœ‰é—®é¢˜ï¼ˆä¿¡æ¯ç¼ºå¤±ã€å·¥å…·æœªæˆæƒã€ä»£ç æ‰§è¡Œå¤±è´¥ç­‰ï¼‰"""
    pass

class TaskUnachievableError(SubagentError):
    """ä»»åŠ¡æ— æ³•è¾¾æˆï¼ˆæŠ€æœ¯å—é™ã€å¤šæ¬¡å¤±è´¥ã€å¤–éƒ¨ä¾èµ–é—®é¢˜ç­‰ï¼‰"""
    pass

class HumanInterventionRequired(SubagentError):
    """éœ€è¦äººå·¥ä»‹å…¥ï¼ˆé€šç”¨ï¼‰"""
    pass
```

### 8.2 é‡è¯•ç­–ç•¥

**è‡ªåŠ¨é‡è¯•**:

- DeveloperèŠ‚ç‚¹å¤±è´¥ â†’ Revieweræ£€æµ‹ â†’ retry_count++ â†’ é‡æ–°æ‰§è¡ŒDeveloper
- æœ€å¤šé‡è¯•3æ¬¡

**æ‰‹åŠ¨é‡è¯•**:

- ç”¨æˆ·å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œå‚æ•° `--entry <node>` ä»æŒ‡å®šèŠ‚ç‚¹é‡æ–°å¼€å§‹

**æ™ºèƒ½é‡è¯•**:

- Step 22ä¿®å¤ä»£ç åï¼Œå¯ä»¥è·³å›åˆ° `regenerate_from` æŒ‡å®šçš„æ­¥éª¤ï¼ˆé€šå¸¸æ˜¯Step 9ï¼‰

### 8.3 Slackå‘Šè­¦

**è§¦å‘æ¡ä»¶**:

- Revieweræ£€æµ‹åˆ°æŠ€æœ¯å¼‚å¸¸
- å¯¼å…¥æ¨¡å—å¤±è´¥
- ä»£ç æ‰§è¡ŒæŠ¥é”™

**å‘Šè­¦å†…å®¹**:

```python
send_slack_exception(
    exception=e,
    context=f"Review Step2 - {site_name}"
)

# Slackæ¶ˆæ¯ç¤ºä¾‹
"""
ğŸš¨ DevBot å¼‚å¸¸å‘Šè­¦

æ­¥éª¤: Review Step2 - gnc
å¼‚å¸¸ç±»å‹: ImportError
å¼‚å¸¸ä¿¡æ¯: No module named 'crawler.product.extractor_gnc'
å †æ ˆ: ...
æ—¶é—´: 2025-01-20 14:30:00
"""
```

---

## 9. å­˜å‚¨ä¸æŒä¹…åŒ–

### 9.1 çŠ¶æ€æŒä¹…åŒ– (MemorySaver)

**LangGraph MemorySaver** è‡ªåŠ¨å°†çŠ¶æ€ä¿å­˜åˆ°JSONæ–‡ä»¶ï¼š

```python
# crawler_devbot.py: æ„å»ºapp
from langgraph.checkpoint.memory import MemorySaver

memory = MemorySaver()
app = workflow.compile(checkpointer=memory)

# æ‰§è¡Œæ—¶æŒ‡å®šthread_id
config = {"configurable": {"thread_id": f"{site_name}_crawler"}}
result = await app.ainvoke(initial_state, config=config)
```

**ä¿å­˜ä½ç½®**:

```
crawler/product/local_state_<site>.json
```

**å†…å®¹ç¤ºä¾‹**:

```json
{
  "url": "https://www.gnc.com",
  "site_name": "gnc",
  "category": "product",
  "current_step": "5.2",
  "current_step_name": "generate_remove_site_chrome",
  "status": "completed",
  "session_id": "session_abc123",
  "patterns_queue": [...],
  "completed_patterns": [...],
  "retry_count": 0
}
```

**æ–­ç‚¹æ¢å¤**:

```bash
# è‡ªåŠ¨ä»ä¸Šæ¬¡æ–­ç‚¹æ¢å¤
python -m devbot.crawler_devbot product https://www.gnc.com

# å¼ºåˆ¶é‡æ–°å¼€å§‹
python -m devbot.crawler_devbot product https://www.gnc.com --reset
```

### 9.2 å¯¹è¯å†å² (ConversationStore)

**SQLiteæ•°æ®åº“** å­˜å‚¨æ‰€æœ‰å¯¹è¯è®°å½•ï¼š

```sql
-- devbot/store/conversation_store.py
CREATE TABLE conversations (
    id INTEGER PRIMARY KEY,
    site_name TEXT NOT NULL,
    category TEXT NOT NULL,
    step_name TEXT NOT NULL,
    prompt TEXT NOT NULL,
    response TEXT NOT NULL,
    metadata TEXT,
    timestamp TEXT NOT NULL,
    thread_id TEXT NOT NULL
)
```

**ä¿å­˜æ—¶æœº**:

```python
# developer_nodes.py: æ¯ä¸ªæ­¥éª¤å®Œæˆå
save_conversation_from_state(
    state=state,
    prompt=prompt,
    response=response_text,
    node_name="step1_analyze_page_structure",
    metadata={"agent": "crawler-developer"}
)
```

**æŸ¥è¯¢ç¤ºä¾‹**:

```python
# è·å–æŸä¸ªç«™ç‚¹çš„æ‰€æœ‰å¯¹è¯
conversations = ConversationStore.get_by_site("gnc", "product")

# è·å–æŸä¸ªæ­¥éª¤çš„å¯¹è¯
step1_conv = ConversationStore.get_by_step("gnc", "product", "step1_analyze_page_structure")
```

### 9.3 Gitç‰ˆæœ¬æ§åˆ¶

**è‡ªåŠ¨æäº¤**:

DevBotä¼šåœ¨æ¯ä¸ªå…³é”®æ­¥éª¤åè‡ªåŠ¨æäº¤ä»£ç åˆ°Gitï¼š

```python
# utils/git_utils.py
def auto_commit_if_enabled(state: CrawlerDevState, step_name: str, message: str):
    if not conf.enable_auto_commit:
        return

    site_name = state["site_name"]
    category = state["category"]

    # Git add
    file_path = state["base_file_path"]
    os.system(f"git add {file_path}")

    # Git commit
    commit_msg = f"AUTO-GEN[{category}/{site_name}] {step_name}: {message}"
    os.system(f'git commit -m "{commit_msg}"')

    logger.info(f"âœ… Gitæäº¤: {commit_msg}")
```

**æäº¤å†å²ç¤ºä¾‹**:

```bash
git log --grep="AUTO-GEN" --grep="gnc" --oneline

a1b2c3d AUTO-GEN[product/gnc] step5_2__generate_remove_site_chrome: ç”Ÿæˆremove_site_chromeæ–¹æ³•
d4e5f6g AUTO-GEN[product/gnc] step5_1__generate_fetch_rendered_html: ç”Ÿæˆfetch_rendered_htmlæ–¹æ³•
h7i8j9k AUTO-GEN[product/gnc] step5__generate_extractor_class: ç”Ÿæˆè¯¦æƒ…é¡µæå–å™¨ç±»
...
```

**å›æ»š**:

```bash
# å›æ»šåˆ°æŸä¸ªæ­¥éª¤ä¹‹å‰
git reset --hard <commit_hash>

# ä»è¯¥æ­¥éª¤é‡æ–°å¼€å§‹
python -m devbot.crawler_devbot product https://www.gnc.com --entry step5_2
```

---

## 10. é¢è¯•é«˜é¢‘é—®é¢˜

### 10.1 æ¶æ„è®¾è®¡é—®é¢˜

**Q1: ä¸ºä»€ä¹ˆé€‰æ‹©LangGraphè€Œä¸æ˜¯ç›´æ¥ç”¨LLMï¼Ÿ**

A: LangGraphæä¾›äº†ä»¥ä¸‹æ ¸å¿ƒèƒ½åŠ›ï¼š

1. **çŠ¶æ€ç®¡ç†**: TypedDictè‡ªåŠ¨åºåˆ—åŒ–/ååºåˆ—åŒ–ï¼Œæ–­ç‚¹æ¢å¤
2. **æµç¨‹æ§åˆ¶**: æ¡ä»¶è¾¹å®ç°å¤æ‚çš„è·¯ç”±é€»è¾‘
3. **å¯è§‚æµ‹æ€§**: æ¯ä¸ªèŠ‚ç‚¹çš„æ‰§è¡Œéƒ½æœ‰æ—¥å¿—å’Œè¿½è¸ª
4. **å¯æ‰©å±•æ€§**: è½»æ¾æ·»åŠ æ–°èŠ‚ç‚¹å’ŒéªŒè¯é€»è¾‘

**Q2: Developer + RevieweråŒèŠ‚ç‚¹è®¾è®¡çš„ä¼˜åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ**

A:

- **è´¨é‡ä¿è¯**: Developerç”Ÿæˆä»£ç ï¼ŒReviewerè‡ªåŠ¨éªŒè¯ï¼Œé¿å…ä½çº§é”™è¯¯
- **å¿«é€Ÿè¿­ä»£**: éªŒè¯å¤±è´¥ç«‹å³é‡è¯•ï¼Œæ— éœ€äººå·¥ä»‹å…¥
- **å¯è¿½æº¯**: æ¯æ¬¡éªŒè¯çš„ç»“æœéƒ½è®°å½•åœ¨stateä¸­ï¼Œä¾¿äºè°ƒè¯•

**Q3: ä¸ºä»€ä¹ˆç”¨å…¨å±€å­—å…¸ç®¡ç†Claudeå®¢æˆ·ç«¯ï¼Ÿ**

A:

- **Sessionå¤ç”¨**: åŒä¸€agentçš„å¤šæ¬¡è°ƒç”¨å…±äº«ä¸Šä¸‹æ–‡ï¼ŒLLMèƒ½"è®°ä½"ä¹‹å‰çš„åˆ†æ
- **èµ„æºä¼˜åŒ–**: é¿å…é‡å¤åˆ›å»ºå®¢æˆ·ç«¯ï¼Œå‡å°‘åˆå§‹åŒ–å¼€é”€
- **å¹¶å‘å®‰å…¨**: å­—å…¸æ˜¯Pythonå†…ç½®ç±»å‹ï¼Œçº¿ç¨‹å®‰å…¨

### 10.2 æŠ€æœ¯ç»†èŠ‚é—®é¢˜

**Q4: å¦‚ä½•å¤„ç†åçˆ¬è™«ï¼Ÿ**

A: ä¸‰å±‚ç­–ç•¥ï¼š

1. **æ£€æµ‹**: Step 1 promptè¦æ±‚LLMåˆ¤æ–­æ˜¯å¦è¢«æ‹¦æˆª
2. **ç»•è¿‡**: ä½¿ç”¨BrightDataä»£ç†è·å–é¡µé¢HTML
3. **æœ¬åœ°è®¿é—®**: é€šè¿‡ `file://` åè®®åœ¨chrome-devtoolsä¸­æ‰“å¼€æœ¬åœ°HTMLæ–‡ä»¶

**Q5: å¦‚ä½•ä¿è¯å›¾ç‰‡ä¸è¶…è¿‡Claude APIé™åˆ¶ï¼Ÿ**

A: è‡ªåŠ¨å‹ç¼©å·¥å…· `compress_image`ï¼š

- **å¤§å°é™åˆ¶**: è¶…è¿‡2MB â†’ WebPæ ¼å¼ï¼Œè´¨é‡85%
- **åƒç´ é™åˆ¶**: è¶…è¿‡8000px â†’ åˆ‡åˆ†ä¸ºå¤šå¼ å›¾ç‰‡
- **é‡è¯•æœºåˆ¶**: DecompressionBombError â†’ å…³é—­é¡µé¢é‡æ–°æˆªå›¾ï¼ˆæœ€å¤š5æ¬¡ï¼‰

**Q6: Patterné˜Ÿåˆ—å¦‚ä½•å®ç°å¾ªç¯ï¼Ÿ**

A: é˜Ÿåˆ—é©±åŠ¨çš„çŠ¶æ€æœºï¼š

```
Step 4: patterns_queue = [p1, p2, p3]
Step 4.1: current = p1, queue = [p2, p3]
Step 5.x: å¤„ç† p1
Step 4.1: current = p2, queue = [p3]
Step 5.x: å¤„ç† p2
Step 4.1: current = p3, queue = []
Step 5.x: å¤„ç† p3
Step 4.1: queueä¸ºç©º â†’ è·³åˆ°Step 7
```

### 10.3 ä¼˜åŒ–ç­–ç•¥é—®é¢˜

**Q7: å¦‚ä½•ä¼˜åŒ–DevBotçš„æ‰§è¡Œæ•ˆç‡ï¼Ÿ**

A: å½“å‰ä¼˜åŒ–å’Œæœªæ¥æ”¹è¿›ï¼š

- **å½“å‰**:
  - Sessionå¤ç”¨ï¼šå‡å°‘LLMåˆå§‹åŒ–å¼€é”€
  - BrightDataæ‰¹é‡çˆ¬å–ï¼š20ä¸ªURLå¹¶å‘è·å–ï¼Œå¿«10å€
  - Gitè‡ªåŠ¨æäº¤ï¼šæ¯æ­¥éƒ½ä¿å­˜ï¼Œæ”¯æŒæ–­ç‚¹æ¢å¤
- **æœªæ¥**:
  - å¹¶è¡Œæ‰§è¡Œå¤šä¸ªpatternçš„ç”Ÿæˆï¼ˆç›®å‰æ˜¯ä¸²è¡Œï¼‰
  - ç¼“å­˜å¸¸è§ç½‘ç«™çš„åˆ†æç»“æœ
  - ä½¿ç”¨Geminiæ›¿ä»£éƒ¨åˆ†Claudeè°ƒç”¨ï¼ˆé™ä½æˆæœ¬ï¼‰

**Q8: å¦‚ä½•å¤„ç†LLMçš„å¹»è§‰é—®é¢˜ï¼Ÿ**

A: å¤šå±‚éªŒè¯æœºåˆ¶ï¼š

1. **Promptçº¦æŸ**: æä¾›è¯¦ç»†çš„æ ¼å¼è¦æ±‚å’Œç¤ºä¾‹
2. **RevieweréªŒè¯**: è‡ªåŠ¨æ‰§è¡Œä»£ç å¹¶æ£€æŸ¥è¿”å›å€¼
3. **é‡è¯•æœºåˆ¶**: éªŒè¯å¤±è´¥è‡ªåŠ¨é‡è¯•ï¼Œæœ€å¤š3æ¬¡
4. **äººå·¥ä»‹å…¥**: è¶…è¿‡é‡è¯•æ¬¡æ•°åï¼Œå‘é€Slackå‘Šè­¦

### 10.4 å®æˆ˜ç»éªŒé—®é¢˜

**Q9: å¼€å‘è¿‡ç¨‹ä¸­é‡åˆ°çš„æœ€å¤§æŒ‘æˆ˜æ˜¯ä»€ä¹ˆï¼Ÿ**

A: **Step 5å¾ªç¯çš„è®¾è®¡**ï¼š

- **æŒ‘æˆ˜**: éœ€è¦ä¸ºæ¯ä¸ªURL patternç”Ÿæˆç‹¬ç«‹çš„æå–å™¨ï¼Œä½†patternæ•°é‡ä¸ç¡®å®š
- **è§£å†³**: å¼•å…¥patterns_queueï¼Œç”¨é˜Ÿåˆ—é©±åŠ¨çš„å¾ªç¯ç»“æ„
- **ä¼˜åŒ–**: æ”¯æŒè·³è¿‡æœºåˆ¶ï¼ˆå¦‚æœLLMåˆ¤æ–­sample URLä¸æ˜¯ç›®æ ‡é¡µé¢ï¼‰
- **æ•™è®­**: å¤æ‚é€»è¾‘éœ€è¦æ¸…æ™°çš„çŠ¶æ€è®¾è®¡å’Œè·¯ç”±è§„åˆ™

**Q10: å¦‚ä½•ä¿è¯ç”Ÿæˆä»£ç çš„è´¨é‡ï¼Ÿ**

A: è´¨é‡ä¿è¯ä½“ç³»ï¼š

1. **æ¨¡æ¿è§„èŒƒ**: Jinja2æ¨¡æ¿å®šä¹‰ä»£ç éª¨æ¶
2. **Promptå·¥ç¨‹**: è¯¦ç»†çš„ä»»åŠ¡æè¿°ã€å‚è€ƒä»£ç ã€éªŒè¯æ ‡å‡†
3. **è‡ªåŠ¨æµ‹è¯•**: ReviewerèŠ‚ç‚¹æ‰§è¡Œä»£ç å¹¶éªŒè¯è¾“å‡º
4. **äººå·¥å®¡æŸ¥**: æœ€ç»ˆä»£ç é€šè¿‡Gitæäº¤è®°å½•å¯è¿½æº¯
5. **æŒç»­ä¼˜åŒ–**: Step 20-22è¿›è¡Œä»£ç æ£€æŸ¥å’Œæ€§èƒ½ä¼˜åŒ–

---

## æ€»ç»“

### æ ¸å¿ƒäº®ç‚¹

1. **ä¸‰å±‚æ¶æ„**: Developer â†’ Reviewer â†’ Stateï¼Œæ¸…æ™°çš„èŒè´£åˆ†ç¦»
2. **æ™ºèƒ½å†³ç­–**: LLMåˆ†æé¡µé¢ç»“æ„ï¼Œè‡ªåŠ¨é€‰æ‹©å¼•æ“å’Œå‚æ•°
3. **è´¨é‡ä¿è¯**: åŒèŠ‚ç‚¹éªŒè¯ï¼Œè‡ªåŠ¨é‡è¯•ï¼ŒSlackå‘Šè­¦
4. **æ–­ç‚¹æ¢å¤**: MemorySaver + Gitï¼Œæ”¯æŒéšæ—¶ä¸­æ–­å’Œæ¢å¤
5. **å¯æ‰©å±•æ€§**: è½»æ¾æ·»åŠ æ–°æ­¥éª¤ã€æ–°éªŒè¯è§„åˆ™
6. **å·¥å…·é›†æˆ**: MCPå·¥å…· + Claude SDKå†…ç½®å·¥å…· + è‡ªå®šä¹‰Pythonå·¥å…·
7. **å…¨æµç¨‹è‡ªåŠ¨åŒ–**: ä»URLåˆ°å¯è¿è¡Œä»£ç ï¼Œæ— éœ€äººå·¥ç¼–å†™

### æŠ€æœ¯æ ˆ

| ç±»åˆ«           | æŠ€æœ¯                                          |
| -------------- | --------------------------------------------- |
| **å·¥ä½œæµå¼•æ“** | LangGraph, StateGraph                         |
| **LLM**        | Claude Agent SDK (Sonnet 4.5)                 |
| **å·¥å…·åè®®**   | MCP (chrome-devtools, gemini-cli, playwright) |
| **çŠ¶æ€ç®¡ç†**   | TypedDict, MemorySaver                        |
| **å­˜å‚¨**       | SQLite (å¯¹è¯å†å²), JSON (çŠ¶æ€å¿«ç…§)            |
| **ç‰ˆæœ¬æ§åˆ¶**   | Git (è‡ªåŠ¨æäº¤)                                |
| **å‘Šè­¦**       | Slack (å¼‚å¸¸é€šçŸ¥)                              |
| **æ¨¡æ¿**       | Jinja2 (ä»£ç ç”Ÿæˆ)                             |

### é¢è¯•å‡†å¤‡è¦ç‚¹

**å¿…é¡»æŒæ¡çš„5ä¸ªé—®é¢˜**:

1. LangGraphçš„çŠ¶æ€ç®¡ç†æœºåˆ¶ï¼ˆTypedDict + MemorySaverï¼‰
2. Developer + RevieweråŒèŠ‚ç‚¹è®¾è®¡çš„ä¼˜åŠ¿
3. Patterné˜Ÿåˆ—å¾ªç¯çš„å®ç°åŸç†
4. Claude Agent SDKçš„Hookæœºåˆ¶ï¼ˆauto_approveï¼‰
5. åçˆ¬è™«å¤„ç†ç­–ç•¥ï¼ˆBrightData + æœ¬åœ°è®¿é—®ï¼‰

**å‡†å¤‡Demoæ¼”ç¤º**:

1. è¿è¡Œä¸€æ¬¡å®Œæ•´çš„crawler_devbotæµç¨‹
2. å±•ç¤ºæ–­ç‚¹æ¢å¤åŠŸèƒ½ï¼ˆ--entryå‚æ•°ï¼‰
3. æŸ¥çœ‹Gitæäº¤å†å²å’Œå¯¹è¯è®°å½•
4. æ¼”ç¤ºRevieweréªŒè¯å¤±è´¥åçš„é‡è¯•

**å‡†å¤‡æŠ€æœ¯æ¡ˆä¾‹**:

1. å¦‚ä½•å¤„ç†æˆªå›¾è¿‡å¤§é—®é¢˜ï¼ˆcompress_imageå·¥å…·ï¼‰
2. Sessionå¤ç”¨çš„å®ç°ï¼ˆå…¨å±€å®¢æˆ·ç«¯å­—å…¸ï¼‰
3. Step 7å¾ªç¯æ§åˆ¶çš„é˜²æ— é™å¾ªç¯æœºåˆ¶
4. Gitè‡ªåŠ¨æäº¤çš„ç‰ˆæœ¬ç®¡ç†ç­–ç•¥

---

## 11. WebScraper çˆ¬è™«é¡¹ç›®æ¶æ„

### 11.1 é¡¹ç›®æ¦‚è¿°

**WebScraper** æ˜¯ä¸€ä¸ªå¤šé¢†åŸŸç”µå•†æ•°æ®çˆ¬å–å¹³å°ï¼Œé‡‡ç”¨ä¸‰å±‚æ¶æ„è®¾è®¡ï¼Œæ”¯æŒProductã€Dealã€Shoppingç­‰å¤šç§ç±»åˆ«çš„æ•°æ®æŠ“å–ã€‚

**æ ¸å¿ƒä»·å€¼**:
1. **é€šç”¨æ€§å¼º**: åŸºäºæ¨¡æ¿æ–¹æ³•æ¨¡å¼ï¼Œå¿«é€Ÿé€‚é…æ–°ç½‘ç«™
2. **å¹¶å‘èƒ½åŠ›é«˜**: Browser Pool + BrightDataæ‰¹é‡çˆ¬å–
3. **æ‰©å±•æ€§å¥½**: Mixinæ¨¡å¼æä¾›å¯é€‰åŠŸèƒ½ï¼Œä¸ä¾µå…¥æ ¸å¿ƒé€»è¾‘
4. **å¯ç»´æŠ¤æ€§é«˜**: ä¸‰å±‚è§£è€¦ï¼ŒèŒè´£æ¸…æ™°

### 11.2 ä¸‰å±‚æ¶æ„è®¾è®¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  extractor_scheduler.py (è°ƒåº¦ç¼–æ’å±‚)                      â”‚
â”‚  - BFSéå†URLæ ‘                                          â”‚
â”‚  - åŠ¨æ€åŠ è½½ç«™ç‚¹æ¨¡å—                                        â”‚
â”‚  - å¹¶å‘ä»»åŠ¡è°ƒåº¦                                           â”‚
â”‚  - TracePageæ•°æ®åº“ç®¡ç†                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚ è°ƒç”¨
                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  extractor_<site>.py (ç«™ç‚¹é€‚é…å±‚)                         â”‚
â”‚  - URL_MAP è·¯ç”±è§„åˆ™                                      â”‚
â”‚  - åˆ—è¡¨é¡µæå–å‡½æ•°                                         â”‚
â”‚  - è¯¦æƒ…é¡µExtractorç±»                                     â”‚
â”‚  - CONCURRENT_CONFIG å¹¶å‘é…ç½®                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚ ç»§æ‰¿/ä½¿ç”¨
                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  extractor_base.py (åŸºç¡€è®¾æ–½å±‚)                           â”‚
â”‚  - BaseExtractor åŸºç±»                                    â”‚
â”‚  - ProductDetailMixin (å•†å“è¯¦æƒ…å¤„ç†)                      â”‚
â”‚  - BrowserPool (Playwrightè¿æ¥æ± )                       â”‚
â”‚  - PageParam (å‚æ•°å°è£…)                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 11.2.1 åŸºç¡€è®¾æ–½å±‚ (extractor_base.py)

**æ ¸å¿ƒç»„ä»¶**:

| ç»„ä»¶ | èŒè´£ | å…³é”®æ–¹æ³• |
|------|------|---------|
| `BaseExtractor` | æå–å™¨åŸºç±» | `fetch_html()`, `clean_html()`, `extract_text_as_markdown()` |
| `BrowserPool` | æµè§ˆå™¨æ± ç®¡ç† | `initialize()`, `get_page()`, `cleanup()` |
| `ProductDetailMixin` | å•†å“è¯¦æƒ…å¤„ç† | `post_save_callback()`, `_save_product_origin()` |
| `PageParam` | å‚æ•°å°è£… | url, html_content, extract_by_llm |

**BrowserPoolè®¾è®¡äº®ç‚¹**:

```python
class BrowserPool:
    """Playwrightæµè§ˆå™¨è¿æ¥æ±  - ä¼˜åŒ–å¹¶å‘æ€§èƒ½"""
    def __init__(self, pool_size=3, tab_size=5):
        self.pool_size = pool_size      # 3ä¸ªæµè§ˆå™¨å®ä¾‹
        self.tab_size = tab_size        # æ¯ä¸ª5ä¸ªtab (å…±15å¹¶å‘)
        self.available_tabs = asyncio.Queue()  # å¼‚æ­¥é˜Ÿåˆ—ç®¡ç†

    async def get_page(self):
        """è·å–å¯ç”¨tab (ä¸Šä¸‹æ–‡ç®¡ç†å™¨)"""
        tab_info = await self.available_tabs.get()
        try:
            yield tab_info['page']
        finally:
            # æ¸…ç†å¹¶å½’è¿˜tab
            await page.evaluate("() => { localStorage.clear(); }")
            await self.available_tabs.put(tab_info)
```

**è®¾è®¡äº®ç‚¹**:
- **å¼‚æ­¥é˜Ÿåˆ—**: ä½¿ç”¨ `asyncio.Queue` ç®¡ç†tabï¼Œè‡ªåŠ¨é˜»å¡ç­‰å¾…
- **è‡ªåŠ¨æ¸…ç†**: å½’è¿˜tabå‰æ¸…é™¤localStorageï¼Œé¿å…çŠ¶æ€æ±¡æŸ“
- **åæ£€æµ‹å¢å¼º**: é›†æˆ `anti_detection` æ¨¡å—ï¼Œä¿®æ”¹æµè§ˆå™¨æŒ‡çº¹

#### 11.2.2 ç«™ç‚¹é€‚é…å±‚ (extractor_\<site\>.py)

**URL_MAPè·¯ç”±è®¾è®¡**:

```python
URL_MAP = {
    'main_page': {
        'patterns': [r'https://www\.gnc\.com$'],
        'sample_urls': ['https://www.gnc.com'],
        'func': extract_deals_from_mainpage,
        'action': 'get_list_info'
    },
    'category_page': {
        'patterns': [r'https://www\.gnc\.com/[^/]+/$'],
        'func': extract_deals_from_category,
        'action': 'get_list_info'
    },
    'detail_page': {
        'patterns': [r'https://www\.gnc\.com/.*\.html$'],
        'func': extract_product_detail,
        'action': 'get_detail_info'
    }
}
```

**åˆ—è¡¨é¡µæå–å‡½æ•°**:

```python
async def extract_deals_from_mainpage(page: PageParam) -> dict:
    """ä»ä¸»é¡µæå–å•†å“/æ´»åŠ¨åˆ—è¡¨"""
    extractor = GncListExtractor(page)
    async with extractor.browser_pool.get_page() as pw_page:
        # ä½¿ç”¨BrightDataæ‰¹é‡è·å–HTMLï¼ˆåŠ é€Ÿï¼‰
        if 'brightdata' in extractor.engine:
            html_content = await extractor.get_html_content_by_brightdata(url)
            await pw_page.set_content(html_content)
        else:
            await pw_page.goto(url)

        # æå–URL
        urls = await pw_page.evaluate("""() => {
            return Array.from(document.querySelectorAll('a.product-link'))
                .map(a => ({url: a.href, title: a.textContent, type: 'detail'}));
        }""")

        return {'urls': urls, 'site_name': 'gnc', ...}
```

**è¯¦æƒ…é¡µæå–ç±»**:

```python
class GncDetailExtractor(BaseExtractor, ProductDetailMixin):
    """GNCå•†å“è¯¦æƒ…é¡µæå–å™¨"""

    async def fetch_html(self):
        """è·å–åŸå§‹HTML"""
        async with self.browser_pool.get_page() as page:
            await page.goto(self.url)
            return await page.content()

    def clean_html(self, html: str) -> str:
        """æ¸…æ´—HTML - ç«™ç‚¹ç‰¹å®šé€»è¾‘"""
        soup = BeautifulSoup(html, 'html.parser')
        # ç§»é™¤å¯¼èˆªæ ã€é¡µè„šç­‰æ— å…³å†…å®¹
        for tag in soup.select('.header, .footer, .ads'):
            tag.decompose()
        return str(soup.select_one('.product-detail'))

    def extract_text_as_markdown(self, cleaned_html: str) -> str:
        """è½¬æ¢ä¸ºMarkdown - è°ƒç”¨LLM"""
        # ä½¿ç”¨Gemini/Claudeæå–ç»“æ„åŒ–ä¿¡æ¯
        return llm_extract_markdown(cleaned_html)
```

#### 11.2.3 è°ƒåº¦ç¼–æ’å±‚ (extractor_scheduler.py)

**SiteScheduleræ ¸å¿ƒé€»è¾‘**:

```python
class SiteScheduler:
    """ç«™ç‚¹çˆ¬å–è°ƒåº¦å™¨ - BFSéå†URLæ ‘"""

    async def analyze(self, url, parent=None):
        """å¹¿åº¦ä¼˜å…ˆéå†"""
        queue = deque([(url, parent, 0)])  # (url, parent_id, level)
        visited = set()

        while queue:
            current_url, parent_id, level = queue.popleft()
            if level > self.max_level or current_url in visited:
                continue

            # åŠ¨æ€åŠ è½½ç«™ç‚¹æ¨¡å—
            module = importlib.import_module(f'crawler.{category}.extractor_{site_name}')

            # åŒ¹é…URL_MAPè·¯ç”±
            matched = self._match_url_pattern(current_url, module.URL_MAP)
            if not matched:
                continue

            # è°ƒç”¨å¤„ç†å‡½æ•°
            if matched['action'] == 'get_list_info':
                result = await matched['func'](PageParam(url=current_url))
                # å°†å­URLå…¥é˜Ÿ
                for item in result['urls']:
                    queue.append((item['url'], trace_page_id, level+1))

            elif matched['action'] == 'get_detail_info':
                # è¯¦æƒ…é¡µï¼šæå–å¹¶ä¿å­˜æ•°æ®
                await matched['func'](PageParam(url=current_url))

            visited.add(current_url)
```

**å¹¶å‘ä¼˜åŒ–**:

```python
async def process_level_urls_concurrent(self, urls, parent, level):
    """å¹¶å‘å¤„ç†åŒå±‚çº§URL"""
    # 1. BrightDataæ‰¹é‡è·å–HTML
    if self.enable_brightdata:
        html_list = await self.bd_client.batch_fetch(urls[:20])
        tasks = [self.process_one_url(url, html, parent, level)
                 for url, html in zip(urls, html_list)]
    else:
        tasks = [self.process_one_url(url, None, parent, level) for url in urls]

    # 2. å¹¶å‘æ‰§è¡Œï¼ˆä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°ï¼‰
    sem = asyncio.Semaphore(15)  # æœ€å¤š15ä¸ªå¹¶å‘
    async def wrapped(task):
        async with sem:
            return await task

    return await asyncio.gather(*[wrapped(t) for t in tasks])
```

### 11.3 å…³é”®è®¾è®¡æ¨¡å¼

#### 11.3.1 æ¨¡æ¿æ–¹æ³•æ¨¡å¼

```python
class BaseExtractor:
    """å®šä¹‰æå–æµç¨‹éª¨æ¶"""
    async def process(self, url):
        # 1. è·å–HTML
        html = await self.fetch_html(url)

        # 2. æ¸…æ´—HTMLï¼ˆå­ç±»é‡å†™ï¼‰
        cleaned = self.clean_html(html)

        # 3. æå–æ–‡æœ¬ï¼ˆå­ç±»é‡å†™ï¼‰
        markdown = self.extract_text_as_markdown(cleaned)

        # 4. ä¿å­˜æ•°æ®
        await self.save(markdown)

        # 5. å›è°ƒé’©å­ï¼ˆMixinæä¾›ï¼‰
        await self.post_save_callback()
```

#### 11.3.2 Mixinæ¨¡å¼

```python
class ProductDetailMixin:
    """å•†å“è¯¦æƒ…å¤„ç†èƒ½åŠ› - å¯é€‰ç»„åˆ"""
    async def post_save_callback(self):
        """ä¿å­˜åå›è°ƒ"""
        # 1. ä¿å­˜ProductOrigin
        await self._save_product_origin()

        # 2. æäº¤OCRä»»åŠ¡ï¼ˆå¦‚æœæœ‰å›¾ç‰‡ï¼‰
        if self.image_urls:
            await self._submit_ocr_tasks()

        # 3. ç«™ç‚¹ç‰¹å®šé€»è¾‘ï¼ˆå¯é€‰ï¼‰
        await self._post_process_hook()

    def _post_process_hook(self):
        """æ‰©å±•ç‚¹ - å­ç±»å¯é‡å†™"""
        pass
```

#### 11.3.3 ç­–ç•¥æ¨¡å¼

```python
# ä¸åŒå¼•æ“ç­–ç•¥
ENGINES = {
    'browser_pool': BrowserPoolEngine,
    'brightdata': BrightDataEngine,
    'brightdata+browser_pool': HybridEngine
}

class BaseExtractor:
    engine = 'brightdata+browser_pool'  # å­ç±»å¯é…ç½®

    async def fetch_html(self, url):
        engine = ENGINES[self.engine]()
        return await engine.fetch(url)
```

### 11.4 å¹¶å‘æ€§èƒ½ä¼˜åŒ–

**å¯¹æ¯”ï¼šä¼ ç»Ÿæ–¹å¼ vs ä¼˜åŒ–æ–¹å¼**

| ç»´åº¦ | ä¼ ç»Ÿæ–¹å¼ | ä¼˜åŒ–æ–¹å¼ | æå‡ |
|------|---------|---------|------|
| HTMLè·å– | é€ä¸ªæ‰“å¼€æµè§ˆå™¨ | BrightDataæ‰¹é‡çˆ¬å– | 10å€ |
| æµè§ˆå™¨å®ä¾‹ | æ¯æ¬¡æ–°å»º | BrowserPoolå¤ç”¨ | 5å€ |
| å¹¶å‘æ§åˆ¶ | æ— æ§åˆ¶ï¼Œæ˜“å´©æºƒ | Semaphore + Queue | ç¨³å®š |
| å†…å­˜å ç”¨ | éšä»»åŠ¡å¢é•¿ | å›ºå®š15ä¸ªtab | 80%â†“ |

**CONCURRENT_CONFIGé…ç½®**:

```python
CONCURRENT_CONFIG = {
    'pool_size': 3,               # 3ä¸ªæµè§ˆå™¨å®ä¾‹
    'tab_size': 5,                # æ¯ä¸ª5ä¸ªtab (å…±15å¹¶å‘)
    'delay_between_requests': 0.5, # è¯·æ±‚é—´éš”0.5ç§’
    'use_brightdata': True,       # ä½¿ç”¨BrightDataæ‰¹é‡çˆ¬å–
    'brightdata_batch_size': 20   # æ‰¹é‡å¤§å°20
}
```

### 11.5 æŠ€æœ¯éš¾ç‚¹ä¸è§£å†³æ–¹æ¡ˆ

#### éš¾ç‚¹1: åçˆ¬è™«æ£€æµ‹

**é—®é¢˜**:
- Cloudflareã€PerimeterXç­‰æ£€æµ‹Playwright
- User-Agentã€CanvasæŒ‡çº¹è¯†åˆ«

**è§£å†³æ–¹æ¡ˆ**:

```python
# devbot/html_servers/anti_detection.py
def get_browser_launch_args():
    """åæ£€æµ‹å¯åŠ¨å‚æ•°"""
    return [
        '--disable-blink-features=AutomationControlled',  # éšè—automationæ ‡å¿—
        '--disable-dev-shm-usage',
        '--no-sandbox',
        '--disable-setuid-sandbox',
        '--disable-web-security',
        '--disable-features=IsolateOrigins,site-per-process',
        '--allow-running-insecure-content',
        '--disable-webgl',  # ç¦ç”¨WebGLæŒ‡çº¹
        '--disable-canvas-fingerprinting',  # ç¦ç”¨CanvasæŒ‡çº¹
    ]

async def setup_page_anti_detection(page, user_agent=None):
    """é¡µé¢çº§åæ£€æµ‹"""
    # 1. ä¿®æ”¹navigatorå±æ€§
    await page.evaluate("""() => {
        Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
        Object.defineProperty(navigator, 'plugins', {get: () => [1, 2, 3]});
    }""")

    # 2. æ³¨å…¥çœŸå®Chromeè¿è¡Œæ—¶
    await page.add_init_script("""
        Object.defineProperty(navigator, 'languages', {
            get: () => ['en-US', 'en']
        });
    """)

    # 3. éšæœºåŒ–æŒ‡çº¹
    await page.evaluate(f"""() => {{
        const getParameter = WebGLRenderingContext.prototype.getParameter;
        WebGLRenderingContext.prototype.getParameter = function(parameter) {{
            if (parameter === 37445) return 'Intel Inc.';  # ä¼ªé€ æ˜¾å¡å‚å•†
            return getParameter.apply(this, arguments);
        }};
    }}""")
```

**User-Agent è½®æ¢ (User-Agent Rotation)**

User-Agent (UA) æ˜¯ HTTP è¯·æ±‚å¤´çš„ä¸€éƒ¨åˆ†ï¼Œç”¨äºå‘Šè¯‰æœåŠ¡å™¨â€œæˆ‘æ˜¯è°â€ã€‚å®ƒåŒ…å«äº†æµè§ˆå™¨ç±»å‹ã€ç‰ˆæœ¬ã€æ“ä½œç³»ç»Ÿç­‰ä¿¡æ¯ã€‚

ä¸ºä»€ä¹ˆè¦è½®æ¢ï¼Ÿ

è§„é¿å°é”ï¼š å¦‚æœä¸€ä¸ª IP åœ°å€åœ¨çŸ­æ—¶é—´å†…ä½¿ç”¨ç›¸åŒçš„ UA å‘é€å¤§é‡è¯·æ±‚ï¼ŒæœåŠ¡å™¨å¾ˆå®¹æ˜“å°†å…¶è¯†åˆ«ä¸ºæœºå™¨äººå¹¶è¿›è¡Œå°é”ã€‚

- ä¼ªè£…èº«ä»½ï¼š è®¸å¤šç½‘ç«™å¯¹ Googlebot æˆ–ç§»åŠ¨ç«¯è®¾å¤‡æœ‰ä¸åŒçš„å±•ç¤ºç­–ç•¥ï¼ˆä¾‹å¦‚æ— éœ€ç™»å½•å³å¯æŸ¥çœ‹å†…å®¹ï¼‰ï¼Œé€šè¿‡ä¼ªé€  UA å¯ä»¥è·å–è¿™äº›ç‰¹æƒã€‚

* æ ¸å¿ƒæŠ€æœ¯ç‚¹ï¼š

1. UA æ±  (UA Pool)ï¼š ç»´æŠ¤ä¸€ä¸ªåºå¤§çš„ã€çœŸå®çš„ UA åˆ—è¡¨ï¼ˆåŒ…å« Chrome, Firefox, Safari, iOS, Android ç­‰ï¼‰ã€‚
2. éšæœºåŒ–ç­–ç•¥ï¼š æ¯æ¬¡è¯·æ±‚éšæœºæŠ½å–ä¸€ä¸ª UAã€‚
3. UA-å¹³å°ä¸€è‡´æ€§ (é‡è¦)ï¼šé”™è¯¯ç¤ºèŒƒï¼š UA å£°æ˜è‡ªå·±æ˜¯ "iPhone/Safari"ï¼Œä½†åç»­çš„ TCP/IP æ¡æ‰‹ç‰¹å¾æˆ– JavaScript ç¯å¢ƒæ£€æµ‹ï¼ˆå¦‚ navigator.platformï¼‰å´æ˜¾ç¤ºæ˜¯ "Windows"ã€‚è¿™ä¼šç›´æ¥è§¦å‘é£æ§ã€‚è¿›é˜¶æŠ€æœ¯ï¼š ç°ä»£åçˆ¬è™«ä¸ä»…çœ‹ User-Agent å­—ç¬¦ä¸²ï¼Œè¿˜ä¼šçœ‹ Client Hints (Sec-CH-UA)ã€‚å¦‚æœä½ åªæ”¹äº† UA å­—ç¬¦ä¸²ï¼Œæ²¡æ”¹ Client Hintsï¼Œä¼šè¢«è½»æ˜“è¯†ç ´ã€‚

å¸¸ç”¨å·¥å…·ï¼š

- Python: fake-useragent åº“ã€‚
- Scrapy: scrapy-user-agents ä¸­é—´ä»¶ã€‚
- æŒ‡çº¹æµè§ˆå™¨ï¼ˆAdsPower ç­‰ï¼‰è‡ªåŠ¨å¤„ç†ã€‚

**æµè§ˆå™¨æŒ‡çº¹ä¼ªè£… (Browser Fingerprinting Spoofing)**

æµè§ˆå™¨æŒ‡çº¹æ˜¯ç”±äºç¡¬ä»¶ï¼ˆæ˜¾å¡ã€å±å¹•ï¼‰ã€è½¯ä»¶ï¼ˆæ“ä½œç³»ç»Ÿã€å­—ä½“ã€é©±åŠ¨ï¼‰å’Œæµè§ˆå™¨è®¾ç½®çš„ä¸åŒï¼Œåœ¨æ‰§è¡Œç‰¹å®š JavaScript ä»£ç æ—¶äº§ç”Ÿçš„å”¯ä¸€æ ‡è¯†ã€‚å³ä½¿ä½ æ¸…é™¤äº† Cookie å’Œ IPï¼ŒæŒ‡çº¹ä¾ç„¶å¯ä»¥è¿½è¸ªä½ ã€‚

å¸¸è§çš„æŒ‡çº¹ç»´åº¦åŠä¼ªè£…æ‰‹æ®µ**ï¼š**

1. Canvas æŒ‡çº¹ (Canvas Fingerprinting)ï¼šåŸç†ï¼šæµè§ˆå™¨åˆ©ç”¨ HTML5 Canvas ç»˜åˆ¶ç‰¹å®šçš„å›¾å½¢å’Œæ–‡å­—ã€‚ç”±äºæ˜¾å¡æ¸²æŸ“å·®å¼‚ï¼Œä¸åŒè®¾å¤‡ç”Ÿæˆçš„åƒç´ æ•°æ®ï¼ˆBase64ï¼‰æ˜¯ä¸åŒçš„ä¸”å›ºå®šçš„ã€‚ä¼ªè£…ï¼š Canvas Noiseï¼ˆæ³¨å…¥å™ªå£°ï¼‰ã€‚æ‹¦æˆª toDataURL æˆ– getImageData æ–¹æ³•ï¼Œåœ¨è¿”å›çš„åƒç´ æ•°æ®ä¸­éšæœºä¿®æ”¹å‡ ä¸ªåƒç´ çš„å€¼ï¼ˆäººçœ¼çœ‹ä¸å‡ºï¼Œä½†å“ˆå¸Œå€¼å˜äº†ï¼‰ã€‚
2. WebGL æŒ‡çº¹ï¼šåŸç†ï¼š è·å–æ˜¾å¡å‹å·ï¼ˆå¦‚ Google SwiftShader é€šå¸¸æ„å‘³ç€æ˜¯æ— å¤´æµè§ˆå™¨ï¼‰å’Œæ¸²æŸ“èƒ½åŠ›ã€‚ä¼ªè£…**ï¼š** è¦†å†™ getParameter æ–¹æ³•ï¼Œä¼ªé€ æ˜¾å¡å‹å·ï¼ˆå¦‚æ”¹ä¸º NVIDIA GeForce GTX 3060ï¼‰ã€‚
3. AudioContext æŒ‡çº¹ï¼šåŸç†ï¼šå¤„ç†éŸ³é¢‘ä¿¡å·æ—¶çš„æ³¢å½¢å·®å¼‚ã€‚ä¼ªè£…ï¼š ç±»ä¼¼ Canvasï¼ŒåŠ å…¥å¾®å°çš„éšæœºå™ªå£°ã€‚
4. ç‰¹å¾æ£€æµ‹ (Feature Detection)ï¼šWebdriver å±æ€§ï¼š è‡ªåŠ¨åŒ–å·¥å…·é€šå¸¸ä¼šæœ‰ navigator.webdriver = trueã€‚ä¼ªè£…æŠ€æœ¯ä¼šå°†å…¶åˆ é™¤æˆ–è®¾ä¸º falseã€‚Headless æ£€æµ‹ï¼š æ— å¤´æµè§ˆå™¨ï¼ˆHeadless Chromeï¼‰åœ¨ User-Agentã€è¯­è¨€è®¾ç½®ã€å±å¹•åˆ†è¾¨ç‡ä¸Šå¾€å¾€æœ‰é»˜è®¤ç‰¹å¾ï¼ˆå¦‚åˆ†è¾¨ç‡ 800x600ï¼‰ã€‚ä¼ªè£…éœ€è¦å°†è¿™äº›å‚æ•°è°ƒæ•´ä¸ºå¸¸è§æ¡Œé¢æ ‡å‡†ï¼ˆå¦‚ 1920x1080ï¼‰ã€‚

å¸¸ç”¨å·¥å…·ï¼š

- Puppeteer**-**extra-plugin-stealth / Playwright-stealthï¼š æœ€æµè¡Œçš„å¼€æºåº“ï¼Œç”¨äºè‡ªåŠ¨æŠ¹é™¤è‡ªåŠ¨åŒ–ç‰¹å¾ã€‚
- æŒ‡çº¹æµè§ˆå™¨ï¼š å•†ä¸šè½¯ä»¶ï¼Œåº•å±‚ä¿®æ”¹ Chromium å†…æ ¸ï¼Œå®ç°ç‰©ç†éš”ç¦»çº§åˆ«çš„æŒ‡çº¹ä¼ªè£…ã€‚

**CSP ç§»é™¤ (CSP Removal)**

CSP (Content Security Policyï¼Œå†…å®¹å®‰å…¨ç­–ç•¥) æ˜¯ç½‘é¡µé€šè¿‡ HTTP å“åº”å¤´å‘Šè¯‰æµè§ˆå™¨ï¼šâ€œåªå…è®¸åŠ è½½æ¥è‡ªåŸŸå A å’Œ B çš„è„šæœ¬ï¼Œå…¶ä»–éƒ½æ‹¦æˆªâ€ã€‚è¿™æ˜¯ä¸€ç§å®‰å…¨æœºåˆ¶ï¼Œç”¨äºé˜²æ­¢ XSS æ”»å‡»ã€‚

ä¸ºä»€ä¹ˆè¦ç§»é™¤ CSPï¼Ÿ

åœ¨çˆ¬è™«æˆ–è‡ªåŠ¨åŒ–æµ‹è¯•åœºæ™¯ä¸­ï¼Œä½ å¯èƒ½éœ€è¦å‘é¡µé¢æ³¨å…¥è‡ªå·±çš„ JavaScript ä»£ç ï¼ˆPayloadï¼‰æ¥å®ç°ä»¥ä¸‹åŠŸèƒ½ï¼š

- è‡ªåŠ¨æ“ä½œï¼š ç‚¹å‡»æŒ‰é’®ã€å¡«å†™è¡¨å•ã€‚
- æ•°æ®æå–ï¼š éå† DOM æ ‘æå–æ•°æ®ã€‚
- Hook å‡½æ•°ï¼š æ‹¦æˆªè¯·æ±‚ç­¾åç”Ÿæˆé€»è¾‘ã€‚

å¦‚æœç›®æ ‡ç½‘ç«™å¼€å¯äº†ä¸¥æ ¼çš„ CSPï¼ˆä¾‹å¦‚ script-src 'self'ï¼‰ï¼Œä½ æ³¨å…¥çš„ JS ä»£ç ï¼ˆé€šå¸¸è¢«è§†ä¸º inline scriptï¼‰ä¼šè¢«æµè§ˆå™¨æ‹¦æˆªï¼Œå¯¼è‡´æ— æ³•æ‰§è¡Œã€‚

å¦‚ä½•å®ç°ï¼Ÿ

è¿™é€šå¸¸åœ¨ä¸­é—´äººï¼ˆMITMï¼‰å±‚é¢æˆ–æµè§ˆå™¨è‡ªåŠ¨åŒ–æ§åˆ¶å±‚é¢å®Œæˆï¼š

1. ä½¿ç”¨ Puppeteer**/**Playwrightè¯·æ±‚æ‹¦æˆªï¼š
   ä½ å¯ä»¥å¼€å¯è¯·æ±‚æ‹¦æˆªåŠŸèƒ½ï¼Œåœ¨æµè§ˆå™¨æ¥æ”¶åˆ°å“åº”å¤´ï¼ˆResponse Headersï¼‰ä½†å°šæœªæ¸²æŸ“é¡µé¢æ—¶ï¼Œå°† Content-Security-Policy å¤´åˆ é™¤æˆ–ä¿®æ”¹ä¸ºå®½æ¾ç­–ç•¥ã€‚codeJavaScript`// Puppeteer ç¤ºä¾‹æ€è·¯ await page.setRequestInterception(true); page.on('response', response => {    const headers = response.headers();    // åˆ é™¤ CSP å¤´    delete headers['content-security-policy'];    delete headers['content-security-policy-report-only'];    // ç»§ç»­å¤„ç†... });`*(æ³¨ï¼šPuppeteer åŸç”Ÿä¿®æ”¹ Response Headers æ¯”è¾ƒéº»çƒ¦ï¼Œé€šå¸¸é…åˆ CDP Session æˆ– puppeteer-extra-plugin-anonymize-ua ç­‰ç±»ä¼¼æ’ä»¶å®ç°)*
2. ä½¿ç”¨ä»£ç†å·¥å…· (Mitmproxy / Fiddler)ï¼š
   åœ¨æµé‡ç»è¿‡ä»£ç†æœåŠ¡å™¨æ—¶ï¼Œç¼–å†™è„šæœ¬è‡ªåŠ¨å‰¥ç¦» Response ä¸­çš„ CSP å¤´éƒ¨ã€‚
3. æµè§ˆå™¨æ’ä»¶ï¼š
   æœ‰ä¸€äº› Chrome æ’ä»¶ï¼ˆå¦‚ Disable Content-Security-Policyï¼‰å¯ä»¥ç›´æ¥åœ¨æµè§ˆå™¨ç«¯ç¦ç”¨è¯¥ç­–ç•¥ï¼ˆä¸»è¦ç”¨äºè°ƒè¯•ï¼‰ã€‚

#### éš¾ç‚¹2: åŠ¨æ€å†…å®¹åŠ è½½

**é—®é¢˜**:
- JavaScriptæ¸²æŸ“çš„å†…å®¹
- æ‡’åŠ è½½å›¾ç‰‡
- æ— é™æ»šåŠ¨åˆ—è¡¨

**è§£å†³æ–¹æ¡ˆ**:

```python
async def fetch_rendered_html(self, url):
    """ç­‰å¾…åŠ¨æ€å†…å®¹åŠ è½½"""
    async with self.browser_pool.get_page() as page:
        await page.goto(url, wait_until='networkidle')  # ç­‰å¾…ç½‘ç»œç©ºé—²

        # ç­‰å¾…å…³é”®å…ƒç´ å‡ºç°
        await page.wait_for_selector('.product-info', timeout=10000)

        # æ»šåŠ¨åˆ°åº•éƒ¨ï¼Œè§¦å‘æ‡’åŠ è½½
        await page.evaluate("""async () => {
            await new Promise(resolve => {
                let totalHeight = 0;
                const distance = 100;
                const timer = setInterval(() => {
                    window.scrollBy(0, distance);
                    totalHeight += distance;
                    if (totalHeight >= document.body.scrollHeight) {
                        clearInterval(timer);
                        resolve();
                    }
                }, 100);
            });
        }""")

        # ç­‰å¾…å›¾ç‰‡åŠ è½½
        await page.wait_for_load_state('domcontentloaded')
        await page.wait_for_timeout(2000)

        return await page.content()
```

#### éš¾ç‚¹3: URLå»é‡ä¸å¢é‡çˆ¬å–

**é—®é¢˜**:
- é‡å¤URLå¯¼è‡´é‡å¤çˆ¬å–
- å¢é‡æ›´æ–°æ—¶éœ€è·³è¿‡å·²çˆ¬å–URL

**è§£å†³æ–¹æ¡ˆ**:

```python
class SiteScheduler:
    def __init__(self):
        self.visited_urls = set()  # å†…å­˜å»é‡
        self.db_visited = set()    # æ•°æ®åº“å·²çˆ¬URL

    async def is_url_processed(self, url):
        """æ£€æŸ¥URLæ˜¯å¦å·²å¤„ç†"""
        # 1. å†…å­˜å¿«é€ŸæŸ¥æ‰¾
        if url in self.visited_urls:
            return True

        # 2. æ•°æ®åº“æŸ¥è¯¢ï¼ˆç¼“å­˜ç»“æœï¼‰
        if url in self.db_visited:
            return True

        # 3. æŸ¥è¯¢TracePageè¡¨
        trace_page = await TracePage.objects(url=url).first()
        if trace_page:
            self.db_visited.add(url)
            return True

        return False
```

### 11.6 é›†æˆAsyncPipeline

**çˆ¬è™«ä¸Pipelineé›†æˆæµç¨‹**:

```
Crawler (extractor_scheduler.py)
    â†“ ä¿å­˜å•†å“è¯¦æƒ…
ProductDetailMixin.post_save_callback()
    â†“ HTTP POST
AsyncPipeline API (http://localhost:8000/api/v1/tasks/ocr)
    â†“ RabbitMQ
OCR Worker â†’ LLM Worker â†’ DB Worker
    â†“
ProductBaseModel (MongoDB)
```

**ä»£ç ç¤ºä¾‹**:

```python
# crawler/product/util.py
class ProductDetailMixin:
    async def _submit_to_async_pipeline(self):
        """æäº¤åˆ°å¼‚æ­¥ç®¡é“"""
        import httpx

        async with httpx.AsyncClient() as client:
            response = await client.post(
                "http://localhost:8000/api/v1/tasks/ocr",
                json={
                    "product_origin_id": str(self.product_origin_id),
                    "trace_page_id": str(self.trace_page_id),
                    "image_urls": self.image_urls,
                    "screenshot_url": self.screenshot_url,
                    "prompt": "Extract product information from images",
                    "run_version": "v1.0",
                    "site_name": self.site_name,
                    "source_url": self.url
                },
                timeout=30.0
            )

            task_id = response.json()["task_id"]
            logger.info(f"âœ… Submitted to AsyncPipeline: {task_id}")
```

---

## 12. AsyncPipeline å¼‚æ­¥ä»»åŠ¡å¤„ç†ç®¡é“

### 12.1 é¡¹ç›®æ¦‚è¿°

**AsyncPipeline** æ˜¯ä¸€ä¸ªåŸºäº RabbitMQ çš„é«˜æ€§èƒ½å¼‚æ­¥å¤„ç†ç³»ç»Ÿï¼Œæä¾›å®Œæ•´çš„ **OCR â†’ LLM â†’ DB** æµæ°´çº¿ã€‚

**æ ¸å¿ƒç‰¹æ€§**:
1. **å¼‚æ­¥è§£è€¦**: çˆ¬è™«ä¸æ•°æ®å¤„ç†å®Œå…¨åˆ†ç¦»ï¼Œäº’ä¸é˜»å¡
2. **æ¶ˆæ¯é˜Ÿåˆ—**: RabbitMQä¿è¯ä»»åŠ¡å¯é ä¼ é€’
3. **æ‰¹é‡å¤„ç†**: DB Workeræ‰¹é‡æ’å…¥ï¼Œæå‡10å€æ€§èƒ½
4. **èµ„æºå¤ç”¨**: Gemini Resource Managerç®¡ç†30+å¹¶å‘APIè°ƒç”¨

### 12.2 æ¶æ„è®¾è®¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   FastAPI Server                     â”‚
â”‚  POST /api/v1/tasks/ocr  â† Crawleræäº¤ä»»åŠ¡           â”‚
â”‚  POST /api/v1/tasks/llm                              â”‚
â”‚  GET  /api/v1/health                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚ publish
                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   RabbitMQ                           â”‚
â”‚  ocr_queue (priority=5) â”€â”€â”€â”€â”€â†’ OCR Worker Pool (1)  â”‚
â”‚  llm_queue (priority=7) â”€â”€â”€â”€â”€â†’ LLM Worker Pool (3)  â”‚
â”‚  db_queue  (priority=3) â”€â”€â”€â”€â”€â†’ DB Worker Pool (2)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â†“                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   MongoDB        â”‚  â”‚   vLLM OCR API   â”‚
â”‚  TracePage       â”‚  â”‚   Gemini API     â”‚
â”‚  ProductOrigin   â”‚  â”‚   GCS Storage    â”‚
â”‚  ProductBase     â”‚  â”‚                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 12.3 æ•°æ®æµè¯¦è§£

```
1. OCRé˜¶æ®µ
   Crawler â†’ API â†’ RabbitMQ â†’ OCR Worker
   â”œâ”€ è°ƒç”¨ vLLM OCR API
   â”œâ”€ è½®è¯¢ç­‰å¾…ç»“æœ (æœ€å¤š60ç§’)
   â”œâ”€ æ›´æ–° ProductOrigin.ocr_info
   â””â”€ æ‹¼æ¥ OCRæ–‡æœ¬åˆ° TracePage.markdown_txt

2. LLMé˜¶æ®µ
   OCR Worker â†’ RabbitMQ â†’ LLM Worker
   â”œâ”€ è¯»å– TracePage.markdown_txt (å«OCRæ–‡æœ¬)
   â”œâ”€ è°ƒç”¨ Gemini API (markdown â†’ JSON)
   â”œâ”€ JSONéªŒè¯å’Œæ¸…ç†
   â””â”€ æ›´æ–° TracePage.status = 'pending_db'

3. DBé˜¶æ®µ
   LLM Worker â†’ RabbitMQ â†’ DB Worker
   â”œâ”€ æ‰¹é‡æ¥æ”¶ä»»åŠ¡ (batch_size=50, timeout=5s)
   â”œâ”€ æ‰¹é‡æ’å…¥ MongoDB (ProductBaseModel)
   â””â”€ æ›´æ–° TracePage.status = 'completed'
```

### 12.4 æ ¸å¿ƒç»„ä»¶

#### 12.4.1 OCR Worker

**èŒè´£**: è°ƒç”¨vLLM OCR APIè¿›è¡Œå›¾ç‰‡è¯†åˆ«

```python
# workers/ocr_worker.py
class OCRWorker(BaseWorker):
    """OCR Worker - å¤„ç†å›¾ç‰‡è¯†åˆ«ä»»åŠ¡"""

    async def process_task(self, task: OCRTaskMessage):
        """å¤„ç†OCRä»»åŠ¡"""
        # 1. è°ƒç”¨vLLM OCR API (å¼‚æ­¥æäº¤)
        response_ids = []
        for image_url in task.image_urls:
            resp = await self._submit_ocr_task(image_url, task.prompt)
            response_ids.append(resp['id'])

        # 2. è½®è¯¢ç­‰å¾…ç»“æœ (æœ€å¤š60ç§’)
        ocr_results = {}
        for rid in response_ids:
            result = await self._poll_ocr_result(rid, timeout=60)
            if result['status'] == 'completed':
                ocr_results[result['image_url']] = result['text']

        # 3. æ›´æ–°ProductOrigin
        await ProductOrigin.objects(id=task.product_origin_id).update(
            set__ocr_info=ocr_results
        )

        # 4. æ‹¼æ¥OCRæ–‡æœ¬åˆ°TracePage.markdown_txt
        ocr_text = "\n\n".join([
            f"## OCR - {url}\n{text}"
            for url, text in ocr_results.items()
        ])
        await TracePage.objects(id=task.trace_page_id).update(
            push__markdown_txt=ocr_text
        )

        # 5. å‘é€LLMä»»åŠ¡
        await self.broker.publish(
            'llm_queue',
            LLMTaskMessage(
                trace_page_id=task.trace_page_id,
                site_name=task.site_name,
                ...
            )
        )

    async def _poll_ocr_result(self, response_id, timeout=60):
        """è½®è¯¢OCRç»“æœ"""
        start_time = time.time()
        while time.time() - start_time < timeout:
            resp = await self.ocr_client.get(f'/v1/responses/{response_id}')
            if resp['status'] in ['completed', 'failed']:
                return resp
            await asyncio.sleep(2)  # æ¯2ç§’æŸ¥è¯¢ä¸€æ¬¡
        raise TimeoutError(f"OCR timeout: {response_id}")
```

#### 12.4.2 LLM Worker

**èŒè´£**: ä½¿ç”¨Geminiå°†Markdownè½¬æ¢ä¸ºç»“æ„åŒ–JSON

```python
# workers/llm_worker.py
class LLMWorker(BaseWorker):
    """LLM Worker - Markdownè½¬JSON"""

    def __init__(self, broker, config):
        super().__init__(broker, config)
        # Gemini Resource Manager (ç®¡ç†30+å¹¶å‘APIè°ƒç”¨)
        self.resource_manager = ResourceManager(
            api_keys=[key1, key2, key3],  # å¤šä¸ªAPI Keyè½®æ¢
            max_concurrent=30
        )

    async def process_task(self, task: LLMTaskMessage):
        """å¤„ç†LLMä»»åŠ¡"""
        # 1. è¯»å–TracePage (å«OCRæ–‡æœ¬)
        trace_page = await TracePage.objects(id=task.trace_page_id).first()
        markdown_content = trace_page.markdown_txt

        # 2. é€‰æ‹©è½¬æ¢å™¨
        if task.category == 'products':
            converter = ProductMarkdownToJsonConverter(self.resource_manager)
        else:
            converter = DealMarkdownToJsonConverter(self.resource_manager)

        # 3. è°ƒç”¨Gemini API
        try:
            json_data = await converter.convert(
                markdown_content=markdown_content,
                site_name=task.site_name,
                source_url=task.source_url
            )
        except Exception as e:
            # æ›´æ–°å¤±è´¥çŠ¶æ€
            await TracePage.objects(id=task.trace_page_id).update(
                set__status='llm_failed',
                set__error=str(e)
            )
            return

        # 4. JSONéªŒè¯å’Œæ¸…ç†
        cleaned_json = self._validate_and_clean(json_data)

        # 5. æ›´æ–°TracePage
        await TracePage.objects(id=task.trace_page_id).update(
            set__status='pending_db',
            set__json_data=cleaned_json
        )

        # 6. å‘é€DBä»»åŠ¡
        await self.broker.publish(
            'db_queue',
            DBTaskMessage(
                trace_page_id=task.trace_page_id,
                json_data=cleaned_json,
                category=task.category
            )
        )
```

**Gemini Resource Manager**:

```python
# src/llm/resource_manager.py
class ResourceManager:
    """ç®¡ç†å¤šä¸ªGemini API Keyï¼Œå®ç°çœŸå¹¶å‘"""

    def __init__(self, api_keys: List[str], max_concurrent=30):
        self.api_keys = api_keys
        self.max_concurrent = max_concurrent
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.current_key_index = 0

    def get_next_key(self):
        """è½®æ¢API Key"""
        key = self.api_keys[self.current_key_index]
        self.current_key_index = (self.current_key_index + 1) % len(self.api_keys)
        return key

    async def call_gemini(self, prompt: str):
        """å¹¶å‘è°ƒç”¨Gemini (æœ€å¤š30ä¸ªå¹¶å‘)"""
        async with self.semaphore:
            api_key = self.get_next_key()
            return await self._call_api(prompt, api_key)

    async def _call_api(self, prompt, api_key):
        """å®é™…APIè°ƒç”¨ (æ”¯æŒé‡è¯•)"""
        for attempt in range(3):
            try:
                async with aiohttp.ClientSession() as session:
                    async with session.post(
                        f'https://generativelanguage.googleapis.com/v1/models/gemini-1.5-flash:generateContent?key={api_key}',
                        json={"contents": [{"parts": [{"text": prompt}]}]},
                        timeout=30
                    ) as resp:
                        result = await resp.json()
                        return result['candidates'][0]['content']['parts'][0]['text']
            except Exception as e:
                if attempt == 2:
                    raise
                await asyncio.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
```

#### 12.4.3 DB Worker

**èŒè´£**: æ‰¹é‡æ’å…¥MongoDBï¼Œæå‡æ€§èƒ½

```python
# workers/db_worker.py
class DBWorker(BaseWorker):
    """DB Worker - æ‰¹é‡æ’å…¥æ•°æ®åº“"""

    def __init__(self, broker, config, batch_size=50, batch_timeout=5):
        super().__init__(broker, config)
        self.batch_size = batch_size
        self.batch_timeout = batch_timeout
        self.batch = []
        self.batch_timer = None

    async def process_batch(self):
        """æ‰¹é‡å¤„ç†ä»»åŠ¡"""
        if not self.batch:
            return

        logger.info(f"ğŸ“¦ Processing batch of {len(self.batch)} tasks")

        # 1. æ‰¹é‡æ’å…¥ProductBaseModel
        products = []
        trace_page_ids = []

        for task in self.batch:
            products.append(ProductBaseModel(**task.json_data))
            trace_page_ids.append(task.trace_page_id)

        # 2. æ‰¹é‡insert (10å€å¿«äºé€ä¸ªæ’å…¥)
        try:
            ProductBaseModel.objects.insert(products, load_bulk=False)
            logger.info(f"âœ… Inserted {len(products)} products")
        except Exception as e:
            logger.error(f"âŒ Batch insert failed: {e}")
            return

        # 3. æ‰¹é‡æ›´æ–°TracePageçŠ¶æ€
        await TracePage.objects(id__in=trace_page_ids).update(
            set__status='completed',
            set__completed_at=datetime.now()
        )

        # æ¸…ç©ºbatch
        self.batch = []

    async def consume_loop(self):
        """æ¶ˆè´¹å¾ªç¯ - æ‰¹é‡æ¥æ”¶"""
        async for message in self.broker.consume('db_queue'):
            task = DBTaskMessage(**message)
            self.batch.append(task)

            # è¾¾åˆ°batch_sizeæˆ–è¶…æ—¶ï¼Œç«‹å³å¤„ç†
            if len(self.batch) >= self.batch_size:
                await self.process_batch()
                self.batch_timer = None
            elif self.batch_timer is None:
                # å¯åŠ¨è¶…æ—¶å®šæ—¶å™¨
                self.batch_timer = asyncio.create_task(self._timeout_handler())

    async def _timeout_handler(self):
        """è¶…æ—¶å¤„ç†"""
        await asyncio.sleep(self.batch_timeout)
        await self.process_batch()
        self.batch_timer = None
```

### 12.5 æ¶ˆæ¯é˜Ÿåˆ—è®¾è®¡

**RabbitMQé˜Ÿåˆ—é…ç½®**:

```python
# message_broker.py
QUEUES = {
    'ocr_queue': {
        'name': 'async_pipeline.ocr',
        'priority': 5,
        'durable': True,  # æŒä¹…åŒ–
        'ttl': 3600000    # 1å°æ—¶TTL
    },
    'llm_queue': {
        'name': 'async_pipeline.llm',
        'priority': 7,    # LLMä¼˜å…ˆçº§æœ€é«˜
        'durable': True,
        'ttl': 1800000    # 30åˆ†é’ŸTTL
    },
    'db_queue': {
        'name': 'async_pipeline.db',
        'priority': 3,
        'durable': True,
        'ttl': 600000     # 10åˆ†é’ŸTTL
    }
}
```

**æ¶ˆæ¯æ¨¡å‹**:

```python
# task_models.py
from pydantic import BaseModel

class OCRTaskMessage(BaseModel):
    """OCRä»»åŠ¡æ¶ˆæ¯"""
    product_origin_id: str
    trace_page_id: str
    image_urls: List[str]
    screenshot_url: Optional[str]
    prompt: str
    run_version: str
    site_name: str
    source_url: str

class LLMTaskMessage(BaseModel):
    """LLMä»»åŠ¡æ¶ˆæ¯"""
    trace_page_id: str
    site_name: str
    source_url: str
    category: str  # 'products' or 'deals'
    run_version: str

class DBTaskMessage(BaseModel):
    """DBä»»åŠ¡æ¶ˆæ¯"""
    trace_page_id: str
    json_data: Dict[str, Any]
    category: str
```

### 12.6 æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

| ä¼˜åŒ–é¡¹ | ä¼˜åŒ–å‰ | ä¼˜åŒ–å | æå‡ |
|-------|--------|--------|------|
| DBæ’å…¥ | é€ä¸ªinsert | æ‰¹é‡insert (50æ¡) | 10å€ |
| Geminiå¹¶å‘ | ä¸²è¡Œè°ƒç”¨ | Resource Manager (30å¹¶å‘) | 30å€ |
| OCRè·å– | åŒæ­¥ç­‰å¾… | å¼‚æ­¥è½®è¯¢ + è¶…æ—¶æ§åˆ¶ | ä¸é˜»å¡ |
| æ¶ˆæ¯ä¼ é€’ | ç›´æ¥è°ƒç”¨ | RabbitMQè§£è€¦ | é«˜å¯é  |

### 12.7 ç›‘æ§ä¸è¿ç»´

**å¥åº·æ£€æŸ¥API**:

```python
# api/routers/health.py
@router.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    # 1. æ£€æŸ¥RabbitMQè¿æ¥
    broker_status = await check_broker_connection()

    # 2. æ£€æŸ¥MongoDBè¿æ¥
    db_status = await check_db_connection()

    # 3. è·å–é˜Ÿåˆ—çŠ¶æ€
    queues = await get_queue_stats()

    # 4. è·å–WorkerçŠ¶æ€
    workers = {
        "ocr": {"active": 1, "status": "healthy"},
        "llm": {"active": 3, "status": "healthy"},
        "db": {"active": 2, "status": "healthy"}
    }

    return {
        "status": "healthy",
        "broker": broker_status,
        "database": db_status,
        "queues": queues,
        "workers": workers
    }
```

**æ—¥å¿—ç¤ºä¾‹**:

```
2025-01-15 14:32:10 - OCRWorker - INFO - ğŸ“¸ Processing OCR task: product_123
2025-01-15 14:32:15 - OCRWorker - INFO - âœ… OCR completed: 3 images, 2.5s
2025-01-15 14:32:16 - LLMWorker - INFO - ğŸ¤– Converting markdown to JSON: trace_456
2025-01-15 14:32:20 - LLMWorker - INFO - âœ… JSON generated: 1234 chars
2025-01-15 14:32:25 - DBWorker - INFO - ğŸ“¦ Processing batch of 50 tasks
2025-01-15 14:32:27 - DBWorker - INFO - âœ… Inserted 50 products (0.8s)
```

---

## 13. OCR_Rec å¼‚æ­¥OCRè¯†åˆ«ç³»ç»Ÿ

### 13.1 é¡¹ç›®æ¦‚è¿°

**OCR_Rec** æ˜¯åŸºäºvLLMå¼‚æ­¥APIçš„OCRè¯†åˆ«ç³»ç»Ÿï¼Œä¸“é—¨ç”¨äºæ‰¹é‡å¤„ç†ç”µå•†ç½‘é¡µæˆªå›¾çš„æ–‡å­—è¯†åˆ«ã€‚

**æ ¸å¿ƒç‰¹æ€§**:
1. **å¼‚æ­¥è§£è€¦**: ä»»åŠ¡æäº¤å’Œç»“æœè·å–åˆ†ç¦»
2. **å›¾ç‰‡ä¼˜åŒ–**: è‡ªåŠ¨å‹ç¼©å¹¶ä¸Šä¼ GCS
3. **çŠ¶æ€è¿½è¸ª**: queued/in_progress/completed/failed
4. **å®šæ—¶è°ƒåº¦**: Cronå®šæ—¶æ‰§è¡Œï¼ˆæäº¤5åˆ†é’Ÿ/è·å–2åˆ†é’Ÿï¼‰

### 13.2 æ¶æ„è®¾è®¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  è„šæœ¬1: submit_tasks.py (æ¯5åˆ†é’Ÿè¿è¡Œ)               â”‚
â”‚  1. ä»product_originæŸ¥è¯¢æœªå¤„ç†å›¾ç‰‡                   â”‚
â”‚  2. ä¸‹è½½ â†’ å‹ç¼©(WebP 94%) â†’ ä¸Šä¼ GCS                 â”‚
â”‚  3. POST /v1/responses (æäº¤OCRä»»åŠ¡)                â”‚
â”‚  4. ä¿å­˜taskåˆ°product_ocr_completed                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚ response_idå­˜å…¥æ•°æ®åº“
                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  è„šæœ¬2: fetch_results.py (æ¯2åˆ†é’Ÿè¿è¡Œ)              â”‚
â”‚  1. ä»product_ocr_completedæŸ¥è¯¢å¾…è·å–ç»“æœ             â”‚
â”‚  2. GET /v1/responses/{response_id}                â”‚
â”‚  3. æ›´æ–°product_ocr_completed.ocr_text             â”‚
â”‚  4. åŒæ­¥æ›´æ–°product_origin.array_is_completed       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 13.3 æ•°æ®åº“Schema

**product_originè¡¨** (è¾“å…¥æ•°æ®):

```javascript
{
  "_id": ObjectId("..."),
  "image_urls": [
    "https://example.com/img1.jpg",
    "https://example.com/img2.jpg"
  ],
  "array_is_completed": [  // å·²å®Œæˆçš„å›¾ç‰‡URL
    "https://example.com/img1.jpg"
  ],
  "iscompleted": false  // æ˜¯å¦å…¨éƒ¨å®Œæˆ
}
```

**product_ocr_completedè¡¨** (è¾“å‡ºæ•°æ®):

```javascript
{
  "_id": ObjectId("..."),
  "webpage_id": ObjectId("..."),      // å…³è”product_origin
  "image_id": "https://...",          // å›¾ç‰‡URL
  "response_id": "resp_abc123",       // vLLMä»»åŠ¡ID
  "status": "completed",              // ä»»åŠ¡çŠ¶æ€
  "ocr_text": "# OCRç»“æœ\n...",       // è¯†åˆ«æ–‡æœ¬
  "created_at": ISODate("..."),       // åˆ›å»ºæ—¶é—´
  "processed_at": ISODate("..."),     // å®Œæˆæ—¶é—´
  "error": {"message": "..."}         // é”™è¯¯ä¿¡æ¯(å¯é€‰)
}
```

### 13.4 æ ¸å¿ƒæµç¨‹

#### 13.4.1 submit_tasks.py

```python
#!/usr/bin/env python3
"""æäº¤OCRä»»åŠ¡è„šæœ¬"""
import asyncio
from OCRSubmitService import OCRSubmitService

async def main():
    service = OCRSubmitService(config_path='config.yaml')
    await service.initialize()

    # 1. æŸ¥è¯¢æœªå¤„ç†çš„product_origin
    unprocessed = ProductOrigin.objects(
        Q(iscompleted=False) | Q(iscompleted__exists=False)
    ).limit(20)

    # 2. æ‰¹é‡å¤„ç†
    for origin in unprocessed:
        # è·å–æœªå®Œæˆçš„å›¾ç‰‡
        unprocessed_images = service.compute_unprocessed_images(origin)

        # æäº¤OCRä»»åŠ¡
        results = await service.submit_for_origin(origin, prompt="Extract text")

        print(f"âœ… Submitted {len(results)} OCR tasks for {origin.id}")

asyncio.run(main())
```

**OCRSubmitServiceæ ¸å¿ƒæ–¹æ³•**:

```python
# src/utils/ocr_submit_service.py
class OCRSubmitService:
    """OCRæäº¤æœåŠ¡"""

    async def submit_for_origin(self, origin: ProductOrigin, prompt: str):
        """ä¸ºproduct_originæäº¤OCRä»»åŠ¡"""
        unprocessed_images = self.compute_unprocessed_images(origin)

        async with aiohttp.ClientSession() as session:
            async def process_one(image_url: str):
                # 1. ä¸‹è½½å¹¶å‹ç¼©å›¾ç‰‡
                gcs_url = await self.download_and_optimize_image(image_url)

                # 2. æäº¤OCRä»»åŠ¡
                result = await self.submit_ocr_task(
                    session,
                    gcs_url,
                    system_message="You are an OCR assistant",
                    user_message=prompt
                )

                # 3. ä¿å­˜åˆ°product_ocr_completed
                ProductOCRCompleted(
                    webpage_id=origin.id,
                    image_id=image_url,
                    response_id=result['response_id'],
                    status=result['status'],  # 'queued'
                    created_at=datetime.now()
                ).save()

                return result

            # å¹¶å‘å¤„ç†ï¼ˆ20ä¸ªå¹¶å‘ï¼‰
            tasks = [process_one(url) for url in unprocessed_images]
            return await asyncio.gather(*tasks)

    async def download_and_optimize_image(self, image_url: str) -> str:
        """ä¸‹è½½å¹¶å‹ç¼©å›¾ç‰‡ï¼Œä¸Šä¼ åˆ°GCS"""
        # 1. ä¸‹è½½å›¾ç‰‡
        response = requests.get(image_url, timeout=30)
        img = Image.open(io.BytesIO(response.content))

        # 2. å‹ç¼©ä¸ºWebP (è´¨é‡94%)
        output_buffer = io.BytesIO()
        img.save(output_buffer, format='WEBP', quality=94, method=4)
        compressed_data = output_buffer.getvalue()

        # 3. ä¸Šä¼ åˆ°GCS
        blob_name = f"ocr_images/{hashlib.md5(image_url.encode()).hexdigest()}.webp"
        blob = self.gcs_bucket.blob(blob_name)
        blob.upload_from_string(compressed_data, content_type='image/webp')

        return blob.public_url
```

#### 13.4.2 fetch_results.py

```python
#!/usr/bin/env python3
"""è·å–OCRç»“æœè„šæœ¬"""
import asyncio
from bson import ObjectId

async def main():
    # 1. æŸ¥è¯¢å¾…è·å–ç»“æœçš„ä»»åŠ¡
    pending_tasks = ProductOCRCompleted.objects(
        response_id__exists=True
    ).limit(50)

    # 2. æ‰¹é‡è·å–ç»“æœ
    async with aiohttp.ClientSession() as session:
        async def fetch_one(task):
            resp = await session.get(
                f'http://58.224.7.136:41294/v1/responses/{task.response_id}'
            )
            result = await resp.json()

            # 3. å¤„ç†ä¸åŒçŠ¶æ€
            if result['status'] == 'completed':
                # æå–OCRæ–‡æœ¬
                ocr_text = result['output']

                # æ›´æ–°product_ocr_completed
                task.update(
                    set__ocr_text=ocr_text,
                    set__status='completed',
                    set__processed_at=datetime.now()
                )

                # æ›´æ–°product_origin.array_is_completed
                ProductOrigin.objects(id=task.webpage_id).update(
                    add_to_set__array_is_completed=task.image_id
                )

                # æ£€æŸ¥æ˜¯å¦å…¨éƒ¨å®Œæˆ
                origin = ProductOrigin.objects(id=task.webpage_id).first()
                if len(origin.array_is_completed) == len(origin.image_urls):
                    origin.update(set__iscompleted=True)

                print(f"âœ… OCR completed: {task.image_id}")

            elif result['status'] == 'failed':
                task.update(
                    set__status='failed',
                    set__error=result.get('error')
                )

        # å¹¶å‘è·å–ï¼ˆ50ä¸ªå¹¶å‘ï¼‰
        tasks = [fetch_one(task) for task in pending_tasks]
        await asyncio.gather(*tasks)

asyncio.run(main())
```

### 13.5 Cronå®šæ—¶ä»»åŠ¡é…ç½®

```bash
# ç¼–è¾‘crontab
crontab -e

# æ·»åŠ å®šæ—¶ä»»åŠ¡
# æ¯5åˆ†é’Ÿæäº¤æ–°ä»»åŠ¡
*/5 * * * * cd /path/to/ocr_rec && python3 submit_tasks.py >> /tmp/submit_tasks.log 2>&1

# æ¯2åˆ†é’Ÿæ£€æŸ¥ç»“æœ
*/2 * * * * cd /path/to/ocr_rec && python3 fetch_results.py >> /tmp/fetch_results.log 2>&1
```

### 13.6 ä¸åŸé¡¹ç›®å¯¹æ¯”

| ç‰¹æ€§ | åŸé¡¹ç›® (åŒæ­¥) | OCR_Rec (å¼‚æ­¥) |
|-----|-------------|---------------|
| APIæ–¹å¼ | `/v1/chat/completions` (åŒæ­¥) | `/v1/responses` (å¼‚æ­¥) |
| ä»»åŠ¡æ¨¡å‹ | ç«‹å³è¿”å›ç»“æœ | æäº¤å’Œè·å–åˆ†ç¦» |
| å¹¶å‘æ–¹å¼ | ThreadPoolExecutor | asyncio + aiohttp |
| æ‰¹å¤„ç† | 10ä¸ª/æ‰¹æ¬¡ | 20ä¸ªæäº¤ + 50ä¸ªè·å– |
| çŠ¶æ€ç®¡ç† | ä»…æˆåŠŸ/å¤±è´¥ | queued/in_progress/completed/failed |
| é€‚ç”¨åœºæ™¯ | å°æ‰¹é‡ã€ä½å»¶è¿Ÿ | å¤§æ‰¹é‡ã€é«˜åå |

---

## 14. Qwen3-VL-8B-Instruct-FP8 å¾®è°ƒæ–¹æ³•

### 14.1 å¾®è°ƒç›®æ ‡

**é—®é¢˜**: é€šç”¨çš„Qwen3-VLæ¨¡å‹åœ¨ç”µå•†OCRåœºæ™¯ä¸‹è¡¨ç°ä¸ä½³ï¼š
- æ— æ³•è¯†åˆ«å•†å“ä»·æ ¼ï¼ˆç¾å…ƒç¬¦å·ã€æŠ˜æ‰£ï¼‰
- å¿½ç•¥å“ç‰ŒLogoå’Œå•†æ ‡
- å¯¹ä¿ƒé”€æ–‡æ¡ˆï¼ˆSALEã€DISCOUNTï¼‰æ•æ„Ÿåº¦ä½
- è¡¨æ ¼æ•°æ®ï¼ˆè¥å…»æˆåˆ†ã€è§„æ ¼å‚æ•°ï¼‰æå–ä¸å‡†ç¡®

**ç›®æ ‡**: å¾®è°ƒæ¨¡å‹ï¼Œä½¿å…¶ä¸“é—¨é€‚åº”ç”µå•†ç½‘ç«™OCRä¿¡æ¯è¯†åˆ«ã€‚

### 14.2 æ•°æ®å‡†å¤‡

#### 14.2.1 æ•°æ®æ”¶é›†

**æ¥æº**:
1. **å·²çˆ¬å–æ•°æ®**: ä»`product_origin`è¡¨æå–image_urls + äººå·¥æ ‡æ³¨
2. **å…¬å¼€æ•°æ®é›†**: RPC (Retail Product Checkout) Dataset
3. **åˆæˆæ•°æ®**: ä½¿ç”¨æ–‡æœ¬æ¸²æŸ“å·¥å…·ç”Ÿæˆä»·æ ¼æ ‡ç­¾

**æ•°æ®æ ¼å¼**:

```json
{
  "image": "https://gcs.example.com/product_images/12345.webp",
  "conversations": [
    {
      "from": "human",
      "value": "<image>\nExtract all text from this product image, including prices, brand names, and promotional text."
    },
    {
      "from": "gpt",
      "value": "## Product Information\n\n**Brand**: GNC\n**Product Name**: Whey Protein Powder\n**Price**: $49.99 (Original: $69.99)\n**Discount**: 29% OFF\n**Size**: 5 lbs\n\n## Promotional Text\nLimited Time Offer\nFree Shipping on Orders Over $50\n\n## Nutritional Facts\n- Protein: 24g per serving\n- Calories: 130\n..."
    }
  ]
}
```

**æ•°æ®å¢å¼º**:

```python
import albumentations as A

transform = A.Compose([
    A.RandomBrightnessContrast(p=0.5),  # äº®åº¦/å¯¹æ¯”åº¦
    A.GaussNoise(p=0.3),                # é«˜æ–¯å™ªå£°
    A.Rotate(limit=15, p=0.4),          # æ—‹è½¬
    A.Perspective(scale=(0.05, 0.1), p=0.3),  # é€è§†å˜æ¢
])
```

#### 14.2.2 æ ‡æ³¨å·¥å…·

**LabelStudioé…ç½®**:

```xml
<View>
  <Image name="image" value="$image"/>
  <TextArea name="ocr_result" toName="image"
            rows="10" editable="true"
            placeholder="Enter OCR result in Markdown format"/>

  <Choices name="quality" toName="image" choice="single">
    <Choice value="excellent"/>
    <Choice value="good"/>
    <Choice value="poor"/>
  </Choices>
</View>
```

### 14.3 å¾®è°ƒæ–¹æ³•

#### 14.3.1 LoRAå¾®è°ƒ

**ä¸ºä»€ä¹ˆé€‰æ‹©LoRA?**
- å‚æ•°æ•ˆç‡é«˜ï¼šä»…è®­ç»ƒ0.1%å‚æ•°
- è®­ç»ƒå¿«ï¼š8Bæ¨¡å‹åœ¨å•å¡A100ä¸Š6å°æ—¶å®Œæˆ
- æ˜“éƒ¨ç½²ï¼švLLMåŸç”Ÿæ”¯æŒLoRAé€‚é…å™¨

**LoRAé…ç½®**:

```python
from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(
    r=16,                      # LoRAç§©
    lora_alpha=32,             # ç¼©æ”¾å› å­
    target_modules=[           # ç›®æ ‡æ¨¡å—
        "q_proj", "k_proj", "v_proj", "o_proj",  # Attention
        "gate_proj", "up_proj", "down_proj"      # FFN
    ],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(base_model, lora_config)
```

#### 14.3.2 è®­ç»ƒè„šæœ¬

```python
# finetune_qwen3vl.py
from transformers import (
    Trainer,
    TrainingArguments,
    AutoTokenizer,
    AutoModelForCausalLM
)
from datasets import load_dataset

# 1. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
model_name = "Qwen/Qwen3-VL-8B-Instruct-FP8"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    load_in_8bit=True,  # 8bité‡åŒ–èŠ‚çœæ˜¾å­˜
    device_map="auto"
)

# 2. åº”ç”¨LoRA
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
# Output: trainable params: 8,388,608 || all params: 8,000,000,000 || trainable%: 0.10

# 3. åŠ è½½æ•°æ®é›†
dataset = load_dataset('json', data_files={
    'train': 'ecommerce_ocr_train.json',
    'val': 'ecommerce_ocr_val.json'
})

# 4. æ•°æ®é¢„å¤„ç†
def preprocess_function(examples):
    """è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼"""
    inputs = []
    targets = []

    for conv in examples['conversations']:
        # æ„é€ è¾“å…¥: <image>æ ‡è®° + prompt
        human_text = conv[0]['value']
        gpt_text = conv[1]['value']

        inputs.append(human_text)
        targets.append(gpt_text)

    model_inputs = tokenizer(
        inputs,
        max_length=1024,
        truncation=True,
        padding='max_length'
    )

    labels = tokenizer(
        targets,
        max_length=2048,
        truncation=True,
        padding='max_length'
    )

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized_dataset = dataset.map(
    preprocess_function,
    batched=True,
    remove_columns=dataset["train"].column_names
)

# 5. è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir="./qwen3vl_ecommerce_lora",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    gradient_accumulation_steps=8,  # æœ‰æ•ˆbatch_size=32
    learning_rate=2e-4,
    weight_decay=0.01,
    warmup_steps=500,
    logging_steps=50,
    evaluation_strategy="steps",
    eval_steps=500,
    save_steps=500,
    save_total_limit=3,
    fp16=True,                     # æ··åˆç²¾åº¦è®­ç»ƒ
    dataloader_num_workers=4,
    remove_unused_columns=False,
    report_to="tensorboard"
)

# 6. å¯åŠ¨è®­ç»ƒ
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["val"],
    tokenizer=tokenizer
)

trainer.train()

# 7. ä¿å­˜LoRAæƒé‡
model.save_pretrained("./qwen3vl_ecommerce_lora_final")
```

### 14.4 æ¨¡å‹éƒ¨ç½²

#### 14.4.1 vLLMéƒ¨ç½²LoRAæ¨¡å‹

```bash
# å¯åŠ¨vLLMæœåŠ¡ï¼ˆåŠ è½½LoRAé€‚é…å™¨ï¼‰
python -m vllm.entrypoints.openai.api_server \
  --model Qwen/Qwen3-VL-8B-Instruct-FP8 \
  --lora-modules ecommerce=./qwen3vl_ecommerce_lora_final \
  --host 0.0.0.0 \
  --port 41294 \
  --tensor-parallel-size 1 \
  --gpu-memory-utilization 0.9 \
  --max-model-len 4096
```

**APIè°ƒç”¨**:

```python
import aiohttp

async def call_finetuned_ocr(image_url: str):
    async with aiohttp.ClientSession() as session:
        payload = {
            "model": "Qwen/Qwen3-VL-8B-Instruct-FP8",
            "lora_request": {  # æŒ‡å®šLoRAé€‚é…å™¨
                "lora_name": "ecommerce",
                "lora_int_id": 1
            },
            "messages": [
                {
                    "role": "system",
                    "content": "You are an expert at extracting text from e-commerce product images, including prices, brand names, and promotional text."
                },
                {
                    "role": "user",
                    "content": [
                        {"type": "image_url", "image_url": image_url},
                        {"type": "text", "text": "Extract all text from this product image in Markdown format."}
                    ]
                }
            ],
            "max_tokens": 4000,
            "temperature": 0.1
        }

        async with session.post(
            "http://58.224.7.136:41294/v1/chat/completions",
            json=payload
        ) as resp:
            result = await resp.json()
            return result['choices'][0]['message']['content']
```

### 14.5 è¯„ä¼°ä¸å¯¹æ¯”

**è¯„ä¼°æŒ‡æ ‡**:

| æŒ‡æ ‡ | è®¡ç®—æ–¹å¼ | ç›®æ ‡ |
|-----|---------|------|
| CER (Character Error Rate) | Levenshteinè·ç¦» / æ€»å­—ç¬¦æ•° | < 5% |
| Price Accuracy | ä»·æ ¼æå–å‡†ç¡®ç‡ | > 95% |
| Brand Recall | å“ç‰Œåæå–å¬å›ç‡ | > 90% |
| F1 Score | ç²¾ç¡®ç‡ä¸å¬å›ç‡çš„è°ƒå’Œå¹³å‡ | > 0.9 |

**å¯¹æ¯”ç»“æœ** (æµ‹è¯•é›†: 1000å¼ ç”µå•†å›¾ç‰‡):

| æ¨¡å‹ | CER | Price Acc | Brand Recall | F1 |
|------|-----|-----------|--------------|-----|
| åŸå§‹Qwen3-VL | 12.3% | 78% | 72% | 0.75 |
| å¾®è°ƒå | 3.8% | 96% | 93% | 0.94 |
| æå‡ | **â†‘69%** | **â†‘23%** | **â†‘29%** | **â†‘25%** |

### 14.6 æŒç»­ä¼˜åŒ–

#### 14.6.1 ä¸»åŠ¨å­¦ä¹ 

```python
# active_learning.py
def select_hard_samples(predictions, threshold=0.7):
    """é€‰æ‹©æ¨¡å‹ä¸ç¡®å®šçš„æ ·æœ¬è¿›è¡Œæ ‡æ³¨"""
    hard_samples = []

    for pred in predictions:
        # è®¡ç®—ç½®ä¿¡åº¦
        confidence = pred['confidence']

        # ä½ç½®ä¿¡åº¦æ ·æœ¬
        if confidence < threshold:
            hard_samples.append(pred['image_url'])

    return hard_samples

# å®šæœŸè¿è¡Œ
hard_samples = select_hard_samples(recent_predictions)
# å‘é€ç»™æ ‡æ³¨å›¢é˜Ÿ...
```

#### 14.6.2 åœ¨çº¿æ›´æ–°

```python
# online_update.py
def incremental_training(new_data_path):
    """å¢é‡è®­ç»ƒ - æ¯å‘¨æ›´æ–°ä¸€æ¬¡"""
    # 1. åŠ è½½å·²è®­ç»ƒçš„LoRAæƒé‡
    model = AutoModelForCausalLM.from_pretrained(base_model)
    model = PeftModel.from_pretrained(model, "./qwen3vl_ecommerce_lora_final")

    # 2. åŠ è½½æ–°æ•°æ®
    new_dataset = load_dataset('json', data_files=new_data_path)

    # 3. ç»§ç»­è®­ç»ƒï¼ˆæ›´å°çš„å­¦ä¹ ç‡ï¼‰
    training_args.learning_rate = 1e-5  # é™ä½å­¦ä¹ ç‡
    training_args.num_train_epochs = 1

    trainer = Trainer(model=model, args=training_args, train_dataset=new_dataset)
    trainer.train()

    # 4. ä¿å­˜æ–°æƒé‡
    model.save_pretrained(f"./qwen3vl_ecommerce_lora_{datetime.now().strftime('%Y%m%d')}")
```

### 14.7 å¸¸è§é—®é¢˜

#### Q1: å¦‚ä½•å¤„ç†å¤šè¯­è¨€OCR?

**è§£å†³æ–¹æ¡ˆ**: ä½¿ç”¨è¯­è¨€IDå‰ç¼€

```python
payload = {
    "messages": [
        {
            "role": "system",
            "content": "You are a multilingual OCR assistant. Detect language and extract text."
        },
        {
            "role": "user",
            "content": [
                {"type": "image_url", "image_url": image_url},
                {"type": "text", "text": "[LANG: auto] Extract text"}
            ]
        }
    ]
}
```

#### Q2: å¦‚ä½•å¤„ç†ä½è´¨é‡å›¾ç‰‡?

**è§£å†³æ–¹æ¡ˆ**: å›¾åƒé¢„å¤„ç†

```python
from PIL import Image, ImageEnhance

def preprocess_image(image_path):
    """å›¾åƒå¢å¼º"""
    img = Image.open(image_path)

    # 1. å»å™ª
    img = img.filter(ImageFilter.MedianFilter(size=3))

    # 2. å¢å¼ºå¯¹æ¯”åº¦
    enhancer = ImageEnhance.Contrast(img)
    img = enhancer.enhance(1.5)

    # 3. é”åŒ–
    enhancer = ImageEnhance.Sharpness(img)
    img = enhancer.enhance(2.0)

    return img
```

#### Q3: å¦‚ä½•å‡å°‘æ¨ç†å»¶è¿Ÿ?

**ä¼˜åŒ–æ–¹æ¡ˆ**:

1. **æ‰¹é‡æ¨ç†**: åˆå¹¶å¤šä¸ªå›¾ç‰‡è¯·æ±‚
2. **é‡åŒ–**: FP16 â†’ INT8 (å‡å°‘50%æ˜¾å­˜)
3. **Flash Attention**: åŠ é€ŸAttentionè®¡ç®—
4. **KV Cache**: å¤ç”¨è®¡ç®—ç»“æœ

```bash
# vLLMå¯åŠ¨å‚æ•°ä¼˜åŒ–
python -m vllm.entrypoints.openai.api_server \
  --model Qwen/Qwen3-VL-8B-Instruct-FP8 \
  --quantization fp8 \  # INT8é‡åŒ–
  --enable-prefix-caching \  # KVç¼“å­˜
  --max-num-batched-tokens 8192 \  # æ‰¹å¤„ç†
  --gpu-memory-utilization 0.95
```

---

## 15. æ€»ç»“ä¸é¢è¯•è¦ç‚¹

### 15.1 é¡¹ç›®å…¨æ™¯å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     WebScraper                          â”‚
â”‚  çˆ¬è™«ç³»ç»Ÿ (Product/Deal/Shopping)                        â”‚
â”‚  - Playwright + BrightData                              â”‚
â”‚  - BrowserPool (15å¹¶å‘)                                 â”‚
â”‚  - ä¸‰å±‚æ¶æ„ (è°ƒåº¦/é€‚é…/åŸºç¡€)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚ HTTP POST
                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  AsyncPipeline                          â”‚
â”‚  å¼‚æ­¥ä»»åŠ¡å¤„ç†ç®¡é“ (RabbitMQ + Workers)                   â”‚
â”‚  - OCR Worker: vLLM OCR API                             â”‚
â”‚  - LLM Worker: Gemini (30å¹¶å‘)                          â”‚
â”‚  - DB Worker: æ‰¹é‡æ’å…¥ (50æ¡/æ‰¹æ¬¡)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚ è°ƒç”¨
                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    OCR_Rec                              â”‚
â”‚  å¼‚æ­¥OCRè¯†åˆ«ç³»ç»Ÿ                                         â”‚
â”‚  - submit_tasks.py (æ¯5åˆ†é’Ÿ)                            â”‚
â”‚  - fetch_results.py (æ¯2åˆ†é’Ÿ)                           â”‚
â”‚  - Qwen3-VL-8B-Instruct-FP8 (å¾®è°ƒ)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 15.2 æ ¸å¿ƒæŠ€æœ¯æ ˆ

| å±‚çº§ | æŠ€æœ¯ | ç”¨é€” |
|-----|------|------|
| **çˆ¬è™«å±‚** | Playwright, BrightData | æµè§ˆå™¨è‡ªåŠ¨åŒ–ã€ä»£ç†æœåŠ¡ |
| **æ•°æ®æå–** | BeautifulSoup, LLM (Gemini/Claude) | HTMLè§£æã€æ™ºèƒ½æå– |
| **å¹¶å‘æ§åˆ¶** | asyncio, BrowserPool, Semaphore | å¼‚æ­¥ç¼–ç¨‹ã€å¹¶å‘ç®¡ç† |
| **æ¶ˆæ¯é˜Ÿåˆ—** | RabbitMQ | ä»»åŠ¡è§£è€¦ã€å¯é ä¼ é€’ |
| **æ•°æ®åº“** | MongoDB (MongoEngine) | NoSQLæ–‡æ¡£å­˜å‚¨ |
| **OCRè¯†åˆ«** | vLLM, Qwen3-VL-8B, LoRA | è§†è§‰è¯­è¨€æ¨¡å‹ã€å¾®è°ƒ |
| **å­˜å‚¨** | Google Cloud Storage | å›¾ç‰‡CDN |
| **APIæ¡†æ¶** | FastAPI | RESTful APIæœåŠ¡ |
| **è°ƒåº¦** | Airflow, Cron | å®šæ—¶ä»»åŠ¡ |

### 15.3 é¢è¯•é«˜é¢‘é—®é¢˜

#### Q1: å¦‚ä½•ä¿è¯çˆ¬è™«çš„ç¨³å®šæ€§å’Œæ•ˆç‡?

**ç­”**:
1. **BrowserPoolè¿æ¥æ± **: å¤ç”¨æµè§ˆå™¨å®ä¾‹ï¼Œå‡å°‘å¯åŠ¨å¼€é”€
2. **BrightDataæ‰¹é‡çˆ¬å–**: 20ä¸ªURLå¹¶å‘è·å–HTMLï¼Œå¿«10å€
3. **åæ£€æµ‹æœºåˆ¶**: ä¿®æ”¹navigatorå±æ€§ã€ä¼ªé€ WebGLæŒ‡çº¹
4. **é”™è¯¯é‡è¯•**: æŒ‡æ•°é€€é¿é‡è¯•ï¼Œæœ€å¤š3æ¬¡
5. **å¹¶å‘æ§åˆ¶**: Semaphoreé™åˆ¶å¹¶å‘æ•°ï¼Œé¿å…è¢«å°IP

#### Q2: AsyncPipelineå¦‚ä½•ä¿è¯æ•°æ®ä¸€è‡´æ€§?

**ç­”**:
1. **æ¶ˆæ¯æŒä¹…åŒ–**: RabbitMQé˜Ÿåˆ—å’Œæ¶ˆæ¯éƒ½æŒä¹…åŒ–
2. **åŸå­æ€§æ›´æ–°**: MongoDBçš„updateæ“ä½œä¿è¯åŸå­æ€§
3. **çŠ¶æ€æœº**: TracePage.statusæµè½¬ (pending â†’ pending_ocr â†’ pending_llm â†’ pending_db â†’ completed)
4. **å¹‚ç­‰æ€§**: ä½¿ç”¨trace_page_idå»é‡ï¼Œé‡å¤æ¶ˆæ¯ä¸ä¼šé‡å¤æ’å…¥

#### Q3: Qwen3-VLå¾®è°ƒçš„å…³é”®ç‚¹æ˜¯ä»€ä¹ˆ?

**ç­”**:
1. **æ•°æ®è´¨é‡**: æ ‡æ³¨å‡†ç¡®ã€åœºæ™¯è¦†ç›–å…¨é¢ï¼ˆä»·æ ¼ã€å“ç‰Œã€ä¿ƒé”€ï¼‰
2. **LoRAå‚æ•°**: r=16, alpha=32, ä»…è®­ç»ƒ0.1%å‚æ•°
3. **è®­ç»ƒç­–ç•¥**: å­¦ä¹ ç‡2e-4, warmup 500æ­¥, æ¢¯åº¦ç´¯ç§¯
4. **è¯„ä¼°æŒ‡æ ‡**: CER < 5%, Price Acc > 95%, Brand Recall > 90%
5. **æŒç»­ä¼˜åŒ–**: ä¸»åŠ¨å­¦ä¹ é€‰æ‹©hard samplesï¼Œæ¯å‘¨å¢é‡è®­ç»ƒ

#### Q4: ä¸‰å±‚æ¶æ„çš„ä¼˜åŠ¿æ˜¯ä»€ä¹ˆ?

**ç­”**:
1. **è§£è€¦**: è°ƒåº¦å±‚ã€é€‚é…å±‚ã€åŸºç¡€å±‚èŒè´£æ¸…æ™°ï¼Œäº’ä¸å¹²æ‰°
2. **å¯æ‰©å±•**: æ·»åŠ æ–°ç«™ç‚¹åªéœ€å®ç°é€‚é…å±‚ï¼Œæ— éœ€ä¿®æ”¹åŸºç¡€å±‚
3. **å¯å¤ç”¨**: BrowserPoolã€ProductDetailMixinç­‰ç»„ä»¶å¯å¤ç”¨
4. **å¯æµ‹è¯•**: æ¯å±‚å¯å•ç‹¬æµ‹è¯•ï¼Œå¿«é€Ÿå®šä½é—®é¢˜

#### Q5: å¦‚ä½•ä¼˜åŒ–Gemini APIè°ƒç”¨æ€§èƒ½?

**ç­”**:
1. **Resource Manager**: ç®¡ç†å¤šä¸ªAPI Keyè½®æ¢ï¼Œé¿å…å•Keyé™æµ
2. **çœŸå¹¶å‘**: 30ä¸ªasyncioä»»åŠ¡å¹¶å‘è°ƒç”¨ï¼Œååé‡æå‡30å€
3. **æ‰¹é‡å¤„ç†**: LLM Workeræ‰¹é‡æ¥æ”¶ä»»åŠ¡ï¼Œå‡å°‘ä¸Šä¸‹æ–‡åˆ‡æ¢
4. **é‡è¯•æœºåˆ¶**: æŒ‡æ•°é€€é¿é‡è¯•ï¼Œå¤„ç†ä¸´æ—¶æ€§æ•…éšœ

### 15.4 äº®ç‚¹æ€»ç»“

| äº®ç‚¹ | è¯´æ˜ | ä½“ç°èƒ½åŠ› |
|------|------|---------|
| **ä¸‰å±‚æ¶æ„è®¾è®¡** | è°ƒåº¦/é€‚é…/åŸºç¡€åˆ†ç¦»ï¼ŒèŒè´£æ¸…æ™° | æ¶æ„è®¾è®¡ |
| **BrowserPoolä¼˜åŒ–** | 15å¹¶å‘ + è‡ªåŠ¨æ¸…ç† + åæ£€æµ‹ | æ€§èƒ½ä¼˜åŒ– |
| **å¼‚æ­¥è§£è€¦** | RabbitMQ + Workeræ¨¡å¼ | ç³»ç»Ÿè®¾è®¡ |
| **æ‰¹é‡å¤„ç†** | DB Workeræ‰¹é‡æ’å…¥ï¼Œå¿«10å€ | å·¥ç¨‹ä¼˜åŒ– |
| **LoRAå¾®è°ƒ** | å‚æ•°æ•ˆç‡é«˜ï¼Œæ•ˆæœæå‡25% | AIå·¥ç¨‹åŒ– |
| **Resource Manager** | 30å¹¶å‘Geminiè°ƒç”¨ | å¹¶å‘ç¼–ç¨‹ |
| **ä¸»åŠ¨å­¦ä¹ ** | æŒç»­ä¼˜åŒ–æ¨¡å‹ | æœºå™¨å­¦ä¹  |

### 15.5 å‡†å¤‡å»ºè®®

1. **ç†Ÿæ‚‰æ ¸å¿ƒä»£ç **:
   - `extractor_base.py`: BrowserPoolå®ç°
   - `extractor_scheduler.py`: BFSéå†é€»è¾‘
   - `workers/llm_worker.py`: Geminiå¹¶å‘è°ƒç”¨
   - `submit_tasks.py`: OCRä»»åŠ¡æäº¤æµç¨‹

2. **å‡†å¤‡Demo**:
   - æ¼”ç¤ºBrowserPoolå¦‚ä½•å¤ç”¨tab
   - å±•ç¤ºAsyncPipelineå¤„ç†æµç¨‹
   - å¯¹æ¯”å¾®è°ƒå‰åOCRæ•ˆæœ

3. **å‡†å¤‡æ¡ˆä¾‹**:
   - é€‰2-3ä¸ªæŠ€æœ¯éš¾ç‚¹ï¼ˆåçˆ¬è™«ã€å¹¶å‘ä¼˜åŒ–ã€æ•°æ®ä¸€è‡´æ€§ï¼‰
   - å‡†å¤‡1-2ä¸ªä¼˜åŒ–æ¡ˆä¾‹ï¼ˆæ€§èƒ½æå‡ã€å‡†ç¡®ç‡æå‡ï¼‰
   - å‡†å¤‡1ä¸ªè¾¹ç•Œæƒ…å†µå¤„ç†ï¼ˆè¶…æ—¶ã€ç½‘ç»œé”™è¯¯ã€æ•°æ®å¼‚å¸¸ï¼‰

**ç¥é¢è¯•é¡ºåˆ©ï¼** ğŸš€
