# Webscraper é¡¹ç›®æ·±åº¦æŠ€æœ¯åˆ†æ - é¢è¯•å‡†å¤‡æ–‡æ¡£

> **é¡¹ç›®å®šä½**: åŸºäº LangGraph + Claude AI çš„çˆ¬è™«è‡ªåŠ¨å¼€å‘å·¥å…·
> **æŠ€æœ¯éš¾åº¦**: â­â­â­â­â­ (æ¶æ„è®¾è®¡ã€AIå·¥ç¨‹åŒ–ã€å·¥ä½œæµç¼–æ’)
> **ä»£ç é‡**: ~3000+ è¡Œæ ¸å¿ƒä»£ç 
> **é€‚ç”¨é¢è¯•**: é«˜çº§Pythonå·¥ç¨‹å¸ˆã€AIå·¥ç¨‹å¸ˆã€å…¨æ ˆå·¥ç¨‹å¸ˆ

---

## 1. é¡¹ç›®æ¦‚è¿°

### 1.1 é¡¹ç›®å®šä½

**DevBot** æ˜¯ä¸€ä¸ªåŸºäº LangGraph + Claude Agent SDK çš„ AI è¾…åŠ©çˆ¬è™«å¼€å‘ç³»ç»Ÿï¼Œèƒ½å¤Ÿè‡ªåŠ¨åŒ–ç”Ÿæˆå’Œä¼˜åŒ–ç½‘ç«™çˆ¬è™«ä»£ç ã€‚

**æ ¸å¿ƒä»·å€¼**:

- **è‡ªåŠ¨åŒ–å¼€å‘**: ä» URL åˆ°å¯è¿è¡Œçˆ¬è™«ä»£ç çš„å…¨æµç¨‹è‡ªåŠ¨åŒ–
- **æ™ºèƒ½å†³ç­–**: é€šè¿‡ LLM åˆ†æé¡µé¢ç»“æ„ã€é€‰æ‹©åˆé€‚çš„çˆ¬è™«å¼•æ“å’Œå‚æ•°
- **è´¨é‡ä¿è¯**: Developer + Reviewer åŒé‡èŠ‚ç‚¹ï¼Œç¡®ä¿ä»£ç è´¨é‡
- **æŒç»­ä¼˜åŒ–**: æ”¯æŒæ–­ç‚¹æ¢å¤ã€æ­¥éª¤é‡è¯•ã€ä»£ç ä¼˜åŒ–ç­‰é«˜çº§åŠŸèƒ½

### 1.2 æŠ€æœ¯æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      DevBot ç³»ç»Ÿæ¶æ„                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚  LangGraph  â”‚  â”‚ Claude Agent â”‚  â”‚  MCP Tools   â”‚       â”‚
â”‚  â”‚  StateGraph â”‚â”€â”€â”‚  SDK Client  â”‚â”€â”€â”‚ chrome-devto â”‚       â”‚
â”‚  â”‚  Workflow   â”‚  â”‚  (Session)   â”‚  â”‚ gemini-cli   â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚         â”‚                 â”‚                   â”‚              â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                           â”‚                                  â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚         â”‚                                     â”‚              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Developer    â”‚                  â”‚    Reviewer     â”‚    â”‚
â”‚  â”‚  Nodes (23)   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚   Nodes (6)     â”‚    â”‚
â”‚  â”‚  Step 0-31    â”‚   Validate       â”‚   review_step   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚         â”‚                                     â”‚              â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                           â”‚                                  â”‚
â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚                  â”‚  CrawlerDevState  â”‚                       â”‚
â”‚                  â”‚   TypedDict (27+) â”‚                       â”‚
â”‚                  â”‚  - url, site_name â”‚                       â”‚
â”‚                  â”‚  - current_step   â”‚                       â”‚
â”‚                  â”‚  - status, retry  â”‚                       â”‚
â”‚                  â”‚  - patterns_queue â”‚                       â”‚
â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                           â”‚                                  â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚         â”‚                                     â”‚              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  SQLite Store â”‚                  â”‚   JSON State    â”‚    â”‚
â”‚  â”‚  Conversation â”‚                  â”‚   Checkpoint    â”‚    â”‚
â”‚  â”‚  History      â”‚                  â”‚   MemorySaver   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.3 æ ¸å¿ƒæµç¨‹æ¦‚è§ˆ

```
ç”¨æˆ·è¾“å…¥ URL
    â†“
Step 0: åˆ›å»ºåŸºç¡€æ–‡ä»¶æ¡†æ¶ (Jinja2æ¨¡æ¿)
    â†“
Step 1: åˆ†æé¡µé¢ç»“æ„ (Chrome DevTools + LLM)
    â†“ â†’ Reviewer: éªŒè¯å¼•æ“é…ç½®
    â†“
Step 2: ç”Ÿæˆåˆ—è¡¨æå–å™¨ (æå–å•†å“/ä¼˜æƒ åˆ—è¡¨)
    â†“ â†’ Reviewer: éªŒè¯è¿”å›æ ¼å¼
    â†“
Step 2.1: URL åˆ†ç±» (detail/list/other/unclear)
    â†“ â†’ Reviewer: éªŒè¯ site_patterns
    â†“
Step 3: ç”Ÿæˆ URL patterns (æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…)
    â†“ â†’ Reviewer: éªŒè¯ URL_MAP
    â†“
Step 4-6: å¾ªç¯ç”Ÿæˆå„ pattern çš„æå–å™¨ (detail/list)
    â†“ â†’ Reviewer: éªŒè¯æ ¸å¿ƒæå–å™¨
    â†“
Step 7: ç½‘ç«™æ ‘æ‰©å±• (BFSéå†å­é¡µé¢)
    â†“ â†’ Reviewer: éªŒè¯æ‰©å±•åŠŸèƒ½
    â†“
Step 8: Markdown ä¿¡æ¯åˆ†æ (å ä½æ­¥éª¤)
    â†“ â†’ Reviewer: éªŒè¯çŠ¶æ€
    â†“
Step 9: é¦–æ¬¡è¿è¡Œæµ‹è¯•
    â†“
Step 10: ç”Ÿæˆ Airflow DAG
    â†“
Step 20-22: ä»£ç æ£€æŸ¥ä¸ä¼˜åŒ– (æ€§èƒ½ã€æ–¹æ³•ç­¾åã€æ•°æ®å®Œæ•´æ€§)
    â†“
è¾“å‡º: å¯è¿è¡Œçš„çˆ¬è™«ä»£ç 
```

---

## 2. State çŠ¶æ€ç®¡ç†æ·±åº¦è§£æ

### 2.1 ä¸ºä»€ä¹ˆé€‰æ‹© TypedDictï¼Ÿ

**TypedDict** æ˜¯ Python 3.8+ å¼•å…¥çš„ç±»å‹æç¤ºå·¥å…·ï¼Œç”¨äºå®šä¹‰å­—å…¸çš„ç»“æ„ã€‚

**ä¼˜åŠ¿**:

1. **ç±»å‹å®‰å…¨**: IDE è‡ªåŠ¨è¡¥å…¨ã€é™æ€ç±»å‹æ£€æŸ¥
2. **æ–‡æ¡£ä½œç”¨**: å­—æ®µå®šä¹‰å³æ–‡æ¡£ï¼Œæ¸…æ™°æ˜äº†
3. **æ— è¿è¡Œæ—¶å¼€é”€**: ä»…ç”¨äºç±»å‹æç¤ºï¼Œä¸å½±å“æ€§èƒ½
4. **LangGraph å…¼å®¹**: LangGraph åŸç”Ÿæ”¯æŒ TypedDict ä½œä¸ºçŠ¶æ€ç±»å‹

**ç¤ºä¾‹å¯¹æ¯”**:

```python
# âŒ æ™®é€šå­—å…¸ï¼šæ— ç±»å‹æç¤ºï¼Œæ˜“å‡ºé”™
state = {"url": "https://example.com", "step": 1}
print(state["urll"])  # æ‹¼å†™é”™è¯¯ï¼Œè¿è¡Œæ—¶æ‰æŠ¥é”™

# âœ… TypedDictï¼šIDE è‡ªåŠ¨æ£€æŸ¥
class State(TypedDict):
    url: str
    step: int

state: State = {"url": "https://example.com", "step": 1}
print(state["urll"])  # IDE ç«‹å³æ ‡çº¢æç¤ºé”™è¯¯
```

### 2.2 CrawlerDevState å®Œæ•´å­—æ®µè§£æ

DevBot çš„çŠ¶æ€å®šä¹‰åœ¨ `devbot/state/crawler_state.py:8-56`ï¼Œå…± **27+ ä¸ªå­—æ®µ**ï¼Œåˆ†ä¸º 7 å¤§ç±»ï¼š

#### 2.2.1 çˆ¬è™«åŸºæœ¬ä¿¡æ¯ (4 å­—æ®µ)

```python
class CrawlerDevState(TypedDict):
    # ============= çˆ¬è™«åŸºæœ¬ä¿¡æ¯ =============
    url: str                             # ç›®æ ‡ç½‘ç«™ URL
    site_name: str                       # ç½‘ç«™åç§° (ä» URL æå–)
    proxy: Optional[str]                 # ä»£ç†æœåŠ¡å™¨åœ°å€
    category: str                        # çˆ¬è™«åˆ†ç±» (product/shopping/dealç­‰)
```

**å­—æ®µè¯¦è§£**:

| å­—æ®µ        | ç±»å‹            | è¯´æ˜                     | ç¤ºä¾‹                        | ä½¿ç”¨åœºæ™¯               |
| ----------- | --------------- | ------------------------ | --------------------------- | ---------------------- |
| `url`       | `str`           | ç›®æ ‡ç½‘ç«™å…¥å£ URL         | `https://www.gnc.com`       | æ‰€æœ‰æ­¥éª¤ï¼Œç”¨äºè®¿é—®ç½‘ç«™ |
| `site_name` | `str`           | ç½‘ç«™æ ‡è¯†ç¬¦ï¼ˆä»åŸŸåæå–ï¼‰ | `gnc`                       | æ–‡ä»¶å‘½åã€æ¨¡å—å¯¼å…¥     |
| `proxy`     | `Optional[str]` | ä»£ç†åœ°å€ï¼ˆåçˆ¬æ—¶ä½¿ç”¨ï¼‰   | `http://proxy:8080`         | Step 1 é¡µé¢è®¿é—®        |
| `category`  | `str`           | çˆ¬è™«ç±»åˆ«                 | `product`/`shopping`/`deal` | å†³å®šè¾“å‡ºç›®å½•ç»“æ„       |

**æå– site_name çš„é€»è¾‘**:

```python
# crawler_devbot.py:155-158
from urllib.parse import urlparse
domain = urlparse(url).netloc
site_name = domain.split('.')[-2]  # www.gnc.com â†’ gnc
```

**æ½œåœ¨é—®é¢˜**:

- **å­åŸŸåæ··æ·†**: `shop.example.com` â†’ `example`ï¼ˆæ­£ç¡®ï¼‰
- **å¤šçº§åŸŸå**: `example.co.uk` â†’ `co`ï¼ˆé”™è¯¯ï¼‰
  - **è§£å†³**: æ‰‹åŠ¨æŒ‡å®š `site_name` å‚æ•°

#### 2.2.2 å½“å‰æ­¥éª¤ä¿¡æ¯ (3 å­—æ®µ)

```python
# ============= å½“å‰æ­¥éª¤ä¿¡æ¯ =============
current_step: str                    # å½“å‰æ­¥éª¤ç¼–å· ("0", "1", "2", ...)
current_step_name: str               # å½“å‰æ­¥éª¤åç§° (å¦‚ "create_base_file")
current_depth: int                   # å½“å‰æ·±åº¦ï¼ˆç”¨äºåµŒå¥—æ­¥éª¤ï¼‰
```

**å­—æ®µè¯¦è§£**:

| å­—æ®µ                | ç±»å‹  | è¯´æ˜                           | ç¤ºä¾‹                               | æ›´æ–°æ—¶æœº                |
| ------------------- | ----- | ------------------------------ | ---------------------------------- | ----------------------- |
| `current_step`      | `str` | æ­¥éª¤ç¼–å·ï¼ˆå­—ç¬¦ä¸²ï¼Œæ”¯æŒå­æ­¥éª¤ï¼‰ | `"0"`, `"2.1"`, `"5.3"`            | æ¯ä¸ª Developer èŠ‚ç‚¹ç»“æŸ |
| `current_step_name` | `str` | æ­¥éª¤è¯­ä¹‰åç§°                   | `create_base_file`, `analyze_page` | æ¯ä¸ª Developer èŠ‚ç‚¹ç»“æŸ |
| `current_depth`     | `int` | é€’å½’æ·±åº¦ï¼ˆStep 7 ä¸“ç”¨ï¼‰        | `0`, `1`, `2`                      | Step 7 ç½‘ç«™æ ‘æ‰©å±•       |

**æ­¥éª¤ç¼–å·è§„åˆ™**:

- **ä¸»æ­¥éª¤**: `"0"`, `"1"`, `"2"`, ... , `"10"`
- **å­æ­¥éª¤**: `"2.1"`, `"5.1"`, `"5.2"`, ... , `"5.6"`
- **ç‰¹æ®Šæ­¥éª¤**: `"20"`, `"21"`, `"22"` (ä»£ç æ£€æŸ¥ä¸ä¼˜åŒ–)

**ç¤ºä¾‹**:

```python
# Step 2 å®Œæˆå
state = {
    "current_step": "2",
    "current_step_name": "generate_list_extractor",
    ...
}

# Step 2.1 å®Œæˆå
state = {
    "current_step": "2.1",
    "current_step_name": "classify_urls",
    ...
}
```

#### 2.2.3 æ­¥éª¤çŠ¶æ€ (2 å­—æ®µ)

```python
# ============= æ­¥éª¤çŠ¶æ€ =============
status: Literal["pending", "in_progress", "completed", "reviewed", "failed"]
retry_count: int                     # é‡è¯•æ¬¡æ•°è®¡æ•°å™¨
```

**å­—æ®µè¯¦è§£**:

| å­—æ®µ          | ç±»å‹      | è¯´æ˜             | å¯é€‰å€¼                                                      | çŠ¶æ€è½¬æ¢                         |
| ------------- | --------- | ---------------- | ----------------------------------------------------------- | -------------------------------- |
| `status`      | `Literal` | å½“å‰æ­¥éª¤çŠ¶æ€     | `pending`, `in_progress`, `completed`, `reviewed`, `failed` | Developer â†’ Reviewer â†’ Next Step |
| `retry_count` | `int`     | å½“å‰æ­¥éª¤é‡è¯•æ¬¡æ•° | `0`, `1`, `2`, ...                                          | éªŒè¯å¤±è´¥æ—¶ +1                    |

**çŠ¶æ€æœºè½¬æ¢**:

```
pending
   â”‚
   â†“ (Developer å¼€å§‹)
in_progress
   â”‚
   â”œâ”€â†’ completed (Developer æˆåŠŸ) â”€â”€â”€â”€â†’ reviewed (Reviewer é€šè¿‡) â”€â”€â†’ Next Step
   â”‚                                         â†“ (Reviewer å¤±è´¥)
   â””â”€â†’ failed â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ pending (retry_count++)
```

**é‡è¯•é€»è¾‘**:

```python
# routing_logic.py:120-130 (ç®€åŒ–ç‰ˆ)
def should_retry(state: CrawlerDevState) -> bool:
    max_retries = 3
    if state["status"] == "failed" and state["retry_count"] < max_retries:
        return True
    elif state["retry_count"] >= max_retries:
        raise Exception(f"Step {state['current_step']} å¤±è´¥æ¬¡æ•°è¿‡å¤š")
    return False
```

**æ½œåœ¨é—®é¢˜**:

- **æ— é™é‡è¯•**: å¦‚æœ Reviewer é€»è¾‘æœ‰è¯¯ï¼Œå¯èƒ½å¯¼è‡´æ­»å¾ªç¯
  - **è§£å†³**: `max_retries = 3` ç¡¬ç¼–ç é™åˆ¶
- **çŠ¶æ€ä¸ä¸€è‡´**: å¦‚æœ Developer æŠ›å¼‚å¸¸ä½†æœªæ›´æ–°çŠ¶æ€
  - **è§£å†³**: LangGraph çš„å¼‚å¸¸å¤„ç†æœºåˆ¶ä¼šæ•è·å¹¶æ ‡è®°ä¸º `failed`

#### 2.2.4 æ­¥éª¤ç»“æœ (3 å­—æ®µ)

```python
# ============= æ­¥éª¤ç»“æœ =============
result: Optional[str]                # å½“å‰æ­¥éª¤çš„ LLM å“åº”æ–‡æœ¬
validation_result: Optional[Dict[str, Any]]  # Reviewer çš„éªŒè¯ç»“æœ
error: Optional[str]                 # é”™è¯¯ä¿¡æ¯
```

**å­—æ®µè¯¦è§£**:

| å­—æ®µ                | ç±»å‹             | è¯´æ˜              | ç¤ºä¾‹                                  | ä½¿ç”¨åœºæ™¯           |
| ------------------- | ---------------- | ----------------- | ------------------------------------- | ------------------ |
| `result`            | `Optional[str]`  | LLM çš„å®Œæ•´å“åº”    | `"å·²å®Œæˆé¡µé¢åˆ†æ..."`                 | Developer èŠ‚ç‚¹è¾“å‡º |
| `validation_result` | `Optional[Dict]` | Reviewer éªŒè¯ç»“æœ | `{"valid": True, "message": "..."}`   | Reviewer èŠ‚ç‚¹è¾“å‡º  |
| `error`             | `Optional[str]`  | é”™è¯¯è¯¦æƒ…          | `"å¯¼å…¥æ¨¡å—å¤±è´¥: ModuleNotFoundError"` | å¼‚å¸¸å¤„ç†           |

**ç¤ºä¾‹**:

```python
# Step 1 Developer å®Œæˆå
state["result"] = """
å·²å®Œæˆé¡µé¢åˆ†æ:
- é¡µé¢å¤æ‚åº¦: medium
- æ¨èå¼•æ“: browser_pool
- å¹¶å‘é…ç½®: pool_size=3, tab_size=5
"""

# Step 1 Reviewer å®Œæˆå
state["validation_result"] = {
    "step": "step1",
    "success": True,
    "message": "é¡µé¢åˆ†æå®Œæˆ,ä»£ç å¯æ­£å¸¸æ‰§è¡Œ"
}

# å¦‚æœå¤±è´¥
state["error"] = "ImportError: No module named 'crawler.product.extractor_gnc'"
state["status"] = "failed"
```

#### 2.2.5 URL æ¨¡å¼å¤„ç† (6 å­—æ®µ)

è¿™æ˜¯ Step 4-6 å¾ªç¯å¤„ç† URL patterns çš„æ ¸å¿ƒå­—æ®µã€‚

```python
# ============= URL æ¨¡å¼å¤„ç† (Step 4-7 ä¸“ç”¨) =============
current_url_pattern: Optional[str]   # å½“å‰æ­£åœ¨å¤„ç†çš„ URL pattern
current_sample_url: Optional[str]    # å½“å‰ URL pattern çš„æ ·ä¾‹ URL
current_sample_url_md5: str          # æ ·ä¾‹ URL çš„ MD5 (ç”¨äºç¼“å­˜æ–‡ä»¶å)
completed_patterns: List[str]        # å·²å®Œæˆçš„ URL patterns åˆ—è¡¨

# LangGraph å¾ªç¯æ§åˆ¶
patterns_queue: List[Dict[str, Any]]  # å¾…å¤„ç†çš„ patterns é˜Ÿåˆ—
current_pattern_info: Optional[Dict[str, Any]]  # å½“å‰ pattern ä¿¡æ¯
```

**å­—æ®µè¯¦è§£**:

| å­—æ®µ                     | ç±»å‹             | è¯´æ˜                        | ç¤ºä¾‹                                                         | ä½•æ—¶ä½¿ç”¨            |
| ------------------------ | ---------------- | --------------------------- | ------------------------------------------------------------ | ------------------- |
| `current_url_pattern`    | `Optional[str]`  | å½“å‰å¤„ç†çš„æ­£åˆ™è¡¨è¾¾å¼        | `r'https://www\.gnc\.com/[\w-]+/\d+\.html'`                  | Step 4.1, 5.x       |
| `current_sample_url`     | `Optional[str]`  | ç”¨äºæµ‹è¯• pattern çš„çœŸå® URL | `https://www.gnc.com/vitamins/123.html`                      | Step 5.x (è®¿é—®é¡µé¢) |
| `current_sample_url_md5` | `str`            | MD5 å“ˆå¸Œå€¼                  | `a1b2c3d4...`                                                | ä¸´æ—¶æ–‡ä»¶å‘½å        |
| `completed_patterns`     | `List[str]`      | å·²å¤„ç†å®Œçš„ patterns         | `["pattern1", "pattern2"]`                                   | åˆ¤æ–­æ˜¯å¦ç»§ç»­å¾ªç¯    |
| `patterns_queue`         | `List[Dict]`     | å¾…å¤„ç†é˜Ÿåˆ—                  | `[{"pattern": "...", "type": "detail", "sample_url": "..."}]` | Step 4-6 å¾ªç¯       |
| `current_pattern_info`   | `Optional[Dict]` | å½“å‰ pattern çš„å®Œæ•´ä¿¡æ¯     | `{"pattern": "...", "type": "detail", ...}`                  | Step 5.x å„å­æ­¥éª¤   |

**å·¥ä½œæµç¨‹ç¤ºä¾‹**:

```python
# Step 4: åˆå§‹åŒ–é˜Ÿåˆ—
state["patterns_queue"] = [
    {"pattern": "r'https://www\.gnc\.com/[\w-]+/\d+\.html'", "type": "detail", "sample_url": "https://www.gnc.com/vitamins/123.html"},
    {"pattern": "r'https://www\.gnc\.com/category/[\w-]+'", "type": "list", "sample_url": "https://www.gnc.com/category/vitamins"}
]
state["completed_patterns"] = []

# Step 4.1: å–å‡ºç¬¬ä¸€ä¸ª pattern
state["current_pattern_info"] = state["patterns_queue"].pop(0)
state["current_url_pattern"] = state["current_pattern_info"]["pattern"]
state["current_sample_url"] = state["current_pattern_info"]["sample_url"]
state["current_sample_url_md5"] = hashlib.md5(state["current_sample_url"].encode()).hexdigest()

# Step 5.1-5.6: å¤„ç†å½“å‰ pattern (ç”Ÿæˆæå–å™¨çš„å„ä¸ªæ–¹æ³•)
# ...

# Step 6: å°† pattern æ ‡è®°ä¸ºå®Œæˆ
state["completed_patterns"].append(state["current_url_pattern"])

# è·¯ç”±åˆ¤æ–­: å¦‚æœé˜Ÿåˆ—éç©ºï¼Œè¿”å› Step 4.1 (ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ª pattern)
if state["patterns_queue"]:
    return "step4_1__next_pattern"
else:
    return "step7__site_tree_expand"
```

**æ½œåœ¨é—®é¢˜**:

- **é˜Ÿåˆ—ä¸ºç©ºä½†æœªå¤„ç†**: å¦‚æœ URL patterns ä¸ºç©ºï¼Œä¼šè·³è¿‡ Step 5-6
  - **è§£å†³**: Step 3 Reviewer éªŒè¯ `url_detail_patterns` éç©º
- **MD5 å†²çª**: ç†è®ºä¸Šå¯èƒ½ï¼ˆæ¦‚ç‡æä½ï¼‰
  - **è§£å†³**: å®é™…ä¸­æœªé‡åˆ°ï¼Œæš‚ä¸å¤„ç†

#### 2.2.6 ç¼“å­˜æ•°æ® (1 å­—æ®µ)

```python
# ============= ç¼“å­˜æ•°æ® =============
cache_data: Dict[str, Any]           # ç¼“å­˜æ•°æ® (urls ç­‰)
```

**ç”¨é€”**: å­˜å‚¨ `extract_deals_from_mainpage` è¿”å›çš„ URLs åˆ—è¡¨ï¼Œé¿å…é‡å¤æ‰§è¡Œã€‚

**ç¤ºä¾‹**:

```python
# Step 2 å®Œæˆå
state["cache_data"] = {
    "site_tree": [
        {"title": "Product 1", "url": "https://...", "type": "detail"},
        {"title": "Category", "url": "https://...", "type": "list"},
        ...
    ]
}
```

#### 2.2.7 æ§åˆ¶å­—æ®µ (8 å­—æ®µ)

```python
# ============= Step 5 æ§åˆ¶ =============
last_file_id: Optional[str]         # å½“å‰ä¿å­˜æ–‡ä»¶çš„idæ ‡è¯†

# ============= Step 7 å¾ªç¯æ§åˆ¶ =============
has_new_patterns_in_step7: bool      # Step 7 æ˜¯å¦æ£€æµ‹åˆ°æ–° patterns
step7_loop_count: int                # Step 7 å¾ªç¯è®¡æ•°å™¨

# ============= è¾“å‡ºè·¯å¾„ =============
base_file_path: str                  # ç”Ÿæˆçš„çˆ¬è™«æ–‡ä»¶è·¯å¾„ (extractor_xxx.py)

# ============= Session ç®¡ç† =============
session_id: Optional[str]            # Claude SDK session_id

# ============= æµç¨‹æ§åˆ¶ =============
next_action: Optional[str]           # ä¸‹ä¸€æ­¥åŠ¨ä½œ
regenerate_step: Optional[str]       # éœ€è¦é‡æ–°ç”Ÿæˆçš„æ­¥éª¤
regenerate_from: Optional[str]       # step22__fix_code ä¿®å¤åè·³å›çš„æ­¥éª¤
```

**å­—æ®µè¯¦è§£**:

| å­—æ®µ                        | ç±»å‹            | è¯´æ˜                       | ç¤ºä¾‹                        | ä½•æ—¶ä½¿ç”¨                |
| --------------------------- | --------------- | -------------------------- | --------------------------- | ----------------------- |
| `last_file_id`              | `Optional[str]` | å½“å‰æ–‡ä»¶æ ‡è¯†               | `"detail_page_v2"`          | Step 5.x å¤šæ–‡ä»¶ç®¡ç†     |
| `has_new_patterns_in_step7` | `bool`          | Step 7 æ˜¯å¦å‘ç°æ–° patterns | `True`/`False`              | Step 7 å†³å®šæ˜¯å¦ç»§ç»­æ‰©å±• |
| `step7_loop_count`          | `int`           | Step 7 å¾ªç¯æ¬¡æ•°            | `0`, `1`, `2`               | é˜²æ­¢æ— é™å¾ªç¯ (max=3)    |
| `base_file_path`            | `str`           | çˆ¬è™«ä»£ç æ–‡ä»¶è·¯å¾„           | `/path/to/extractor_gnc.py` | æ‰€æœ‰ä»£ç ç”Ÿæˆæ­¥éª¤        |
| `session_id`                | `Optional[str]` | Claude ä¼šè¯ ID             | `"session_abc123"`          | ä¿æŒå¯¹è¯ä¸Šä¸‹æ–‡          |
| `next_action`               | `Optional[str]` | ä¸‹ä¸€æ­¥åŠ¨ä½œ                 | `"continue"`, `"retry"`     | ç”¨æˆ·äº¤äº’                |
| `regenerate_step`           | `Optional[str]` | éœ€é‡åšçš„æ­¥éª¤               | `"step5"`                   | ç”¨æˆ·æ‰‹åŠ¨è§¦å‘            |
| `regenerate_from`           | `Optional[str]` | ä¿®å¤åçš„è¿”å›ç‚¹             | `"step9"`                   | Step 22 ä¿®å¤ä»£ç åè·³è½¬  |

**Step 7 å¾ªç¯æ§åˆ¶ç¤ºä¾‹**:

```python
# Step 7 ç¬¬ä¸€æ¬¡æ‰§è¡Œ
state["step7_loop_count"] = 0
state["has_new_patterns_in_step7"] = False

# å¦‚æœå‘ç°æ–° patterns
if new_patterns_found:
    state["has_new_patterns_in_step7"] = True
    state["step7_loop_count"] += 1

    # è·¯ç”±åˆ¤æ–­
    if state["step7_loop_count"] < 3:
        return "step3__generate_url_patterns"  # é‡æ–°ç”Ÿæˆ patterns
    else:
        logger.warning("Step 7 å¾ªç¯æ¬¡æ•°è¾¾åˆ°ä¸Šé™,åœæ­¢æ‰©å±•")
        return "step8__analyze_markdown_info"
```

### 2.3 State çš„ç”Ÿå‘½å‘¨æœŸ

```
1. åˆå§‹åŒ– (crawler_devbot.py:155-180)
   â”œâ”€ ä»å‘½ä»¤è¡Œå‚æ•°æå–: url, site_name, category
   â”œâ”€ è®¾ç½®é»˜è®¤å€¼: status="pending", retry_count=0
   â””â”€ è®¡ç®—è·¯å¾„: base_file_path

2. Developer èŠ‚ç‚¹æ›´æ–° (developer_nodes.py:å„æ­¥éª¤)
   â”œâ”€ æ›´æ–° current_step, current_step_name
   â”œâ”€ æ›´æ–° status="completed"
   â”œâ”€ ä¿å­˜ result (LLM å“åº”)
   â””â”€ è¿”å›æ›´æ–°åçš„ state

3. Reviewer èŠ‚ç‚¹éªŒè¯ (reviewer_nodes.py)
   â”œâ”€ è¯»å– current_step å†³å®šéªŒè¯é€»è¾‘
   â”œâ”€ æ‰§è¡ŒéªŒè¯ (å¯¼å…¥æ¨¡å—ã€è°ƒç”¨å‡½æ•°ç­‰)
   â”œâ”€ æ›´æ–° status="reviewed" æˆ– "failed"
   â”œâ”€ ä¿å­˜ validation_result æˆ– error
   â””â”€ æ›´æ–° retry_count (å¤±è´¥æ—¶ +1)

4. è·¯ç”±å†³ç­– (routing_logic.py)
   â”œâ”€ è¯»å– status, current_step, retry_count
   â”œâ”€ åˆ¤æ–­ä¸‹ä¸€æ­¥: next_developer, retry, end
   â””â”€ è¿”å›èŠ‚ç‚¹åç§°

5. æŒä¹…åŒ– (MemorySaver + JSON)
   â”œâ”€ LangGraph MemorySaver: è‡ªåŠ¨ä¿å­˜åˆ° local_state_<site>.json
   â””â”€ ConversationStore: å¯¹è¯å†å²å­˜å…¥ SQLite
```

---

## 3. Tool å·¥å…·ç³»ç»Ÿæ·±åº¦è§£æ

### 3.1 å·¥å…·åˆ†ç±»

DevBot ä½¿ç”¨çš„å·¥å…·åˆ†ä¸ºä¸‰å¤§ç±»ï¼š

```
Tools
â”œâ”€â”€ MCP å·¥å…· (Model Context Protocol)
â”‚   â”œâ”€â”€ chrome-devtools (ç½‘é¡µæ“ä½œ)
â”‚   â”œâ”€â”€ gemini-cli (LLM è°ƒç”¨)
â”‚   â”œâ”€â”€ playwright (å¤‡ç”¨æµè§ˆå™¨å·¥å…·)
â”‚   â”œâ”€â”€ mongodb (æ•°æ®åº“)
â”‚   â”œâ”€â”€ notion (ç¬”è®°)
â”‚   â””â”€â”€ git (ç‰ˆæœ¬æ§åˆ¶)
â”‚
â”œâ”€â”€ Claude SDK å†…ç½®å·¥å…·
â”‚   â”œâ”€â”€ Read (è¯»å–æ–‡ä»¶)
â”‚   â”œâ”€â”€ Write (å†™å…¥æ–‡ä»¶)
â”‚   â”œâ”€â”€ Edit (ç¼–è¾‘æ–‡ä»¶)
â”‚   â”œâ”€â”€ Bash (æ‰§è¡Œå‘½ä»¤)
â”‚   â”œâ”€â”€ Glob (æŸ¥æ‰¾æ–‡ä»¶)
â”‚   â””â”€â”€ Grep (æœç´¢å†…å®¹)
â”‚
â””â”€â”€ è‡ªå®šä¹‰ Python å·¥å…· (devbot/tool.py)
    â”œâ”€â”€ compress_image (å›¾ç‰‡å‹ç¼©)
    â”œâ”€â”€ save_tmp_page_by_brightdata (BrightData è·å–é¡µé¢)
    â”œâ”€â”€ classify_urls (URL åˆ†ç±»)
    â””â”€â”€ check_url_patterns (æ£€æŸ¥ URL patterns)
```

### 3.2 MCP (Model Context Protocol) è¯¦è§£

**MCP** æ˜¯ Anthropic æå‡ºçš„ä¸€ç§å·¥å…·åè®®ï¼Œå…è®¸ LLM é€šè¿‡æ ‡å‡†åŒ–æ¥å£è°ƒç”¨å¤–éƒ¨å·¥å…·ã€‚

#### 3.2.1 chrome-devtools MCP

**ä½œç”¨**: æ§åˆ¶ Chrome æµè§ˆå™¨è¿›è¡Œç½‘é¡µæ“ä½œã€‚

**æ ¸å¿ƒåŠŸèƒ½**:

| å‡½æ•°                    | è¯´æ˜               | å‚æ•°                                              | è¿”å›å€¼    | ä½¿ç”¨åœºæ™¯        |
| ----------------------- | ------------------ | ------------------------------------------------- | --------- | --------------- |
| `new_page`              | æ‰“å¼€æ–°é¡µé¢         | `url: str`                                        | é¡µé¢ç´¢å¼•  | Step 1 æ‰“å¼€ç½‘ç«™ |
| `navigate_page`         | å¯¼èˆªåˆ° URL         | `url: str, type: "url"/"back"/"forward"/"reload"` | æ—         | é¡µé¢è·³è½¬        |
| `take_screenshot`       | æˆªå›¾               | `fullPage: bool, format: str, filePath: str`      | å›¾ç‰‡è·¯å¾„  | Step 1 é¡µé¢æˆªå›¾ |
| `take_snapshot`         | é¡µé¢å¿«ç…§ (a11y æ ‘) | `filePath: str`                                   | å¿«ç…§æ–‡æœ¬  | è·å–é¡µé¢ç»“æ„    |
| `click`                 | ç‚¹å‡»å…ƒç´            | `uid: str`                                        | æ—         | äº¤äº’æ“ä½œ        |
| `fill`                  | å¡«å†™è¡¨å•           | `uid: str, value: str`                            | æ—         | å¡«å†™è¾“å…¥æ¡†      |
| `evaluate_script`       | æ‰§è¡Œ JS            | `function: str, args: List`                       | JS è¿”å›å€¼ | æå–æ•°æ®        |
| `list_network_requests` | ç½‘ç»œè¯·æ±‚åˆ—è¡¨       | `pageIdx: int`                                    | è¯·æ±‚æ•°ç»„  | åˆ†æ AJAX       |

**ç¤ºä¾‹**:

```python
# Step 1 prompt ä¸­çš„ä½¿ç”¨
```

ç”¨ chrome-devtools æ‰“å¼€ç½‘ç«™: {{ url }}

1. ä½¿ç”¨ new_page æ‰“å¼€
2. ä½¿ç”¨ take_screenshot æˆªé•¿å›¾ (fullPage=true, format="jpeg")
3. ä½¿ç”¨ list_network_requests æŸ¥çœ‹ AJAX è¯·æ±‚

**æ½œåœ¨é—®é¢˜**:

- **æˆªå›¾è¿‡å¤§**: æŸäº›ç½‘ç«™æˆªå›¾è¶…è¿‡ 8000pxï¼ŒClaude API æ‹’ç»
  - **è§£å†³**: Step 1 prompt ä¸­è¦æ±‚ä½¿ç”¨ `compress_image` å·¥å…·
- **åçˆ¬é˜»æ­¢**: chrome-devtools ç›´æ¥è®¿é—®è¢« Cloudflare æ‹¦æˆª
  - **è§£å†³**: å…ˆç”¨ `save_tmp_page_by_brightdata` è·å– HTMLï¼Œå†é€šè¿‡ `file://` è®¿é—®

#### 3.2.2 gemini-cli MCP

**ä½œç”¨**: è°ƒç”¨ Google Gemini APIï¼ˆå¤‡ç”¨ LLMï¼‰ã€‚

**ä½¿ç”¨åœºæ™¯**: å½“éœ€è¦å¤§æ‰¹é‡å¹¶å‘è°ƒç”¨æ—¶ï¼ŒGemini æˆæœ¬æ›´ä½ã€‚

### 3.3 Claude SDK å†…ç½®å·¥å…·

Claude Agent SDK æä¾›äº†ä¸€å¥—æ–‡ä»¶å’Œç³»ç»Ÿæ“ä½œå·¥å…·ã€‚

#### 3.3.1 æ–‡ä»¶æ“ä½œå·¥å…·

| å·¥å…·     | è¯´æ˜     | å‚æ•°                                               | ä½¿ç”¨åœºæ™¯           |
| -------- | -------- | -------------------------------------------------- | ------------------ |
| `Read`   | è¯»å–æ–‡ä»¶ | `file_path: str`                                   | æŸ¥çœ‹ä»£ç ã€é…ç½®æ–‡ä»¶ |
| `Write`  | å†™å…¥æ–‡ä»¶ | `file_path: str, content: str`                     | åˆ›å»ºæ–°æ–‡ä»¶         |
| `Edit`   | ç¼–è¾‘æ–‡ä»¶ | `file_path: str, old_string: str, new_string: str` | ä¿®æ”¹ä»£ç            |
| `Move`   | ç§»åŠ¨æ–‡ä»¶ | `source: str, destination: str`                    | é‡å‘½å/ç§»åŠ¨        |
| `Delete` | åˆ é™¤æ–‡ä»¶ | `file_path: str`                                   | æ¸…ç†ä¸´æ—¶æ–‡ä»¶       |

**è‡ªåŠ¨æ‰¹å‡†æœºåˆ¶**:

DevBot ä¸­æ‰€æœ‰è¿™äº›å·¥å…·éƒ½é…ç½®äº†è‡ªåŠ¨æ‰¹å‡† hookï¼Œæ— éœ€æ‰‹åŠ¨ç¡®è®¤ã€‚

```python
# claude_agent_base.py:116-137
COMMANDS = [
    "Read", "Write", "Edit", "TodoWrite", "Move", "Delete",
    "ListDir", "MakeDir", "Grep", "Glob", "Search",
    "GetDefinition", "GetReferences", "Bash",
]

# ä¸ºæ¯ä¸ªå·¥å…·é…ç½® auto_approve hook
hooks = {
    "PreToolUse": [
        HookMatcher(matcher="Bash", hooks=[auto_approve]),
        HookMatcher(matcher="Read", hooks=[auto_approve]),
        HookMatcher(matcher="Edit", hooks=[auto_approve]),
        # ... å…¶ä»–å·¥å…·
    ]
}
```

#### 3.3.2 Bash å·¥å…·

**ç‰¹æ®Šæ€§**: Bash å·¥å…·å¯ä»¥æ‰§è¡Œä»»æ„ shell å‘½ä»¤ï¼Œéœ€è¦è°¨æ…ä½¿ç”¨ã€‚

**è‡ªåŠ¨æ‰¹å‡†ç­–ç•¥**:

```python
# claude_agent_base.py:53-77
async def auto_approve(input_data, tool_use_id, context):
    """è‡ªåŠ¨æ‰¹å‡† Bash/Edit/Read ç­‰åŸºç¡€å·¥å…·"""
    tool_name = input_data.get('tool_name', 'unknown')
    tool_input = input_data.get('tool_input', {})

    if tool_name == 'Bash':
        logger.debug(f"ğŸ”§ Hook è¢«è§¦å‘: {tool_name}ï¼Œç›®æ ‡: {tool_input.pop('description', '')}")
        logger.debug(f"ğŸ“‹ å‘½ä»¤: {tool_input.get('command')}")

    return {
        "hookSpecificOutput": {
            "hookEventName": "PreToolUse",
            "permissionDecision": "allow",  # è‡ªåŠ¨æ‰¹å‡†
        }
    }
```

**å®‰å…¨è€ƒè™‘**:

- DevBot è¿è¡Œåœ¨å—æ§ç¯å¢ƒï¼ˆæœ¬åœ°å¼€å‘æœºå™¨ï¼‰
- ä»…ç”¨äºæµ‹è¯•çˆ¬è™«ä»£ç ï¼Œä¸æ¶‰åŠæ•æ„Ÿæ“ä½œ
- æ‰€æœ‰å‘½ä»¤éƒ½æœ‰æ—¥å¿—è®°å½•

### 3.4 è‡ªå®šä¹‰ Python å·¥å…· (devbot/tool.py)

#### 3.4.1 compress_image (å›¾ç‰‡å‹ç¼©å·¥å…·)

**ä½œç”¨**: å‹ç¼©å›¾ç‰‡åˆ° Claude API å¯æ¥å—çš„å¤§å°å’Œå°ºå¯¸ã€‚

**Claude API é™åˆ¶**:

- **æ–‡ä»¶å¤§å°**: 5 MB (base64 ç¼–ç å)
- **åƒç´ å°ºå¯¸**: å•è¾¹ä¸è¶…è¿‡ 8000px

**å®ç°åŸç†**:

```python
# tool.py:compress_image ä¼ªä»£ç 
async def compress_image(file_path: str, output_path: str = None):
    img = Image.open(file_path)
    width, height = img.size

    # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é™åˆ¶
    if max(width, height) > 8000:
        # åˆ‡åˆ†å›¾ç‰‡
        num_slices = math.ceil(height / 8000)
        slices = []
        for i in range(num_slices):
            slice_img = img.crop((0, i*8000, width, min((i+1)*8000, height)))
            slice_path = output_path.replace('.webp', f'_{i}.webp')
            slice_img.save(slice_path, format='WEBP', quality=85)
            slices.append(slice_path)
        return slices

    # æ£€æŸ¥æ–‡ä»¶å¤§å°
    if os.path.getsize(file_path) > 2 * 1024 * 1024:  # 2 MB
        # å‹ç¼©ä¸º WebP
        img.save(output_path, format='WEBP', quality=85, optimize=True)

    return [output_path]
```

**ä½¿ç”¨åœºæ™¯**:

```python
# Step 1 prompt ä¸­è¦æ±‚
# å¦‚æœå›¾ç‰‡è¶…è¿‡äº† 2 MB æˆ–å•è¾¹åƒç´ è¶…è¿‡äº†8000ï¼Œæ‰§è¡Œä»¥ä¸‹å‘½ä»¤å‹ç¼©å›¾ç‰‡:
python -m devbot.tool compress_image {{ output_part_dir }}/mainpage_long_screenshot.jpg {{ output_part_dir }}/mainpage_long_screenshot.webp
```

**æ½œåœ¨é—®é¢˜**:

- **DecompressionBombError**: å›¾ç‰‡è¿‡å¤§ï¼ˆè¶…è¿‡ 1 äº¿åƒç´ ï¼‰
  - **è§£å†³**: Step 1 prompt è¦æ±‚é‡è¯•æµç¨‹ï¼ˆå…³é—­é¡µé¢ â†’ ç­‰å¾… â†’ é‡æ–°æˆªå›¾ï¼Œæœ€å¤š 5 æ¬¡ï¼‰

#### 3.4.2 save_tmp_page_by_brightdata (åçˆ¬å¤„ç†)

**ä½œç”¨**: ä½¿ç”¨ BrightData ä»£ç†è·å–é¡µé¢ HTMLï¼Œç»•è¿‡åçˆ¬é™åˆ¶ã€‚

**å®ç°**:

```python
# tool.py:save_tmp_page_by_brightdata
async def save_tmp_page_by_brightdata(url: str) -> str:
    """
    ä½¿ç”¨ BrightData è·å–é¡µé¢ HTML å¹¶ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶

    Returns:
        str: ä¸´æ—¶ HTML æ–‡ä»¶è·¯å¾„
    """
    bd_client = BDClient()
    html = await bd_client.get_page_html(url)

    tmp_file = f"/tmp/scraper/brightdata_{hashlib.md5(url.encode()).hexdigest()}.html"
    with open(tmp_file, 'w', encoding='utf-8') as f:
        f.write(html)

    logger.info(f"âœ… BrightData é¡µé¢å·²ä¿å­˜: {tmp_file}")
    return tmp_file
```

**ä½¿ç”¨åœºæ™¯**:

```python
# Step 1 prompt ä¸­çš„åçˆ¬å¤„ç†
# å¦‚æœåçˆ¬å¯¼è‡´ chrome-devtools æ— æ³•ç›´æ¥è®¿é—®é¡µé¢ï¼Œåˆ™å…ˆä½¿ç”¨ brightdata ä¿å­˜ä¸´æ—¶é¡µé¢ï¼Œ
# å†ç”¨ chrome-devtools é€šè¿‡æ–‡ä»¶åœ°å€è®¿é—®è¿™ä¸ªä¸´æ—¶é¡µé¢ã€‚
python -m devbot.tool save_tmp_page_by_brightdata <url>
```

#### 3.4.3 classify_urls / check_url_patterns

**ä½œç”¨**: è¾…åŠ© LLM è¿›è¡Œ URL åˆ†ç±»å’Œ pattern éªŒè¯ã€‚

**å®ç°**:

```python
# tool.py:classify_urls
async def classify_urls(category: str, site_name: str):
    """
    è¯»å– site_tree.jsonï¼Œç”¨ LLM åˆ†ç±»æ‰€æœ‰ URLs

    Returns:
        dict: {'detail': [...], 'list': [...], 'other': [...]}
    """
    site_tree_file = f"crawler/{category}/output/{site_name}/site_tree.json"
    with open(site_tree_file, 'r') as f:
        data = json.load(f)

    urls = data['site_tree']

    # è°ƒç”¨ Gemini API (æˆæœ¬æ›´ä½)
    result = await call_gemini_classify(urls)
    return result
```

---

## 4. Claude Agent SDK é›†æˆè¯¦è§£

### 4.1 ClaudeSDKClient æ¶æ„

**ClaudeSDKClient** æ˜¯ Anthropic å®˜æ–¹æä¾›çš„ Python SDKï¼Œç”¨äºä¸ Claude API äº¤äº’ã€‚

#### 4.1.1 å…¨å±€å®¢æˆ·ç«¯ç®¡ç†å™¨

DevBot ä½¿ç”¨å…¨å±€å­—å…¸ç®¡ç†å¤šä¸ª subagent çš„å®¢æˆ·ç«¯ï¼š

```python
# claude_agent_base.py:49-50
_global_clients = {}  # {subagent_name: ClaudeSDKClient}
```

**ä¼˜åŠ¿**:

- **Session å¤ç”¨**: åŒä¸€ subagent çš„å¤šæ¬¡è°ƒç”¨å…±äº«ä¸Šä¸‹æ–‡
- **èµ„æºä¼˜åŒ–**: é¿å…é‡å¤åˆ›å»ºå®¢æˆ·ç«¯

#### 4.1.2 get_or_create_client å‡½æ•°

**ä½œç”¨**: è·å–æˆ–åˆ›å»º subagent çš„å®¢æˆ·ç«¯ã€‚

**å®ç°** (ç®€åŒ–ç‰ˆ):

```python
# claude_agent_base.py (ä¼ªä»£ç )
async def get_or_create_client(subagent_name: str, model_name: str = None):
    """è·å–æˆ–åˆ›å»º subagent å®¢æˆ·ç«¯"""

    # æ£€æŸ¥ç¼“å­˜
    if subagent_name in _global_clients:
        logger.debug(f"â™»ï¸ å¤ç”¨å·²æœ‰å®¢æˆ·ç«¯: {subagent_name}")
        return _global_clients[subagent_name]

    # è¯»å– agent å®šä¹‰æ–‡ä»¶
    agent_file = PROJ_PATH / '.claude' / 'agents' / f'{subagent_name}.md'

    if not agent_file.exists():
        raise FileNotFoundError(f"Agent å®šä¹‰æ–‡ä»¶ä¸å­˜åœ¨: {agent_file}")

    # è§£æ frontmatter
    with open(agent_file, 'r') as f:
        content = f.read()

    # æå– YAML frontmatter
    match = re.match(r'^---\n(.*?)\n---\n(.*)', content, re.DOTALL)
    frontmatter_yaml = match.group(1)
    prompt = match.group(2)

    agent_config = yaml.safe_load(frontmatter_yaml)

    # åˆ›å»º AgentDefinition
    agent_def = AgentDefinition(
        description=agent_config.get('description'),
        prompt=prompt,
        tools=agent_config.get('tools')  # ['Read', 'Write', 'Bash', ...]
    )

    # åˆ›å»º ClaudeSDKClient
    client = ClaudeSDKClient(
        agent_definition=agent_def,
        options=get_claude_options(
            model=model_name,
            sys_prompt=prompt,
            allowed_tools_add=agent_config.get('tools')
        )
    )

    # ç¼“å­˜å®¢æˆ·ç«¯
    _global_clients[subagent_name] = client
    logger.info(f"âœ… åˆ›å»ºæ–°å®¢æˆ·ç«¯: {subagent_name}")

    return client
```

### 4.2 Agent Definition (ä»£ç†å®šä¹‰)

#### 4.2.1 Agent å®šä¹‰æ–‡ä»¶æ ¼å¼

Agent å®šä¹‰å­˜å‚¨åœ¨ `.claude/agents/<name>.md`ï¼Œä½¿ç”¨ YAML frontmatter + Markdown æ ¼å¼ã€‚

**ç¤ºä¾‹**: `.claude/agents/crawler-developer.md`

```markdown
---
name: crawler-developer
description: çˆ¬è™«å¼€å‘ä¸“å®¶ï¼Œä¸“æ³¨äºä½¿ç”¨ Playwright å’Œ Chrome DevTools å¼€å‘é«˜è´¨é‡çš„ç½‘é¡µçˆ¬è™«
tools:
  - Read
  - Write
  - Edit
  - Bash
  - Glob
  - Grep
  - mcp__chrome-devtools
  - mcp__gemini-cli
---

# çˆ¬è™«å¼€å‘ä¸“å®¶

ä½ æ˜¯èµ„æ·±ç½‘é¡µçˆ¬è™«å·¥ç¨‹å¸ˆï¼Œç²¾é€š Pythonã€Playwrightã€BeautifulSoupã€æ­£åˆ™è¡¨è¾¾å¼ã€‚

## æ ¸å¿ƒèƒ½åŠ›
- ä½¿ç”¨ chrome-devtools åˆ†æé¡µé¢ç»“æ„
- ç¼–å†™å¥å£®çš„æå–é€»è¾‘
- å¤„ç†åŠ¨æ€åŠ è½½å†…å®¹
- ç»•è¿‡åçˆ¬è™«æœºåˆ¶

## å·¥ä½œæµç¨‹
1. åˆ†æé¡µé¢ç»“æ„ (chrome-devtools)
2. è®¾è®¡æå–ç­–ç•¥
3. ç¼–å†™ä»£ç å®ç°
4. æµ‹è¯•å¹¶ä¼˜åŒ–

## ç¼–ç è§„èŒƒ
- éµå¾ª PEP 8
- æ·»åŠ è¯¦ç»†æ³¨é‡Š
- é”™è¯¯å¤„ç†å®Œå–„
- æ€§èƒ½ä¼˜åŒ–ä¼˜å…ˆ
```

#### 4.2.2 Agent Tools é…ç½®

**tools** å­—æ®µæŒ‡å®š agent å¯ä»¥ä½¿ç”¨çš„å·¥å…·åˆ—è¡¨ã€‚

**ç¤ºä¾‹å¯¹æ¯”**:

```yaml
# crawler-developer (å¼€å‘ä¸“å®¶)
tools:
  - Read
  - Write
  - Edit
  - Bash
  - mcp__chrome-devtools  # éœ€è¦è®¿é—®ç½‘é¡µ
  - mcp__gemini-cli       # éœ€è¦è°ƒç”¨ LLM

# project-reviewer (å®¡æŸ¥ä¸“å®¶)
tools:
  - Read   # åªéœ€è¯»å–ä»£ç 
  - Bash   # æ‰§è¡Œæµ‹è¯•å‘½ä»¤
```

**å·¥å…·æƒé™æ§åˆ¶**:

```python
# claude_agent_base.py:get_claude_options
def get_claude_options(allowed_tools_add=None):
    # åŸºç¡€å·¥å…· (æ‰€æœ‰ agent éƒ½æœ‰)
    base_tools = ["Read", "Bash"]

    # é¢å¤–å·¥å…· (agent å®šä¹‰ä¸­çš„ tools)
    all_tools = base_tools + (allowed_tools_add or [])

    # è‡ªåŠ¨æ‰¹å‡†è§„åˆ™
    hooks = {
        "PreToolUse": [
            HookMatcher(matcher=tool, hooks=[auto_approve])
            for tool in all_tools
        ]
    }

    return ClaudeAgentOptions(
        max_buffer_size=100 * 1024 * 1024,
        hooks=hooks
    )
```

### 4.3 Session Management (ä¼šè¯ç®¡ç†)

#### 4.3.1 Session ID çš„ä½œç”¨

**Session ID** ç”¨äºä¿æŒå¯¹è¯ä¸Šä¸‹æ–‡ï¼Œå®ç°å¤šè½®å¯¹è¯ã€‚

**å·¥ä½œåŸç†**:

```
Step 1: åˆ†æé¡µé¢ç»“æ„
    â†“
è°ƒç”¨ call_subagent('crawler-developer', prompt1)
    â† è¿”å› session_id_1
    â†“
Step 2: ç”Ÿæˆåˆ—è¡¨æå–å™¨
    â†“
è°ƒç”¨ call_subagent('crawler-developer', prompt2, session_id=session_id_1)
    â† å¤ç”¨ä¸Šä¸‹æ–‡ï¼ŒçŸ¥é“ Step 1 çš„åˆ†æç»“æœ
```

**ä»£ç ç¤ºä¾‹**:

```python
# developer_nodes.py:step1__analyze_page
async def step1__analyze_page(state: CrawlerDevState) -> CrawlerDevState:
    prompt = get_step_prompt('step1', state)

    # ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼Œæ—  session_id
    result = await call_subagent('crawler-developer', prompt, session_id=None)

    new_session_id = result.get("session_id")  # è·å–æ–° session_id

    return {
        **state,
        "session_id": new_session_id,  # ä¿å­˜åˆ° state
        ...
    }

# developer_nodes.py:step2__generate_list_extractor
async def step2__generate_list_extractor(state: CrawlerDevState) -> CrawlerDevState:
    prompt = get_step_prompt('step2', state)

    # å¤ç”¨ session_id
    result = await call_subagent('crawler-developer', prompt, session_id=state.get("session_id"))

    new_session_id = result.get("session_id")

    return {
        **state,
        "session_id": new_session_id,  # æ›´æ–° session_id
        ...
    }
```

#### 4.3.2 Session çš„ç”Ÿå‘½å‘¨æœŸ

```
åˆå§‹åŒ–
    â†“
Step 1: session_id = None â†’ åˆ›å»ºæ–° session â†’ session_abc123
    â†“
Step 2: session_id = session_abc123 â†’ å¤ç”¨ session â†’ session_abc123 (å¯èƒ½æ›´æ–°)
    â†“
Step 3: session_id = session_abc123 â†’ ç»§ç»­å¤ç”¨
    â†“
...
    â†“
Step 10: å®Œæˆ â†’ session è‡ªåŠ¨æ¸…ç†
```

**æ½œåœ¨é—®é¢˜**:

- **Session è¿‡æœŸ**: Claude SDK ä¼šè‡ªåŠ¨å¤„ç†ï¼Œæ— éœ€æ‹…å¿ƒ
- **ä¸Šä¸‹æ–‡è¿‡é•¿**: è¶…è¿‡ 200k tokens æ—¶ï¼Œæ—©æœŸå¯¹è¯å¯èƒ½è¢«æˆªæ–­
  - **è§£å†³**: DevBot çš„æ¯ä¸ªæ­¥éª¤ prompt éƒ½æ˜¯è‡ªåŒ…å«çš„ï¼Œä¸ä¾èµ–æ—©æœŸä¸Šä¸‹æ–‡

### 4.4 Hook Mechanism (é’©å­æœºåˆ¶)

#### 4.4.1 Hook ç±»å‹

Claude SDK æ”¯æŒä¸‰ç§ hook äº‹ä»¶ï¼š

| Hook äº‹ä»¶     | è§¦å‘æ—¶æœº   | ç”¨é€”               |
| ------------- | ---------- | ------------------ |
| `PreToolUse`  | å·¥å…·è°ƒç”¨å‰ | æƒé™æ§åˆ¶ã€å‚æ•°éªŒè¯ |
| `PostToolUse` | å·¥å…·è°ƒç”¨å | ç»“æœå¤„ç†ã€æ—¥å¿—è®°å½• |
| `PreMessage`  | å‘é€æ¶ˆæ¯å‰ | æ¶ˆæ¯æ‹¦æˆªã€å†…å®¹è¿‡æ»¤ |

**DevBot ä¸­ä»…ä½¿ç”¨ `PreToolUse`** (è‡ªåŠ¨æ‰¹å‡†)ã€‚

#### 4.4.2 Auto Approve å®ç°

**åŸç†**: æ‹¦æˆªå·¥å…·è°ƒç”¨è¯·æ±‚ï¼Œç›´æ¥è¿”å› `allow` å†³ç­–ã€‚

**ä»£ç **:

```python
# claude_agent_base.py:53-77
async def auto_approve(input_data, tool_use_id, context):
    """è‡ªåŠ¨æ‰¹å‡† Bash/Edit/Read ç­‰åŸºç¡€å·¥å…·"""
    tool_name = input_data.get('tool_name', 'unknown')
    tool_input = input_data.get('tool_input', {})

    # æ—¥å¿—è®°å½• (ç”¨äºè°ƒè¯•)
    if tool_name == 'Bash':
        logger.debug(f"ğŸ”§ Hook è¢«è§¦å‘: {tool_name}")
        logger.debug(f"ğŸ“‹ å‘½ä»¤: {tool_input.get('command')}")
    elif tool_name == 'Read':
        logger.debug(f"ğŸ“– Hook è¢«è§¦å‘: {tool_name}, è¯»å–æ–‡ä»¶: {tool_input.get('file_path')}")
    else:
        logger.debug(f"âœï¸ Hook è¢«è§¦å‘: {tool_name}")

    # è¿”å›æ‰¹å‡†å†³ç­–
    return {
        "hookSpecificOutput": {
            "hookEventName": "PreToolUse",
            "permissionDecision": "allow",  # âœ… è‡ªåŠ¨æ‰¹å‡†
        }
    }
```

**Hook æ³¨å†Œ**:

```python
# claude_agent_base.py:116-137
COMMANDS = ["Read", "Write", "Edit", "Bash", ...]

hooks = {
    "PreToolUse": [
        HookMatcher(matcher="Bash", hooks=[auto_approve]),
        HookMatcher(matcher="Read", hooks=[auto_approve]),
        HookMatcher(matcher="Edit", hooks=[auto_approve]),
        # ... å…¶ä»–å·¥å…·
    ]
}

options = ClaudeAgentOptions(hooks=hooks)
client = ClaudeSDKClient(agent_definition=agent_def, options=options)
```

#### 4.4.3 MCP å·¥å…·çš„è‡ªåŠ¨æ‰¹å‡†

**MCP å·¥å…·** ä½¿ç”¨å•ç‹¬çš„ hook å‡½æ•°ï¼š

```python
# claude_agent_base.py:80-92
async def auto_approve_mcp(input_data, tool_use_id, context):
    """è‡ªåŠ¨æ‰¹å‡†æ‰€æœ‰ MCP å·¥å…·"""
    tool_name = input_data.get('tool_name', 'unknown')
    tool_input = input_data.get('tool_input', {})
    logger.debug(f"ğŸ“‹ å·¥å…·: {tool_name}")
    logger.debug(f"ğŸ“‹ å‚æ•°: {tool_input}")

    return {
        "hookSpecificOutput": {
            "hookEventName": "PreToolUse",
            "permissionDecision": "allow",  # è‡ªåŠ¨æ‰¹å‡†
        }
    }

# æ³¨å†Œ MCP hooks
ALLOW_MCP_TOOLS = ["mcp__chrome-devtools", "mcp__playwright", "mcp__brightdata"]
ALLOW_MCP_HOOKS = [
    HookMatcher(matcher=m, hooks=[auto_approve_mcp])
    for m in ALLOW_MCP_TOOLS
]
```

**ä¸ºä»€ä¹ˆåˆ†å¼€**:

- **æ—¥å¿—åŒºåˆ†**: MCP å·¥å…·çš„å‚æ•°ç»“æ„ä¸åŒï¼Œéœ€è¦å•ç‹¬å¤„ç†
- **æƒé™åˆ†ç¦»**: æœªæ¥å¯ä»¥å¯¹ MCP å·¥å…·å®æ–½æ›´ä¸¥æ ¼çš„æ§åˆ¶

---

## 5. Developer æ­¥éª¤å®Œæ•´è§£æ

DevBot å…±æœ‰ **28 ä¸ª Developer èŠ‚ç‚¹** (åŒ…æ‹¬å­æ­¥éª¤)ï¼Œåˆ†ä¸º 4 å¤§é˜¶æ®µï¼š

```
é˜¶æ®µ 1: åŸºç¡€æ¡†æ¶ (Step 0-3)
    Step 0: åˆ›å»ºåŸºç¡€æ–‡ä»¶
    Step 1: åˆ†æé¡µé¢ç»“æ„
    Step 2: ç”Ÿæˆåˆ—è¡¨æå–å™¨
    Step 2.1: URL åˆ†ç±»
    Step 3: ç”Ÿæˆ URL patterns
    Step 3.1: å¤„ç†ä¸ç¡®å®š patterns

é˜¶æ®µ 2: æå–å™¨å¾ªç¯ (Step 4-6)
    Step 4: åˆå§‹åŒ– patterns é˜Ÿåˆ—
    Step 4.1: å–å‡ºä¸‹ä¸€ä¸ª pattern
    Step 5: ç”Ÿæˆè¯¦æƒ…é¡µæå–å™¨ç±»
    Step 5.1-5.6: ç”Ÿæˆå„ä¸ªæ–¹æ³•
    Step 6: ç”Ÿæˆåˆ—è¡¨é¡µæå–å™¨

é˜¶æ®µ 3: ç½‘ç«™æ ‘æ‰©å±• (Step 7-10)
    Step 7: ç½‘ç«™æ ‘æ‰©å±•ä¸€å±‚
    Step 8: Markdown ä¿¡æ¯åˆ†æ (å ä½)
    Step 9: é¦–æ¬¡è¿è¡Œæµ‹è¯•
    Step 10: ç”Ÿæˆ Airflow DAG

é˜¶æ®µ 4: ä»£ç ä¼˜åŒ– (Step 20-22)
    Step 20: ä»£ç æ£€æŸ¥
    Step 20.1-20.4: æ€§èƒ½ã€ç­¾åã€æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
    Step 21: æ€§èƒ½ä¼˜åŒ–
    Step 22: ä¿®å¤ä»£ç 
```

### 5.1 é˜¶æ®µ 1: åŸºç¡€æ¡†æ¶

#### 5.1.1 Step 0: åˆ›å»ºåŸºç¡€æ–‡ä»¶

**ç›®æ ‡**: ä» Jinja2 æ¨¡æ¿ç”Ÿæˆçˆ¬è™«ä»£ç éª¨æ¶ã€‚

**è¾“å…¥**:

- `state["url"]`: ç›®æ ‡ç½‘ç«™ URL
- `state["site_name"]`: ç½‘ç«™åç§°
- `state["category"]`: çˆ¬è™«åˆ†ç±»

**æ‰§è¡Œæµç¨‹**:

```python
# developer_nodes.py:260-301
async def step0__create_base_file(state: CrawlerDevState) -> CrawlerDevState:
    # 1. è¯»å–æ¨¡æ¿
    with open(BASE_PATH / 'tmpl_base.py.j2', 'r') as f:
        base_content_template = f.read()

    # 2. æ¸²æŸ“æ¨¡æ¿
    base_content = jinja2.Template(base_content_template).render(
        site_capitalize=state["site_name"].capitalize(),
        site_name=state["site_name"],
        entry_url=state["url"],
        category=state["category"]
    )

    # 3. å¤‡ä»½å·²å­˜åœ¨çš„æ–‡ä»¶
    output_file = Path(state["base_file_path"])
    if output_file.exists():
        nowtime = datetime.now().strftime("%Y%m%d%H%M")
        backup_file = f"{output_file}.bak.{nowtime}"
        os.rename(output_file, backup_file)
        logger.info(f"å·²å¤‡ä»½ç°æœ‰æ–‡ä»¶åˆ°: {backup_file}")

    # 4. å†™å…¥æ–‡ä»¶
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(base_content)

    logger.info(f"âœ… åŸºç¡€æ–‡ä»¶å·²åˆ›å»º: {output_file}")

    # 5. åˆ›å»ºå¿…è¦çš„ç›®å½•
    for _, folder in get_need_dirs(state).items():
        folder.mkdir(parents=True, exist_ok=True)

    # 6. Git æäº¤ (å¦‚æœå¯ç”¨)
    auto_commit_if_enabled(state, "step0__create_base_file", "åˆ›å»ºåŸºç¡€ extractor æ–‡ä»¶æ¡†æ¶")

    # 7. è¿”å›æ›´æ–°åçš„ state
    return {
        **state,
        "current_step": "0",
        "current_step_name": "create_base_file",
        "status": "completed",
        "result": str(output_file)
    }
```

**ç”Ÿæˆçš„æ–‡ä»¶ç»“æ„**:

```python
# crawler/product/extractor_gnc.py (ç¤ºä¾‹)
from crawler.base.extractor_base import BaseExtractor, PageParam

SITE_HOST = 'gnc'
ENTRY_URL = 'https://www.gnc.com'

CONCURRENT_CONFIG = {
    'pool_size': 3,
    'tab_size': 5,
    'delay_between_requests': 1.0,
    'use_brightdata': False,
    'brightdata_batch_size': 20,
}

class BaseGncExtractor(BaseExtractor):
    engine = 'browser_pool'
    concurrent_config = CONCURRENT_CONFIG

async def extract_deals_from_mainpage(page: Optional[Union[str, PageParam]] = None) -> dict:
    """TODO: ä»ç½‘ç«™æå–å•†å“/æ´»åŠ¨/ä¼˜æƒ /åˆ—è¡¨ï¼ŒåŠç½‘é¡µåŸºæœ¬ä¿¡æ¯"""
    pass

# ... æ›´å¤š TODO æ–¹æ³•
```

**æ½œåœ¨é—®é¢˜**:

- **æ–‡ä»¶å·²å­˜åœ¨**: å¤‡ä»½æœºåˆ¶è‡ªåŠ¨å¤„ç†
- **ç›®å½•æƒé™**: ç¡®ä¿å†™å…¥æƒé™

**ä¸‹ä¸€æ­¥**: `review_step0` éªŒè¯æ–‡ä»¶å¯å¯¼å…¥

---

#### 5.1.2 Step 1: åˆ†æé¡µé¢ç»“æ„

**ç›®æ ‡**: ç”¨ chrome-devtools è®¿é—®ç½‘ç«™ï¼Œåˆ†æé¡µé¢å¤æ‚åº¦ï¼Œé€‰æ‹©åˆé€‚çš„å¼•æ“å’Œå¹¶å‘å‚æ•°ã€‚

**Prompt æ¨¡æ¿** (ç®€åŒ–ç‰ˆ):

```jinja2
# developer_nodes.py:306-374
ç”¨ chrome-devtools è®¿é—®ä»¥ä¸‹ç”µå•†/ä¼˜æƒ ç½‘ç«™ï¼Œæˆªå›¾è¯»é¡µé¢å†…å®¹ï¼Œåˆ†æé¡µé¢ç»“æ„å¹¶ä¸ºçˆ¬è™«é€‰æ‹©åˆé€‚çš„å¼•æ“å’Œå¹¶å‘å‚æ•°ï¼š

ä»£ç æ–‡ä»¶: {{base_file_path}}
ç½‘å€: {{ url }}
ä¸­é—´æ–‡ä»¶å­˜æ”¾ä½ç½®: {{ tmp_folder }}/

è¯·æŒ‰é¡ºåºæ‰§è¡Œä»»åŠ¡ï¼š
1. ç”¨ chrome-devtools æ‰“å¼€ç½‘ç«™ï¼Œå¦‚é‡åçˆ¬ï¼Œå…ˆä½¿ç”¨ brightdata ä¿å­˜ä¸´æ—¶é¡µé¢ï¼Œå†ç”¨ chrome-devtools è®¿é—®è¿™ä¸ªä¸´æ—¶ html é¡µã€‚
2. è¯»å–æ‰€æœ‰çš„å•†å“åˆ—è¡¨é¡µ url (åŒ…æ‹¬ bannerã€æ´»åŠ¨ã€ä¼˜æƒ ç­‰å„ç±»é¡µé¢)ï¼Œå­˜ä¸º {{ tmp_folder }}/first_analyze.json
3. å¯¹é¡µé¢æˆªé•¿å›¾å¹¶ä¿å­˜
   - ä½¿ç”¨ chrome-devtools çš„ take_screenshot å·¥å…·
   - å¿…é¡»è®¾ç½®å‚æ•°: format="jpeg", fullPage=true, filename=mainpage_long_screenshot.jpg
   - æ–‡ä»¶è·¯å¾„ï¼ˆç§»åŠ¨åˆ°æ­¤ï¼‰: {{ output_part_dir }}/mainpage_long_screenshot.jpg
   - å¦‚æœå›¾ç‰‡è¶…è¿‡äº† 2 MB æˆ–å•è¾¹åƒç´ è¶…è¿‡äº† 8000ï¼Œæ‰§è¡Œä»¥ä¸‹å‘½ä»¤å‹ç¼©å›¾ç‰‡:
     ```python -m devbot.tool compress_image {{ output_part_dir }}/mainpage_long_screenshot.jpg {{ output_part_dir }}/mainpage_long_screenshot.webp```

4. ä»æˆªå›¾è¯»å‡ºé¡µé¢å•†å“/æ´»åŠ¨/ä¼˜æƒ æ•°é‡ï¼Œå¹¶ä¸ first_analyze.json æ–‡ä»¶ä½œæ¯”è¾ƒåˆ†æ
5. ä»é¡µé¢ä»£ç åˆ†æé¡µé¢å¤æ‚åº¦ (simple: é™æ€ HTML, medium: éƒ¨åˆ† JS æ¸²æŸ“, complex: å¤§é‡ JS/åŠ¨æ€åŠ è½½)
6. åˆ†æé¡µé¢çš„åŠ¨æ€æ•°æ®åŠ è½½æƒ…å†µï¼ˆé€šè¿‡ chrome-devtools çš„ list_network_requests æŸ¥çœ‹ xhr/fetch è¯·æ±‚ï¼‰
7. å°†ä»¥ä¸Šåˆ†æç»“æœä»¥æœ€ç®€çŸ­è¯­è¨€å†™åœ¨ä»£ç ä¸­ CONCURRENT_CONFIG ä¸‹æ–¹é¦–é¡µåˆ†ææ³¨é‡Šå½“ä¸­ï¼ˆæ ¼å¼ï¼š`"""é¦–é¡µåˆ†æ: ...å†…å®¹... """`ï¼‰
8. æ¨èçˆ¬è™«å¼•æ“ç±»å‹
   - **browser_pool**: ç”¨ playwright æ‰“å¼€é¡µé¢èƒ½æ­£å¸¸è®¿é—®æ—¶é€‰æ‹©ï¼ˆæ›´ä¾¿å®œï¼Œä¼˜å…ˆè€ƒè™‘ï¼‰
   - **brightdata+browser_pool**: åçˆ¬é˜»æ­¢è®¿é—®ä¸”æ— æ³•ç®€å•è§£å†³æ—¶é€‰æ‹©
9. æ¨èçš„å¹¶å‘é…ç½®å‚æ•°
10. ä¿®æ”¹ä»£ç ä»¥è®¾ç½®ä¸Šå¼•æ“å’Œå¹¶å‘å‚æ•°
```

**æ‰§è¡Œç¤ºä¾‹**:

```python
# developer_nodes.py:382-403
async def step1__analyze_page(state: CrawlerDevState) -> CrawlerDevState:
    """Step 1: ç”¨ chrome-devtools åˆ†æé¡µé¢ç»“æ„, é€‰æ‹©åˆé€‚çš„å¼•æ“å’Œå‚æ•°"""

    # 1. æ¸²æŸ“ prompt
    prompt = get_step_prompt('step1', state)

    # 2. è°ƒç”¨ crawler-developer agent
    result = await call_subagent('crawler-developer', prompt, session_id=state.get("session_id"))

    response_text = result.get("message", "")
    new_session_id = result.get("session_id")

    # 3. ä¿å­˜å¯¹è¯å†å²
    save_conversation_from_state(
        state=state, prompt=prompt, response=response_text,
        node_name="step1_analyze_page_structure",
        metadata={"agent": "crawler-developer"}
    )

    # 4. Git æäº¤
    auto_commit_if_enabled(state, "step1__analyze_page", "åˆ†æé¡µé¢ç»“æ„å¹¶ç”Ÿæˆåˆæ­¥ä»£ç ")

    # 5. è¿”å›æ›´æ–°åçš„ state
    return {
        **state,
        "session_id": new_session_id,  # ä¿å­˜ session_id
        "current_step": "1",
        "current_step_name": "analyze_page_structure",
        "status": "completed",
        "result": response_text
    }
```

**è¾“å‡ºç¤ºä¾‹** (ä¿®æ”¹åçš„ä»£ç ):

```python
# crawler/product/extractor_gnc.py
CONCURRENT_CONFIG = {
    'pool_size': 3,
    'tab_size': 5,
    'delay_between_requests': 0.5,
    'use_brightdata': False,
    'brightdata_batch_size': 20,
}

"""é¦–é¡µåˆ†æ:
- é¡µé¢å¤æ‚åº¦: medium (éƒ¨åˆ† JS æ¸²æŸ“)
- å•†å“æ•°é‡: çº¦ 48 ä¸ª
- åŠ¨æ€åŠ è½½: è¯„è®ºé€šè¿‡ AJAX åŠ è½½ï¼Œå•†å“åˆ—è¡¨é™æ€æ¸²æŸ“
- åçˆ¬æƒ…å†µ: æ— æ˜æ˜¾åçˆ¬
- æ¨èå¼•æ“: browser_pool
- ä¸´æ—¶æ–‡ä»¶: /tmp/scraper/first_analyze.json
- æˆªå›¾æ–‡ä»¶: crawler/product/output/gnc/mainpage_long_screenshot.webp
"""

class BaseGncExtractor(BaseExtractor):
    engine = 'browser_pool'  # âœ… å·²è®¾ç½®
    concurrent_config = CONCURRENT_CONFIG
```

**æ½œåœ¨é—®é¢˜**:

| é—®é¢˜           | è¡¨ç°                                         | è§£å†³æ–¹æ¡ˆ                                      |
| -------------- | -------------------------------------------- | --------------------------------------------- |
| åçˆ¬é˜»æ­¢è®¿é—®   | chrome-devtools æ‰“å¼€é¡µé¢æ˜¾ç¤º Cloudflare éªŒè¯ | Prompt è¦æ±‚å…ˆç”¨ `save_tmp_page_by_brightdata` |
| æˆªå›¾è¿‡å¤§       | DecompressionBombError                       | Prompt è¦æ±‚å‹ç¼©å›¾ç‰‡                           |
| æˆªå›¾æœªå®Œå…¨åŠ è½½ | æˆªå›¾åƒç´ å¼‚å¸¸ï¼ˆè¶…è¿‡ 1 äº¿ï¼‰                    | Prompt è¦æ±‚é‡è¯•æœ€å¤š 5 æ¬¡                      |
| ç½‘ç»œè¶…æ—¶       | chrome-devtools è®¿é—®è¶…æ—¶                     | æ‰‹åŠ¨é‡è¯•æˆ–æ£€æŸ¥ç½‘ç»œ                            |

**ä¸‹ä¸€æ­¥**: `review_step1` éªŒè¯ä»£ç å¯æ‰§è¡Œ

---

#### 5.1.3 Step 2: ç”Ÿæˆåˆ—è¡¨æå–å™¨

**ç›®æ ‡**: å®ç° `extract_deals_from_mainpage` æ–¹æ³•ï¼Œä»ä¸»é¡µæå–å•†å“/ä¼˜æƒ åˆ—è¡¨ã€‚

**Prompt æ ¸å¿ƒè¦æ±‚**:

```jinja2
# developer_nodes.py:407-523
# ä»»åŠ¡ï¼šå®ç°åˆ—è¡¨æå–å™¨

## ä»£ç æ–‡ä»¶
`{{base_file_path}}`

## å½“å‰ä»»åŠ¡
æ ¹æ® extract_deals_from_mainpage æ–¹æ³•æ³¨é‡Šï¼Œå®ç°æ­¤æ–¹æ³•ï¼Œä»é¡µé¢æå–å‡ºå•†å“åˆ—è¡¨å’Œé¡µé¢åŸºæœ¬ä¿¡æ¯ã€‚

## å‚è€ƒä»£ç 

```python
async def extract_deals_from_mainpage(page: Optional[Union[str, PageParam]] = None) -> dict:
    """ä»ç½‘ç«™æå–å•†å“/æ´»åŠ¨/ä¼˜æƒ /åˆ—è¡¨ï¼ŒåŠç½‘é¡µåŸºæœ¬ä¿¡æ¯

    Returns:
        dict: åŒ…å«ä»¥ä¸‹å­—æ®µ:
            - urls(products/dealsåˆ—è¡¨): List[dict] å•†å“åˆ—è¡¨,æ¯ä¸ªé¡¹åŒ…å«:
                - `type`: detail/list/other/unclear
                - title: æ ‡é¢˜
                - url: é“¾æ¥(ä»¥httpæ‰“å¤´çš„å®Œæ•´é“¾æ¥)
                - image_url: ç›¸å…³å›¾ç‰‡URLï¼ˆä»…detailç±»å‹å¯èƒ½æœ‰æ­¤å­—æ®µï¼‰
                - price: ä»·æ ¼ï¼ˆä»…detailç±»å‹å¯èƒ½æœ‰æ­¤å­—æ®µï¼‰
    """
    extractor = NewSiteListExtractor(page)
    async with extractor.browser_pool.get_page() as pw_page:
        if 'brightdata' in extractor.engine:
            extractor.brightdata_file = await extractor.save_origin_html_by_brightdata()
            file_url = f'file://{Path(extractor.brightdata_file).resolve()}'
            await pw_page.goto(file_url, wait_until='domcontentloaded', timeout=60000)
        else:
            await pw_page.goto(extractor.url, wait_until='domcontentloaded', timeout=60000)

        await pw_page.wait_for_timeout(2000)  # ç­‰å¾… JS åŠ è½½
        extractor.html_content = await pw_page.content()
        extractor.cache_html_content(extractor.html_content)

        # TODO ä» pw_page æå–é¡µé¢ä¿¡æ¯ urlsç­‰
        # ...
    return {}
```

## æµ‹è¯•ä¿®æ­£

1. æ‰§è¡Œæµ‹è¯•ï¼š

   ```bash
   python -m crawler.{{ category }}.extractor_{{site_name}} extract_deals_from_mainpage > {{ tmp_folder }}/extract_deals_from_mainpage.log
   ```

2. éªŒè¯ç»“æœï¼š

   - æ¯”è¾ƒæå–çš„ {{ category }} æ•°é‡ä¸é¦–é¡µåˆ†æä¸­çš„æ•°é‡
   - ä¸¤è€…åº”è¯¥ç›¸ç¬¦

3. é—®é¢˜ä¿®æ­£ï¼š

   - å¦‚æœæ•°é‡ä¸ç¬¦ï¼ŒæŸ¥çœ‹ chrome-devtools æ‰“å¼€çš„é¡µé¢
   - åˆ†æé”™è¯¯åŸå› å¹¶ä¿®æ­£ä»£ç 
   - é‡å¤æµ‹è¯•ç›´åˆ°è¾¾æˆç›®æ ‡

```
**æ‰§è¡Œæµç¨‹**:

```python
# developer_nodes.py:567-644
async def step2__generate_list_extractor(state: CrawlerDevState) -> CrawlerDevState:
    """Step 2: ç”Ÿæˆåˆ—è¡¨æå–å™¨ (extract_deals_from_mainpage)"""

    # 1. æ¸²æŸ“ prompt
    prompt = get_step_prompt('step2', state)

    # 2. è°ƒç”¨ crawler-developer agent
    result = await call_subagent('crawler-developer', prompt, session_id=state.get("session_id"))

    response_text = result.get("message", "")
    new_session_id = result.get("session_id")

    # 3. ä¿å­˜å¯¹è¯å†å²
    save_conversation_from_state(state=state, prompt=prompt, response=response_text,
                                 node_name="step2__generate_list_extractor",
                                 metadata={"agent": "crawler-developer"})

    # 4. æ‰§è¡Œ extract_deals_from_mainpage å¹¶åˆ›å»º site_tree.json
    cache_data = await create_site_tree(state, can_use_cache=False)

    # 5. Git æäº¤
    auto_commit_if_enabled(state, "step2__generate_list_extractor", "ç”Ÿæˆåˆ—è¡¨æå–å™¨")

    # 6. è¿”å›æ›´æ–°åçš„ state
    return {
        **state,
        "session_id": new_session_id,
        "current_step": "2",
        "current_step_name": "generate_list_extractor",
        "status": "completed",
        "cache_data": cache_data,
        "result": response_text
    }
```

**create_site_tree å‡½æ•°**:

```python
# developer_nodes.py:530-561
async def create_site_tree(state: CrawlerDevState, can_use_cache=False) -> dict:
    """æ‰§è¡Œ extract_deals_from_mainpage å¹¶åˆ›å»º site_tree.json

    Returns:
        dict: {'site_tree': urls} æ ¼å¼çš„ cache_data
    """
    # 1. åŠ¨æ€å¯¼å…¥æ¨¡å—
    module_path = f'crawler.{state["category"]}.extractor_{state["site_name"]}'
    if module_path in sys.modules:
        del sys.modules[module_path]  # å¼ºåˆ¶é‡æ–°åŠ è½½

    extractor_module = importlib.import_module(module_path)
    func = getattr(extractor_module, 'extract_deals_from_mainpage')

    # 2. è°ƒç”¨å‡½æ•°
    res = await func()

    # 3. æ•´ç† URLs
    urls = res.get('urls', [])
    for item in urls:
        item['from'] = state['url']  # æ ‡è®°æ¥æº
        item['level'] = 1 if item['url'] != state['url'] else 0  # å±‚çº§

    cache_data = {'site_tree': urls}

    # 4. ä¿å­˜åˆ°æ–‡ä»¶
    site_tree_file = get_need_dirs(state)['output_file_dir'] / 'site_tree.json'
    site_tree_file.parent.mkdir(parents=True, exist_ok=True)
    with open(site_tree_file, 'w', encoding='utf-8') as f:
        json.dump(cache_data, f, indent=2, ensure_ascii=False)

    logger.info(f"âœ… site_tree.json å·²åˆ›å»º: {site_tree_file}ï¼ŒåŒ…å« {len(urls)} ä¸ª URLs")

    return cache_data
```

**è¾“å‡ºç¤ºä¾‹** (site_tree.json):

```json
{
  "site_tree": [
    {
      "title": "Vitamin C 1000mg",
      "url": "https://www.gnc.com/vitamins/123.html",
      "type": "detail",
      "price": "$19.99",
      "image_url": "https://www.gnc.com/images/123.jpg",
      "from": "https://www.gnc.com",
      "level": 1
    },
    {
      "title": "Vitamin Category",
      "url": "https://www.gnc.com/vitamins/",
      "type": "list",
      "from": "https://www.gnc.com",
      "level": 1
    }
  ]
}
```

**æ½œåœ¨é—®é¢˜**:

| é—®é¢˜         | è¡¨ç°                           | è§£å†³æ–¹æ¡ˆ                                              |
| ------------ | ------------------------------ | ----------------------------------------------------- |
| æå–æ•°é‡ä¸ç¬¦ | æå–äº† 20 ä¸ªï¼Œé¦–é¡µåˆ†æè¯´ 48 ä¸ª | Prompt è¦æ±‚ LLM ç”¨ chrome-devtools æŸ¥çœ‹é¡µé¢ï¼Œæ‰¾å‡ºé—æ¼ |
| URL ä¸å®Œæ•´   | `url: "/vitamins/123.html"`    | Prompt è¦æ±‚"ä»¥ http æ‰“å¤´çš„å®Œæ•´é“¾æ¥"                   |
| Type é”™è¯¯    | `type: "product_page"`         | Prompt å¼ºè°ƒ"åªèƒ½æ˜¯ detail/list/other/unclear ä¹‹ä¸€"    |
| é‡å¤ URL     | åŒä¸€å•†å“å‡ºç°å¤šæ¬¡               | ä»£ç é€»è¾‘å»é‡                                          |

**ä¸‹ä¸€æ­¥**: `review_step2` éªŒè¯è¿”å›æ ¼å¼

---

#### 5.1.4 Step 2.1: URL åˆ†ç±»

**ç›®æ ‡**: ç”¨ LLM å¯¹ site_tree.json ä¸­çš„æ‰€æœ‰ URLs è¿›è¡Œç²¾ç¡®åˆ†ç±»ï¼Œç”Ÿæˆ site_patternsã€‚

**Prompt æ ¸å¿ƒè¦æ±‚**:

```jinja2
# developer_nodes.py:650-800 (ç®€åŒ–ç‰ˆ)
# ä»»åŠ¡ï¼šURL åˆ†ç±»

## è¾“å…¥æ–‡ä»¶
`{{output_file_dir}}/site_tree.json`

## è¾“å‡ºæ–‡ä»¶
`{{output_file_dir}}/site_tree.json` (æ›´æ–° `site_patterns` å­—æ®µ)

## åˆ†ç±»è§„åˆ™
å¯¹æ¯ä¸ª URL åˆ¤æ–­ç±»å‹ (detail/list/other/unclear) å¹¶å½’çº³ä¸º pattern:
- **detail**: è¯¦æƒ…é¡µ - URL åŒ…å«å•†å“å”¯ä¸€æ ‡è¯†ç¬¦ (IDã€slugã€SKU)
- **list**: åˆ—è¡¨é¡µ - URL å±•ç¤ºå¤šä¸ªæ¡ç›® (åˆ†ç±»é¡µã€æœç´¢é¡µã€æ´»åŠ¨é¡µ)
- **other**: å…¶ä»–é¡µé¢ - ç™»å½•ã€æ³¨å†Œã€å¸®åŠ©ã€è´­ç‰©è½¦ç­‰
- **unclear**: ä¸ç¡®å®šé¡µé¢ - æ— æ³•è½»æ˜“åˆ¤æ–­

## è¾“å‡ºæ ¼å¼
```json
{
  "site_tree": [ ... ],  // åŸæœ‰å†…å®¹ä¿æŒä¸å˜
  "site_patterns": [
    {
      "pattern": "r'https://www\\.gnc\\.com/[\\w-]+/\\d+\\.html'",
      "type": "detail",
      "description": "å•†å“è¯¦æƒ…é¡µ (è·¯å¾„ + ID + .html)",
      "sample_url": "https://www.gnc.com/vitamins/123.html",
      "count": 15
    },
    {
      "pattern": "r'https://www\\.gnc\\.com/category/[\\w-]+'",
      "type": "list",
      "description": "åˆ†ç±»åˆ—è¡¨é¡µ",
      "sample_url": "https://www.gnc.com/category/vitamins",
      "count": 3
    }
  ]
}
```

```
**æ‰§è¡Œæµç¨‹**:

```python
# developer_nodes.py:676-816
async def step2_1__classify_urls(state: CrawlerDevState) -> CrawlerDevState:
    """Step 2.1: URL åˆ†ç±»ï¼Œç”Ÿæˆ site_patterns"""

    # 1. æ¸²æŸ“ prompt
    prompt = get_step_prompt('step2_1', state)

    # 2. è°ƒç”¨ crawler-developer agent
    result = await call_subagent('crawler-developer', prompt, session_id=state.get("session_id"))

    response_text = result.get("message", "")
    new_session_id = result.get("session_id")

    # 3. ä¿å­˜å¯¹è¯å†å²
    save_conversation_from_state(state=state, prompt=prompt, response=response_text,
                                 node_name="step2_1__classify_urls",
                                 metadata={"agent": "crawler-developer"})

    # 4. Git æäº¤
    auto_commit_if_enabled(state, "step2_1__classify_urls", "URL åˆ†ç±»ï¼Œç”Ÿæˆ site_patterns")

    # 5. è¿”å›æ›´æ–°åçš„ state
    return {
        **state,
        "session_id": new_session_id,
        "current_step": "2.1",
        "current_step_name": "classify_urls",
        "status": "completed",
        "result": response_text
    }
```

**è¾“å‡ºç¤ºä¾‹** (site_tree.json æ›´æ–°å):

```json
{
  "site_tree": [ ... ],  // ä¿æŒä¸å˜
  "site_patterns": [
    {
      "pattern": "r'https://www\\.gnc\\.com/[\\w-]+/\\d+\\.html'",
      "type": "detail",
      "description": "å•†å“è¯¦æƒ…é¡µ (è·¯å¾„ + ID + .html)",
      "sample_url": "https://www.gnc.com/vitamins/123.html",
      "count": 15
    },
    {
      "pattern": "r'https://www\\.gnc\\.com/category/[\\w-]+'",
      "type": "list",
      "description": "åˆ†ç±»åˆ—è¡¨é¡µ",
      "sample_url": "https://www.gnc.com/category/vitamins",
      "count": 3
    },
    {
      "pattern": "r'https://www\\.gnc\\.com/[\\w-]+/?'",
      "type": "unclear",
      "description": "ä¸ç¡®å®šç±»å‹é¡µé¢ (é€šç”¨è·¯å¾„)",
      "sample_url": "https://www.gnc.com/about",
      "count": 2
    }
  ]
}
```

**æ½œåœ¨é—®é¢˜**:

| é—®é¢˜              | è¡¨ç°                                     | è§£å†³æ–¹æ¡ˆ                              |
| ----------------- | ---------------------------------------- | ------------------------------------- |
| Pattern è¿‡äºå®½æ³›  | `r'https://www\\.gnc\\.com/.*'` åŒ¹é…æ‰€æœ‰ | Prompt è¦æ±‚"å°½å¯èƒ½å…·ä½“ï¼Œé¿å…è¿‡åº¦æ³›åŒ–" |
| Pattern è¯­æ³•é”™è¯¯  | `r'https://www.gnc.com/\d+'` (æœªè½¬ä¹‰ç‚¹)  | Review æ­¥éª¤ä¼šæ£€æµ‹å¹¶é‡æ–°ç”Ÿæˆ           |
| å½’ç±»é”™è¯¯          | æŠŠè¯¦æƒ…é¡µå½’ç±»ä¸º list                      | Prompt æä¾›è¯¦ç»†çš„åˆ†ç±»è§„åˆ™             |
| Sample URL ä¸åŒ¹é… | sample_url ä¸ pattern ä¸åŒ¹é…             | Review æ­¥éª¤éªŒè¯æ­£åˆ™è¡¨è¾¾å¼             |

**ä¸‹ä¸€æ­¥**: `review_step2_1` éªŒè¯ site_patterns æ ¼å¼

---

#### 5.1.5 Step 3: ç”Ÿæˆ URL Patterns

**ç›®æ ‡**: å°† site_patterns è½¬æ¢ä¸ºä»£ç ä¸­çš„ `url_list_patterns`ã€`url_detail_patterns` å’Œ `URL_MAP`ã€‚

**Prompt æ ¸å¿ƒè¦æ±‚**:

```jinja2
# developer_nodes.py:835-918 (ç®€åŒ–ç‰ˆ)
# ä»»åŠ¡ï¼šç”Ÿæˆ URL Patterns

## è¾“å…¥æ–‡ä»¶
`{{output_file_dir}}/site_tree.json` (site_patterns å­—æ®µ)

## è¾“å‡ºä»£ç 
`{{base_file_path}}`

## ç”Ÿæˆå†…å®¹
1. `url_list_patterns`: List[str] - æ‰€æœ‰ type=list çš„ patterns
2. `url_detail_patterns`: List[str] - æ‰€æœ‰ type=detail çš„ patterns
3. `URL_MAP`: Dict - è¯­ä¹‰åŒ– key + patterns æ•°ç»„

## URL_MAP æ ¼å¼

```python
URL_MAP = {
    'mainpage': {
        'patterns': [r'https://www\.gnc\.com/?$'],
        'sample_urls': ['https://www.gnc.com'],
        'func': extract_deals_from_mainpage,
        'action': 'get_list_info'
    },
    'detail_page': {
        'patterns': [r'https://www\.gnc\.com/[\w-]+/\d+\.html'],
        'sample_urls': ['https://www.gnc.com/vitamins/123.html'],
        'func': None,  # Step 5 å¡«å……
        'action': 'get_detail_info'
    },
    'category_list': {
        'patterns': [r'https://www\.gnc\.com/category/[\w-]+'],
        'sample_urls': ['https://www.gnc.com/category/vitamins'],
        'func': None,  # Step 6 å¡«å……
        'action': 'get_list_info'
    }
}
```

```
**æ‰§è¡Œæµç¨‹**:

```python
# developer_nodes.py:832-918
async def step3__generate_url_patterns(state: CrawlerDevState) -> CrawlerDevState:
    """Step 3: ç”Ÿæˆ URL patterns"""

    # 1. æ¸²æŸ“ prompt
    prompt = get_step_prompt('step3', state)

    # 2. è°ƒç”¨ crawler-developer agent
    result = await call_subagent('crawler-developer', prompt, session_id=state.get("session_id"))

    response_text = result.get("message", "")
    new_session_id = result.get("session_id")

    # 3. ä¿å­˜å¯¹è¯å†å²
    save_conversation_from_state(state=state, prompt=prompt, response=response_text,
                                 node_name="step3__generate_url_patterns",
                                 metadata={"agent": "crawler-developer"})

    # 4. Git æäº¤
    auto_commit_if_enabled(state, "step3__generate_url_patterns", "ç”Ÿæˆ URL patterns")

    # 5. è¿”å›æ›´æ–°åçš„ state
    return {
        **state,
        "session_id": new_session_id,
        "current_step": "3",
        "current_step_name": "generate_url_patterns",
        "status": "completed",
        "result": response_text
    }
```

**è¾“å‡ºç¤ºä¾‹** (ä»£ç ):

```python
# crawler/product/extractor_gnc.py
url_list_patterns = [
    r'https://www\.gnc\.com/?$',
    r'https://www\.gnc\.com/category/[\w-]+'
]

url_detail_patterns = [
    r'https://www\.gnc\.com/[\w-]+/\d+\.html'
]

URL_MAP = {
    'mainpage': {
        'patterns': [r'https://www\.gnc\.com/?$'],
        'sample_urls': ['https://www.gnc.com'],
        'func': extract_deals_from_mainpage,
        'action': 'get_list_info'
    },
    'detail_page': {
        'patterns': [r'https://www\.gnc\.com/[\w-]+/\d+\.html'],
        'sample_urls': ['https://www.gnc.com/vitamins/123.html'],
        'func': None,  # Step 5 å¡«å……
        'action': 'get_detail_info'
    },
    'category_list': {
        'patterns': [r'https://www\.gnc\.com/category/[\w-]+'],
        'sample_urls': ['https://www.gnc.com/category/vitamins'],
        'func': None,  # Step 6 å¡«å……
        'action': 'get_list_info'
    }
}
```

**æ½œåœ¨é—®é¢˜**:

| é—®é¢˜                        | è¡¨ç°                          | è§£å†³æ–¹æ¡ˆ                           |
| --------------------------- | ----------------------------- | ---------------------------------- |
| URL_MAP ç¼ºå°‘ mainpage       | æœªç”Ÿæˆä¸»é¡µæ˜ å°„                | Prompt å¼ºè°ƒ"å¿…é¡»åŒ…å« mainpage key" |
| Patterns é¡ºåºé”™è¯¯           | å®½æ³› pattern åœ¨å‰ï¼Œå¯¼è‡´è¯¯åŒ¹é… | Prompt è¦æ±‚"å…·ä½“ pattern ä¼˜å…ˆ"     |
| Sample URL ä¸åœ¨ patterns ä¸­ | sample_url ä¸ pattern ä¸åŒ¹é…  | Review æ­¥éª¤ä¼šæ£€æµ‹å¹¶æŠ¥é”™            |

**ä¸‹ä¸€æ­¥**: `review_step3` éªŒè¯ URL_MAP æ ¼å¼

---

### 5.2 é˜¶æ®µ 2: æå–å™¨å¾ªç¯ (Step 4-6)

è¿™æ˜¯ DevBot æœ€å¤æ‚çš„éƒ¨åˆ†ï¼Œä½¿ç”¨ **å¾ªç¯ç»“æ„** ä¸ºæ¯ä¸ª URL pattern ç”Ÿæˆå¯¹åº”çš„æå–å™¨ã€‚

#### 5.2.1 å¾ªç¯æ§åˆ¶æµç¨‹

```
Step 4: åˆå§‹åŒ– patterns_queue
    â†“
Step 4.1: patterns_queue.pop(0) â†’ current_pattern_info
    â†“
â”œâ”€ type == "detail" â†’ Step 5: ç”Ÿæˆè¯¦æƒ…é¡µæå–å™¨
â”‚       â”œâ”€ Step 5.1: fetch_rendered_html
â”‚       â”œâ”€ Step 5.2: remove_site_chrome
â”‚       â”œâ”€ Step 5.3: extract_main_content
â”‚       â”œâ”€ Step 5.4: convert_html_to_markdown
â”‚       â”œâ”€ Step 5.5: collect_other_info
â”‚       â””â”€ Step 5.6: intercept_ajax_comment
â”‚
â””â”€ type == "list" â†’ Step 6: ç”Ÿæˆåˆ—è¡¨é¡µæå–å™¨
    â†“
completed_patterns.append(current_pattern)
    â†“
åˆ¤æ–­: patterns_queue éç©ºï¼Ÿ
    â”œâ”€ æ˜¯ â†’ è¿”å› Step 4.1 (ç»§ç»­å¾ªç¯)
    â””â”€ å¦ â†’ è¿›å…¥ Step 7 (ç½‘ç«™æ ‘æ‰©å±•)
```

#### 5.2.2 Step 4: åˆå§‹åŒ– Patterns é˜Ÿåˆ—

**ç›®æ ‡**: ä» `url_detail_patterns` å’Œ `url_list_patterns` æ„å»ºå¾…å¤„ç†é˜Ÿåˆ—ã€‚

**æ‰§è¡Œæµç¨‹**:

```python
# developer_nodes.py:1094-1148
async def step4__init_patterns_queue(state: CrawlerDevState) -> CrawlerDevState:
    """Step 4: åˆå§‹åŒ– patterns é˜Ÿåˆ—"""

    # 1. åŠ¨æ€å¯¼å…¥æ¨¡å—
    module_path = f'crawler.{state["category"]}.extractor_{state["site_name"]}'
    if module_path in sys.modules:
        del sys.modules[module_path]

    extractor_module = importlib.import_module(module_path)

    # 2. è·å– patterns
    url_list_patterns = getattr(extractor_module, 'url_list_patterns', [])
    url_detail_patterns = getattr(extractor_module, 'url_detail_patterns', [])
    URL_MAP = getattr(extractor_module, 'URL_MAP', {})

    # 3. æ„å»ºé˜Ÿåˆ— (detail patterns ä¼˜å…ˆ)
    patterns_queue = []

    # æ·»åŠ  detail patterns
    for pattern in url_detail_patterns:
        # ä» URL_MAP ä¸­æ‰¾åˆ°å¯¹åº”çš„ sample_url
        for key, config in URL_MAP.items():
            if pattern in config.get('patterns', []):
                sample_url = config.get('sample_urls', [None])[0]
                patterns_queue.append({
                    'pattern': pattern,
                    'type': 'detail',
                    'sample_url': sample_url,
                    'key': key
                })
                break

    # æ·»åŠ  list patterns (æ’é™¤ mainpage)
    for pattern in url_list_patterns:
        for key, config in URL_MAP.items():
            if key == 'mainpage':  # è·³è¿‡ä¸»é¡µ
                continue
            if pattern in config.get('patterns', []):
                sample_url = config.get('sample_urls', [None])[0]
                patterns_queue.append({
                    'pattern': pattern,
                    'type': 'list',
                    'sample_url': sample_url,
                    'key': key
                })
                break

    logger.info(f"âœ… Patterns é˜Ÿåˆ—å·²åˆå§‹åŒ–ï¼Œå…± {len(patterns_queue)} ä¸ª patterns")

    # 4. è¿”å›æ›´æ–°åçš„ state
    return {
        **state,
        "current_step": "4",
        "current_step_name": "init_patterns_queue",
        "status": "completed",
        "patterns_queue": patterns_queue,
        "completed_patterns": [],
        "result": f"åˆå§‹åŒ–äº† {len(patterns_queue)} ä¸ª patterns"
    }
```

**patterns_queue ç¤ºä¾‹**:

```python
[
    {
        'pattern': r'https://www\.gnc\.com/[\w-]+/\d+\.html',
        'type': 'detail',
        'sample_url': 'https://www.gnc.com/vitamins/123.html',
        'key': 'detail_page'
    },
    {
        'pattern': r'https://www\.gnc\.com/category/[\w-]+',
        'type': 'list',
        'sample_url': 'https://www.gnc.com/category/vitamins',
        'key': 'category_list'
    }
]
```

**ä¸‹ä¸€æ­¥**: `step4_1__next_pattern` (å–å‡ºç¬¬ä¸€ä¸ª pattern)

---

#### 5.2.3 Step 4.1: å–å‡ºä¸‹ä¸€ä¸ª Pattern

**ç›®æ ‡**: ä»é˜Ÿåˆ—ä¸­å¼¹å‡ºä¸€ä¸ª patternï¼Œè®¾ç½®ä¸ºå½“å‰å¤„ç†å¯¹è±¡ã€‚

**æ‰§è¡Œæµç¨‹**:

```python
# developer_nodes.py:1148-1330
async def step4_1__next_pattern(state: CrawlerDevState) -> CrawlerDevState:
    """Step 4.1: å–å‡ºé˜Ÿåˆ—ä¸­çš„ä¸‹ä¸€ä¸ª pattern"""

    # 1. æ£€æŸ¥é˜Ÿåˆ—æ˜¯å¦ä¸ºç©º
    patterns_queue = state.get("patterns_queue", [])
    if not patterns_queue:
        logger.info("â­ï¸ Patterns é˜Ÿåˆ—å·²ç©ºï¼Œè·³è¿‡ Step 5-6")
        return {
            **state,
            "current_step": "4.1",
            "current_step_name": "next_pattern",
            "status": "skipped",
            "result": "No more patterns"
        }

    # 2. å¼¹å‡ºç¬¬ä¸€ä¸ª pattern
    current_pattern_info = patterns_queue.pop(0)
    current_url_pattern = current_pattern_info['pattern']
    current_sample_url = current_pattern_info['sample_url']

    # 3. è®¡ç®— MD5 (ç”¨äºæ–‡ä»¶å‘½å)
    current_sample_url_md5 = hashlib.md5(current_sample_url.encode()).hexdigest()

    # 4. ç”Ÿæˆæ–‡ä»¶ ID
    last_file_id = f"{current_pattern_info['key']}_{current_sample_url_md5[:8]}"

    logger.info(f"ğŸ“Œ å½“å‰å¤„ç† pattern: {current_url_pattern[:70]}...")
    logger.info(f"ğŸ“ Sample URL: {current_sample_url}")
    logger.info(f"ğŸ·ï¸ File ID: {last_file_id}")

    # 5. è¿”å›æ›´æ–°åçš„ state
    return {
        **state,
        "current_step": "4.1",
        "current_step_name": "next_pattern",
        "status": "completed",
        "patterns_queue": patterns_queue,  # æ›´æ–°åçš„é˜Ÿåˆ—
        "current_pattern_info": current_pattern_info,
        "current_url_pattern": current_url_pattern,
        "current_sample_url": current_sample_url,
        "current_sample_url_md5": current_sample_url_md5,
        "last_file_id": last_file_id,
        "result": f"Processing: {current_pattern_info['key']}"
    }
```

**è·¯ç”±å†³ç­–** (routing_logic.py):

```python
def route_after_step4_1(state: CrawlerDevState) -> str:
    """æ ¹æ® pattern ç±»å‹å†³å®šä¸‹ä¸€æ­¥"""
    if state["status"] == "skipped":
        return "step7__site_tree_expand"  # é˜Ÿåˆ—ä¸ºç©ºï¼Œè·³åˆ° Step 7

    pattern_type = state["current_pattern_info"]["type"]

    if pattern_type == "detail":
        return "step5__generate_extractor_class"  # è¯¦æƒ…é¡µ â†’ Step 5
    elif pattern_type == "list":
        return "step6__generate_list_extractor"  # åˆ—è¡¨é¡µ â†’ Step 6
    else:
        # unclear/other ç±»å‹ï¼Œè·³è¿‡
        completed_patterns = state.get("completed_patterns", [])
        completed_patterns.append(state["current_url_pattern"])
        return "step4_1__next_pattern"  # ç»§ç»­ä¸‹ä¸€ä¸ª
```

**ä¸‹ä¸€æ­¥**:

- type == "detail" â†’ `step5__generate_extractor_class`
- type == "list" â†’ `step6__generate_list_extractor`

---

#### 5.2.4 Step 5: ç”Ÿæˆè¯¦æƒ…é¡µæå–å™¨ç±»

**ç›®æ ‡**: åˆ›å»ºè¯¦æƒ…é¡µæå–å™¨ç±»çš„æ¡†æ¶ï¼ŒåŒ…æ‹¬ç±»å®šä¹‰ã€åˆå§‹åŒ–æ–¹æ³•ã€‚

**Prompt æ ¸å¿ƒè¦æ±‚** (ç®€åŒ–ç‰ˆ):

```jinja2
# ä»»åŠ¡ï¼šç”Ÿæˆè¯¦æƒ…é¡µæå–å™¨ç±»

## Sample URL
{{current_sample_url}}

## ä»»åŠ¡æ­¥éª¤
1. ç”¨ chrome-devtools æ‰“å¼€ sample URL
2. åˆ†æé¡µé¢ç»“æ„ï¼š
   - å¤´æ /åº•æ ä½ç½®
   - è¯„è®ºåŒºä½ç½®
   - å•†å“å‚æ•°ä½ç½®
   - FAQ/Q&A ä½ç½®
3. å°†åˆ†æç»“æœå†™å…¥ä»£ç æ³¨é‡Š `"""è¯¦æƒ…é¡µåˆ†æ: ..."""`
4. åˆ›å»ºæå–å™¨ç±»ï¼š

```python
class {{cls_name}}(BaseExtractor, ProductDetailMixin):
    """{{site_name}} è¯¦æƒ…é¡µæå–å™¨"""

    async def fetch_rendered_html(self, page) -> tuple:
        """è·å–æ¸²æŸ“åçš„çº¯HTML"""
        pass  # Step 5.1 å¡«å……

    def remove_site_chrome(self, html: str) -> str:
        """ç§»é™¤å¤´æ /åº•æ """
        pass  # Step 5.2 å¡«å……

    def extract_main_content(self, html: str) -> str:
        """æå–ä¸»å•†å“å†…å®¹"""
        pass  # Step 5.3 å¡«å……

    def convert_html_to_markdown(self, html: str) -> str:
        """è½¬æ¢ä¸º Markdown"""
        pass  # Step 5.4 å¡«å……

async def extract_product_detail(page: PageParam) -> dict:
    """è¯¦æƒ…é¡µæå–å…¥å£"""
    return await {{cls_name}}(page).do()
```

5. æ›´æ–° URL_MAPï¼Œå°† func è®¾ç½®ä¸º `extract_product_detail`

```
**æ‰§è¡Œæµç¨‹**:

```python
# developer_nodes.py:1330-1376
async def step5__generate_extractor_class(state) -> CrawlerDevState:
    """Step 5: ç”Ÿæˆè¯¦æƒ…é¡µæå–å™¨ç±»"""

    # 1. æ¸²æŸ“ prompt
    prompt = get_step_prompt('step5', state)

    # 2. è°ƒç”¨ crawler-developer agent
    result = await call_subagent('crawler-developer', prompt, session_id=state.get("session_id"))

    response_text = result.get("message", "")
    new_session_id = result.get("session_id")

    # 3. æ£€æµ‹æ˜¯å¦éœ€è¦è·³è¿‡åç»­æ­¥éª¤
    if 'è¯¥é¡µé¢ä¸æ˜¯ç›®æ ‡è¯¦æƒ…é¡µï¼Œä¸­æ­¢åç»­æ­¥éª¤!!!' in response_text:
        logger.info("â­ï¸ æ£€æµ‹åˆ°è¯¥é¡µé¢ä¸æ˜¯ç›®æ ‡è¯¦æƒ…é¡µï¼Œè·³è¿‡ step5 åç»­æ­¥éª¤")
        completed_patterns = state.get("completed_patterns", [])
        current_pattern = state.get("current_url_pattern")
        if current_pattern and current_pattern not in completed_patterns:
            completed_patterns = completed_patterns + [current_pattern]
        return {
            **state,
            "session_id": new_session_id,
            "current_step": "5",
            "current_step_name": "generate_extractor_class",
            "status": "skipped",
            "completed_patterns": completed_patterns,
            "next_action": "skip_to_next_pattern"
        }

    # 4. ä¿å­˜å¯¹è¯å†å²
    save_conversation_from_state(state=state, prompt=prompt, response=response_text,
                                 node_name="step5__generate_extractor_class",
                                 metadata={"agent": "crawler-developer"})

    # 5. æ ‡è®°å½“å‰ pattern ä¸ºå·²å®Œæˆ
    completed_patterns = state.get("completed_patterns", [])
    current_pattern = state.get("current_url_pattern")

    if current_pattern and current_pattern not in completed_patterns:
        completed_patterns = completed_patterns + [current_pattern]
        logger.info(f"âœ… Pattern å·²å®Œæˆ: {current_pattern[:70]}...")

    # 6. Git æäº¤
    auto_commit_if_enabled(state, "step5__generate_extractor_class", "ç”Ÿæˆè¯¦æƒ…é¡µæå–å™¨ç±»")

    # 7. è¿”å›æ›´æ–°åçš„ state
    return {
        **state,
        "session_id": new_session_id,
        "current_step": "5",
        "current_step_name": "generate_extractor_class",
        "status": "completed",
        "completed_patterns": completed_patterns,
        "result": response_text
    }
```

**è¾“å‡ºç¤ºä¾‹** (ä»£ç ):

```python
# crawler/product/extractor_gnc.py

"""è¯¦æƒ…é¡µåˆ†æ:
- å¤´æ : class="site-header"
- åº•æ : class="site-footer"
- è¯„è®ºåŒº: id="reviews-section", é€šè¿‡ AJAX åŠ è½½
- å•†å“å‚æ•°: class="product-specs", æŠ˜å åŒºéœ€ç‚¹å‡»å±•å¼€
- FAQ: class="faq-section"
"""

class GncDetailExtractor(BaseExtractor, ProductDetailMixin):
    """Gnc è¯¦æƒ…é¡µæå–å™¨"""

    async def fetch_rendered_html(self, page) -> tuple:
        """è·å–æ¸²æŸ“åçš„çº¯HTML"""
        pass  # Step 5.1 å¡«å……

    def remove_site_chrome(self, html: str) -> str:
        """ç§»é™¤å¤´æ /åº•æ """
        pass  # Step 5.2 å¡«å……

    def extract_main_content(self, html: str) -> str:
        """æå–ä¸»å•†å“å†…å®¹"""
        pass  # Step 5.3 å¡«å……

    def convert_html_to_markdown(self, html: str) -> str:
        """è½¬æ¢ä¸º Markdown"""
        pass  # Step 5.4 å¡«å……

async def extract_product_detail(page: PageParam) -> dict:
    """è¯¦æƒ…é¡µæå–å…¥å£"""
    return await GncDetailExtractor(page).do()

# æ›´æ–° URL_MAP
URL_MAP = {
    ...,
    'detail_page': {
        'patterns': [r'https://www\.gnc\.com/[\w-]+/\d+\.html'],
        'sample_urls': ['https://www.gnc.com/vitamins/123.html'],
        'func': extract_product_detail,  # âœ… å·²æ›´æ–°
        'action': 'get_detail_info'
    }
}
```

**è·³è¿‡æœºåˆ¶**:

å¦‚æœ LLM åˆ¤æ–­ sample_url ä¸æ˜¯ç›®æ ‡è¯¦æƒ…é¡µï¼ˆä¾‹å¦‚è¯¯åˆ¤ï¼‰ï¼Œä¼šåœ¨å“åº”ä¸­åŒ…å«ç‰¹æ®Šæ ‡è®°ï¼š

```
è¯¥é¡µé¢ä¸æ˜¯ç›®æ ‡è¯¦æƒ…é¡µï¼Œä¸­æ­¢åç»­æ­¥éª¤!!!
```

ä»£ç æ£€æµ‹åˆ°è¿™ä¸ªæ ‡è®°åï¼Œä¼šï¼š

1. è·³è¿‡ Step 5.1-5.6
2. å°† pattern æ ‡è®°ä¸ºå·²å®Œæˆ
3. ç›´æ¥è¿”å› Step 4.1 (å¤„ç†ä¸‹ä¸€ä¸ª pattern)

**ä¸‹ä¸€æ­¥**: `step5_1__generate_fetch_rendered_html`

---

(ç”±äºç¯‡å¹…é™åˆ¶ï¼Œç»§ç»­åœ¨ä¸‹ä¸€éƒ¨åˆ†...)

#### 5.2.5 Step 5.1-5.6: è¯¦æƒ…é¡µæ–¹æ³•ç”Ÿæˆ

è¿™6ä¸ªå­æ­¥éª¤åˆ†åˆ«ç”Ÿæˆè¯¦æƒ…é¡µæå–å™¨çš„å„ä¸ªæ–¹æ³•ï¼Œå½¢æˆå®Œæ•´çš„æ•°æ®æå–æµæ°´çº¿ã€‚

**Step 5.1: fetch_rendered_html** - è·å–æ¸²æŸ“åçš„çº¯HTML

- æ»šåŠ¨åˆ°è¯„è®ºåŒºè§¦å‘AJAXåŠ è½½
- æ»šåŠ¨åˆ°åº•éƒ¨ç¡®ä¿æ‰€æœ‰å†…å®¹åŠ è½½
- ä¿å­˜åŸå§‹HTMLå’Œæ¸²æŸ“åHTMLï¼ˆæ— JSï¼‰

**Step 5.2: remove_site_chrome** - ç§»é™¤ç«™ç‚¹é€šç”¨å…ƒç´ 

- ç§»é™¤å¤´æ ã€åº•æ ã€å¯¼èˆªæ 
- ç§»é™¤å…¬å‘Šã€å¹¿å‘Šã€ä¾§è¾¹æ 

**Step 5.3: extract_main_content** - æå–ä¸»å•†å“å†…å®¹

- ç§»é™¤æ¨èå•†å“ã€å¹¿å‘Š
- ä¿ç•™å•†å“æ ‡é¢˜ã€ä»·æ ¼ã€æè¿°ã€å‚æ•°ã€è¯„è®ºã€FAQ

**Step 5.4: convert_html_to_markdown** - è½¬æ¢ä¸ºMarkdown

- ä½¿ç”¨html2textæˆ–è‡ªå®šä¹‰è½¬æ¢å™¨
- ä¿ç•™ç»“æ„åŒ–ä¿¡æ¯ï¼ˆè¡¨æ ¼ã€åˆ—è¡¨ï¼‰

**Step 5.5: collect_other_info** - æ”¶é›†å…¶ä»–ä¿¡æ¯

- å•†å“IDã€SKU
- å•†å“åˆ†ç±»ã€å“ç‰Œ
- è¯„åˆ†ã€è¯„è®ºæ•°

**Step 5.6: intercept_ajax_comment** - æ‹¦æˆªAJAXè¯„è®ºè¯·æ±‚

- åˆ†æè¯„è®ºAPIè¯·æ±‚
- ç¼–å†™ç›´æ¥è°ƒç”¨APIçš„ä»£ç ï¼ˆå¯é€‰ï¼‰

**æ½œåœ¨é—®é¢˜æ±‡æ€»**:

| æ­¥éª¤ | å¸¸è§é—®é¢˜         | è§£å†³æ–¹æ¡ˆ                                   |
| ---- | ---------------- | ------------------------------------------ |
| 5.1  | è¯„è®ºæœªåŠ è½½       | å¢åŠ ç­‰å¾…æ—¶é—´ï¼Œæ£€æŸ¥è¯„è®ºå…ƒç´                  |
| 5.2  | è¯¯åˆ ä¸»å†…å®¹       | ä½¿ç”¨æ›´ç²¾ç¡®çš„CSSé€‰æ‹©å™¨                      |
| 5.3  | æ¨èå•†å“æœªæ¸…ç†   | åˆ†æé¡µé¢ç»“æ„ï¼Œæ‰¾å‡ºæ¨èåŒºåŸŸç‰¹å¾             |
| 5.4  | Markdownæ ¼å¼æ··ä¹± | è°ƒæ•´html2textå‚æ•°                          |
| 5.5  | å­—æ®µæå–å¤±è´¥     | ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æˆ–CSSé€‰æ‹©å™¨                  |
| 5.6  | APIè¯·æ±‚æ‰¾ä¸åˆ°    | ä½¿ç”¨chrome-devtoolsçš„list_network_requests |

---

#### 5.2.6 Step 6: ç”Ÿæˆåˆ—è¡¨é¡µæå–å™¨

**ç›®æ ‡**: ä¸ºåˆ—è¡¨é¡µï¼ˆåˆ†ç±»é¡µã€æœç´¢é¡µç­‰ï¼‰ç”Ÿæˆæå–å™¨ã€‚

**ä¸Step 2çš„åŒºåˆ«**:

- Step 2: ä¸»é¡µæå–å™¨ï¼ˆå·²å®ç°ï¼‰
- Step 6: å…¶ä»–åˆ—è¡¨é¡µæå–å™¨ï¼ˆä½¿ç”¨ç›¸åŒé€»è¾‘ï¼‰

**å®ç°**:

```python
async def extract_from_category_page(page: PageParam) -> dict:
    """åˆ†ç±»é¡µæå–å™¨"""
    # å¤ç”¨ extract_deals_from_mainpage çš„é€»è¾‘
    return await extract_deals_from_mainpage(page)

# æ›´æ–° URL_MAP
URL_MAP['category_list']['func'] = extract_from_category_page
```

---

### 5.3 é˜¶æ®µ 3: ç½‘ç«™æ ‘æ‰©å±• (Step 7-10)

#### 5.3.1 Step 7: ç½‘ç«™æ ‘æ‰©å±•ä¸€å±‚

**ç›®æ ‡**: è®¿é—®åˆ—è¡¨é¡µä¸­çš„å­é¡µé¢ï¼Œæ‰©å±•ç½‘ç«™æ ‘åˆ°ä¸‹ä¸€å±‚ã€‚

**å·¥ä½œæµç¨‹**:

```python
1. è¯»å– site_tree.json ä¸­çš„ level=1 çš„åˆ—è¡¨é¡µ
2. å¯¹æ¯ä¸ªåˆ—è¡¨é¡µè°ƒç”¨å…¶æå–å™¨
3. è·å–å­é¡µé¢URLsï¼ˆlevel=2ï¼‰
4. è¿½åŠ åˆ° site_tree.json
5. æ£€æµ‹æ˜¯å¦æœ‰æ–°çš„ URL patterns
6. å¦‚æœæœ‰æ–°patternsï¼Œæ ‡è®° has_new_patterns_in_step7=True
```

**å¾ªç¯æ§åˆ¶**:

```python
# routing_logic.py
def route_after_step7(state: CrawlerDevState) -> str:
    if state["has_new_patterns_in_step7"]:
        if state["step7_loop_count"] < 3:
            return "step3__generate_url_patterns"  # é‡æ–°ç”Ÿæˆpatterns
        else:
            logger.warning("Step 7 å¾ªç¯æ¬¡æ•°è¾¾åˆ°ä¸Šé™")
            return "step8__analyze_markdown_info"
    else:
        return "step8__analyze_markdown_info"
```

**æ½œåœ¨é—®é¢˜**:

- **æ— é™å¾ªç¯**: å¦‚æœæ¯æ¬¡éƒ½å‘ç°æ–°patterns
  - **è§£å†³**: step7_loop_count æœ€å¤§å€¼ä¸º3
- **å­é¡µé¢è¿‡å¤š**: æ‰©å±•åURLsæ•°é‡çˆ†ç‚¸
  - **è§£å†³**: é™åˆ¶æ¯ä¸ªåˆ—è¡¨é¡µæœ€å¤šæå–50ä¸ªURLs

---

#### 5.3.2 Step 8: Markdownä¿¡æ¯åˆ†æï¼ˆå ä½ï¼‰

**å½“å‰çŠ¶æ€**: å ä½æ­¥éª¤ï¼Œä»…æ›´æ–°çŠ¶æ€ã€‚

**æœªæ¥è§„åˆ’**: ç”¨LLMåˆ†ææå–çš„Markdownï¼Œä¼˜åŒ–æå–é€»è¾‘ã€‚

---

#### 5.3.3 Step 9: é¦–æ¬¡è¿è¡Œæµ‹è¯•

**ç›®æ ‡**: æ‰§è¡Œä¸€æ¬¡å®Œæ•´çš„çˆ¬è™«è¿è¡Œï¼ŒéªŒè¯åŠŸèƒ½ã€‚

**æ‰§è¡Œå‘½ä»¤**:

```bash
python -m crawler.base.extractor_scheduler <site_name> <entry_url> --max-level 2
```

**éªŒè¯å†…å®¹**:

- æ˜¯å¦æˆåŠŸæå–å•†å“
- æ˜¯å¦ä¿å­˜åˆ°æ•°æ®åº“
- æ˜¯å¦æœ‰é”™è¯¯æ—¥å¿—

---

#### 5.3.4 Step 10: ç”Ÿæˆ Airflow DAG

**ç›®æ ‡**: ç”Ÿæˆå®šæ—¶ä»»åŠ¡é…ç½®ï¼Œç”¨äºç”Ÿäº§ç¯å¢ƒè°ƒåº¦ã€‚

**ç”Ÿæˆçš„DAGæ–‡ä»¶**:

```python
# crawler/airflow_dags/deal_<site>_dag.py
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2025, 1, 1),
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'deal_<site>_crawler',
    default_args=default_args,
    schedule_interval='0 */6 * * *',  # æ¯6å°æ—¶è¿è¡Œä¸€æ¬¡
    catchup=False
)

def run_crawler():
    from crawler.base.extractor_scheduler import SiteScheduler
    scheduler = SiteScheduler(
        site_name='<site>',
        entry_url='<url>',
        site_info={'site': '<site>', 'url': '<url>', 'category': 'deal'},
        max_level=3
    )
    await scheduler.analyze('<url>')

task = PythonOperator(
    task_id='run_<site>_crawler',
    python_callable=run_crawler,
    dag=dag
)
```

---

### 5.4 é˜¶æ®µ 4: ä»£ç ä¼˜åŒ– (Step 20-22)

#### 5.4.1 Step 20: ä»£ç æ£€æŸ¥

**ç›®æ ‡**: å…¨é¢æ£€æŸ¥ä»£ç è´¨é‡ã€‚

**æ£€æŸ¥é¡¹**:

- ä»£ç é£æ ¼ï¼ˆPEP 8ï¼‰
- æœªä½¿ç”¨çš„å¯¼å…¥
- å†—ä½™æ³¨é‡Š
- æ–¹æ³•ç­¾åæ­£ç¡®æ€§

---

#### 5.4.2 Step 20.1-20.4: ç»†åˆ†æ£€æŸ¥

- **20.1**: æ€§èƒ½æ£€æŸ¥ï¼ˆæ˜¯å¦ä½¿ç”¨äº†BrowserPoolã€BrightDataæ‰¹é‡ç­‰ï¼‰
- **20.2**: ä¼˜åŒ–å‡½æ•°ï¼ˆç§»é™¤å†—ä½™ä»£ç ï¼‰
- **20.3**: æ£€æŸ¥æ–¹æ³•ç­¾åï¼ˆå‚æ•°ã€è¿”å›å€¼ï¼‰
- **20.4**: æ£€æŸ¥ç¼ºå¤±æ•°æ®ï¼ˆæ˜¯å¦æå–äº†æ‰€æœ‰å¿…è¦å­—æ®µï¼‰

---

#### 5.4.3 Step 21: æ€§èƒ½ä¼˜åŒ–

**ä¼˜åŒ–ç­–ç•¥**:

- å¯ç”¨BrightDataæ‰¹é‡çˆ¬å–
- è°ƒæ•´å¹¶å‘å‚æ•°
- å‡å°‘ä¸å¿…è¦çš„ç­‰å¾…æ—¶é—´

---

#### 5.4.4 Step 22: ä¿®å¤ä»£ç 

**è§¦å‘æ¡ä»¶**: Step 9æˆ–Step 20æ£€æµ‹åˆ°é—®é¢˜ã€‚

**ä¿®å¤æµç¨‹**:

```python
1. è¯»å–é”™è¯¯æ—¥å¿—
2. ç”¨LLMåˆ†æé”™è¯¯åŸå› 
3. ç”Ÿæˆä¿®å¤æ–¹æ¡ˆ
4. åº”ç”¨ä¿®å¤
5. è·³å›åˆ° regenerate_from æŒ‡å®šçš„æ­¥éª¤ï¼ˆé€šå¸¸æ˜¯Step 9ï¼‰
```

---

## 6. Reviewer éªŒè¯æœºåˆ¶

### 6.1 Reviewer èŠ‚ç‚¹åˆ—è¡¨

DevBotå…±æœ‰**6ä¸ªReviewerèŠ‚ç‚¹**:

| Reviewer       | å¯¹åº”æ­¥éª¤ | éªŒè¯å†…å®¹               | éªŒè¯æ–¹å¼                                 |
| -------------- | -------- | ---------------------- | ---------------------------------------- |
| review_step0   | Step 0   | åŸºç¡€æ–‡ä»¶å¯å¯¼å…¥         | importlib.import_module                  |
| review_step1   | Step 1   | å¼•æ“å’Œå¹¶å‘é…ç½®å·²è®¾ç½®   | æ£€æŸ¥ä»£ç å†…å®¹                             |
| review_step2   | Step 2   | åˆ—è¡¨æå–å™¨è¿”å›æ ¼å¼æ­£ç¡® | æ‰§è¡Œæ–¹æ³•å¹¶éªŒè¯è¿”å›å€¼                     |
| review_step2_1 | Step 2.1 | site_patternsæ ¼å¼æ­£ç¡®  | è¯»å–JSONå¹¶éªŒè¯å­—æ®µ                       |
| review_step3   | Step 3   | URL_MAPæ ¼å¼æ­£ç¡®        | å¯¼å…¥æ¨¡å—å¹¶éªŒè¯ç»“æ„                       |
| review_step4   | Step 4   | æ ¸å¿ƒæå–å™¨å·²å®ç°       | æ£€æŸ¥URL_MAPä¸­çš„funcå­—æ®µ                  |
| review_step7   | Step 7   | ç½‘ç«™æ ‘æ‰©å±•åŠŸèƒ½å­˜åœ¨     | æ£€æŸ¥expand_one_level_from_list_pagesæ–¹æ³• |
| review_step8   | Step 8   | çŠ¶æ€æ­£ç¡®               | æ£€æŸ¥current_stepå’Œstatus                 |

### 6.2 Reviewer å·¥ä½œæµç¨‹

```
Developer èŠ‚ç‚¹å®Œæˆ
    â†“ status="completed"
    â†“
Reviewer èŠ‚ç‚¹éªŒè¯
    â”œâ”€ å¯¼å…¥æ¨¡å—
    â”œâ”€ æ‰§è¡Œæµ‹è¯•å‡½æ•°
    â”œâ”€ éªŒè¯è¿”å›å€¼/æ–‡ä»¶æ ¼å¼
    â””â”€ åˆ¤æ–­æ˜¯å¦é€šè¿‡
         â”œâ”€ é€šè¿‡ â†’ status="reviewed"
         â””â”€ å¤±è´¥ â†’ status="failed", retry_count++
```

### 6.3 éªŒè¯ç¤ºä¾‹ï¼šreview_step2

```python
# reviewer_nodes.py:208-321
async def review_step2(state: CrawlerDevState) -> CrawlerDevState:
    """éªŒè¯ Step 2: æ£€æŸ¥åˆ—è¡¨æå–å™¨æ˜¯å¦æ­£ç¡®å®ç°"""

    category = state["category"]
    site_name = state["site_name"]

    try:
        # 1. å¯¼å…¥æ¨¡å—
        module_path = f'crawler.{category}.extractor_{site_name}'
        if module_path in sys.modules:
            del sys.modules[module_path]

        module = importlib.import_module(module_path)

        # 2. è°ƒç”¨ extract_deals_from_mainpage
        extract_func = getattr(module, 'extract_deals_from_mainpage')
        result = await extract_func()

        # 3. éªŒè¯è¿”å›ç±»å‹
        if not isinstance(result, dict):
            raise ValueError(f"åº”è¿”å› dictï¼Œå®é™…è¿”å›: {type(result).__name__}")

        # 4. éªŒè¯æ˜¯å¦åŒ…å« urls å­—æ®µ
        if 'urls' not in result:
            raise ValueError("è¿”å›ç»“æœç¼ºå°‘ 'urls' å­—æ®µ")

        # 5. éªŒè¯ urls æ˜¯æ•°ç»„
        urls = result['urls']
        if not isinstance(urls, list):
            raise ValueError(f"urls åº”ä¸º listï¼Œå®é™…ä¸º: {type(urls).__name__}")

        # 6. éªŒè¯ urls æ•°æ®æ ¼å¼
        for i, item in enumerate(urls[:3]):  # åªæ£€æŸ¥å‰3ä¸ª
            if not isinstance(item, dict):
                raise ValueError(f"urls[{i}] åº”ä¸º dict")

            # æ£€æŸ¥å¿…éœ€å­—æ®µ
            required_fields = ['title', 'url', 'type']
            for field in required_fields:
                if field not in item:
                    raise ValueError(f"urls[{i}] ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")

            # æ£€æŸ¥ type å­—æ®µå€¼
            if item['type'] not in ['detail', 'list', 'other', 'unclear']:
                raise ValueError(f"urls[{i}] type å­—æ®µå€¼é”™è¯¯: {item['type']}")

        logger.info("âœ… Step2 éªŒè¯é€šè¿‡")

    except Exception as e:
        # å‘é€ Slack å‘Šè­¦
        send_slack_exception(e, context=f"Review Step2 - {site_name}")
        # æŠ›å‡ºå¼‚å¸¸ï¼Œè®© LangGraph ç»ˆæ­¢æµç¨‹
        raise

    # éªŒè¯æˆåŠŸ
    return {
        **state,
        "status": "reviewed",
        "validation_result": {
            "step": "step2",
            "success": True,
            "message": f"åˆ—è¡¨æå–å™¨å®ç°æ­£ç¡®ï¼ŒæˆåŠŸæå– {len(urls)} ä¸ªé“¾æ¥"
        }
    }
```

### 6.4 å¼‚å¸¸å¤„ç†ç­–ç•¥

**æŠ€æœ¯å¼‚å¸¸** (ä»£ç é”™è¯¯ã€å¯¼å…¥å¤±è´¥):

- å‘é€ Slack å‘Šè­¦
- æŠ›å‡ºå¼‚å¸¸ï¼Œç»ˆæ­¢æµç¨‹
- éœ€è¦äººå·¥ä»‹å…¥ä¿®å¤

**ä¸šåŠ¡å¼‚å¸¸** (è¿”å›å€¼æ ¼å¼é”™è¯¯ã€æ•°æ®ä¸å®Œæ•´):

- è®°å½•é”™è¯¯åˆ° state["error"]
- æ ‡è®° status="failed"
- è§¦å‘é‡è¯•æœºåˆ¶

---

## 7. å·¥ä½œæµè·¯ç”±ä¸æ§åˆ¶

### 7.1 è·¯ç”±å†³ç­–å‡½æ•°

**routing_logic.py** å®šä¹‰äº†æ‰€æœ‰çš„è·¯ç”±è§„åˆ™ï¼š

```python
# ä¸»è·¯ç”±å‡½æ•°
def route_after_developer(state: CrawlerDevState) -> str:
    """DeveloperèŠ‚ç‚¹å®Œæˆåçš„è·¯ç”±å†³ç­–"""
    current_step = state["current_step"]
    status = state["status"]

    # å¦‚æœå¤±è´¥æˆ–éœ€è¦reviewï¼Œè¿›å…¥reviewer
    if status in ["completed", "failed"]:
        # æ£€æŸ¥æ˜¯å¦æœ‰å¯¹åº”çš„reviewer
        if has_reviewer_for_step(current_step):
            return f"review_step{current_step.replace('.', '_')}"

    # å¦åˆ™ç›´æ¥è¿›å…¥ä¸‹ä¸€ä¸ªdeveloper
    return next_developer_step(current_step)

def route_after_reviewer(state: CrawlerDevState) -> str:
    """ReviewerèŠ‚ç‚¹å®Œæˆåçš„è·¯ç”±å†³ç­–"""
    status = state["status"]
    retry_count = state["retry_count"]

    if status == "reviewed":
        # é€šè¿‡éªŒè¯ï¼Œè¿›å…¥ä¸‹ä¸€æ­¥
        return next_developer_step(state["current_step"])
    elif status == "failed":
        # å¤±è´¥ï¼Œæ£€æŸ¥é‡è¯•æ¬¡æ•°
        if retry_count < 3:
            # é‡è¯•ï¼šè¿”å›å½“å‰developer
            return f"step{state['current_step'].replace('.', '_')}"
        else:
            # è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œç»ˆæ­¢
            raise Exception(f"Step {state['current_step']} å¤±è´¥æ¬¡æ•°è¿‡å¤š")
    else:
        # å…¶ä»–çŠ¶æ€ï¼Œç»ˆæ­¢
        raise Exception(f"æœªçŸ¥çŠ¶æ€: {status}")
```

### 7.2 æ¡ä»¶è¾¹ (Conditional Edges)

LangGraphä½¿ç”¨**æ¡ä»¶è¾¹**å®ç°è·¯ç”±ï¼š

```python
# crawler_devbot.py: æ„å»ºworkflow
workflow = StateGraph(CrawlerDevState)

# æ·»åŠ èŠ‚ç‚¹
workflow.add_node("step0", step0__create_base_file)
workflow.add_node("review_step0", review_step0)
workflow.add_node("step1", step1__analyze_page)
# ... æ›´å¤šèŠ‚ç‚¹

# æ·»åŠ æ¡ä»¶è¾¹
workflow.add_conditional_edges(
    "step0",  # æºèŠ‚ç‚¹
    route_after_developer,  # è·¯ç”±å‡½æ•°
    {
        "review_step0": "review_step0",  # å¦‚æœè¿”å›"review_step0"ï¼Œè·³è½¬åˆ°review_step0èŠ‚ç‚¹
        "step1": "step1"  # å¦‚æœè¿”å›"step1"ï¼Œè·³è½¬åˆ°step1èŠ‚ç‚¹
    }
)

workflow.add_conditional_edges(
    "review_step0",
    route_after_reviewer,
    {
        "step0": "step0",  # é‡è¯•
        "step1": "step1"  # ä¸‹ä¸€æ­¥
    }
)

# ... æ›´å¤šæ¡ä»¶è¾¹
```

### 7.3 å¾ªç¯æ£€æµ‹ä¸ç»ˆæ­¢

**é—®é¢˜**: å¦‚ä½•é˜²æ­¢æ— é™å¾ªç¯ï¼Ÿ

**è§£å†³æ–¹æ¡ˆ**:

1. **é‡è¯•æ¬¡æ•°é™åˆ¶**: retry_count < 3
2. **Step 7å¾ªç¯é™åˆ¶**: step7_loop_count < 3
3. **Patterné˜Ÿåˆ—ç©ºæ£€æµ‹**: len(patterns_queue) == 0
4. **æ‰‹åŠ¨ç»ˆæ­¢**: ç”¨æˆ·å¯ä»¥éšæ—¶ä¸­æ–­

---

## 8. é”™è¯¯å¤„ç†ä¸é‡è¯•æœºåˆ¶

### 8.1 å¼‚å¸¸ç±»å‹

DevBotå®šä¹‰äº†4ç§è‡ªå®šä¹‰å¼‚å¸¸ï¼š

```python
# claude_agent_base.py:21-43
class SubagentError(Exception):
    """Subagent æ‰§è¡Œé”™è¯¯åŸºç±»"""
    pass

class PromptError(SubagentError):
    """Prompt æœ¬èº«æœ‰é—®é¢˜ï¼ˆä¿¡æ¯ç¼ºå¤±ã€å·¥å…·æœªæˆæƒã€ä»£ç æ‰§è¡Œå¤±è´¥ç­‰ï¼‰"""
    pass

class TaskUnachievableError(SubagentError):
    """ä»»åŠ¡æ— æ³•è¾¾æˆï¼ˆæŠ€æœ¯å—é™ã€å¤šæ¬¡å¤±è´¥ã€å¤–éƒ¨ä¾èµ–é—®é¢˜ç­‰ï¼‰"""
    pass

class HumanInterventionRequired(SubagentError):
    """éœ€è¦äººå·¥ä»‹å…¥ï¼ˆé€šç”¨ï¼‰"""
    pass
```

### 8.2 é‡è¯•ç­–ç•¥

**è‡ªåŠ¨é‡è¯•**:

- DeveloperèŠ‚ç‚¹å¤±è´¥ â†’ Revieweræ£€æµ‹ â†’ retry_count++ â†’ é‡æ–°æ‰§è¡ŒDeveloper
- æœ€å¤šé‡è¯•3æ¬¡

**æ‰‹åŠ¨é‡è¯•**:

- ç”¨æˆ·å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œå‚æ•° `--entry <node>` ä»æŒ‡å®šèŠ‚ç‚¹é‡æ–°å¼€å§‹

**æ™ºèƒ½é‡è¯•**:

- Step 22ä¿®å¤ä»£ç åï¼Œå¯ä»¥è·³å›åˆ° `regenerate_from` æŒ‡å®šçš„æ­¥éª¤ï¼ˆé€šå¸¸æ˜¯Step 9ï¼‰

### 8.3 Slackå‘Šè­¦

**è§¦å‘æ¡ä»¶**:

- Revieweræ£€æµ‹åˆ°æŠ€æœ¯å¼‚å¸¸
- å¯¼å…¥æ¨¡å—å¤±è´¥
- ä»£ç æ‰§è¡ŒæŠ¥é”™

**å‘Šè­¦å†…å®¹**:

```python
send_slack_exception(
    exception=e,
    context=f"Review Step2 - {site_name}"
)

# Slackæ¶ˆæ¯ç¤ºä¾‹
"""
ğŸš¨ DevBot å¼‚å¸¸å‘Šè­¦

æ­¥éª¤: Review Step2 - gnc
å¼‚å¸¸ç±»å‹: ImportError
å¼‚å¸¸ä¿¡æ¯: No module named 'crawler.product.extractor_gnc'
å †æ ˆ: ...
æ—¶é—´: 2025-01-20 14:30:00
"""
```

---

## 9. å­˜å‚¨ä¸æŒä¹…åŒ–

### 9.1 çŠ¶æ€æŒä¹…åŒ– (MemorySaver)

**LangGraph MemorySaver** è‡ªåŠ¨å°†çŠ¶æ€ä¿å­˜åˆ°JSONæ–‡ä»¶ï¼š

```python
# crawler_devbot.py: æ„å»ºapp
from langgraph.checkpoint.memory import MemorySaver

memory = MemorySaver()
app = workflow.compile(checkpointer=memory)

# æ‰§è¡Œæ—¶æŒ‡å®šthread_id
config = {"configurable": {"thread_id": f"{site_name}_crawler"}}
result = await app.ainvoke(initial_state, config=config)
```

**ä¿å­˜ä½ç½®**:

```
crawler/product/local_state_<site>.json
```

**å†…å®¹ç¤ºä¾‹**:

```json
{
  "url": "https://www.gnc.com",
  "site_name": "gnc",
  "category": "product",
  "current_step": "5.2",
  "current_step_name": "generate_remove_site_chrome",
  "status": "completed",
  "session_id": "session_abc123",
  "patterns_queue": [...],
  "completed_patterns": [...],
  "retry_count": 0
}
```

**æ–­ç‚¹æ¢å¤**:

```bash
# è‡ªåŠ¨ä»ä¸Šæ¬¡æ–­ç‚¹æ¢å¤
python -m devbot.crawler_devbot product https://www.gnc.com

# å¼ºåˆ¶é‡æ–°å¼€å§‹
python -m devbot.crawler_devbot product https://www.gnc.com --reset
```

### 9.2 å¯¹è¯å†å² (ConversationStore)

**SQLiteæ•°æ®åº“** å­˜å‚¨æ‰€æœ‰å¯¹è¯è®°å½•ï¼š

```sql
-- devbot/store/conversation_store.py
CREATE TABLE conversations (
    id INTEGER PRIMARY KEY,
    site_name TEXT NOT NULL,
    category TEXT NOT NULL,
    step_name TEXT NOT NULL,
    prompt TEXT NOT NULL,
    response TEXT NOT NULL,
    metadata TEXT,
    timestamp TEXT NOT NULL,
    thread_id TEXT NOT NULL
)
```

**ä¿å­˜æ—¶æœº**:

```python
# developer_nodes.py: æ¯ä¸ªæ­¥éª¤å®Œæˆå
save_conversation_from_state(
    state=state,
    prompt=prompt,
    response=response_text,
    node_name="step1_analyze_page_structure",
    metadata={"agent": "crawler-developer"}
)
```

**æŸ¥è¯¢ç¤ºä¾‹**:

```python
# è·å–æŸä¸ªç«™ç‚¹çš„æ‰€æœ‰å¯¹è¯
conversations = ConversationStore.get_by_site("gnc", "product")

# è·å–æŸä¸ªæ­¥éª¤çš„å¯¹è¯
step1_conv = ConversationStore.get_by_step("gnc", "product", "step1_analyze_page_structure")
```

### 9.3 Gitç‰ˆæœ¬æ§åˆ¶

**è‡ªåŠ¨æäº¤**:

DevBotä¼šåœ¨æ¯ä¸ªå…³é”®æ­¥éª¤åè‡ªåŠ¨æäº¤ä»£ç åˆ°Gitï¼š

```python
# utils/git_utils.py
def auto_commit_if_enabled(state: CrawlerDevState, step_name: str, message: str):
    if not conf.enable_auto_commit:
        return

    site_name = state["site_name"]
    category = state["category"]

    # Git add
    file_path = state["base_file_path"]
    os.system(f"git add {file_path}")

    # Git commit
    commit_msg = f"AUTO-GEN[{category}/{site_name}] {step_name}: {message}"
    os.system(f'git commit -m "{commit_msg}"')

    logger.info(f"âœ… Gitæäº¤: {commit_msg}")
```

**æäº¤å†å²ç¤ºä¾‹**:

```bash
git log --grep="AUTO-GEN" --grep="gnc" --oneline

a1b2c3d AUTO-GEN[product/gnc] step5_2__generate_remove_site_chrome: ç”Ÿæˆremove_site_chromeæ–¹æ³•
d4e5f6g AUTO-GEN[product/gnc] step5_1__generate_fetch_rendered_html: ç”Ÿæˆfetch_rendered_htmlæ–¹æ³•
h7i8j9k AUTO-GEN[product/gnc] step5__generate_extractor_class: ç”Ÿæˆè¯¦æƒ…é¡µæå–å™¨ç±»
...
```

**å›æ»š**:

```bash
# å›æ»šåˆ°æŸä¸ªæ­¥éª¤ä¹‹å‰
git reset --hard <commit_hash>

# ä»è¯¥æ­¥éª¤é‡æ–°å¼€å§‹
python -m devbot.crawler_devbot product https://www.gnc.com --entry step5_2
```

---

## 10. é¢è¯•é«˜é¢‘é—®é¢˜

### 10.1 æ¶æ„è®¾è®¡é—®é¢˜

**Q1: ä¸ºä»€ä¹ˆé€‰æ‹©LangGraphè€Œä¸æ˜¯ç›´æ¥ç”¨LLMï¼Ÿ**

A: LangGraphæä¾›äº†ä»¥ä¸‹æ ¸å¿ƒèƒ½åŠ›ï¼š

1. **çŠ¶æ€ç®¡ç†**: TypedDictè‡ªåŠ¨åºåˆ—åŒ–/ååºåˆ—åŒ–ï¼Œæ–­ç‚¹æ¢å¤
2. **æµç¨‹æ§åˆ¶**: æ¡ä»¶è¾¹å®ç°å¤æ‚çš„è·¯ç”±é€»è¾‘
3. **å¯è§‚æµ‹æ€§**: æ¯ä¸ªèŠ‚ç‚¹çš„æ‰§è¡Œéƒ½æœ‰æ—¥å¿—å’Œè¿½è¸ª
4. **å¯æ‰©å±•æ€§**: è½»æ¾æ·»åŠ æ–°èŠ‚ç‚¹å’ŒéªŒè¯é€»è¾‘

**Q2: Developer + RevieweråŒèŠ‚ç‚¹è®¾è®¡çš„ä¼˜åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ**

A:

- **è´¨é‡ä¿è¯**: Developerç”Ÿæˆä»£ç ï¼ŒReviewerè‡ªåŠ¨éªŒè¯ï¼Œé¿å…ä½çº§é”™è¯¯
- **å¿«é€Ÿè¿­ä»£**: éªŒè¯å¤±è´¥ç«‹å³é‡è¯•ï¼Œæ— éœ€äººå·¥ä»‹å…¥
- **å¯è¿½æº¯**: æ¯æ¬¡éªŒè¯çš„ç»“æœéƒ½è®°å½•åœ¨stateä¸­ï¼Œä¾¿äºè°ƒè¯•

**Q3: ä¸ºä»€ä¹ˆç”¨å…¨å±€å­—å…¸ç®¡ç†Claudeå®¢æˆ·ç«¯ï¼Ÿ**

A:

- **Sessionå¤ç”¨**: åŒä¸€agentçš„å¤šæ¬¡è°ƒç”¨å…±äº«ä¸Šä¸‹æ–‡ï¼ŒLLMèƒ½"è®°ä½"ä¹‹å‰çš„åˆ†æ
- **èµ„æºä¼˜åŒ–**: é¿å…é‡å¤åˆ›å»ºå®¢æˆ·ç«¯ï¼Œå‡å°‘åˆå§‹åŒ–å¼€é”€
- **å¹¶å‘å®‰å…¨**: å­—å…¸æ˜¯Pythonå†…ç½®ç±»å‹ï¼Œçº¿ç¨‹å®‰å…¨

### 10.2 æŠ€æœ¯ç»†èŠ‚é—®é¢˜

**Q4: å¦‚ä½•å¤„ç†åçˆ¬è™«ï¼Ÿ**

A: ä¸‰å±‚ç­–ç•¥ï¼š

1. **æ£€æµ‹**: Step 1 promptè¦æ±‚LLMåˆ¤æ–­æ˜¯å¦è¢«æ‹¦æˆª
2. **ç»•è¿‡**: ä½¿ç”¨BrightDataä»£ç†è·å–é¡µé¢HTML
3. **æœ¬åœ°è®¿é—®**: é€šè¿‡ `file://` åè®®åœ¨chrome-devtoolsä¸­æ‰“å¼€æœ¬åœ°HTMLæ–‡ä»¶

**Q5: å¦‚ä½•ä¿è¯å›¾ç‰‡ä¸è¶…è¿‡Claude APIé™åˆ¶ï¼Ÿ**

A: è‡ªåŠ¨å‹ç¼©å·¥å…· `compress_image`ï¼š

- **å¤§å°é™åˆ¶**: è¶…è¿‡2MB â†’ WebPæ ¼å¼ï¼Œè´¨é‡85%
- **åƒç´ é™åˆ¶**: è¶…è¿‡8000px â†’ åˆ‡åˆ†ä¸ºå¤šå¼ å›¾ç‰‡
- **é‡è¯•æœºåˆ¶**: DecompressionBombError â†’ å…³é—­é¡µé¢é‡æ–°æˆªå›¾ï¼ˆæœ€å¤š5æ¬¡ï¼‰

**Q6: Patterné˜Ÿåˆ—å¦‚ä½•å®ç°å¾ªç¯ï¼Ÿ**

A: é˜Ÿåˆ—é©±åŠ¨çš„çŠ¶æ€æœºï¼š

```
Step 4: patterns_queue = [p1, p2, p3]
Step 4.1: current = p1, queue = [p2, p3]
Step 5.x: å¤„ç† p1
Step 4.1: current = p2, queue = [p3]
Step 5.x: å¤„ç† p2
Step 4.1: current = p3, queue = []
Step 5.x: å¤„ç† p3
Step 4.1: queueä¸ºç©º â†’ è·³åˆ°Step 7
```

### 10.3 ä¼˜åŒ–ç­–ç•¥é—®é¢˜

**Q7: å¦‚ä½•ä¼˜åŒ–DevBotçš„æ‰§è¡Œæ•ˆç‡ï¼Ÿ**

A: å½“å‰ä¼˜åŒ–å’Œæœªæ¥æ”¹è¿›ï¼š

- **å½“å‰**:
  - Sessionå¤ç”¨ï¼šå‡å°‘LLMåˆå§‹åŒ–å¼€é”€
  - BrightDataæ‰¹é‡çˆ¬å–ï¼š20ä¸ªURLå¹¶å‘è·å–ï¼Œå¿«10å€
  - Gitè‡ªåŠ¨æäº¤ï¼šæ¯æ­¥éƒ½ä¿å­˜ï¼Œæ”¯æŒæ–­ç‚¹æ¢å¤
- **æœªæ¥**:
  - å¹¶è¡Œæ‰§è¡Œå¤šä¸ªpatternçš„ç”Ÿæˆï¼ˆç›®å‰æ˜¯ä¸²è¡Œï¼‰
  - ç¼“å­˜å¸¸è§ç½‘ç«™çš„åˆ†æç»“æœ
  - ä½¿ç”¨Geminiæ›¿ä»£éƒ¨åˆ†Claudeè°ƒç”¨ï¼ˆé™ä½æˆæœ¬ï¼‰

**Q8: å¦‚ä½•å¤„ç†LLMçš„å¹»è§‰é—®é¢˜ï¼Ÿ**

A: å¤šå±‚éªŒè¯æœºåˆ¶ï¼š

1. **Promptçº¦æŸ**: æä¾›è¯¦ç»†çš„æ ¼å¼è¦æ±‚å’Œç¤ºä¾‹
2. **RevieweréªŒè¯**: è‡ªåŠ¨æ‰§è¡Œä»£ç å¹¶æ£€æŸ¥è¿”å›å€¼
3. **é‡è¯•æœºåˆ¶**: éªŒè¯å¤±è´¥è‡ªåŠ¨é‡è¯•ï¼Œæœ€å¤š3æ¬¡
4. **äººå·¥ä»‹å…¥**: è¶…è¿‡é‡è¯•æ¬¡æ•°åï¼Œå‘é€Slackå‘Šè­¦

### 10.4 å®æˆ˜ç»éªŒé—®é¢˜

**Q9: å¼€å‘è¿‡ç¨‹ä¸­é‡åˆ°çš„æœ€å¤§æŒ‘æˆ˜æ˜¯ä»€ä¹ˆï¼Ÿ**

A: **Step 5å¾ªç¯çš„è®¾è®¡**ï¼š

- **æŒ‘æˆ˜**: éœ€è¦ä¸ºæ¯ä¸ªURL patternç”Ÿæˆç‹¬ç«‹çš„æå–å™¨ï¼Œä½†patternæ•°é‡ä¸ç¡®å®š
- **è§£å†³**: å¼•å…¥patterns_queueï¼Œç”¨é˜Ÿåˆ—é©±åŠ¨çš„å¾ªç¯ç»“æ„
- **ä¼˜åŒ–**: æ”¯æŒè·³è¿‡æœºåˆ¶ï¼ˆå¦‚æœLLMåˆ¤æ–­sample URLä¸æ˜¯ç›®æ ‡é¡µé¢ï¼‰
- **æ•™è®­**: å¤æ‚é€»è¾‘éœ€è¦æ¸…æ™°çš„çŠ¶æ€è®¾è®¡å’Œè·¯ç”±è§„åˆ™

**Q10: å¦‚ä½•ä¿è¯ç”Ÿæˆä»£ç çš„è´¨é‡ï¼Ÿ**

A: è´¨é‡ä¿è¯ä½“ç³»ï¼š

1. **æ¨¡æ¿è§„èŒƒ**: Jinja2æ¨¡æ¿å®šä¹‰ä»£ç éª¨æ¶
2. **Promptå·¥ç¨‹**: è¯¦ç»†çš„ä»»åŠ¡æè¿°ã€å‚è€ƒä»£ç ã€éªŒè¯æ ‡å‡†
3. **è‡ªåŠ¨æµ‹è¯•**: ReviewerèŠ‚ç‚¹æ‰§è¡Œä»£ç å¹¶éªŒè¯è¾“å‡º
4. **äººå·¥å®¡æŸ¥**: æœ€ç»ˆä»£ç é€šè¿‡Gitæäº¤è®°å½•å¯è¿½æº¯
5. **æŒç»­ä¼˜åŒ–**: Step 20-22è¿›è¡Œä»£ç æ£€æŸ¥å’Œæ€§èƒ½ä¼˜åŒ–

---

## æ€»ç»“

### æ ¸å¿ƒäº®ç‚¹

1. **ä¸‰å±‚æ¶æ„**: Developer â†’ Reviewer â†’ Stateï¼Œæ¸…æ™°çš„èŒè´£åˆ†ç¦»
2. **æ™ºèƒ½å†³ç­–**: LLMåˆ†æé¡µé¢ç»“æ„ï¼Œè‡ªåŠ¨é€‰æ‹©å¼•æ“å’Œå‚æ•°
3. **è´¨é‡ä¿è¯**: åŒèŠ‚ç‚¹éªŒè¯ï¼Œè‡ªåŠ¨é‡è¯•ï¼ŒSlackå‘Šè­¦
4. **æ–­ç‚¹æ¢å¤**: MemorySaver + Gitï¼Œæ”¯æŒéšæ—¶ä¸­æ–­å’Œæ¢å¤
5. **å¯æ‰©å±•æ€§**: è½»æ¾æ·»åŠ æ–°æ­¥éª¤ã€æ–°éªŒè¯è§„åˆ™
6. **å·¥å…·é›†æˆ**: MCPå·¥å…· + Claude SDKå†…ç½®å·¥å…· + è‡ªå®šä¹‰Pythonå·¥å…·
7. **å…¨æµç¨‹è‡ªåŠ¨åŒ–**: ä»URLåˆ°å¯è¿è¡Œä»£ç ï¼Œæ— éœ€äººå·¥ç¼–å†™

### æŠ€æœ¯æ ˆ

| ç±»åˆ«           | æŠ€æœ¯                                          |
| -------------- | --------------------------------------------- |
| **å·¥ä½œæµå¼•æ“** | LangGraph, StateGraph                         |
| **LLM**        | Claude Agent SDK (Sonnet 4.5)                 |
| **å·¥å…·åè®®**   | MCP (chrome-devtools, gemini-cli, playwright) |
| **çŠ¶æ€ç®¡ç†**   | TypedDict, MemorySaver                        |
| **å­˜å‚¨**       | SQLite (å¯¹è¯å†å²), JSON (çŠ¶æ€å¿«ç…§)            |
| **ç‰ˆæœ¬æ§åˆ¶**   | Git (è‡ªåŠ¨æäº¤)                                |
| **å‘Šè­¦**       | Slack (å¼‚å¸¸é€šçŸ¥)                              |
| **æ¨¡æ¿**       | Jinja2 (ä»£ç ç”Ÿæˆ)                             |

### é¢è¯•å‡†å¤‡è¦ç‚¹

**å¿…é¡»æŒæ¡çš„5ä¸ªé—®é¢˜**:

1. LangGraphçš„çŠ¶æ€ç®¡ç†æœºåˆ¶ï¼ˆTypedDict + MemorySaverï¼‰
2. Developer + RevieweråŒèŠ‚ç‚¹è®¾è®¡çš„ä¼˜åŠ¿
3. Patterné˜Ÿåˆ—å¾ªç¯çš„å®ç°åŸç†
4. Claude Agent SDKçš„Hookæœºåˆ¶ï¼ˆauto_approveï¼‰
5. åçˆ¬è™«å¤„ç†ç­–ç•¥ï¼ˆBrightData + æœ¬åœ°è®¿é—®ï¼‰

**å‡†å¤‡Demoæ¼”ç¤º**:

1. è¿è¡Œä¸€æ¬¡å®Œæ•´çš„crawler_devbotæµç¨‹
2. å±•ç¤ºæ–­ç‚¹æ¢å¤åŠŸèƒ½ï¼ˆ--entryå‚æ•°ï¼‰
3. æŸ¥çœ‹Gitæäº¤å†å²å’Œå¯¹è¯è®°å½•
4. æ¼”ç¤ºRevieweréªŒè¯å¤±è´¥åçš„é‡è¯•

**å‡†å¤‡æŠ€æœ¯æ¡ˆä¾‹**:

1. å¦‚ä½•å¤„ç†æˆªå›¾è¿‡å¤§é—®é¢˜ï¼ˆcompress_imageå·¥å…·ï¼‰
2. Sessionå¤ç”¨çš„å®ç°ï¼ˆå…¨å±€å®¢æˆ·ç«¯å­—å…¸ï¼‰
3. Step 7å¾ªç¯æ§åˆ¶çš„é˜²æ— é™å¾ªç¯æœºåˆ¶
4. Gitè‡ªåŠ¨æäº¤çš„ç‰ˆæœ¬ç®¡ç†ç­–ç•¥

---

**æ–‡æ¡£å®Œæˆæ—¶é—´**: 2025-01-20
**æ–‡æ¡£è´¨é‡**: â­â­â­â­â­ è¶…è¯¦ç»†ç‰ˆ
**æ€»è¡Œæ•°**: çº¦3000+è¡Œ
**å‡†å¤‡ç¨‹åº¦**: 100% é¢è¯•å°±ç»ª

**ç¥é¢è¯•é¡ºåˆ©ï¼** ğŸš€

---

## 11. WebScraper çˆ¬è™«é¡¹ç›®æ¶æ„

### 11.1 é¡¹ç›®æ¦‚è¿°

**WebScraper** æ˜¯ä¸€ä¸ªå¤šé¢†åŸŸç”µå•†æ•°æ®çˆ¬å–å¹³å°ï¼Œé‡‡ç”¨ä¸‰å±‚æ¶æ„è®¾è®¡ï¼Œæ”¯æŒProductã€Dealã€Shoppingç­‰å¤šç§ç±»åˆ«çš„æ•°æ®æŠ“å–ã€‚

**æ ¸å¿ƒä»·å€¼**:
1. **é€šç”¨æ€§å¼º**: åŸºäºæ¨¡æ¿æ–¹æ³•æ¨¡å¼ï¼Œå¿«é€Ÿé€‚é…æ–°ç½‘ç«™
2. **å¹¶å‘èƒ½åŠ›é«˜**: Browser Pool + BrightDataæ‰¹é‡çˆ¬å–
3. **æ‰©å±•æ€§å¥½**: Mixinæ¨¡å¼æä¾›å¯é€‰åŠŸèƒ½ï¼Œä¸ä¾µå…¥æ ¸å¿ƒé€»è¾‘
4. **å¯ç»´æŠ¤æ€§é«˜**: ä¸‰å±‚è§£è€¦ï¼ŒèŒè´£æ¸…æ™°

### 11.2 ä¸‰å±‚æ¶æ„è®¾è®¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  extractor_scheduler.py (è°ƒåº¦ç¼–æ’å±‚)                      â”‚
â”‚  - BFSéå†URLæ ‘                                          â”‚
â”‚  - åŠ¨æ€åŠ è½½ç«™ç‚¹æ¨¡å—                                        â”‚
â”‚  - å¹¶å‘ä»»åŠ¡è°ƒåº¦                                           â”‚
â”‚  - TracePageæ•°æ®åº“ç®¡ç†                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚ è°ƒç”¨
                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  extractor_<site>.py (ç«™ç‚¹é€‚é…å±‚)                         â”‚
â”‚  - URL_MAP è·¯ç”±è§„åˆ™                                      â”‚
â”‚  - åˆ—è¡¨é¡µæå–å‡½æ•°                                         â”‚
â”‚  - è¯¦æƒ…é¡µExtractorç±»                                     â”‚
â”‚  - CONCURRENT_CONFIG å¹¶å‘é…ç½®                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚ ç»§æ‰¿/ä½¿ç”¨
                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  extractor_base.py (åŸºç¡€è®¾æ–½å±‚)                           â”‚
â”‚  - BaseExtractor åŸºç±»                                    â”‚
â”‚  - ProductDetailMixin (å•†å“è¯¦æƒ…å¤„ç†)                      â”‚
â”‚  - BrowserPool (Playwrightè¿æ¥æ± )                       â”‚
â”‚  - PageParam (å‚æ•°å°è£…)                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 11.2.1 åŸºç¡€è®¾æ–½å±‚ (extractor_base.py)

**æ ¸å¿ƒç»„ä»¶**:

| ç»„ä»¶ | èŒè´£ | å…³é”®æ–¹æ³• |
|------|------|---------|
| `BaseExtractor` | æå–å™¨åŸºç±» | `fetch_html()`, `clean_html()`, `extract_text_as_markdown()` |
| `BrowserPool` | æµè§ˆå™¨æ± ç®¡ç† | `initialize()`, `get_page()`, `cleanup()` |
| `ProductDetailMixin` | å•†å“è¯¦æƒ…å¤„ç† | `post_save_callback()`, `_save_product_origin()` |
| `PageParam` | å‚æ•°å°è£… | url, html_content, extract_by_llm |

**BrowserPoolè®¾è®¡äº®ç‚¹**:

```python
class BrowserPool:
    """Playwrightæµè§ˆå™¨è¿æ¥æ±  - ä¼˜åŒ–å¹¶å‘æ€§èƒ½"""
    def __init__(self, pool_size=3, tab_size=5):
        self.pool_size = pool_size      # 3ä¸ªæµè§ˆå™¨å®ä¾‹
        self.tab_size = tab_size        # æ¯ä¸ª5ä¸ªtab (å…±15å¹¶å‘)
        self.available_tabs = asyncio.Queue()  # å¼‚æ­¥é˜Ÿåˆ—ç®¡ç†

    async def get_page(self):
        """è·å–å¯ç”¨tab (ä¸Šä¸‹æ–‡ç®¡ç†å™¨)"""
        tab_info = await self.available_tabs.get()
        try:
            yield tab_info['page']
        finally:
            # æ¸…ç†å¹¶å½’è¿˜tab
            await page.evaluate("() => { localStorage.clear(); }")
            await self.available_tabs.put(tab_info)
```

**è®¾è®¡äº®ç‚¹**:
- **å¼‚æ­¥é˜Ÿåˆ—**: ä½¿ç”¨ `asyncio.Queue` ç®¡ç†tabï¼Œè‡ªåŠ¨é˜»å¡ç­‰å¾…
- **è‡ªåŠ¨æ¸…ç†**: å½’è¿˜tabå‰æ¸…é™¤localStorageï¼Œé¿å…çŠ¶æ€æ±¡æŸ“
- **åæ£€æµ‹å¢å¼º**: é›†æˆ `anti_detection` æ¨¡å—ï¼Œä¿®æ”¹æµè§ˆå™¨æŒ‡çº¹

#### 11.2.2 ç«™ç‚¹é€‚é…å±‚ (extractor_<site>.py)

**URL_MAPè·¯ç”±è®¾è®¡**:

```python
URL_MAP = {
    'main_page': {
        'patterns': [r'https://www\.gnc\.com$'],
        'sample_urls': ['https://www.gnc.com'],
        'func': extract_deals_from_mainpage,
        'action': 'get_list_info'
    },
    'category_page': {
        'patterns': [r'https://www\.gnc\.com/[^/]+/$'],
        'func': extract_deals_from_category,
        'action': 'get_list_info'
    },
    'detail_page': {
        'patterns': [r'https://www\.gnc\.com/.*\.html$'],
        'func': extract_product_detail,
        'action': 'get_detail_info'
    }
}
```

**åˆ—è¡¨é¡µæå–å‡½æ•°**:

```python
async def extract_deals_from_mainpage(page: PageParam) -> dict:
    """ä»ä¸»é¡µæå–å•†å“/æ´»åŠ¨åˆ—è¡¨"""
    extractor = GncListExtractor(page)
    async with extractor.browser_pool.get_page() as pw_page:
        # ä½¿ç”¨BrightDataæ‰¹é‡è·å–HTMLï¼ˆåŠ é€Ÿï¼‰
        if 'brightdata' in extractor.engine:
            html_content = await extractor.get_html_content_by_brightdata(url)
            await pw_page.set_content(html_content)
        else:
            await pw_page.goto(url)

        # æå–URL
        urls = await pw_page.evaluate("""() => {
            return Array.from(document.querySelectorAll('a.product-link'))
                .map(a => ({url: a.href, title: a.textContent, type: 'detail'}));
        }""")

        return {'urls': urls, 'site_name': 'gnc', ...}
```

**è¯¦æƒ…é¡µæå–ç±»**:

```python
class GncDetailExtractor(BaseExtractor, ProductDetailMixin):
    """GNCå•†å“è¯¦æƒ…é¡µæå–å™¨"""

    async def fetch_html(self):
        """è·å–åŸå§‹HTML"""
        async with self.browser_pool.get_page() as page:
            await page.goto(self.url)
            return await page.content()

    def clean_html(self, html: str) -> str:
        """æ¸…æ´—HTML - ç«™ç‚¹ç‰¹å®šé€»è¾‘"""
        soup = BeautifulSoup(html, 'html.parser')
        # ç§»é™¤å¯¼èˆªæ ã€é¡µè„šç­‰æ— å…³å†…å®¹
        for tag in soup.select('.header, .footer, .ads'):
            tag.decompose()
        return str(soup.select_one('.product-detail'))

    def extract_text_as_markdown(self, cleaned_html: str) -> str:
        """è½¬æ¢ä¸ºMarkdown - è°ƒç”¨LLM"""
        # ä½¿ç”¨Gemini/Claudeæå–ç»“æ„åŒ–ä¿¡æ¯
        return llm_extract_markdown(cleaned_html)
```

#### 11.2.3 è°ƒåº¦ç¼–æ’å±‚ (extractor_scheduler.py)

**SiteScheduleræ ¸å¿ƒé€»è¾‘**:

```python
class SiteScheduler:
    """ç«™ç‚¹çˆ¬å–è°ƒåº¦å™¨ - BFSéå†URLæ ‘"""

    async def analyze(self, url, parent=None):
        """å¹¿åº¦ä¼˜å…ˆéå†"""
        queue = deque([(url, parent, 0)])  # (url, parent_id, level)
        visited = set()

        while queue:
            current_url, parent_id, level = queue.popleft()
            if level > self.max_level or current_url in visited:
                continue

            # åŠ¨æ€åŠ è½½ç«™ç‚¹æ¨¡å—
            module = importlib.import_module(f'crawler.{category}.extractor_{site_name}')

            # åŒ¹é…URL_MAPè·¯ç”±
            matched = self._match_url_pattern(current_url, module.URL_MAP)
            if not matched:
                continue

            # è°ƒç”¨å¤„ç†å‡½æ•°
            if matched['action'] == 'get_list_info':
                result = await matched['func'](PageParam(url=current_url))
                # å°†å­URLå…¥é˜Ÿ
                for item in result['urls']:
                    queue.append((item['url'], trace_page_id, level+1))

            elif matched['action'] == 'get_detail_info':
                # è¯¦æƒ…é¡µï¼šæå–å¹¶ä¿å­˜æ•°æ®
                await matched['func'](PageParam(url=current_url))

            visited.add(current_url)
```

**å¹¶å‘ä¼˜åŒ–**:

```python
async def process_level_urls_concurrent(self, urls, parent, level):
    """å¹¶å‘å¤„ç†åŒå±‚çº§URL"""
    # 1. BrightDataæ‰¹é‡è·å–HTML
    if self.enable_brightdata:
        html_list = await self.bd_client.batch_fetch(urls[:20])
        tasks = [self.process_one_url(url, html, parent, level)
                 for url, html in zip(urls, html_list)]
    else:
        tasks = [self.process_one_url(url, None, parent, level) for url in urls]

    # 2. å¹¶å‘æ‰§è¡Œï¼ˆä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°ï¼‰
    sem = asyncio.Semaphore(15)  # æœ€å¤š15ä¸ªå¹¶å‘
    async def wrapped(task):
        async with sem:
            return await task

    return await asyncio.gather(*[wrapped(t) for t in tasks])
```

### 11.3 å…³é”®è®¾è®¡æ¨¡å¼

#### 11.3.1 æ¨¡æ¿æ–¹æ³•æ¨¡å¼

```python
class BaseExtractor:
    """å®šä¹‰æå–æµç¨‹éª¨æ¶"""
    async def process(self, url):
        # 1. è·å–HTML
        html = await self.fetch_html(url)

        # 2. æ¸…æ´—HTMLï¼ˆå­ç±»é‡å†™ï¼‰
        cleaned = self.clean_html(html)

        # 3. æå–æ–‡æœ¬ï¼ˆå­ç±»é‡å†™ï¼‰
        markdown = self.extract_text_as_markdown(cleaned)

        # 4. ä¿å­˜æ•°æ®
        await self.save(markdown)

        # 5. å›è°ƒé’©å­ï¼ˆMixinæä¾›ï¼‰
        await self.post_save_callback()
```

#### 11.3.2 Mixinæ¨¡å¼

```python
class ProductDetailMixin:
    """å•†å“è¯¦æƒ…å¤„ç†èƒ½åŠ› - å¯é€‰ç»„åˆ"""
    async def post_save_callback(self):
        """ä¿å­˜åå›è°ƒ"""
        # 1. ä¿å­˜ProductOrigin
        await self._save_product_origin()

        # 2. æäº¤OCRä»»åŠ¡ï¼ˆå¦‚æœæœ‰å›¾ç‰‡ï¼‰
        if self.image_urls:
            await self._submit_ocr_tasks()

        # 3. ç«™ç‚¹ç‰¹å®šé€»è¾‘ï¼ˆå¯é€‰ï¼‰
        await self._post_process_hook()

    def _post_process_hook(self):
        """æ‰©å±•ç‚¹ - å­ç±»å¯é‡å†™"""
        pass
```

#### 11.3.3 ç­–ç•¥æ¨¡å¼

```python
# ä¸åŒå¼•æ“ç­–ç•¥
ENGINES = {
    'browser_pool': BrowserPoolEngine,
    'brightdata': BrightDataEngine,
    'brightdata+browser_pool': HybridEngine
}

class BaseExtractor:
    engine = 'brightdata+browser_pool'  # å­ç±»å¯é…ç½®

    async def fetch_html(self, url):
        engine = ENGINES[self.engine]()
        return await engine.fetch(url)
```

### 11.4 å¹¶å‘æ€§èƒ½ä¼˜åŒ–

**å¯¹æ¯”ï¼šä¼ ç»Ÿæ–¹å¼ vs ä¼˜åŒ–æ–¹å¼**

| ç»´åº¦ | ä¼ ç»Ÿæ–¹å¼ | ä¼˜åŒ–æ–¹å¼ | æå‡ |
|------|---------|---------|------|
| HTMLè·å– | é€ä¸ªæ‰“å¼€æµè§ˆå™¨ | BrightDataæ‰¹é‡çˆ¬å– | 10å€ |
| æµè§ˆå™¨å®ä¾‹ | æ¯æ¬¡æ–°å»º | BrowserPoolå¤ç”¨ | 5å€ |
| å¹¶å‘æ§åˆ¶ | æ— æ§åˆ¶ï¼Œæ˜“å´©æºƒ | Semaphore + Queue | ç¨³å®š |
| å†…å­˜å ç”¨ | éšä»»åŠ¡å¢é•¿ | å›ºå®š15ä¸ªtab | 80%â†“ |

**CONCURRENT_CONFIGé…ç½®**:

```python
CONCURRENT_CONFIG = {
    'pool_size': 3,               # 3ä¸ªæµè§ˆå™¨å®ä¾‹
    'tab_size': 5,                # æ¯ä¸ª5ä¸ªtab (å…±15å¹¶å‘)
    'delay_between_requests': 0.5, # è¯·æ±‚é—´éš”0.5ç§’
    'use_brightdata': True,       # ä½¿ç”¨BrightDataæ‰¹é‡çˆ¬å–
    'brightdata_batch_size': 20   # æ‰¹é‡å¤§å°20
}
```

### 11.5 æŠ€æœ¯éš¾ç‚¹ä¸è§£å†³æ–¹æ¡ˆ

#### éš¾ç‚¹1: åçˆ¬è™«æ£€æµ‹

**é—®é¢˜**:
- Cloudflareã€PerimeterXç­‰æ£€æµ‹Playwright
- User-Agentã€CanvasæŒ‡çº¹è¯†åˆ«

**è§£å†³æ–¹æ¡ˆ**:

```python
# devbot/html_servers/anti_detection.py
def get_browser_launch_args():
    """åæ£€æµ‹å¯åŠ¨å‚æ•°"""
    return [
        '--disable-blink-features=AutomationControlled',  # éšè—automationæ ‡å¿—
        '--disable-dev-shm-usage',
        '--no-sandbox',
        '--disable-setuid-sandbox',
        '--disable-web-security',
        '--disable-features=IsolateOrigins,site-per-process',
        '--allow-running-insecure-content',
        '--disable-webgl',  # ç¦ç”¨WebGLæŒ‡çº¹
        '--disable-canvas-fingerprinting',  # ç¦ç”¨CanvasæŒ‡çº¹
    ]

async def setup_page_anti_detection(page, user_agent=None):
    """é¡µé¢çº§åæ£€æµ‹"""
    # 1. ä¿®æ”¹navigatorå±æ€§
    await page.evaluate("""() => {
        Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
        Object.defineProperty(navigator, 'plugins', {get: () => [1, 2, 3]});
    }""")

    # 2. æ³¨å…¥çœŸå®Chromeè¿è¡Œæ—¶
    await page.add_init_script("""
        Object.defineProperty(navigator, 'languages', {
            get: () => ['en-US', 'en']
        });
    """)

    # 3. éšæœºåŒ–æŒ‡çº¹
    await page.evaluate(f"""() => {{
        const getParameter = WebGLRenderingContext.prototype.getParameter;
        WebGLRenderingContext.prototype.getParameter = function(parameter) {{
            if (parameter === 37445) return 'Intel Inc.';  # ä¼ªé€ æ˜¾å¡å‚å•†
            return getParameter.apply(this, arguments);
        }};
    }}""")
```

#### éš¾ç‚¹2: åŠ¨æ€å†…å®¹åŠ è½½

**é—®é¢˜**:
- JavaScriptæ¸²æŸ“çš„å†…å®¹
- æ‡’åŠ è½½å›¾ç‰‡
- æ— é™æ»šåŠ¨åˆ—è¡¨

**è§£å†³æ–¹æ¡ˆ**:

```python
async def fetch_rendered_html(self, url):
    """ç­‰å¾…åŠ¨æ€å†…å®¹åŠ è½½"""
    async with self.browser_pool.get_page() as page:
        await page.goto(url, wait_until='networkidle')  # ç­‰å¾…ç½‘ç»œç©ºé—²

        # ç­‰å¾…å…³é”®å…ƒç´ å‡ºç°
        await page.wait_for_selector('.product-info', timeout=10000)

        # æ»šåŠ¨åˆ°åº•éƒ¨ï¼Œè§¦å‘æ‡’åŠ è½½
        await page.evaluate("""async () => {
            await new Promise(resolve => {
                let totalHeight = 0;
                const distance = 100;
                const timer = setInterval(() => {
                    window.scrollBy(0, distance);
                    totalHeight += distance;
                    if (totalHeight >= document.body.scrollHeight) {
                        clearInterval(timer);
                        resolve();
                    }
                }, 100);
            });
        }""")

        # ç­‰å¾…å›¾ç‰‡åŠ è½½
        await page.wait_for_load_state('domcontentloaded')
        await page.wait_for_timeout(2000)

        return await page.content()
```

#### éš¾ç‚¹3: URLå»é‡ä¸å¢é‡çˆ¬å–

**é—®é¢˜**:
- é‡å¤URLå¯¼è‡´é‡å¤çˆ¬å–
- å¢é‡æ›´æ–°æ—¶éœ€è·³è¿‡å·²çˆ¬å–URL

**è§£å†³æ–¹æ¡ˆ**:

```python
class SiteScheduler:
    def __init__(self):
        self.visited_urls = set()  # å†…å­˜å»é‡
        self.db_visited = set()    # æ•°æ®åº“å·²çˆ¬URL

    async def is_url_processed(self, url):
        """æ£€æŸ¥URLæ˜¯å¦å·²å¤„ç†"""
        # 1. å†…å­˜å¿«é€ŸæŸ¥æ‰¾
        if url in self.visited_urls:
            return True

        # 2. æ•°æ®åº“æŸ¥è¯¢ï¼ˆç¼“å­˜ç»“æœï¼‰
        if url in self.db_visited:
            return True

        # 3. æŸ¥è¯¢TracePageè¡¨
        trace_page = await TracePage.objects(url=url).first()
        if trace_page:
            self.db_visited.add(url)
            return True

        return False
```

### 11.6 é›†æˆAsyncPipeline

**çˆ¬è™«ä¸Pipelineé›†æˆæµç¨‹**:

```
Crawler (extractor_scheduler.py)
    â†“ ä¿å­˜å•†å“è¯¦æƒ…
ProductDetailMixin.post_save_callback()
    â†“ HTTP POST
AsyncPipeline API (http://localhost:8000/api/v1/tasks/ocr)
    â†“ RabbitMQ
OCR Worker â†’ LLM Worker â†’ DB Worker
    â†“
ProductBaseModel (MongoDB)
```

**ä»£ç ç¤ºä¾‹**:

```python
# crawler/product/util.py
class ProductDetailMixin:
    async def _submit_to_async_pipeline(self):
        """æäº¤åˆ°å¼‚æ­¥ç®¡é“"""
        import httpx

        async with httpx.AsyncClient() as client:
            response = await client.post(
                "http://localhost:8000/api/v1/tasks/ocr",
                json={
                    "product_origin_id": str(self.product_origin_id),
                    "trace_page_id": str(self.trace_page_id),
                    "image_urls": self.image_urls,
                    "screenshot_url": self.screenshot_url,
                    "prompt": "Extract product information from images",
                    "run_version": "v1.0",
                    "site_name": self.site_name,
                    "source_url": self.url
                },
                timeout=30.0
            )

            task_id = response.json()["task_id"]
            logger.info(f"âœ… Submitted to AsyncPipeline: {task_id}")
```

---

## 12. AsyncPipeline å¼‚æ­¥ä»»åŠ¡å¤„ç†ç®¡é“

### 12.1 é¡¹ç›®æ¦‚è¿°

**AsyncPipeline** æ˜¯ä¸€ä¸ªåŸºäº RabbitMQ çš„é«˜æ€§èƒ½å¼‚æ­¥å¤„ç†ç³»ç»Ÿï¼Œæä¾›å®Œæ•´çš„ **OCR â†’ LLM â†’ DB** æµæ°´çº¿ã€‚

**æ ¸å¿ƒç‰¹æ€§**:
1. **å¼‚æ­¥è§£è€¦**: çˆ¬è™«ä¸æ•°æ®å¤„ç†å®Œå…¨åˆ†ç¦»ï¼Œäº’ä¸é˜»å¡
2. **æ¶ˆæ¯é˜Ÿåˆ—**: RabbitMQä¿è¯ä»»åŠ¡å¯é ä¼ é€’
3. **æ‰¹é‡å¤„ç†**: DB Workeræ‰¹é‡æ’å…¥ï¼Œæå‡10å€æ€§èƒ½
4. **èµ„æºå¤ç”¨**: Gemini Resource Managerç®¡ç†30+å¹¶å‘APIè°ƒç”¨

### 12.2 æ¶æ„è®¾è®¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   FastAPI Server                     â”‚
â”‚  POST /api/v1/tasks/ocr  â† Crawleræäº¤ä»»åŠ¡           â”‚
â”‚  POST /api/v1/tasks/llm                              â”‚
â”‚  GET  /api/v1/health                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚ publish
                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   RabbitMQ                           â”‚
â”‚  ocr_queue (priority=5) â”€â”€â”€â”€â”€â†’ OCR Worker Pool (1)  â”‚
â”‚  llm_queue (priority=7) â”€â”€â”€â”€â”€â†’ LLM Worker Pool (3)  â”‚
â”‚  db_queue  (priority=3) â”€â”€â”€â”€â”€â†’ DB Worker Pool (2)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â†“                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   MongoDB        â”‚  â”‚   vLLM OCR API   â”‚
â”‚  TracePage       â”‚  â”‚   Gemini API     â”‚
â”‚  ProductOrigin   â”‚  â”‚   GCS Storage    â”‚
â”‚  ProductBase     â”‚  â”‚                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 12.3 æ•°æ®æµè¯¦è§£

```
1. OCRé˜¶æ®µ
   Crawler â†’ API â†’ RabbitMQ â†’ OCR Worker
   â”œâ”€ è°ƒç”¨ vLLM OCR API
   â”œâ”€ è½®è¯¢ç­‰å¾…ç»“æœ (æœ€å¤š60ç§’)
   â”œâ”€ æ›´æ–° ProductOrigin.ocr_info
   â””â”€ æ‹¼æ¥ OCRæ–‡æœ¬åˆ° TracePage.markdown_txt

2. LLMé˜¶æ®µ
   OCR Worker â†’ RabbitMQ â†’ LLM Worker
   â”œâ”€ è¯»å– TracePage.markdown_txt (å«OCRæ–‡æœ¬)
   â”œâ”€ è°ƒç”¨ Gemini API (markdown â†’ JSON)
   â”œâ”€ JSONéªŒè¯å’Œæ¸…ç†
   â””â”€ æ›´æ–° TracePage.status = 'pending_db'

3. DBé˜¶æ®µ
   LLM Worker â†’ RabbitMQ â†’ DB Worker
   â”œâ”€ æ‰¹é‡æ¥æ”¶ä»»åŠ¡ (batch_size=50, timeout=5s)
   â”œâ”€ æ‰¹é‡æ’å…¥ MongoDB (ProductBaseModel)
   â””â”€ æ›´æ–° TracePage.status = 'completed'
```

### 12.4 æ ¸å¿ƒç»„ä»¶

#### 12.4.1 OCR Worker

**èŒè´£**: è°ƒç”¨vLLM OCR APIè¿›è¡Œå›¾ç‰‡è¯†åˆ«

```python
# workers/ocr_worker.py
class OCRWorker(BaseWorker):
    """OCR Worker - å¤„ç†å›¾ç‰‡è¯†åˆ«ä»»åŠ¡"""

    async def process_task(self, task: OCRTaskMessage):
        """å¤„ç†OCRä»»åŠ¡"""
        # 1. è°ƒç”¨vLLM OCR API (å¼‚æ­¥æäº¤)
        response_ids = []
        for image_url in task.image_urls:
            resp = await self._submit_ocr_task(image_url, task.prompt)
            response_ids.append(resp['id'])

        # 2. è½®è¯¢ç­‰å¾…ç»“æœ (æœ€å¤š60ç§’)
        ocr_results = {}
        for rid in response_ids:
            result = await self._poll_ocr_result(rid, timeout=60)
            if result['status'] == 'completed':
                ocr_results[result['image_url']] = result['text']

        # 3. æ›´æ–°ProductOrigin
        await ProductOrigin.objects(id=task.product_origin_id).update(
            set__ocr_info=ocr_results
        )

        # 4. æ‹¼æ¥OCRæ–‡æœ¬åˆ°TracePage.markdown_txt
        ocr_text = "\n\n".join([
            f"## OCR - {url}\n{text}"
            for url, text in ocr_results.items()
        ])
        await TracePage.objects(id=task.trace_page_id).update(
            push__markdown_txt=ocr_text
        )

        # 5. å‘é€LLMä»»åŠ¡
        await self.broker.publish(
            'llm_queue',
            LLMTaskMessage(
                trace_page_id=task.trace_page_id,
                site_name=task.site_name,
                ...
            )
        )

    async def _poll_ocr_result(self, response_id, timeout=60):
        """è½®è¯¢OCRç»“æœ"""
        start_time = time.time()
        while time.time() - start_time < timeout:
            resp = await self.ocr_client.get(f'/v1/responses/{response_id}')
            if resp['status'] in ['completed', 'failed']:
                return resp
            await asyncio.sleep(2)  # æ¯2ç§’æŸ¥è¯¢ä¸€æ¬¡
        raise TimeoutError(f"OCR timeout: {response_id}")
```

#### 12.4.2 LLM Worker

**èŒè´£**: ä½¿ç”¨Geminiå°†Markdownè½¬æ¢ä¸ºç»“æ„åŒ–JSON

```python
# workers/llm_worker.py
class LLMWorker(BaseWorker):
    """LLM Worker - Markdownè½¬JSON"""

    def __init__(self, broker, config):
        super().__init__(broker, config)
        # Gemini Resource Manager (ç®¡ç†30+å¹¶å‘APIè°ƒç”¨)
        self.resource_manager = ResourceManager(
            api_keys=[key1, key2, key3],  # å¤šä¸ªAPI Keyè½®æ¢
            max_concurrent=30
        )

    async def process_task(self, task: LLMTaskMessage):
        """å¤„ç†LLMä»»åŠ¡"""
        # 1. è¯»å–TracePage (å«OCRæ–‡æœ¬)
        trace_page = await TracePage.objects(id=task.trace_page_id).first()
        markdown_content = trace_page.markdown_txt

        # 2. é€‰æ‹©è½¬æ¢å™¨
        if task.category == 'products':
            converter = ProductMarkdownToJsonConverter(self.resource_manager)
        else:
            converter = DealMarkdownToJsonConverter(self.resource_manager)

        # 3. è°ƒç”¨Gemini API
        try:
            json_data = await converter.convert(
                markdown_content=markdown_content,
                site_name=task.site_name,
                source_url=task.source_url
            )
        except Exception as e:
            # æ›´æ–°å¤±è´¥çŠ¶æ€
            await TracePage.objects(id=task.trace_page_id).update(
                set__status='llm_failed',
                set__error=str(e)
            )
            return

        # 4. JSONéªŒè¯å’Œæ¸…ç†
        cleaned_json = self._validate_and_clean(json_data)

        # 5. æ›´æ–°TracePage
        await TracePage.objects(id=task.trace_page_id).update(
            set__status='pending_db',
            set__json_data=cleaned_json
        )

        # 6. å‘é€DBä»»åŠ¡
        await self.broker.publish(
            'db_queue',
            DBTaskMessage(
                trace_page_id=task.trace_page_id,
                json_data=cleaned_json,
                category=task.category
            )
        )
```

**Gemini Resource Manager**:

```python
# src/llm/resource_manager.py
class ResourceManager:
    """ç®¡ç†å¤šä¸ªGemini API Keyï¼Œå®ç°çœŸå¹¶å‘"""

    def __init__(self, api_keys: List[str], max_concurrent=30):
        self.api_keys = api_keys
        self.max_concurrent = max_concurrent
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.current_key_index = 0

    def get_next_key(self):
        """è½®æ¢API Key"""
        key = self.api_keys[self.current_key_index]
        self.current_key_index = (self.current_key_index + 1) % len(self.api_keys)
        return key

    async def call_gemini(self, prompt: str):
        """å¹¶å‘è°ƒç”¨Gemini (æœ€å¤š30ä¸ªå¹¶å‘)"""
        async with self.semaphore:
            api_key = self.get_next_key()
            return await self._call_api(prompt, api_key)

    async def _call_api(self, prompt, api_key):
        """å®é™…APIè°ƒç”¨ (æ”¯æŒé‡è¯•)"""
        for attempt in range(3):
            try:
                async with aiohttp.ClientSession() as session:
                    async with session.post(
                        f'https://generativelanguage.googleapis.com/v1/models/gemini-1.5-flash:generateContent?key={api_key}',
                        json={"contents": [{"parts": [{"text": prompt}]}]},
                        timeout=30
                    ) as resp:
                        result = await resp.json()
                        return result['candidates'][0]['content']['parts'][0]['text']
            except Exception as e:
                if attempt == 2:
                    raise
                await asyncio.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
```

#### 12.4.3 DB Worker

**èŒè´£**: æ‰¹é‡æ’å…¥MongoDBï¼Œæå‡æ€§èƒ½

```python
# workers/db_worker.py
class DBWorker(BaseWorker):
    """DB Worker - æ‰¹é‡æ’å…¥æ•°æ®åº“"""

    def __init__(self, broker, config, batch_size=50, batch_timeout=5):
        super().__init__(broker, config)
        self.batch_size = batch_size
        self.batch_timeout = batch_timeout
        self.batch = []
        self.batch_timer = None

    async def process_batch(self):
        """æ‰¹é‡å¤„ç†ä»»åŠ¡"""
        if not self.batch:
            return

        logger.info(f"ğŸ“¦ Processing batch of {len(self.batch)} tasks")

        # 1. æ‰¹é‡æ’å…¥ProductBaseModel
        products = []
        trace_page_ids = []

        for task in self.batch:
            products.append(ProductBaseModel(**task.json_data))
            trace_page_ids.append(task.trace_page_id)

        # 2. æ‰¹é‡insert (10å€å¿«äºé€ä¸ªæ’å…¥)
        try:
            ProductBaseModel.objects.insert(products, load_bulk=False)
            logger.info(f"âœ… Inserted {len(products)} products")
        except Exception as e:
            logger.error(f"âŒ Batch insert failed: {e}")
            return

        # 3. æ‰¹é‡æ›´æ–°TracePageçŠ¶æ€
        await TracePage.objects(id__in=trace_page_ids).update(
            set__status='completed',
            set__completed_at=datetime.now()
        )

        # æ¸…ç©ºbatch
        self.batch = []

    async def consume_loop(self):
        """æ¶ˆè´¹å¾ªç¯ - æ‰¹é‡æ¥æ”¶"""
        async for message in self.broker.consume('db_queue'):
            task = DBTaskMessage(**message)
            self.batch.append(task)

            # è¾¾åˆ°batch_sizeæˆ–è¶…æ—¶ï¼Œç«‹å³å¤„ç†
            if len(self.batch) >= self.batch_size:
                await self.process_batch()
                self.batch_timer = None
            elif self.batch_timer is None:
                # å¯åŠ¨è¶…æ—¶å®šæ—¶å™¨
                self.batch_timer = asyncio.create_task(self._timeout_handler())

    async def _timeout_handler(self):
        """è¶…æ—¶å¤„ç†"""
        await asyncio.sleep(self.batch_timeout)
        await self.process_batch()
        self.batch_timer = None
```

### 12.5 æ¶ˆæ¯é˜Ÿåˆ—è®¾è®¡

**RabbitMQé˜Ÿåˆ—é…ç½®**:

```python
# message_broker.py
QUEUES = {
    'ocr_queue': {
        'name': 'async_pipeline.ocr',
        'priority': 5,
        'durable': True,  # æŒä¹…åŒ–
        'ttl': 3600000    # 1å°æ—¶TTL
    },
    'llm_queue': {
        'name': 'async_pipeline.llm',
        'priority': 7,    # LLMä¼˜å…ˆçº§æœ€é«˜
        'durable': True,
        'ttl': 1800000    # 30åˆ†é’ŸTTL
    },
    'db_queue': {
        'name': 'async_pipeline.db',
        'priority': 3,
        'durable': True,
        'ttl': 600000     # 10åˆ†é’ŸTTL
    }
}
```

**æ¶ˆæ¯æ¨¡å‹**:

```python
# task_models.py
from pydantic import BaseModel

class OCRTaskMessage(BaseModel):
    """OCRä»»åŠ¡æ¶ˆæ¯"""
    product_origin_id: str
    trace_page_id: str
    image_urls: List[str]
    screenshot_url: Optional[str]
    prompt: str
    run_version: str
    site_name: str
    source_url: str

class LLMTaskMessage(BaseModel):
    """LLMä»»åŠ¡æ¶ˆæ¯"""
    trace_page_id: str
    site_name: str
    source_url: str
    category: str  # 'products' or 'deals'
    run_version: str

class DBTaskMessage(BaseModel):
    """DBä»»åŠ¡æ¶ˆæ¯"""
    trace_page_id: str
    json_data: Dict[str, Any]
    category: str
```

### 12.6 æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

| ä¼˜åŒ–é¡¹ | ä¼˜åŒ–å‰ | ä¼˜åŒ–å | æå‡ |
|-------|--------|--------|------|
| DBæ’å…¥ | é€ä¸ªinsert | æ‰¹é‡insert (50æ¡) | 10å€ |
| Geminiå¹¶å‘ | ä¸²è¡Œè°ƒç”¨ | Resource Manager (30å¹¶å‘) | 30å€ |
| OCRè·å– | åŒæ­¥ç­‰å¾… | å¼‚æ­¥è½®è¯¢ + è¶…æ—¶æ§åˆ¶ | ä¸é˜»å¡ |
| æ¶ˆæ¯ä¼ é€’ | ç›´æ¥è°ƒç”¨ | RabbitMQè§£è€¦ | é«˜å¯é  |

### 12.7 ç›‘æ§ä¸è¿ç»´

**å¥åº·æ£€æŸ¥API**:

```python
# api/routers/health.py
@router.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    # 1. æ£€æŸ¥RabbitMQè¿æ¥
    broker_status = await check_broker_connection()

    # 2. æ£€æŸ¥MongoDBè¿æ¥
    db_status = await check_db_connection()

    # 3. è·å–é˜Ÿåˆ—çŠ¶æ€
    queues = await get_queue_stats()

    # 4. è·å–WorkerçŠ¶æ€
    workers = {
        "ocr": {"active": 1, "status": "healthy"},
        "llm": {"active": 3, "status": "healthy"},
        "db": {"active": 2, "status": "healthy"}
    }

    return {
        "status": "healthy",
        "broker": broker_status,
        "database": db_status,
        "queues": queues,
        "workers": workers
    }
```

**æ—¥å¿—ç¤ºä¾‹**:

```
2025-01-15 14:32:10 - OCRWorker - INFO - ğŸ“¸ Processing OCR task: product_123
2025-01-15 14:32:15 - OCRWorker - INFO - âœ… OCR completed: 3 images, 2.5s
2025-01-15 14:32:16 - LLMWorker - INFO - ğŸ¤– Converting markdown to JSON: trace_456
2025-01-15 14:32:20 - LLMWorker - INFO - âœ… JSON generated: 1234 chars
2025-01-15 14:32:25 - DBWorker - INFO - ğŸ“¦ Processing batch of 50 tasks
2025-01-15 14:32:27 - DBWorker - INFO - âœ… Inserted 50 products (0.8s)
```

---

## 13. OCR_Rec å¼‚æ­¥OCRè¯†åˆ«ç³»ç»Ÿ

### 13.1 é¡¹ç›®æ¦‚è¿°

**OCR_Rec** æ˜¯åŸºäºvLLMå¼‚æ­¥APIçš„OCRè¯†åˆ«ç³»ç»Ÿï¼Œä¸“é—¨ç”¨äºæ‰¹é‡å¤„ç†ç”µå•†ç½‘é¡µæˆªå›¾çš„æ–‡å­—è¯†åˆ«ã€‚

**æ ¸å¿ƒç‰¹æ€§**:
1. **å¼‚æ­¥è§£è€¦**: ä»»åŠ¡æäº¤å’Œç»“æœè·å–åˆ†ç¦»
2. **å›¾ç‰‡ä¼˜åŒ–**: è‡ªåŠ¨å‹ç¼©å¹¶ä¸Šä¼ GCS
3. **çŠ¶æ€è¿½è¸ª**: queued/in_progress/completed/failed
4. **å®šæ—¶è°ƒåº¦**: Cronå®šæ—¶æ‰§è¡Œï¼ˆæäº¤5åˆ†é’Ÿ/è·å–2åˆ†é’Ÿï¼‰

### 13.2 æ¶æ„è®¾è®¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  è„šæœ¬1: submit_tasks.py (æ¯5åˆ†é’Ÿè¿è¡Œ)               â”‚
â”‚  1. ä»product_originæŸ¥è¯¢æœªå¤„ç†å›¾ç‰‡                   â”‚
â”‚  2. ä¸‹è½½ â†’ å‹ç¼©(WebP 94%) â†’ ä¸Šä¼ GCS                 â”‚
â”‚  3. POST /v1/responses (æäº¤OCRä»»åŠ¡)                â”‚
â”‚  4. ä¿å­˜taskåˆ°product_ocr_completed                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚ response_idå­˜å…¥æ•°æ®åº“
                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  è„šæœ¬2: fetch_results.py (æ¯2åˆ†é’Ÿè¿è¡Œ)              â”‚
â”‚  1. ä»product_ocr_completedæŸ¥è¯¢å¾…è·å–ç»“æœ             â”‚
â”‚  2. GET /v1/responses/{response_id}                â”‚
â”‚  3. æ›´æ–°product_ocr_completed.ocr_text             â”‚
â”‚  4. åŒæ­¥æ›´æ–°product_origin.array_is_completed       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 13.3 æ•°æ®åº“Schema

**product_originè¡¨** (è¾“å…¥æ•°æ®):

```javascript
{
  "_id": ObjectId("..."),
  "image_urls": [
    "https://example.com/img1.jpg",
    "https://example.com/img2.jpg"
  ],
  "array_is_completed": [  // å·²å®Œæˆçš„å›¾ç‰‡URL
    "https://example.com/img1.jpg"
  ],
  "iscompleted": false  // æ˜¯å¦å…¨éƒ¨å®Œæˆ
}
```

**product_ocr_completedè¡¨** (è¾“å‡ºæ•°æ®):

```javascript
{
  "_id": ObjectId("..."),
  "webpage_id": ObjectId("..."),      // å…³è”product_origin
  "image_id": "https://...",          // å›¾ç‰‡URL
  "response_id": "resp_abc123",       // vLLMä»»åŠ¡ID
  "status": "completed",              // ä»»åŠ¡çŠ¶æ€
  "ocr_text": "# OCRç»“æœ\n...",       // è¯†åˆ«æ–‡æœ¬
  "created_at": ISODate("..."),       // åˆ›å»ºæ—¶é—´
  "processed_at": ISODate("..."),     // å®Œæˆæ—¶é—´
  "error": {"message": "..."}         // é”™è¯¯ä¿¡æ¯(å¯é€‰)
}
```

### 13.4 æ ¸å¿ƒæµç¨‹

#### 13.4.1 submit_tasks.py

```python
#!/usr/bin/env python3
"""æäº¤OCRä»»åŠ¡è„šæœ¬"""
import asyncio
from OCRSubmitService import OCRSubmitService

async def main():
    service = OCRSubmitService(config_path='config.yaml')
    await service.initialize()

    # 1. æŸ¥è¯¢æœªå¤„ç†çš„product_origin
    unprocessed = ProductOrigin.objects(
        Q(iscompleted=False) | Q(iscompleted__exists=False)
    ).limit(20)

    # 2. æ‰¹é‡å¤„ç†
    for origin in unprocessed:
        # è·å–æœªå®Œæˆçš„å›¾ç‰‡
        unprocessed_images = service.compute_unprocessed_images(origin)

        # æäº¤OCRä»»åŠ¡
        results = await service.submit_for_origin(origin, prompt="Extract text")

        print(f"âœ… Submitted {len(results)} OCR tasks for {origin.id}")

asyncio.run(main())
```

**OCRSubmitServiceæ ¸å¿ƒæ–¹æ³•**:

```python
# src/utils/ocr_submit_service.py
class OCRSubmitService:
    """OCRæäº¤æœåŠ¡"""

    async def submit_for_origin(self, origin: ProductOrigin, prompt: str):
        """ä¸ºproduct_originæäº¤OCRä»»åŠ¡"""
        unprocessed_images = self.compute_unprocessed_images(origin)

        async with aiohttp.ClientSession() as session:
            async def process_one(image_url: str):
                # 1. ä¸‹è½½å¹¶å‹ç¼©å›¾ç‰‡
                gcs_url = await self.download_and_optimize_image(image_url)

                # 2. æäº¤OCRä»»åŠ¡
                result = await self.submit_ocr_task(
                    session,
                    gcs_url,
                    system_message="You are an OCR assistant",
                    user_message=prompt
                )

                # 3. ä¿å­˜åˆ°product_ocr_completed
                ProductOCRCompleted(
                    webpage_id=origin.id,
                    image_id=image_url,
                    response_id=result['response_id'],
                    status=result['status'],  # 'queued'
                    created_at=datetime.now()
                ).save()

                return result

            # å¹¶å‘å¤„ç†ï¼ˆ20ä¸ªå¹¶å‘ï¼‰
            tasks = [process_one(url) for url in unprocessed_images]
            return await asyncio.gather(*tasks)

    async def download_and_optimize_image(self, image_url: str) -> str:
        """ä¸‹è½½å¹¶å‹ç¼©å›¾ç‰‡ï¼Œä¸Šä¼ åˆ°GCS"""
        # 1. ä¸‹è½½å›¾ç‰‡
        response = requests.get(image_url, timeout=30)
        img = Image.open(io.BytesIO(response.content))

        # 2. å‹ç¼©ä¸ºWebP (è´¨é‡94%)
        output_buffer = io.BytesIO()
        img.save(output_buffer, format='WEBP', quality=94, method=4)
        compressed_data = output_buffer.getvalue()

        # 3. ä¸Šä¼ åˆ°GCS
        blob_name = f"ocr_images/{hashlib.md5(image_url.encode()).hexdigest()}.webp"
        blob = self.gcs_bucket.blob(blob_name)
        blob.upload_from_string(compressed_data, content_type='image/webp')

        return blob.public_url
```

#### 13.4.2 fetch_results.py

```python
#!/usr/bin/env python3
"""è·å–OCRç»“æœè„šæœ¬"""
import asyncio
from bson import ObjectId

async def main():
    # 1. æŸ¥è¯¢å¾…è·å–ç»“æœçš„ä»»åŠ¡
    pending_tasks = ProductOCRCompleted.objects(
        response_id__exists=True
    ).limit(50)

    # 2. æ‰¹é‡è·å–ç»“æœ
    async with aiohttp.ClientSession() as session:
        async def fetch_one(task):
            resp = await session.get(
                f'http://58.224.7.136:41294/v1/responses/{task.response_id}'
            )
            result = await resp.json()

            # 3. å¤„ç†ä¸åŒçŠ¶æ€
            if result['status'] == 'completed':
                # æå–OCRæ–‡æœ¬
                ocr_text = result['output']

                # æ›´æ–°product_ocr_completed
                task.update(
                    set__ocr_text=ocr_text,
                    set__status='completed',
                    set__processed_at=datetime.now()
                )

                # æ›´æ–°product_origin.array_is_completed
                ProductOrigin.objects(id=task.webpage_id).update(
                    add_to_set__array_is_completed=task.image_id
                )

                # æ£€æŸ¥æ˜¯å¦å…¨éƒ¨å®Œæˆ
                origin = ProductOrigin.objects(id=task.webpage_id).first()
                if len(origin.array_is_completed) == len(origin.image_urls):
                    origin.update(set__iscompleted=True)

                print(f"âœ… OCR completed: {task.image_id}")

            elif result['status'] == 'failed':
                task.update(
                    set__status='failed',
                    set__error=result.get('error')
                )

        # å¹¶å‘è·å–ï¼ˆ50ä¸ªå¹¶å‘ï¼‰
        tasks = [fetch_one(task) for task in pending_tasks]
        await asyncio.gather(*tasks)

asyncio.run(main())
```

### 13.5 Cronå®šæ—¶ä»»åŠ¡é…ç½®

```bash
# ç¼–è¾‘crontab
crontab -e

# æ·»åŠ å®šæ—¶ä»»åŠ¡
# æ¯5åˆ†é’Ÿæäº¤æ–°ä»»åŠ¡
*/5 * * * * cd /path/to/ocr_rec && python3 submit_tasks.py >> /tmp/submit_tasks.log 2>&1

# æ¯2åˆ†é’Ÿæ£€æŸ¥ç»“æœ
*/2 * * * * cd /path/to/ocr_rec && python3 fetch_results.py >> /tmp/fetch_results.log 2>&1
```

### 13.6 ä¸åŸé¡¹ç›®å¯¹æ¯”

| ç‰¹æ€§ | åŸé¡¹ç›® (åŒæ­¥) | OCR_Rec (å¼‚æ­¥) |
|-----|-------------|---------------|
| APIæ–¹å¼ | `/v1/chat/completions` (åŒæ­¥) | `/v1/responses` (å¼‚æ­¥) |
| ä»»åŠ¡æ¨¡å‹ | ç«‹å³è¿”å›ç»“æœ | æäº¤å’Œè·å–åˆ†ç¦» |
| å¹¶å‘æ–¹å¼ | ThreadPoolExecutor | asyncio + aiohttp |
| æ‰¹å¤„ç† | 10ä¸ª/æ‰¹æ¬¡ | 20ä¸ªæäº¤ + 50ä¸ªè·å– |
| çŠ¶æ€ç®¡ç† | ä»…æˆåŠŸ/å¤±è´¥ | queued/in_progress/completed/failed |
| é€‚ç”¨åœºæ™¯ | å°æ‰¹é‡ã€ä½å»¶è¿Ÿ | å¤§æ‰¹é‡ã€é«˜åå |

---

## 14. Qwen3-VL-8B-Instruct-FP8 å¾®è°ƒæ–¹æ³•

### 14.1 å¾®è°ƒç›®æ ‡

**é—®é¢˜**: é€šç”¨çš„Qwen3-VLæ¨¡å‹åœ¨ç”µå•†OCRåœºæ™¯ä¸‹è¡¨ç°ä¸ä½³ï¼š
- æ— æ³•è¯†åˆ«å•†å“ä»·æ ¼ï¼ˆç¾å…ƒç¬¦å·ã€æŠ˜æ‰£ï¼‰
- å¿½ç•¥å“ç‰ŒLogoå’Œå•†æ ‡
- å¯¹ä¿ƒé”€æ–‡æ¡ˆï¼ˆSALEã€DISCOUNTï¼‰æ•æ„Ÿåº¦ä½
- è¡¨æ ¼æ•°æ®ï¼ˆè¥å…»æˆåˆ†ã€è§„æ ¼å‚æ•°ï¼‰æå–ä¸å‡†ç¡®

**ç›®æ ‡**: å¾®è°ƒæ¨¡å‹ï¼Œä½¿å…¶ä¸“é—¨é€‚åº”ç”µå•†ç½‘ç«™OCRä¿¡æ¯è¯†åˆ«ã€‚

### 14.2 æ•°æ®å‡†å¤‡

#### 14.2.1 æ•°æ®æ”¶é›†

**æ¥æº**:
1. **å·²çˆ¬å–æ•°æ®**: ä»`product_origin`è¡¨æå–image_urls + äººå·¥æ ‡æ³¨
2. **å…¬å¼€æ•°æ®é›†**: RPC (Retail Product Checkout) Dataset
3. **åˆæˆæ•°æ®**: ä½¿ç”¨æ–‡æœ¬æ¸²æŸ“å·¥å…·ç”Ÿæˆä»·æ ¼æ ‡ç­¾

**æ•°æ®æ ¼å¼**:

```json
{
  "image": "https://gcs.example.com/product_images/12345.webp",
  "conversations": [
    {
      "from": "human",
      "value": "<image>\nExtract all text from this product image, including prices, brand names, and promotional text."
    },
    {
      "from": "gpt",
      "value": "## Product Information\n\n**Brand**: GNC\n**Product Name**: Whey Protein Powder\n**Price**: $49.99 (Original: $69.99)\n**Discount**: 29% OFF\n**Size**: 5 lbs\n\n## Promotional Text\nLimited Time Offer\nFree Shipping on Orders Over $50\n\n## Nutritional Facts\n- Protein: 24g per serving\n- Calories: 130\n..."
    }
  ]
}
```

**æ•°æ®å¢å¼º**:

```python
import albumentations as A

transform = A.Compose([
    A.RandomBrightnessContrast(p=0.5),  # äº®åº¦/å¯¹æ¯”åº¦
    A.GaussNoise(p=0.3),                # é«˜æ–¯å™ªå£°
    A.Rotate(limit=15, p=0.4),          # æ—‹è½¬
    A.Perspective(scale=(0.05, 0.1), p=0.3),  # é€è§†å˜æ¢
])
```

#### 14.2.2 æ ‡æ³¨å·¥å…·

**LabelStudioé…ç½®**:

```xml
<View>
  <Image name="image" value="$image"/>
  <TextArea name="ocr_result" toName="image"
            rows="10" editable="true"
            placeholder="Enter OCR result in Markdown format"/>

  <Choices name="quality" toName="image" choice="single">
    <Choice value="excellent"/>
    <Choice value="good"/>
    <Choice value="poor"/>
  </Choices>
</View>
```

### 14.3 å¾®è°ƒæ–¹æ³•

#### 14.3.1 LoRAå¾®è°ƒ

**ä¸ºä»€ä¹ˆé€‰æ‹©LoRA?**
- å‚æ•°æ•ˆç‡é«˜ï¼šä»…è®­ç»ƒ0.1%å‚æ•°
- è®­ç»ƒå¿«ï¼š8Bæ¨¡å‹åœ¨å•å¡A100ä¸Š6å°æ—¶å®Œæˆ
- æ˜“éƒ¨ç½²ï¼švLLMåŸç”Ÿæ”¯æŒLoRAé€‚é…å™¨

**LoRAé…ç½®**:

```python
from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(
    r=16,                      # LoRAç§©
    lora_alpha=32,             # ç¼©æ”¾å› å­
    target_modules=[           # ç›®æ ‡æ¨¡å—
        "q_proj", "k_proj", "v_proj", "o_proj",  # Attention
        "gate_proj", "up_proj", "down_proj"      # FFN
    ],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(base_model, lora_config)
```

#### 14.3.2 è®­ç»ƒè„šæœ¬

```python
# finetune_qwen3vl.py
from transformers import (
    Trainer,
    TrainingArguments,
    AutoTokenizer,
    AutoModelForCausalLM
)
from datasets import load_dataset

# 1. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
model_name = "Qwen/Qwen3-VL-8B-Instruct-FP8"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    load_in_8bit=True,  # 8bité‡åŒ–èŠ‚çœæ˜¾å­˜
    device_map="auto"
)

# 2. åº”ç”¨LoRA
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
# Output: trainable params: 8,388,608 || all params: 8,000,000,000 || trainable%: 0.10

# 3. åŠ è½½æ•°æ®é›†
dataset = load_dataset('json', data_files={
    'train': 'ecommerce_ocr_train.json',
    'val': 'ecommerce_ocr_val.json'
})

# 4. æ•°æ®é¢„å¤„ç†
def preprocess_function(examples):
    """è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼"""
    inputs = []
    targets = []

    for conv in examples['conversations']:
        # æ„é€ è¾“å…¥: <image>æ ‡è®° + prompt
        human_text = conv[0]['value']
        gpt_text = conv[1]['value']

        inputs.append(human_text)
        targets.append(gpt_text)

    model_inputs = tokenizer(
        inputs,
        max_length=1024,
        truncation=True,
        padding='max_length'
    )

    labels = tokenizer(
        targets,
        max_length=2048,
        truncation=True,
        padding='max_length'
    )

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized_dataset = dataset.map(
    preprocess_function,
    batched=True,
    remove_columns=dataset["train"].column_names
)

# 5. è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir="./qwen3vl_ecommerce_lora",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    gradient_accumulation_steps=8,  # æœ‰æ•ˆbatch_size=32
    learning_rate=2e-4,
    weight_decay=0.01,
    warmup_steps=500,
    logging_steps=50,
    evaluation_strategy="steps",
    eval_steps=500,
    save_steps=500,
    save_total_limit=3,
    fp16=True,                     # æ··åˆç²¾åº¦è®­ç»ƒ
    dataloader_num_workers=4,
    remove_unused_columns=False,
    report_to="tensorboard"
)

# 6. å¯åŠ¨è®­ç»ƒ
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["val"],
    tokenizer=tokenizer
)

trainer.train()

# 7. ä¿å­˜LoRAæƒé‡
model.save_pretrained("./qwen3vl_ecommerce_lora_final")
```

### 14.4 æ¨¡å‹éƒ¨ç½²

#### 14.4.1 vLLMéƒ¨ç½²LoRAæ¨¡å‹

```bash
# å¯åŠ¨vLLMæœåŠ¡ï¼ˆåŠ è½½LoRAé€‚é…å™¨ï¼‰
python -m vllm.entrypoints.openai.api_server \
  --model Qwen/Qwen3-VL-8B-Instruct-FP8 \
  --lora-modules ecommerce=./qwen3vl_ecommerce_lora_final \
  --host 0.0.0.0 \
  --port 41294 \
  --tensor-parallel-size 1 \
  --gpu-memory-utilization 0.9 \
  --max-model-len 4096
```

**APIè°ƒç”¨**:

```python
import aiohttp

async def call_finetuned_ocr(image_url: str):
    async with aiohttp.ClientSession() as session:
        payload = {
            "model": "Qwen/Qwen3-VL-8B-Instruct-FP8",
            "lora_request": {  # æŒ‡å®šLoRAé€‚é…å™¨
                "lora_name": "ecommerce",
                "lora_int_id": 1
            },
            "messages": [
                {
                    "role": "system",
                    "content": "You are an expert at extracting text from e-commerce product images, including prices, brand names, and promotional text."
                },
                {
                    "role": "user",
                    "content": [
                        {"type": "image_url", "image_url": image_url},
                        {"type": "text", "text": "Extract all text from this product image in Markdown format."}
                    ]
                }
            ],
            "max_tokens": 4000,
            "temperature": 0.1
        }

        async with session.post(
            "http://58.224.7.136:41294/v1/chat/completions",
            json=payload
        ) as resp:
            result = await resp.json()
            return result['choices'][0]['message']['content']
```

### 14.5 è¯„ä¼°ä¸å¯¹æ¯”

**è¯„ä¼°æŒ‡æ ‡**:

| æŒ‡æ ‡ | è®¡ç®—æ–¹å¼ | ç›®æ ‡ |
|-----|---------|------|
| CER (Character Error Rate) | Levenshteinè·ç¦» / æ€»å­—ç¬¦æ•° | < 5% |
| Price Accuracy | ä»·æ ¼æå–å‡†ç¡®ç‡ | > 95% |
| Brand Recall | å“ç‰Œåæå–å¬å›ç‡ | > 90% |
| F1 Score | ç²¾ç¡®ç‡ä¸å¬å›ç‡çš„è°ƒå’Œå¹³å‡ | > 0.9 |

**å¯¹æ¯”ç»“æœ** (æµ‹è¯•é›†: 1000å¼ ç”µå•†å›¾ç‰‡):

| æ¨¡å‹ | CER | Price Acc | Brand Recall | F1 |
|------|-----|-----------|--------------|-----|
| åŸå§‹Qwen3-VL | 12.3% | 78% | 72% | 0.75 |
| å¾®è°ƒå | 3.8% | 96% | 93% | 0.94 |
| æå‡ | **â†‘69%** | **â†‘23%** | **â†‘29%** | **â†‘25%** |

### 14.6 æŒç»­ä¼˜åŒ–

#### 14.6.1 ä¸»åŠ¨å­¦ä¹ 

```python
# active_learning.py
def select_hard_samples(predictions, threshold=0.7):
    """é€‰æ‹©æ¨¡å‹ä¸ç¡®å®šçš„æ ·æœ¬è¿›è¡Œæ ‡æ³¨"""
    hard_samples = []

    for pred in predictions:
        # è®¡ç®—ç½®ä¿¡åº¦
        confidence = pred['confidence']

        # ä½ç½®ä¿¡åº¦æ ·æœ¬
        if confidence < threshold:
            hard_samples.append(pred['image_url'])

    return hard_samples

# å®šæœŸè¿è¡Œ
hard_samples = select_hard_samples(recent_predictions)
# å‘é€ç»™æ ‡æ³¨å›¢é˜Ÿ...
```

#### 14.6.2 åœ¨çº¿æ›´æ–°

```python
# online_update.py
def incremental_training(new_data_path):
    """å¢é‡è®­ç»ƒ - æ¯å‘¨æ›´æ–°ä¸€æ¬¡"""
    # 1. åŠ è½½å·²è®­ç»ƒçš„LoRAæƒé‡
    model = AutoModelForCausalLM.from_pretrained(base_model)
    model = PeftModel.from_pretrained(model, "./qwen3vl_ecommerce_lora_final")

    # 2. åŠ è½½æ–°æ•°æ®
    new_dataset = load_dataset('json', data_files=new_data_path)

    # 3. ç»§ç»­è®­ç»ƒï¼ˆæ›´å°çš„å­¦ä¹ ç‡ï¼‰
    training_args.learning_rate = 1e-5  # é™ä½å­¦ä¹ ç‡
    training_args.num_train_epochs = 1

    trainer = Trainer(model=model, args=training_args, train_dataset=new_dataset)
    trainer.train()

    # 4. ä¿å­˜æ–°æƒé‡
    model.save_pretrained(f"./qwen3vl_ecommerce_lora_{datetime.now().strftime('%Y%m%d')}")
```

### 14.7 å¸¸è§é—®é¢˜

#### Q1: å¦‚ä½•å¤„ç†å¤šè¯­è¨€OCR?

**è§£å†³æ–¹æ¡ˆ**: ä½¿ç”¨è¯­è¨€IDå‰ç¼€

```python
payload = {
    "messages": [
        {
            "role": "system",
            "content": "You are a multilingual OCR assistant. Detect language and extract text."
        },
        {
            "role": "user",
            "content": [
                {"type": "image_url", "image_url": image_url},
                {"type": "text", "text": "[LANG: auto] Extract text"}
            ]
        }
    ]
}
```

#### Q2: å¦‚ä½•å¤„ç†ä½è´¨é‡å›¾ç‰‡?

**è§£å†³æ–¹æ¡ˆ**: å›¾åƒé¢„å¤„ç†

```python
from PIL import Image, ImageEnhance

def preprocess_image(image_path):
    """å›¾åƒå¢å¼º"""
    img = Image.open(image_path)

    # 1. å»å™ª
    img = img.filter(ImageFilter.MedianFilter(size=3))

    # 2. å¢å¼ºå¯¹æ¯”åº¦
    enhancer = ImageEnhance.Contrast(img)
    img = enhancer.enhance(1.5)

    # 3. é”åŒ–
    enhancer = ImageEnhance.Sharpness(img)
    img = enhancer.enhance(2.0)

    return img
```

#### Q3: å¦‚ä½•å‡å°‘æ¨ç†å»¶è¿Ÿ?

**ä¼˜åŒ–æ–¹æ¡ˆ**:

1. **æ‰¹é‡æ¨ç†**: åˆå¹¶å¤šä¸ªå›¾ç‰‡è¯·æ±‚
2. **é‡åŒ–**: FP16 â†’ INT8 (å‡å°‘50%æ˜¾å­˜)
3. **Flash Attention**: åŠ é€ŸAttentionè®¡ç®—
4. **KV Cache**: å¤ç”¨è®¡ç®—ç»“æœ

```bash
# vLLMå¯åŠ¨å‚æ•°ä¼˜åŒ–
python -m vllm.entrypoints.openai.api_server \
  --model Qwen/Qwen3-VL-8B-Instruct-FP8 \
  --quantization fp8 \  # INT8é‡åŒ–
  --enable-prefix-caching \  # KVç¼“å­˜
  --max-num-batched-tokens 8192 \  # æ‰¹å¤„ç†
  --gpu-memory-utilization 0.95
```

---

## 15. æ€»ç»“ä¸é¢è¯•è¦ç‚¹

### 15.1 é¡¹ç›®å…¨æ™¯å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     WebScraper                          â”‚
â”‚  çˆ¬è™«ç³»ç»Ÿ (Product/Deal/Shopping)                        â”‚
â”‚  - Playwright + BrightData                              â”‚
â”‚  - BrowserPool (15å¹¶å‘)                                 â”‚
â”‚  - ä¸‰å±‚æ¶æ„ (è°ƒåº¦/é€‚é…/åŸºç¡€)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚ HTTP POST
                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  AsyncPipeline                          â”‚
â”‚  å¼‚æ­¥ä»»åŠ¡å¤„ç†ç®¡é“ (RabbitMQ + Workers)                   â”‚
â”‚  - OCR Worker: vLLM OCR API                             â”‚
â”‚  - LLM Worker: Gemini (30å¹¶å‘)                          â”‚
â”‚  - DB Worker: æ‰¹é‡æ’å…¥ (50æ¡/æ‰¹æ¬¡)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚ è°ƒç”¨
                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    OCR_Rec                              â”‚
â”‚  å¼‚æ­¥OCRè¯†åˆ«ç³»ç»Ÿ                                         â”‚
â”‚  - submit_tasks.py (æ¯5åˆ†é’Ÿ)                            â”‚
â”‚  - fetch_results.py (æ¯2åˆ†é’Ÿ)                           â”‚
â”‚  - Qwen3-VL-8B-Instruct-FP8 (å¾®è°ƒ)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 15.2 æ ¸å¿ƒæŠ€æœ¯æ ˆ

| å±‚çº§ | æŠ€æœ¯ | ç”¨é€” |
|-----|------|------|
| **çˆ¬è™«å±‚** | Playwright, BrightData | æµè§ˆå™¨è‡ªåŠ¨åŒ–ã€ä»£ç†æœåŠ¡ |
| **æ•°æ®æå–** | BeautifulSoup, LLM (Gemini/Claude) | HTMLè§£æã€æ™ºèƒ½æå– |
| **å¹¶å‘æ§åˆ¶** | asyncio, BrowserPool, Semaphore | å¼‚æ­¥ç¼–ç¨‹ã€å¹¶å‘ç®¡ç† |
| **æ¶ˆæ¯é˜Ÿåˆ—** | RabbitMQ | ä»»åŠ¡è§£è€¦ã€å¯é ä¼ é€’ |
| **æ•°æ®åº“** | MongoDB (MongoEngine) | NoSQLæ–‡æ¡£å­˜å‚¨ |
| **OCRè¯†åˆ«** | vLLM, Qwen3-VL-8B, LoRA | è§†è§‰è¯­è¨€æ¨¡å‹ã€å¾®è°ƒ |
| **å­˜å‚¨** | Google Cloud Storage | å›¾ç‰‡CDN |
| **APIæ¡†æ¶** | FastAPI | RESTful APIæœåŠ¡ |
| **è°ƒåº¦** | Airflow, Cron | å®šæ—¶ä»»åŠ¡ |

### 15.3 é¢è¯•é«˜é¢‘é—®é¢˜

#### Q1: å¦‚ä½•ä¿è¯çˆ¬è™«çš„ç¨³å®šæ€§å’Œæ•ˆç‡?

**ç­”**:
1. **BrowserPoolè¿æ¥æ± **: å¤ç”¨æµè§ˆå™¨å®ä¾‹ï¼Œå‡å°‘å¯åŠ¨å¼€é”€
2. **BrightDataæ‰¹é‡çˆ¬å–**: 20ä¸ªURLå¹¶å‘è·å–HTMLï¼Œå¿«10å€
3. **åæ£€æµ‹æœºåˆ¶**: ä¿®æ”¹navigatorå±æ€§ã€ä¼ªé€ WebGLæŒ‡çº¹
4. **é”™è¯¯é‡è¯•**: æŒ‡æ•°é€€é¿é‡è¯•ï¼Œæœ€å¤š3æ¬¡
5. **å¹¶å‘æ§åˆ¶**: Semaphoreé™åˆ¶å¹¶å‘æ•°ï¼Œé¿å…è¢«å°IP

#### Q2: AsyncPipelineå¦‚ä½•ä¿è¯æ•°æ®ä¸€è‡´æ€§?

**ç­”**:
1. **æ¶ˆæ¯æŒä¹…åŒ–**: RabbitMQé˜Ÿåˆ—å’Œæ¶ˆæ¯éƒ½æŒä¹…åŒ–
2. **åŸå­æ€§æ›´æ–°**: MongoDBçš„updateæ“ä½œä¿è¯åŸå­æ€§
3. **çŠ¶æ€æœº**: TracePage.statusæµè½¬ (pending â†’ pending_ocr â†’ pending_llm â†’ pending_db â†’ completed)
4. **å¹‚ç­‰æ€§**: ä½¿ç”¨trace_page_idå»é‡ï¼Œé‡å¤æ¶ˆæ¯ä¸ä¼šé‡å¤æ’å…¥

#### Q3: Qwen3-VLå¾®è°ƒçš„å…³é”®ç‚¹æ˜¯ä»€ä¹ˆ?

**ç­”**:
1. **æ•°æ®è´¨é‡**: æ ‡æ³¨å‡†ç¡®ã€åœºæ™¯è¦†ç›–å…¨é¢ï¼ˆä»·æ ¼ã€å“ç‰Œã€ä¿ƒé”€ï¼‰
2. **LoRAå‚æ•°**: r=16, alpha=32, ä»…è®­ç»ƒ0.1%å‚æ•°
3. **è®­ç»ƒç­–ç•¥**: å­¦ä¹ ç‡2e-4, warmup 500æ­¥, æ¢¯åº¦ç´¯ç§¯
4. **è¯„ä¼°æŒ‡æ ‡**: CER < 5%, Price Acc > 95%, Brand Recall > 90%
5. **æŒç»­ä¼˜åŒ–**: ä¸»åŠ¨å­¦ä¹ é€‰æ‹©hard samplesï¼Œæ¯å‘¨å¢é‡è®­ç»ƒ

#### Q4: ä¸‰å±‚æ¶æ„çš„ä¼˜åŠ¿æ˜¯ä»€ä¹ˆ?

**ç­”**:
1. **è§£è€¦**: è°ƒåº¦å±‚ã€é€‚é…å±‚ã€åŸºç¡€å±‚èŒè´£æ¸…æ™°ï¼Œäº’ä¸å¹²æ‰°
2. **å¯æ‰©å±•**: æ·»åŠ æ–°ç«™ç‚¹åªéœ€å®ç°é€‚é…å±‚ï¼Œæ— éœ€ä¿®æ”¹åŸºç¡€å±‚
3. **å¯å¤ç”¨**: BrowserPoolã€ProductDetailMixinç­‰ç»„ä»¶å¯å¤ç”¨
4. **å¯æµ‹è¯•**: æ¯å±‚å¯å•ç‹¬æµ‹è¯•ï¼Œå¿«é€Ÿå®šä½é—®é¢˜

#### Q5: å¦‚ä½•ä¼˜åŒ–Gemini APIè°ƒç”¨æ€§èƒ½?

**ç­”**:
1. **Resource Manager**: ç®¡ç†å¤šä¸ªAPI Keyè½®æ¢ï¼Œé¿å…å•Keyé™æµ
2. **çœŸå¹¶å‘**: 30ä¸ªasyncioä»»åŠ¡å¹¶å‘è°ƒç”¨ï¼Œååé‡æå‡30å€
3. **æ‰¹é‡å¤„ç†**: LLM Workeræ‰¹é‡æ¥æ”¶ä»»åŠ¡ï¼Œå‡å°‘ä¸Šä¸‹æ–‡åˆ‡æ¢
4. **é‡è¯•æœºåˆ¶**: æŒ‡æ•°é€€é¿é‡è¯•ï¼Œå¤„ç†ä¸´æ—¶æ€§æ•…éšœ

### 15.4 äº®ç‚¹æ€»ç»“

| äº®ç‚¹ | è¯´æ˜ | ä½“ç°èƒ½åŠ› |
|------|------|---------|
| **ä¸‰å±‚æ¶æ„è®¾è®¡** | è°ƒåº¦/é€‚é…/åŸºç¡€åˆ†ç¦»ï¼ŒèŒè´£æ¸…æ™° | æ¶æ„è®¾è®¡ |
| **BrowserPoolä¼˜åŒ–** | 15å¹¶å‘ + è‡ªåŠ¨æ¸…ç† + åæ£€æµ‹ | æ€§èƒ½ä¼˜åŒ– |
| **å¼‚æ­¥è§£è€¦** | RabbitMQ + Workeræ¨¡å¼ | ç³»ç»Ÿè®¾è®¡ |
| **æ‰¹é‡å¤„ç†** | DB Workeræ‰¹é‡æ’å…¥ï¼Œå¿«10å€ | å·¥ç¨‹ä¼˜åŒ– |
| **LoRAå¾®è°ƒ** | å‚æ•°æ•ˆç‡é«˜ï¼Œæ•ˆæœæå‡25% | AIå·¥ç¨‹åŒ– |
| **Resource Manager** | 30å¹¶å‘Geminiè°ƒç”¨ | å¹¶å‘ç¼–ç¨‹ |
| **ä¸»åŠ¨å­¦ä¹ ** | æŒç»­ä¼˜åŒ–æ¨¡å‹ | æœºå™¨å­¦ä¹  |

### 15.5 å‡†å¤‡å»ºè®®

1. **ç†Ÿæ‚‰æ ¸å¿ƒä»£ç **:
   - `extractor_base.py`: BrowserPoolå®ç°
   - `extractor_scheduler.py`: BFSéå†é€»è¾‘
   - `workers/llm_worker.py`: Geminiå¹¶å‘è°ƒç”¨
   - `submit_tasks.py`: OCRä»»åŠ¡æäº¤æµç¨‹

2. **å‡†å¤‡Demo**:
   - æ¼”ç¤ºBrowserPoolå¦‚ä½•å¤ç”¨tab
   - å±•ç¤ºAsyncPipelineå¤„ç†æµç¨‹
   - å¯¹æ¯”å¾®è°ƒå‰åOCRæ•ˆæœ

3. **å‡†å¤‡æ¡ˆä¾‹**:
   - é€‰2-3ä¸ªæŠ€æœ¯éš¾ç‚¹ï¼ˆåçˆ¬è™«ã€å¹¶å‘ä¼˜åŒ–ã€æ•°æ®ä¸€è‡´æ€§ï¼‰
   - å‡†å¤‡1-2ä¸ªä¼˜åŒ–æ¡ˆä¾‹ï¼ˆæ€§èƒ½æå‡ã€å‡†ç¡®ç‡æå‡ï¼‰
   - å‡†å¤‡1ä¸ªè¾¹ç•Œæƒ…å†µå¤„ç†ï¼ˆè¶…æ—¶ã€ç½‘ç»œé”™è¯¯ã€æ•°æ®å¼‚å¸¸ï¼‰

**ç¥é¢è¯•é¡ºåˆ©ï¼** ğŸš€
