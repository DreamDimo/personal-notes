## å¤§æ¨¡å‹å¼€å‘

### ä¸€ã€MCPæ¨¡å‹ä¸Šä¸‹æ–‡åè®®

#### 1. ä»‹ç»

æ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼ˆ$MCP$ï¼‰æ˜¯ä¸€ç§ç”¨äºè¯­è¨€æ¨¡å‹è°ƒç”¨çš„æ ‡å‡†åè®®ï¼Œç”¨äºåœ¨å®¢æˆ·ç«¯å’ŒæœåŠ¡ç«¯ä¹‹é—´ç»Ÿä¸€ä¼ è¾“ä¸Šä¸‹æ–‡ï¼ˆå¦‚æŒ‡ä»¤ã€å†å²æ¶ˆæ¯ã€å·¥å…·è°ƒç”¨ä¿¡æ¯ç­‰ï¼‰ï¼Œå°¤å…¶é€‚åº”**å¤šè½®å¯¹è¯ã€å¤š Agent åä½œã€æ’ä»¶ã€å·¥å…·ã€ç½‘é¡µä»£ç†ã€ä¸Šä¸‹æ–‡æŒä¹…åŒ–** ç­‰åœºæ™¯ä¸­ã€‚ä¹Ÿå°±æ˜¯å¦‚æœå¤§æ¨¡å‹æƒ³ä½¿ç”¨å¤–éƒ¨å·¥å…·çš„è¯ï¼Œåœ¨ä¹‹å‰éœ€è¦ä¸ºæ¯ä¸ªå¤§æ¨¡å‹å•ç‹¬å¼€å‘ä¸€å¥—å·¥å…·ï¼Œå› ä¸ºæ²¡æœ‰ç»Ÿä¸€çš„æ¥å£ï¼Œç°åœ¨åªéœ€è¦æŒ‰ç…§ $MCP$ åè®®çš„æ ‡å‡†ï¼Œå°±å¯ä»¥ç”±å·¥å…·å¼€å‘äººå‘˜åˆ›å»ºæœåŠ¡å™¨ï¼Œç„¶åå¤§æ¨¡å‹ä½œä¸ºå®¢æˆ·ç«¯ç›´æ¥ä½¿ç”¨å³å¯ã€‚

![image-20250715101441602](assets/image-20250715101441602.png)

#### 2. MCP Server æœåŠ¡ç«¯

åˆ©ç”¨ $FastMCP$ æ„å»ºï¼Œå·¥å…·ç”¨è£…é¥°å™¨ `mcp.tool()` æ ‡æ³¨

æœåŠ¡ç«¯ä¼ é€ä¿¡æ¯çš„æ–¹å¼æœ‰ä¸‰ç§ï¼š1.stdioå³æœ¬åœ°æ ‡å‡†è¾“å…¥è¾“å‡ºï¼ˆç”¨äºæœ¬åœ°è¿æ¥ï¼‰ 2. http+sseæœ‰çŠ¶æ€è¿æ¥ 3.streamable HTTPå…è®¸æ— çŠ¶æ€æˆ–è€…æœ‰çŠ¶æ€è¿æ¥

![image-20250715101822032](assets/image-20250715101822032.png)

```python
import arxiv
import json
import os
from typing import List
from mcp.server.fastmcp import FastMCP


PAPER_DIR = "papers"

# Initialize FastMCP server
mcp = FastMCP("research") # ç»™æœåŠ¡èµ·å

@mcp.tool()
def search_papers(topic: str, max_results: int = 5) -> List[str]:
    """
    Search for papers on arXiv based on a topic and store their information.
    
    Args:
        topic: The topic to search for
        max_results: Maximum number of results to retrieve (default: 5)
        
    Returns:
        List of paper IDs found in the search
    """
    
    # Use arxiv to find the papers 
    client = arxiv.Client()

    # Search for the most relevant articles matching the queried topic
    search = arxiv.Search(
        query = topic,
        max_results = max_results,
        sort_by = arxiv.SortCriterion.Relevance
    )

    papers = client.results(search)
    
    # Create directory for this topic
    path = os.path.join(PAPER_DIR, topic.lower().replace(" ", "_"))
    os.makedirs(path, exist_ok=True)
    
    file_path = os.path.join(path, "papers_info.json")

    # Try to load existing papers info
    try:
        with open(file_path, "r") as json_file:
            papers_info = json.load(json_file)
    except (FileNotFoundError, json.JSONDecodeError):
        papers_info = {}

    # Process each paper and add to papers_info  
    paper_ids = []
    for paper in papers:
        paper_ids.append(paper.get_short_id())
        paper_info = {
            'title': paper.title,
            'authors': [author.name for author in paper.authors],
            'summary': paper.summary,
            'pdf_url': paper.pdf_url,
            'published': str(paper.published.date())
        }
        papers_info[paper.get_short_id()] = paper_info
    
    # Save updated papers_info to json file
    with open(file_path, "w") as json_file:
        json.dump(papers_info, json_file, indent=2)
    
    print(f"Results are saved in: {file_path}")
    
    return paper_ids

@mcp.tool()
def extract_info(paper_id: str) -> str:
    """
    Search for information about a specific paper across all topic directories.
    
    Args:
        paper_id: The ID of the paper to look for
        
    Returns:
        JSON string with paper information if found, error message if not found
    """
 
    for item in os.listdir(PAPER_DIR):
        item_path = os.path.join(PAPER_DIR, item)
        if os.path.isdir(item_path):
            file_path = os.path.join(item_path, "papers_info.json")
            if os.path.isfile(file_path):
                try:
                    with open(file_path, "r") as json_file:
                        papers_info = json.load(json_file)
                        if paper_id in papers_info:
                            return json.dumps(papers_info[paper_id], indent=2)
                except (FileNotFoundError, json.JSONDecodeError) as e:
                    print(f"Error reading {file_path}: {str(e)}")
                    continue
    
    return f"There's no saved information related to paper {paper_id}."



if __name__ == "__main__":
    # Initialize and run the server
    mcp.run(transport='stdio')
```

ç¼–å†™å¥½åï¼Œè¦å†™ä¸€ä¸ªserver-config.jsonæ–‡ä»¶æ¥ç»™å®¢æˆ·ç«¯ä½¿ç”¨

```json
{
    "mcpServers": {

        "filesystem": {
            "command": "npx",
            "args": [
                "-y",
                "@modelcontextprotocol/server-filesystem",
                "."
            ]
        },

        "research": {
            "command": "uv",
            "args": ["run", "research_server.py"]
        },

        "fetch": {
            "command": "uvx",
            "args": ["mcp-server-fetch"]
        }
    }
}
```

æœåŠ¡å™¨å¯ä»¥å°†èµ„æºæš´éœ²ç»™å®¢æˆ·ç«¯

```python
@mcp.resource("papers://{topic}")
def get_topic_papers(topic: str) -> str:
    """
    Get detailed information about papers on a specific topic.
    
    Args:
        topic: The research topic to retrieve papers for
    """
    topic_dir = topic.lower().replace(" ", "_")
    papers_file = os.path.join(PAPER_DIR, topic_dir, "papers_info.json")
    
    if not os.path.exists(papers_file):
        return f"# No papers found for topic: {topic}\n\nTry searching for papers on this topic first."
    
    try:
        with open(papers_file, 'r') as f:
            papers_data = json.load(f)
        
        # Create markdown content with paper details
        content = f"# Papers on {topic.replace('_', ' ').title()}\n\n"
        content += f"Total papers: {len(papers_data)}\n\n"
        
        for paper_id, paper_info in papers_data.items():
            content += f"## {paper_info['title']}\n"
            content += f"- **Paper ID**: {paper_id}\n"
            content += f"- **Authors**: {', '.join(paper_info['authors'])}\n"
            content += f"- **Published**: {paper_info['published']}\n"
            content += f"- **PDF URL**: [{paper_info['pdf_url']}]({paper_info['pdf_url']})\n\n"
            content += f"### Summary\n{paper_info['summary'][:500]}...\n\n"
            content += "---\n\n"
        
        return content
    except json.JSONDecodeError:
        return f"# Error reading papers data for {topic}\n\nThe papers data file is corrupted."
```

æœåŠ¡ç«¯ä¹Ÿå¯ä»¥å‘å®¢æˆ·ç«¯æä¾›promptæ¨¡ç‰ˆ

```python
@mcp.prompt()
def generate_search_prompt(topic: str, num_papers: int = 5) -> str:
    """Generate a prompt for Claude to find and discuss academic papers on a specific topic."""
    return f"""Search for {num_papers} academic papers about '{topic}' using the search_papers tool. Follow these instructions:
    1. First, search for papers using search_papers(topic='{topic}', max_results={num_papers})
    2. For each paper found, extract and organize the following information:
       - Paper title
       - Authors
       - Publication date
       - Brief summary of the key findings
       - Main contributions or innovations
       - Methodologies used
       - Relevance to the topic '{topic}'
    
    3. Provide a comprehensive summary that includes:
       - Overview of the current state of research in '{topic}'
       - Common themes and trends across the papers
       - Key research gaps or areas for future investigation
       - Most impactful or influential papers in this area
    
    4. Organize your findings in a clear, structured format with headings and bullet points for easy readability.
    
    Please present both detailed information about each paper and a high-level synthesis of the research landscape in {topic}."""
```

é€šè¿‡ `npx @modelcontextprotocol/inspector uv run research_server.py` å¯ä»¥æ‰“å¼€ `inspector` æŸ¥çœ‹æœåŠ¡å™¨æƒ…å†µ

#### 3. MCP Client å®¢æˆ·ç«¯

`ClientSession` æ˜¯ MCP åè®®å®¢æˆ·ç«¯çš„ä¼šè¯å¯¹è±¡ï¼Œè´Ÿè´£å’ŒæœåŠ¡å™¨å»ºç«‹ã€ç»´æŒã€å…³é—­è¿æ¥ï¼Œåˆå§‹åŒ–ä¸æœåŠ¡å™¨çš„è¿æ¥ï¼Œè·å–æœåŠ¡å™¨æä¾›çš„å·¥å…·åˆ—è¡¨ï¼Œè°ƒç”¨æœåŠ¡ç«¯çš„å·¥å…·ï¼Œè¿™ä¸ªå°±æ˜¯å®¢æˆ·ç«¯å’ŒæœåŠ¡å™¨äº¤äº’çš„é€šé“ï¼Œæ¯ä¸ªæœåŠ¡å™¨éƒ½è¦ç»´æŠ¤ä¸€ä¸ªå•ç‹¬çš„ `session` ã€‚

`AsyncExitStack` æ˜¯å¼‚æ­¥é€€å‡ºæ ˆï¼Œç”¨æ¥ç®¡ç†å¤šä¸ªå¼‚æ­¥èµ„æºï¼ˆå³ç®¡ç†å¤šä¸ªä¸åŒçš„å·¥å…·ï¼‰

`stdio-client` æ˜¯ä¼šå¯åŠ¨ä¸€ä¸ª `MCP` æœåŠ¡å™¨çš„å­è¿›ç¨‹ï¼Œå¹¶è¿”å›ä¸€ä¸ªå¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œç”¨æ¥å’ŒæœåŠ¡å™¨é€šä¿¡

```python
from dotenv import load_dotenv
from anthropic import Anthropic
from mcp import ClientSession, StdioServerParameters, types
from mcp.client.stdio import stdio_client
from typing import List, Dict, TypedDict
from contextlib import AsyncExitStack
import json
import asyncio

load_dotenv()

class ToolDefinition(TypedDict):
    name: str
    description: str
    input_schema: dict

class MCP_ChatBot:

    def __init__(self):
        # Initialize session and client objects
        self.sessions: List[ClientSession] = [] # new
        self.exit_stack = AsyncExitStack() # new
        self.anthropic = Anthropic()
        self.available_tools: List[ToolDefinition] = [] # new
        self.tool_to_session: Dict[str, ClientSession] = {} # new


    async def connect_to_server(self, server_name: str, server_config: dict) -> None:
        """Connect to a single MCP server."""
        try:
            server_params = StdioServerParameters(**server_config)
            stdio_transport = await self.exit_stack.enter_async_context(
                stdio_client(server_params)
            ) # new
            read, write = stdio_transport
            session = await self.exit_stack.enter_async_context(
                ClientSession(read, write)
            ) # new
            await session.initialize()
            self.sessions.append(session)
            
            # List available tools for this session
            response = await session.list_tools()
            tools = response.tools
            print(f"\nConnected to {server_name} with tools:", [t.name for t in tools])
            
            for tool in tools: # new
                self.tool_to_session[tool.name] = session
                self.available_tools.append({
                    "name": tool.name,
                    "description": tool.description,
                    "input_schema": tool.inputSchema
                })
        except Exception as e:
            print(f"Failed to connect to {server_name}: {e}")

    async def connect_to_servers(self): # new
        """Connect to all configured MCP servers."""
        try:
            with open("server_config.json", "r") as file:
                data = json.load(file)
            
            servers = data.get("mcpServers", {})
            
            for server_name, server_config in servers.items():
                await self.connect_to_server(server_name, server_config)
        except Exception as e:
            print(f"Error loading server configuration: {e}")
            raise
    
    async def process_query(self, query):
        messages = [{'role':'user', 'content':query}]
        response = self.anthropic.messages.create(max_tokens = 2024,
                                      model = 'claude-3-7-sonnet-20250219', 
                                      tools = self.available_tools,
                                      messages = messages)
        process_query = True
        while process_query:
            assistant_content = []
            for content in response.content:
                if content.type =='text':
                    print(content.text)
                    assistant_content.append(content)
                    if(len(response.content) == 1):
                        process_query= False
                elif content.type == 'tool_use':
                    assistant_content.append(content)
                    messages.append({'role':'assistant', 'content':assistant_content})
                    tool_id = content.id
                    tool_args = content.input
                    tool_name = content.name
                    
    
                    print(f"Calling tool {tool_name} with args {tool_args}")
                    
                    # Call a tool
                    session = self.tool_to_session[tool_name] # new
                    result = await session.call_tool(tool_name, arguments=tool_args)
                    messages.append({"role": "user", 
                                      "content": [
                                          {
                                              "type": "tool_result",
                                              "tool_use_id":tool_id,
                                              "content": result.content
                                          }
                                      ]
                                    })
                    response = self.anthropic.messages.create(max_tokens = 2024,
                                      model = 'claude-3-7-sonnet-20250219', 
                                      tools = self.available_tools,
                                      messages = messages) 
                    
                    if(len(response.content) == 1 and response.content[0].type == "text"):
                        print(response.content[0].text)
                        process_query= False

    
    
    async def chat_loop(self):
        """Run an interactive chat loop"""
        print("\nMCP Chatbot Started!")
        print("Type your queries or 'quit' to exit.")
        
        while True:
            try:
                query = input("\nQuery: ").strip()
        
                if query.lower() == 'quit':
                    break
                    
                await self.process_query(query)
                print("\n")
                    
            except Exception as e:
                print(f"\nError: {str(e)}")
    
    async def cleanup(self): # new
        """Cleanly close all resources using AsyncExitStack."""
        await self.exit_stack.aclose()


async def main():
    chatbot = MCP_ChatBot()
    try:
        # the mcp clients and sessions are not initialized using "with"
        # like in the previous lesson
        # so the cleanup should be manually handled
        await chatbot.connect_to_servers() # new! 
        await chatbot.chat_loop()
    finally:
        await chatbot.cleanup() #new! 


if __name__ == "__main__":
    asyncio.run(main())
```

å®¢æˆ·ç«¯äº¤äº’èµ„æºçš„æ–¹å¼ï¼š@æ˜¯èµ„æºåé¢çš„æ˜¯å‚æ•°ï¼Œ/promptæ˜¯æç¤ºè¯

- **@ai_interpretability**
- **/prompts**
- **/prompt generate_search_prompt topic=history num_papers=2**

### äºŒã€LangChain

#### 0. ç®€ä»‹

##### 1. åŒ…

langchain åŒ…å«çš„æ¨¡å— langchainï¼Œlangchain_coreï¼Œlangchain_community ä¸‰åŒ…

##### 2. æ ¸å¿ƒéƒ¨åˆ†

llm æ˜¯å°è£…çš„åŸºç¡€æ¨¡å‹ï¼Œchat_models æ˜¯ä¸ºå¯¹è¯è®¾è®¡çš„å¯ä»¥æ¥å—ä¸€ç»„è¾“å…¥ï¼Œå®˜æ–¹æä¾›çš„æ˜¯ `langchain_openai`

messages æ˜¯èŠå¤©çš„å†…å®¹ï¼ˆåŒ…æ‹¬langchainé£æ ¼çš„ HumanMessage å’Œ openai é£æ ¼çš„æ¨¡å¼ï¼‰ï¼Œæç¤ºè¯æ˜¯ç®¡ç†æ¶ˆæ¯

output parsers è¾“å‡ºè§£é‡Šï¼Œå°†å¤§æ¨¡å‹çš„è¾“å‡ºè§£æä¸º json ç­‰æ ¼å¼

retrievers æ£€ç´¢å™¨ï¼Œè¿›è¡ŒçŸ¥è¯†æ£€ç´¢å³ ragï¼Œvector stores æ¥å…¥å¾ˆå¤šå‘é‡æ•°æ®åº“

agents æ™ºèƒ½ä½“

#### 1. openai

```python
import openai
import os
from dotenv import load_dotenv, find_dotenv
# åŠ è½½å‚æ•°å€¼å­˜å…¥os.environä¸­
_ = load_dotenv(find_dotenv())
openai.api_key = os.environ['OPENAI_API_KEY']

def get_completion(prompt, model = 'gpt-3.5-turbo'):
  messages = [{"role":"user", "content":prompt}]
  response = openai.ChatCompletion.create(
  	model = model, 
    messages = messages,
    temperature = 0,
  )
  return response.choices[0].message["content"]

get_completion("explain ai in one sentence")
```

openai chat apiæ ¼å¼ `role` åŒ…æ‹¬ä¸‰ç§ `system, user, assistant` ï¼Œä»¥åŠ `content` å†…å®¹

```python
messages=[
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç¿»è¯‘åŠ©æ‰‹ã€‚"},
    {"role": "user", "content": "è¯·å°†ä¸‹é¢è¿™å¥è¯ç¿»è¯‘æˆè‹±æ–‡ï¼šæˆ‘å–œæ¬¢å­¦ä¹ ã€‚"},
    {"role": "assistant", "content": "I like learning."},
    {"role": "user", "content": "å†ç¿»è¯‘ä¸€å¥ï¼šå¤©æ°”å¾ˆå¥½ã€‚"}
]
```

#### 2. Chat models

ç‰¹ç‚¹ï¼šæ”¯æŒå¤šæ¨¡æ€ï¼Œå¯ä»¥è¿›è¡Œå·¥å…·è°ƒç”¨ï¼ˆtool callingï¼‰ï¼Œè¿›è¡Œç»“æ„åŒ–è¾“å‡ºã€‚

è°ƒç”¨ï¼šå®˜æ–¹æä¾›çš„æ¨¡å‹åœ¨ `langchain-<provider>` åŒ…ï¼Œç¤¾åŒºç‰ˆåœ¨ `langchain-community` åŒ…ã€‚

æ–¹æ³•ï¼š

```python
from langchain.chat_models import init_chat_model
model = init_chat_model("gemini-2.0-flash", model_provider="google_genai")
```

`ChatOpenAI()` å‚æ•°æœ‰ model, temperature ç­‰

```python
from langchain.chat_models import ChatOpenAI
chat = ChatOpenAI()

response = chat(prompt)
print(response.content)
```

ç­‰å¾…å¤§æ¨¡å‹å®Œå…¨æ¨ç†å®Œåè¿”å›è°ƒç”¨ç»“æœ

```python
chat_model = ChatOpenAI(model='gpt-4o', temperature=0.0)
response = chat_model.invoke(prompt)
```

$stream$ æµå¼è°ƒç”¨

```python
model = ChatOpenAI(model='gpt-4o', streaming = True)
for chunk in model.stream(prompt):
    print(chunk.content, end="", flush=True)
```

æ‰¹é‡è°ƒç”¨

```python
message = [message1, message2]
response = model.invoke(message)
```

å¼‚æ­¥è°ƒç”¨å°±æ˜¯åœ¨å‰é¢åŠ  $a$ï¼Œæ¯”å¦‚ $astream$ æˆ–è€… $ainvoke$

ç„¶åå¿…é¡»æ˜¯åœ¨ä¸»çº¿ç¨‹ä¸­é‡‡ç”¨å¼‚æ­¥æ–¹æ³•å¯åŠ¨

```python
if __name__ == "__main__":
    result = asyncio.run(run_async_tasks())
    print(restult)
```

#### 3. Prompt Templates

æ³¨æ„ `ChatPromptTemplate.from_template()` é‡Œé¢åº”è¯¥ç›´æ¥å¡«å­—ç¬¦ä¸²ä½œä¸ºæç¤ºè¯ï¼Œè€Œ `ChatPromptTemplate` å¯ä»¥æ˜¯ `([(),()])` å°æ‹¬å·é‡Œä¸º openai é£æ ¼æˆ–è€… HumanMessage ç­‰

* String PromptTemplates

  invoke è¾“å…¥çš„æ˜¯å­—å…¸ï¼Œè¿”å›å€¼æ˜¯ PromptValueï¼Œformat è¾“å…¥çš„æ˜¯é”®å€¼å¯¹ï¼Œè¿”å›å€¼æ˜¯ str

  ```python
  from langchain_core.prompts import PromptTemplate
  
  prompt_template = PromptTemplate.from_template("Tell me a joke about {topic}")
  prompt_template = PromptTemplate.from_template(
      template = "Tell me a joke about {topic} ",
      partial_variables={"topic":"dogs"} # éƒ¨åˆ†å¡«å…¥å‚æ•°ï¼Œä¸‹é¢ä¸ç”¨å†™äº†
  )
  # ä¸‹é¢ä¸¤è€…ç­‰ä»·
  prompt_template.invoke({"topic": "cats"})
  prompt_template.format(topic='cat')
  ```

  æ³¨æ„ `partial()` ä¸ä¼šæ”¹å˜åŸæ¥çš„æ¨¡æ¿ï¼Œè€Œæ˜¯ä¼šè¿”å›ä¸€ä¸ªæ–°çš„æ¨¡æ¿

  ```python
  template1 = template.partial(topic="cat")
  template1.invoke()
  ```

* ChatPromptTemplates

  `from_template` æ˜¯å­—ç¬¦ä¸²ï¼Œ`from_message` æ˜¯æ¶ˆæ¯æ ¼å¼

  ```python
  prompt = ChatPromptTemplate.from_template(
      """
          Translate the following text from English to French:
          {text}
      """
  )
  
  prompt.invoke({"text": "I love programming in Python"})
  ```

  ```python
  prompt = ChatPromptTemplate.from_messages(
      [
          ("system", "You are a helpful assistant that translates text from English to French."),
          ("human", "{text}"),
      ]
  )
  
  prompt.invoke({"text": "I love programming in Python"})
  
  
  prompt.invoke({"text": "I love programming in Python"})
  ```

  ```python
  from langchain_core.prompts import ChatPromptTemplate
  
  prompt_template = ChatPromptTemplate([
      ("system", "You are a helpful assistant"),
      ("user", "Tell me a joke about {topic}")
  ])
  
  prompt_template.invoke({"topic": "cats"})
  ```

  æ³¨æ„å¦‚æœæ˜¯ä¸‹é¢çš„è¾“å…¥çš„è¯ï¼Œå˜é‡æ˜¯èµ‹å€¼ä¸äº†çš„

  ```python
  prompt = ChatPromptTemplate.from_messages(
      [
          SystemMessage(
              content="""
              Translate the following text from English to French:
              """
          ),
          HumanMessage(content="{text}"),
      ]
  )
  
  prompt.invoke("text":"I love langchian")
  ```

  è¦æƒ³å®ç°èµ‹å€¼ï¼Œæ˜¯éœ€è¦å†™æˆæ¨¡æ¿çš„å½¢å¼

  ```python
  from langchain_core.prompts import HumanMessagePromptTemplate
  
  prompt = ChatPromptTemplate.from_messages(
      [
          SystemMessage(
              content="""
              Translate the following text from English to French:
              """
          ),
          HumanMessagePromptTemplate.from_template(template="{text}"),
      ]
  )
  
  response = prompt.invoke({"text":"I love langchian"})
  
  response.to_string()
  ```

  

  å®ç° ChatPromptValue å’Œ list[messages] ä»¥åŠå­—ç¬¦ä¸²ä¹‹é—´çš„è½¬åŒ–

  ```python
  response_messages=response.to_messages() # å°†ChatPromptValueè½¬åŒ–ä¸ºlist<Messages>
  response_messages=response.to_string()
  ```

* MessagesPlaceholder

  ä½œç”¨æ˜¯æ’å…¥èŠå¤©å†å²ï¼Œç›®å‰ä¸çŸ¥é“æ¶ˆæ¯çš„è§’è‰²ï¼Œéœ€è¦åæœŸè°ƒç”¨æ‰çŸ¥é“
  
  ```python
  # In addition to Human/AI/Tool/Function messages,
  # you can initialize the template with a MessagesPlaceholder
  # either using the class directly or with the shorthand tuple syntax:
  
  template = ChatPromptTemplate([
      ("system", "You are a helpful AI bot."),
      # Means the template will receive an optional list of messages under
      # the "conversation" key
      ("placeholder", "{conversation}")
      # Equivalently:
      # MessagesPlaceholder(variable_name="conversation", optional=True)
  ])
  
  prompt_value = template.invoke(
      {
          "conversation": [
              ("human", "Hi!"),
              ("ai", "How can I assist you today?"),
              ("human", "Can you make me an ice cream sundae?"),
              ("ai", "No.")
          ]
      }
  )
  
  # Output:
  # ChatPromptValue(
  #    messages=[
  #        SystemMessage(content='You are a helpful AI bot.'),
  #        HumanMessage(content='Hi!'),
  #        AIMessage(content='How can I assist you today?'),
  #        HumanMessage(content='Can you make me an ice cream sundae?'),
  #        AIMessage(content='No.'),
  #    ]
  #)
  ```

* FewShotPromptTemplateï¼ˆä¸Templateä¸€èµ·ï¼Œéå¯¹è¯å¼ï¼‰ã€FewShotChatMessagePromptTemplateï¼ˆä¸ChatPromptTemplateä¸€èµ·ï¼Œå¯¹è¯å¼ï¼‰ã€Example selectorsï¼ˆç¤ºä¾‹é€‰æ‹©å™¨ï¼Œç±»ä¼¼äºragï¼‰

  ```python
  from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate
  
  examples = [
      {"text": "I love programming in Python", "translation": "J'aime programmer en Python"},
      {"text": "I love programming in Python", "translation": "J'aime programmer en Python"},
  ]
  
  example_prompt = PromptTemplate(
      input_variables=["text", "translation"],
      template="""
      Text: {text}
      Translation: {translation}
      """
  )
  
  prompt = FewShotPromptTemplate(
      examples=examples, # è¯´æ˜ç¤ºä¾‹
      example_prompt=example_prompt, # è¯´æ˜ç¤ºä¾‹çš„æ ¼å¼
      suffix="Translate the following text from English to French: {text}",
      input_variables=["text"],
  )
  
  response = prompt.invoke({"text":"I love langchian"})
  ```

  ```python
  from langchain_core.prompts import FewShotChatMessagePromptTemplate
  
  examples = [
      {"text": "I love programming in Python", "translation": "J'aime programmer en Python"},
  ]
  
  example_prompt = ChatPromptTemplate.from_messages(
      [
          ("human", "{text}"),
          ("ai", "{translation}"),
      ]
  )
  
  few_shot_prompt = FewShotChatMessagePromptTemplate(
      examples=examples,
      example_prompt=example_prompt,
  )
  
  final_prompt = ChatPromptTemplate.from_messages(
      [
          ("system", "You are a helpful trasnslation assistant"),
          few_shot_prompt,
          ("human", "{text}"),
      ]
  )
  
  response = final_prompt.invoke({"text":"I love langchian"})
  
  model.invoke(response).content
  ```

  

#### 4. Output parsers

å°†å¤§æ¨¡å‹è¾“å‡ºçš„å†…å®¹æ ¼å¼åŒ–è¾“å‡ºåˆ†ä¸ºä¸¤æ­¥Schema definitionå’ŒReturning structured output

```python
from pydantic import BaseModel, Field

# æ–¹æ³•1ï¼šä½¿ç”¨Pydanticæ¨¡å‹å®šä¹‰schemaï¼ˆæ¨èï¼‰
class CapitalInfo(BaseModel):
    capital: str = Field(description="The capital city name")
    country: str = Field(description="The country name")
    population: int = Field(description="Approximate population of the capital")
```

ç»“æ„åŒ–è¾“å‡º

```python
model_with_structure = model.with_structured_output(CapitalInfo)
structured_output = model_with_structure.invoke(user_input)
print("\nâœ… Pydanticæ¨¡å‹è¾“å‡º:")
print(f"  é¦–éƒ½: {structured_output.capital}")
print(f"  å›½å®¶: {structured_output.country}")
print(f"  äººå£: {structured_output.population}")
```

ä¹Ÿå¯ä»¥å†™æˆæŒ‡ä»¤å½¢å¼æ”¾åœ¨prompté‡Œé¢

```python
from langchain.output_parsers import ResponseSchema, StructuredOutputParser
capital_schema = ResponseShema(name = "capital", description = "Was this a gift? Answer true if yes, false if no")
response_schemas = [capital_schema]
output_parser = StructuredOutputParser.from_response_schemas(response_schemas)
format_instructions = output_parser.get_format_instructions(response_schemas)
```

* StrOutputParse

```python
from langchain_core.output_parsers import StrOutputParser

output_parser = StrOutputParser()
chain = llm | output_parser
result = chain.invoke("Tell me a joke")
print(result)
```

* JsonOutputParser

  éœ€è¦åœ¨æç¤ºè¯ä¸­è¯´æ˜æ¸…æ¥š

```python
prompt = chat_prompt_template.invoke(input={"role":"äººå·¥æ™ºèƒ½ä¸“å®¶","question":"äººå·¥æ™ºèƒ½ç”¨ç”¨è‹±è¯­æ€ä¹ˆè¯´ï¼Œé—®é¢˜ç”¨qè¡¨ç¤ºï¼Œç­”æ¡ˆç”¨aè¡¨ç¤ºï¼Œè¿”å›ä¸€ä¸ªjsonæ ¼å¼çš„æ•°æ®"})
parser = JsonOutputParser()
json_result = parser.invoke(response)
```

è¿™ç§æ–¹å¼å°±æ˜¯æŒ‡å®šæ ¼å¼ä¸ºjsonï¼Œä¸éœ€è¦å†æç¤ºè¯å†™äº†

```python
parser = JsonOutputParser()
prompt_template = PromptTemplate.from_template(
	template = "å›ç­”ç”¨æˆ·çš„æŸ¥è¯¢\n æ»¡è¶³çš„æ ¼å¼ä¸º{format_instructions}\n é—®é¢˜ä¸º{question}\n"ï¼Œ
    partial_variables={"format_instructions":parser.get_format_instructions}
)
```

#### 5. memory

ç”±äºllmæœ¬èº«æ˜¯æ— çŠ¶æ€çš„ï¼Œæ‰€ä»¥éœ€è¦è®°å¿†æ¥ç®¡ç†

* è‡ªå®šä¹‰å®ç°

åœ¨ prompt ä¸­åŠ å…¥ä¿¡æ¯

```python
prompt_template.messages.append(AIMessage(content=response.content))
```

* ChatMessageHistory

```python
from langchain.memory import ChatMessageHistroy

history = ChatMessageHistory()
history.add_user_message("ä½ å¥½")
history.add_ai_message("å¾ˆé«˜å…´è®¤è¯†ä½ ")

print(history.messages)

response = llm.invoke(history.messages)
```

* ConversationBufferMemory

chain é‡Œé¢æœ‰keyå€¼ historyï¼Œæ‰€ä»¥æç¤ºè¯ä¸­ä¹Ÿæœ‰ history 

```python
from langchain.chains import LLMChain
from langchain.memory import ConversationBufferMemory

prompt_template = PromptTemplate.from_template(
	template = """
		ä½ å¯ä»¥ä¸äººç±»å¯¹è¯ã€‚
		å½“å‰å¯¹è¯å†å²ï¼š{history}
		äººç±»é—®é¢˜ï¼š{question}
		å›å¤ï¼š
	"""
)

memory = ConversationBufferMemory()

chain = LLMChain(
    llm=model,
    prompt=prompt_template
    memory=memory,
    verbose=True,
)

chain.invoke(
	{"question":"ä½ å¥½ï¼Œæˆ‘çš„åå­—å«å°æ˜"}
)
# è¾“å‡ºå½“å‰è®°å¿†çš„ä¸‰ä¸ªæ–¹æ³•
print(memory.chat_memory.messages)
print(memory.buffer)
memory.load_memory_variables({})
```

```python
memory.save_context(inputs = {"input":"What's your name"}, outputs = {"output":"My name is John"})
memory.load_memory_variables({})
```

* ConversationBufferWindowMemory

```python
from langchain.memory import ConversationBufferWindowMemory

# æŒ‡å®šä¿ç•™çš„å¯¹è¯æ•°é‡
memory = ConversationBufferWindowMemory(k=2)

memory.chat_memory.add_user_message("Hi, my name is Andrew")
memory.chat_memory.add_ai_message("Hello, Andrew. How may I help you today?")

print(memory.chat_memory.messages)
```

* ConversationTokenBufferMemory

```python
from langchain.memory import ConversationTokenBufferMemory

# æŒ‡å®šä¿ç•™çš„tokenæ•°é‡
memory = ConversationTokenBufferMemory(llm=model, max_token_limit=1000)

memory.chat_memory.add_user_message("Hi, my name is Andrew")
memory.chat_memory.add_ai_message("Hello, Andrew. How may I help you today?")

print(memory.chat_memory.messages)
```

* ConversationSummaryMemory

```python
from langchain.memory import ConversationSummaryMemory

# æ¦‚æ‹¬ä¸Šä¸‹é—®å¯¹è¯
memory = ConversationSummaryMemory(llm=model, max_token_limit=1000)

memory.chat_memory.add_user_message("Hi, my name is Andrew")
memory.chat_memory.add_ai_message("Hello, Andrew. How may I help you today?")

print(memory.chat_memory.messages)
```

* ConversationSummaryBufferMemory

```python
from langchain.memory import ConversationSummaryBufferMemory
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.chains.llm import LLMChain

model = ChatGoogleGenerativeAI(model = "gemini-2.5-flash")

prompt_template = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ç”µå•†å®¢æœåŠ©æ‰‹ï¼Œç”¨ä¸­æ–‡å‹å¥½å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œä¿æŒä¸“ä¸šä¸”äº²åˆ‡çš„è¯­æ°”ã€‚"),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{input}")
]) 

memory = ConversationSummaryBufferMemory(
    llm = model,
    max_token_limit=400,
    memory_key="chat_history",
    return_messages=True
)

chain = LLMChain(
    llm = model, 
    prompt = prompt_template,
    memory = memory
)

dialogue = [
    ("ä½ å¥½ï¼Œæˆ‘æƒ³æŸ¥è¯¢12345è®¢å•çš„çŠ¶æ€", None),
    ("è¿™ä¸ªè®¢å•æ˜¯ä¸Šå‘¨äº”ä¸‹çš„", None)
]

for user_input, _ in dialogue:
    response = chain.invoke({"input":user_input})
    print(f"å®¢æˆ·ï¼š{user_input}")
    print(f"å®¢æœï¼š{response['text']}\n")
```



 è®°å¿†åˆ†ä¸ºä¸¤ç§ï¼šçŸ­æœŸè®°å¿†å’Œé•¿æœŸè®°å¿†

![image-20250726155638516](assets/image-20250726155638516.png)

#### 6. chain

åœ¨ç®¡é“è¿æ¥ç¬¦ä¹‹å‰æ˜¯promptï¼Œllmï¼Œparseréƒ½éœ€è¦invokeæ–¹æ³•å®ç°ï¼Œä»–ä»¬éƒ½ç»§æ‰¿äº†æŠ½è±¡åŸºç±»Runnable

* LLMChain

  ```python
  from langchain.chains import LLMChain
  
  prompt = ChatPromptTemplate.from_messages([
      ("system", "You are a helpful assistant."),
      ("user", "{input}"),
  ])
  
  # chain = prompt | model
  
  chain = LLMChain(llm=model, prompt=prompt)
  
  chain.invoke({"input": "What is the capital of France?"})
  ```

* SimpleSequentialChainï¼šå•é“¾å¼è°ƒç”¨

  ![image-20250727161749291](assets/image-20250727161749291.png)

  ```python
  from langchain.chains import SimpleSequentialChain
  first_prompt = ChatPromptTemplate.from_messages([
      ("system", "You are a helpful assistant."),
      ("user", "{input}"),
  ])
  
  second_prompt = ChatPromptTemplate.from_messages([
      ("system", "You are a helpful assistant."),
      ("user", "translate the text to Chinese:{text}"),
  ])
  
  # chain = first_prompt | model | second_prompt | model
  
  chain_one = LLMChain(llm=model, prompt=first_prompt)
  chain_two = LLMChain(llm=model, prompt=second_prompt)
  
  chain = SimpleSequentialChain(
      chains=[chain_one, chain_two]ï¼Œ
    	verbose=True
  )
  
  chain.invoke({"input": "What is the capital of France?"})
  ```

* SequentialChain

  è¦åœ¨SequentialChainä¸­è¯´æ¸…æ¥šè¾“å…¥å˜é‡å’Œè¾“å‡ºå˜é‡

  ```python
  translate_prompt = PromptTemplate(
      input_variables=["text"],
      template="å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘æˆè‹±æ–‡ï¼š{text}"
  )
  
  summarize_prompt = PromptTemplate(
      input_variables=["translated_text"],
      template="ç”¨ä¸€å¥è¯æ€»ç»“ä»¥ä¸‹æ–‡æœ¬ï¼š{translated_text}"
  )
  
  sentiment_prompt = PromptTemplate(
      input_variables=["summary"],
      template="åˆ†æä»¥ä¸‹æ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘ï¼ˆç§¯æ/æ¶ˆæ/ä¸­æ€§ï¼‰ï¼š{summary}"
  )
  
  # åˆ›å»ºLLMChain
  translate_chain = LLMChain(llm=model, prompt=translate_prompt, output_key="translated_text")
  summarize_chain = LLMChain(llm=model, prompt=summarize_prompt, output_key="summary")
  sentiment_chain = LLMChain(llm=model, prompt=sentiment_prompt, output_key="sentiment")
  
  # åˆ›å»ºSequentialChain
  simple_chain = SequentialChain(
      chains=[translate_chain, summarize_chain, sentiment_chain],
      input_variables=["text"],
      output_variables=["translated_text", "summary", "sentiment"]
  )
  
  # æ‰§è¡Œ
  result = simple_chain.invoke({
      "text": "ä»Šå¤©å¤©æ°”çœŸå¥½ï¼Œé˜³å…‰æ˜åªšï¼Œå¿ƒæƒ…æ„‰å¿«ï¼"
  })
  
  print("\nğŸ”— ç®€å•SequentialChainç¤ºä¾‹")
  print("=" * 30)
  print(f"åŸæ–‡ï¼š{result['text']}")
  print(f"ç¿»è¯‘ï¼š{result['translated_text']}")
  print(f"æ€»ç»“ï¼š{result['summary']}")
  print(f"æƒ…æ„Ÿï¼š{result['sentiment']}")
  ```

* RouterChainï¼šè·¯ç”±é€‰æ‹©

  ![image-20250727161837260](assets/image-20250727161837260.png)

  ```python
  from langchain.chains.router import MultiPromptChain
  from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser
  from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE
  # æ•°å­¦é—®é¢˜å¤„ç†æ¨¡æ¿
  math_template = """
  ä½ æ˜¯ä¸€ä¸ªæ•°å­¦ä¸“å®¶ã€‚è¯·è§£å†³ä»¥ä¸‹æ•°å­¦é—®é¢˜ï¼Œæä¾›è¯¦ç»†çš„æ­¥éª¤å’Œè§£é‡Šï¼š
  
  é—®é¢˜ï¼š{input}
  
  è¯·æä¾›ï¼š
  1. è¯¦ç»†çš„è§£é¢˜æ­¥éª¤
  2. ç›¸å…³çš„æ•°å­¦æ¦‚å¿µ
  3. éªŒè¯ç­”æ¡ˆ
  4. ç±»ä¼¼é—®é¢˜çš„è§£æ³•
  """
  
  # ç¼–ç¨‹é—®é¢˜å¤„ç†æ¨¡æ¿
  programming_template = """
  ä½ æ˜¯ä¸€ä¸ªç¼–ç¨‹ä¸“å®¶ã€‚è¯·å¸®åŠ©è§£å†³ä»¥ä¸‹ç¼–ç¨‹ç›¸å…³é—®é¢˜ï¼š
  
  é—®é¢˜ï¼š{input}
  
  è¯·æä¾›ï¼š
  1. è¯¦ç»†çš„è§£å†³æ–¹æ¡ˆ
  2. ç¤ºä¾‹ä»£ç 
  3. æœ€ä½³å®è·µå»ºè®®
  4. å¯èƒ½çš„é™·é˜±å’Œæ³¨æ„äº‹é¡¹
  """
  
  # 3. åˆ›å»ºç›®æ ‡ä¿¡æ¯ï¼ˆç”¨äºè·¯ç”±é€‰æ‹©ï¼‰
  prompt_infos = [
      {
          "name": "math",
          "description": "é€‚åˆå›ç­”æ•°å­¦ã€ç®—æœ¯ã€å‡ ä½•ã€ä»£æ•°ã€å¾®ç§¯åˆ†ç­‰æ•°å­¦ç›¸å…³é—®é¢˜",
          "prompt_template": math_template
      },
      {
          "name": "programming", 
          "description": "é€‚åˆå›ç­”ç¼–ç¨‹ã€ä»£ç ã€ç®—æ³•ã€è½¯ä»¶å¼€å‘ã€è°ƒè¯•ç­‰ç¼–ç¨‹ç›¸å…³é—®é¢˜",
          "prompt_template": programming_template
      }
  ]
  
  # 4. åˆ›å»ºç›®æ ‡é“¾
  print("ğŸ”— åˆ›å»ºä¸“é—¨çš„å¤„ç†é“¾...")
  
  destination_chains = {}
  for p_info in prompt_infos:
      name = p_info["name"]
      prompt_template = p_info["prompt_template"]
      prompt = PromptTemplate(template=prompt_template, input_variables=["input"])
      chain = LLMChain(llm=model, prompt=prompt)
      destination_chains[name] = chain
  
  # 5. åˆ›å»ºé»˜è®¤é“¾ï¼ˆå¤„ç†æ— æ³•åˆ†ç±»çš„é—®é¢˜ï¼‰
  default_prompt = PromptTemplate(
      template="""
      è¯·å›ç­”ä»¥ä¸‹é—®é¢˜ã€‚å¦‚æœä½ ä¸ç¡®å®šå¦‚ä½•åˆ†ç±»è¿™ä¸ªé—®é¢˜ï¼Œè¯·å°½åŠ›æä¾›æœ‰å¸®åŠ©çš„ç­”æ¡ˆï¼š
  
      é—®é¢˜ï¼š{input}
  
      è¯·æä¾›è¯¦ç»†å’Œæœ‰ç”¨çš„å›ç­”ã€‚
      """,
      input_variables=["input"]
  )
  default_chain = LLMChain(llm=model, prompt=default_prompt)
  
  # 6. åˆ›å»ºè·¯ç”±å™¨æ¨¡æ¿
  destinations = [f"{p['name']}: {p['description']}" for p in prompt_infos]
  destinations_str = "\n".join(destinations)
  
  router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(
      destinations=destinations_str
  )
  
  router_prompt = PromptTemplate(
      template=router_template,
      input_variables=["input"],
      output_parser=RouterOutputParser(),
  )
  
  # 7. åˆ›å»ºè·¯ç”±å™¨é“¾
  print("ğŸ¯ åˆ›å»ºè·¯ç”±å™¨é“¾...")
  
  router_chain = LLMRouterChain.from_llm(model, router_prompt)
  
  # 8. åˆ›å»ºMultiPromptChain
  print("ğŸ”€ åˆ›å»ºMultiPromptChain...")
  
  chain = MultiPromptChain(
      router_chain=router_chain,
      destination_chains=destination_chains,
      default_chain=default_chain,
      verbose=True
  )
  ```

* `create_sql_query_chain`

  å°†è‡ªç„¶è¯­è¨€è½¬åŒ–ä¸º `sql` ï¼Œå‚æ•°ååªèƒ½ä¸º $question$

  ```python
  llm = ChatOpenAI(model="gpt-3.5-turbo", tempreture=0)
  
  chain = create_sql_query_chain(llm, db)
  response = chain.invoke("quesetion":"how many employees are there")
  ```

* `create_stuff_documents_chain`

å°†å¤šä¸ªæ–‡æ¡£åˆå¹¶æˆå•ä¸ªé•¿æ–‡æœ¬çš„é“¾å¼å·¥å…·

```python
prompt_template=PromptTemplate.from_template("å¦‚ä¸‹æ–‡æ¡£{docs}æ‰€è¯´ï¼Œé¦™è•‰æ˜¯ä»€ä¹ˆ")
llm = ChatOpenAI(model="gpt-3.5-turbo")
chain=create_stuff_documents_chain(llm,prompt,document_variable_name="docs")

docs = [
  Document(
  	page_content=""
  ),
  Document(
  	page_content=""
  )
]

chain.invoke("docs":docs)
```



#### 7. åŸºäºæ–‡æ¡£çš„é—®ç­”

é€šè¿‡ $embedding$ åµŒå…¥å‘é‡ï¼Œå¹¶è®¡ç®—å‘é‡ä¹‹é—´çš„ç›¸ä¼¼ç¨‹åº¦

![image-20250728102027152](assets/image-20250728102027152.png)

```python
from langchain.chains import RetrievalQA
from langchain.document_loaders import CSVLoader
from langchain.indexes import VectorstoreIndexCreator
from langchain.vectorstores import DocArrayInMemorySearch

# åŠ è½½æ–‡æ¡£
loader = CSVLoader(file_path="data/titanic.csv")

# åˆ›å»ºç´¢å¼•ï¼ˆéœ€è¦æŒ‡å®šå‘é‡æ•°æ®åº“ç±»å‹ä»¥åŠæ–‡æ¡£ï¼‰
index = VectorstoreIndexCreator(
    vectorstore_cls=DocArrayInMemorySearch,
).from_loaders([loader])

query = "Please list all your shirts"

response = index.query(query)

display(Markdown(response))
```

æ–¹æ³•äºŒï¼š

```python
loader = CSVLoader(file_path = file)

docs = loader.load()

from langchain.embeddings import OpenAIEmbeddings

# å¼•å…¥å‘é‡åµŒå…¥æ–¹æ³•
embeddings = OpenAIEmbeddings()

embed = embeddings.embed_query(docs[0].page_content)

print(len(embed))

print(embed[:5])

print(embeddings.embed_query("Hi my name is Andrew and I live in California"))

# æ„å»ºå‘é‡æ•°æ®åº“
db = DocArrayInMemorySearch.from_documents(
    docs,
    embeddings
)

query = "Please list all your shirts"

docs = db.similarity_search(query)

print(docs[0].page_content)

# æ„å»ºæ£€ç´¢å™¨
retriever = db.as_retriever()

qdocs = "".join([docs[i].page_content for i in range(len(docs))])

response = model.invoke(f"{qdocs}\n\nPlease answer the question: {query}")

qa_stuff = RetrievalQA.from_chain_type(
    llm=model,
    chain_type="stuff",
    retriever=retriever
)

qa_stuff.invoke({"question": query})
```

![image-20250728102800185](assets/image-20250728102800185.png)

![image-20250728102849115](assets/image-20250728102849115.png)

#### 8. è‡ªåŠ¨è¯„ä¼°

æ ¹æ®æ–‡æ¡£å†…å®¹å’Œå¤§æ¨¡å‹çš„é¢„æµ‹è¾“å‡ºè¿›è¡Œæ¯”è¾ƒï¼Œçœ‹çœ‹å¤§æ¨¡å‹æ˜¯å¦æ­£ç¡®

```python
from langchain.evaluation.qa import QAGenerateChain

langchain.debug = True

example_chain = QAGenerateChain.from_llm(llm=model)

# dataæ˜¯csvæ–‡ä»¶
examples = example_chain.apply_and_parse(
    [{"doc" : t} for t in data[:5]]
)

index = VectorstoreIndexCreator(
    vectorstore_cls=DocArrayInMemorySearch,
).from_loaders([loader])

qa = RetrievalQA.from_chain_type(
    llm=model,
    chain_type="stuff",
    retriever=index.vectorstore.as_retriever(),
    verbose=True,
    chain_type_kwargs = {
        "document_separator": "<<<<>>>>>"
    }
)

predictions = qa.invoke(examples)

from langchain.evaluation.qa import QAGenerateChain
eval_chain = QAGenerateChain.from_llm(llm=model)

graded_outputs = eval_chain.evaluate(examples, predictions)

for i,eg in enumerate(examples):
    print(f"Example {i}:")
    print("Question: ", eg["question"])
    print("Real Answer: ", eg["answer"])
    print("Predicted Answer: ", predictions[i]["answer"])
    print("Predicted Answer (no sources): ", predictions[i]["answer_without_sources"])
    print("Grade: ", graded_outputs[i]["text"])
    print("\n")
```

#### 9. agent

* ä¸»è¦æ­¥éª¤

ä¸»è¦æ­¥éª¤åˆ†ä¸ºä¸¤æ­¥ï¼Œä¸€ä¸ªæ˜¯åˆ›å»º $Agent$ ï¼Œé€šè¿‡ $AgentType$ æˆ–è€… $create\_xxx\_agent$ æŒ‡å®šï¼Œåˆ›å»º $AgentExecutor$ ï¼Œé€šè¿‡ $initialize\_agent$ æˆ–è€… $AgentExecutor$ æ„é€ æ–¹æ³•ï¼Œä¹‹åå°±æ‰§è¡Œå³å¯ï¼Œåˆ©ç”¨invoke

```python
from langchain_community.agent_toolkits.load_tools import load_tools
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain.agents import AgentExecutor, create_react_agent
from langchain.agents import AgentType
tools = load_tools(["llm-math", "wikipedia"], llm=llm)
# åˆ›å»ºæç¤ºæ¨¡æ¿
prompt = ChatPromptTemplate.from_messages([
    ("system", """ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ï¼Œå¯ä»¥ä½¿ç”¨å·¥å…·æ¥å›ç­”é—®é¢˜ã€‚

ä½ æœ‰ä»¥ä¸‹å·¥å…·å¯ç”¨:
{tools}

ä½¿ç”¨ä»¥ä¸‹æ ¼å¼:

Question: ç”¨æˆ·çš„é—®é¢˜
Thought: æˆ‘éœ€è¦æ€è€ƒå¦‚ä½•è§£å†³è¿™ä¸ªé—®é¢˜
Action: è¦ä½¿ç”¨çš„å·¥å…·åç§°
Action Input: å·¥å…·çš„è¾“å…¥
Observation: å·¥å…·çš„è¾“å‡ºç»“æœ
... (è¿™ä¸ª Thought/Action/Action Input/Observation å¯ä»¥é‡å¤Næ¬¡)
Thought: æˆ‘ç°åœ¨çŸ¥é“æœ€ç»ˆç­”æ¡ˆäº†
Final Answer: æœ€ç»ˆç­”æ¡ˆ

é‡è¦æç¤º:
- æ€»æ˜¯å…ˆæ€è€ƒå†è¡ŒåŠ¨
- å¦‚æœéœ€è¦è®¡ç®—ï¼Œä½¿ç”¨Calculatorå·¥å…·
- å¦‚æœéœ€è¦æŸ¥æ‰¾ä¿¡æ¯ï¼Œä½¿ç”¨Searchå·¥å…·
- ç»™å‡ºæ¸…æ™°ã€æœ‰ç”¨çš„æœ€ç»ˆç­”æ¡ˆ"""),
    ("human", "{input}"),
    ("assistant", "{agent_scratchpad}")
])

# ä¿®å¤: prompt éœ€è¦ tool_names å˜é‡
tool_names = ", ".join([tool.name for tool in tools])
prompt = prompt.partial(tool_names=tool_names)

# åˆ›å»ºReAct agent
agent = create_react_agent(llm, tools, prompt)

# åˆ›å»ºagent executor
agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    verbose=True,
    max_iterations=5,
    early_stopping_method="generate"
)

result = agent_executor.invoke({
    "input": "è®¡ç®— (25 + 15) * 2 - 10 çš„ç»“æœ"
})
```

* æ–¹å¼ä¸€é€šè¿‡æ„é€ å‡½æ•°å’Œæšä¸¾ç±»å‹è°ƒç”¨

PythonREPLTool æ˜¯ LangChain æä¾›çš„ä¸€ä¸ªPythonä»£ç æ‰§è¡Œå·¥å…·ï¼Œå®ƒå…è®¸ AI Agent åœ¨è¿è¡Œæ—¶æ‰§è¡Œ Python ä»£ç 

```python
from langchain_experimental.tools import PythonREPLTool
from langchain.agents import initialize_agent, AgentType

agent_executor = initialize_agent(
    tools=[PythonREPLTool()],
    llm=llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    # agent = AgentType.OPENAI_FUNCTIONS,
    verbose=True,
)

result = agent_executor.invoke({"ç”¨pythonå†™ä¸€ä¸ªè®¡ç®—å™¨"})
```

* æ–¹å¼äºŒï¼šæ ¹æ®createè°ƒç”¨

agent_scratchpad æ˜¯å¿…é¡»æœ‰çš„å‚æ•°ï¼Œç”¨æ¥ä¿å­˜é“¾å¼è°ƒç”¨çš„ä¿¡æ¯

éœ€è¦æç¤ºè¯æ¨¡æ¿ï¼Œæç¤ºè¯å¿…é¡»å’Œå·¥å…·åå­—å¯¹åº”ï¼Œä¹Ÿå¯ä»¥å†™ä¸º {tool} åé¢æä¾›

å¦‚æœæ˜¯ `create_react_agent` åˆ™å¿…é¡»æ˜¯è¦æ»¡è¶³æç¤ºè¯ä¸­å«æœ‰ tools å’Œ tool_names

```python
prompt = ChatPromptTemplate.from_messages([
    ("system","ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„aiåŠ©æ‰‹ï¼Œæ ¹æ®ç”¨æˆ·çš„æé—®ï¼Œå¿…è¦æ—¶è°ƒç”¨searchå·¥å…·ï¼Œä½¿ç”¨äº’è”ç½‘æ£€ç´¢ä¿¡æ¯"),
    ("human","{input}")ï¼Œ
    ("system","{agent_scratchpad}")
])

agent = create_tool_calling_agent(
	llm=llm, 
    prompt=prompt,
    tools=[search_tool]
)

agent_excutor = AgentExcutor(
	agent = agent,
    tools = [search_tool]
)

agent_excutor.invoke({"input":"æŸ¥è¯¢åŒ—äº¬ä»Šå¤©çš„å¤©æ°”æƒ…å†µ"})
```

* å¸¦è®°å¿†æ¨¡å—çš„agent

å¿…é¡»è¦æœ‰ memory_keyï¼Œä¸”å€¼ä¸º chat_historyï¼Œè¿™ä¸ªæ˜¯å’Œæç¤ºè¯æ¨¡æ¿åŒ¹é…ï¼Œå¯¹äºæšä¸¾ç±»å‹è°ƒç”¨æç¤ºè¯æ¨¡æ¿æ˜¯å†™å¥½çš„

```python
agent = AgentType.CONVERSATIONAL_REACT_DESCRIPTION
memory = ConversationBufferMemory(
	return_messages=True,
    memory_key="chat_history"
)

agent_executor = initalize_agent(
	tools = [search_tool],
    llm = llm, 
    agent = agent,
    verbose = True,
    memory = memory
)

result = agent_executor
```

#### 10. CallbackHandler

##### 10.1 CallbackHandler æ˜¯ä»€ä¹ˆ

å®ƒå°±æ˜¯ä¸€ä¸ª **é’©å­æœºåˆ¶ (Hook)**ã€‚

- Agentã€LLMã€Tool åœ¨è¿è¡Œè¿‡ç¨‹ä¸­çš„å…³é”®èŠ‚ç‚¹ï¼Œä¼šä¸»åŠ¨è§¦å‘å›è°ƒã€‚
- ä½ å¯ä»¥åœ¨è¿™äº›å›è°ƒé‡Œè®°å½•æ—¥å¿—ã€åšå¯è§†åŒ–ã€å®ç°æµå¼è¾“å‡ºï¼Œç”šè‡³æ”¹å˜é€»è¾‘ã€‚

ç›¸å½“äºï¼š

> **Agent/LLM/Tool = ä¸»æµç¨‹**
>  **CallbackHandler = æ—è§‚è€…ï¼Œç›‘å¬+è®°å½•+å¹²é¢„**

------

##### 10.2 å¸¸è§çš„å›è°ƒæ–¹æ³•

$LangChain$ å®šä¹‰äº†å¾ˆå¤šäº‹ä»¶ç‚¹ï¼Œä½ çš„ `StreamingCallbackHandler` è¦†ç›–äº†å…¶ä¸­çš„éƒ¨åˆ†ã€‚

| æ–¹æ³•               | ä»€ä¹ˆæ—¶å€™è°ƒç”¨                        | ä½ ä»£ç é‡Œåšäº†ä»€ä¹ˆ                                 |
| ------------------ | ----------------------------------- | ------------------------------------------------ |
| `on_chain_start`   | æ•´ä¸ªé“¾/Agent å¼€å§‹æ‰§è¡Œæ—¶             | æ²¡å†™ï¼Œä½†å¯ä»¥åšæ—¥å¿—åˆå§‹åŒ–                         |
| `on_chain_end`     | é“¾/Agent æ‰§è¡Œå®Œæ¯•æ—¶                 | æ²¡å†™ï¼Œä½†å¯ä»¥æ”¶å°¾                                 |
| `on_llm_start`     | LLM å¼€å§‹æ¨ç†æ—¶                      | æ²¡å†™ï¼ˆä½ å¯ä»¥åŠ æ—¥å¿—ï¼‰                             |
| `on_llm_new_token` | LLM ç”Ÿæˆä¸€ä¸ª token æ—¶               | ä½ æŠŠ token æ”¾è¿› `queue`ï¼Œå®ç°äº† **æµå¼è¾“å‡º**     |
| `on_llm_end`       | LLM è¾“å‡ºå®Œæˆæ—¶                      | æ²¡å†™                                             |
| `on_agent_action`  | LLM äº§å‡º Actionï¼ˆè¦è°ƒç”¨æŸä¸ªå·¥å…·ï¼‰æ—¶ | ä½ æå–ä»£ç å—ï¼Œç”Ÿæˆä¸´æ—¶æ–‡ä»¶åï¼Œå­˜åˆ° `steps`       |
| `on_tool_start`    | å·¥å…·å¼€å§‹æ‰§è¡Œæ—¶                      | ä½ æ²¡å†™ï¼ˆå¯åŠ ä¸Šï¼‰                                 |
| `on_tool_end`      | å·¥å…·æ‰§è¡Œå®Œæˆæ—¶                      | ä½ è®°å½•äº† `execution_logs`                        |
| `on_agent_finish`  | Agent ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆæ—¶                | ä½ è°ƒç”¨äº† `FinalAnswerCheck` æ£€æŸ¥ï¼Œå¹¶å­˜å‚¨æœ€ç»ˆç»“æœ |

------

##### 10.3 æ‰§è¡Œè¿‡ç¨‹é‡Œçš„è°ƒç”¨é¡ºåº

1. **ç”¨æˆ·è¾“å…¥ â†’ Agent**
   - `on_chain_start`
2. **Agent è°ƒç”¨ LLM å†³å®šä¸‹ä¸€æ­¥**
   - `on_llm_start`
   - `on_llm_new_token`ï¼ˆå¤šæ¬¡è§¦å‘ï¼Œæµå¼è¾“å‡ºï¼‰
   - `on_llm_end`
3. **LLM å†³å®šè°ƒç”¨å·¥å…·**
   - `on_agent_action`
   - `on_tool_start`ï¼ˆå¦‚æœå®ç°äº†ï¼‰
   - `on_tool_end`
4. **å·¥å…·ç»“æœå†é€å› LLM**
   - åˆè§¦å‘ä¸€æ¬¡ `on_llm_start` â†’ `on_llm_new_token` â†’ `on_llm_end`
5. **LLM æœ€ç»ˆäº§å‡ºç»“æœ**
   - `on_agent_finish`
   - `on_chain_end`

```mermaid
sequenceDiagram
    participant User as ç”¨æˆ·
    participant Agent as Agent
    participant LLM as LLM
    participant Tool as å·¥å…·
    participant Callback as CallbackHandler

    User ->> Agent: è¾“å…¥é—®é¢˜
    Callback ->> Agent: on_chain_start
    
    Agent ->> LLM: è¯·æ±‚æ€è€ƒä¸‹ä¸€æ­¥
    Callback ->> LLM: on_llm_start
    LLM ->> Callback: on_llm_new_token* (æµå¼è¾“å‡º)
    LLM ->> Callback: on_llm_end
    
    LLM ->> Agent: äº§å‡ºAction (tool + input)
    Callback ->> Agent: on_agent_action

    Agent ->> Tool: è°ƒç”¨å·¥å…· (æ‰§è¡Œä»£ç /æ“ä½œ)
    Callback ->> Tool: on_tool_start (å¯é€‰)
    Tool -->> Agent: è¿”å›æ‰§è¡Œç»“æœ
    Callback ->> Agent: on_tool_end

    Agent ->> LLM: å†æ¬¡è¯·æ±‚ (ç»“åˆå·¥å…·è¾“å‡º)
    Callback ->> LLM: on_llm_start
    LLM ->> Callback: on_llm_new_token* (æµå¼è¾“å‡º)
    LLM ->> Callback: on_llm_end

    LLM ->> Agent: è¾“å‡ºæœ€ç»ˆç­”æ¡ˆ
    Callback ->> Agent: on_agent_finish

    Agent -->> User: è¿”å›æœ€ç»ˆç»“æœ
    Callback ->> Agent: on_chain_end
```

#### 11. tool

åŒ…å« $name$ å·¥å…·åï¼Œ$description$ æè¿°ï¼Œå·¥å…·è¾“å…¥çš„ $json$ æ¨¡å¼ï¼Œè¦è°ƒç”¨çš„å‡½æ•°ï¼Œ$return\_direct$ ä»…å¯¹ $Agent$ ç›¸å…³ï¼Œå½“ä¸º $True$ æ—¶ï¼Œåœ¨è°ƒç”¨ç»™å®šå·¥å…·åï¼Œå°†åœæ­¢å¹¶å°†ç»“æœç›´æ¥è¿”å›ç»™ç”¨æˆ·

æ­¥éª¤ä¸»è¦æ˜¯å°† $name$ å’Œ $description$ å’Œ $json$ æ¨¡å¼ä½œä¸ºä¸Šä¸‹æ–‡æä¾›ç»™ $LLM$ï¼Œç„¶åå¤§æ¨¡å‹æ ¹æ®æç¤ºè¯æ¨æ–­å‡ºéœ€è¦è°ƒç”¨å“ªäº›å·¥å…·ï¼Œå¹¶æä¾›å…·ä½“çš„è°ƒç”¨å‚æ•°ä¿¡æ¯ï¼Œç„¶åç”¨æˆ·éœ€è¦æ ¹æ®è¿”å›çš„å·¥å…·è°ƒç”¨ä¿¡æ¯ï¼Œè‡ªè¡Œè§¦å‘ç›¸å…³å·¥å…·çš„å›è°ƒ

```python
from langchain_core.tools import tool

@tool(name_or_callable="add_two_number", description="add two numbers", return_direct=True)
def add_number(a:int, b:int) -> int:
    return a + b

add_number.invoke({"a":10, "b":20})
```

è¿˜æœ‰ $StructruredTool$ 

```python
def search_goolge(query:str):
    return "æœ€åæŸ¥è¯¢çš„ç»“æœ"

class FieldInfo(BaseModel):
    query:str = Field(description = "è¦æ£€ç´¢çš„å…³é”®è¯")

search = StructuredTool.from_function(
	func=search_google,
    name="Search",
    description="æŸ¥è¯¢è°·æ­Œæœç´¢å¼•æ“å¹¶å°†ç»“æœè¿”å›",
    args_schema=FieldInfo
)

search.invoke({"query":"ai"})
```

ä¸»è¦æ­¥éª¤ï¼Œä¸€ä¸ªæ˜¯å‘Šè¯‰å¤§æ¨¡å‹èƒ½å¤Ÿè°ƒç”¨å“ªäº›å·¥å…·ï¼Œç¬¬äºŒä¸ªæ˜¯æ‰§è¡Œå·¥å…·

```python
from dotenv import load_dotenv
from langchain_community.tools import MoveFileTool
from langchain_core.messages import HumanMessage
import os

from langchain_core.utils.function_calling import convert_to_openai_function
load_dotenv()

os.environ["GOOGLE_API_KEY"] = os.getenv("GOOGLE_API_KEY")
from langchain.memory import ConversationSummaryBufferMemory
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.chains.llm import LLMChain

model = ChatGoogleGenerativeAI(model = "gemini-2.5-flash")

tools = [MoveFileTool()]

message = [HumanMessage(content = "æˆ‘æƒ³æŠŠdata_agent/requirements.txtæ–‡ä»¶ç§»åŠ¨åˆ°code/requirements.txt")]

function = [convert_to_openai_function(tool) for tool in tools]

response = model.invoke(input=message, functions=function)

print(response)
```

```python
import json

if "function_call" in response.additional_kwargs:
    function_call = response.additional_kwargs["function_call"]
    function_name = function_call["name"]
    function_args = json.loads(function_call["arguments"])
    print(f"æ‰§è¡Œå·¥å…·ï¼š{function_name}")
    tool = next(tool for tool in tools if tool.name == function_name)
    print(f"å·¥å…·ï¼š{tool.name}")
    print(f"å·¥å…·å‚æ•°ï¼š{function_args}")
    result = tool.invoke(function_args)
    print(f"å·¥å…·æ‰§è¡Œç»“æœï¼š{result}")
    
else:
    print(f"æ¨¡å‹å›å¤ï¼š{response.content}")
```

#### 12. RAG

##### 1. æ–‡æ¡£åŠ è½½å™¨

ä½¿ç”¨åŠ è½½å™¨åï¼Œé€šè¿‡ `load` æ–¹æ³•è·å–å®ä¾‹

| æ–‡æ¡£     | æ–‡æ¡£åŠ è½½å™¨                 |
| -------- | -------------------------- |
| txt      | TextLoader                 |
| pdf      | PyPDFLoader                |
| csv      | CSVLoader                  |
| json     | JSONLoader                 |
| html     | UnStructuredHTMLLoader     |
| md       | UnSturcturedMarkdownLoader |
| æ–‡ä»¶ç›®å½• | DirectoryLoader            |

textåŠ è½½å™¨ï¼Œpdfå’Œcsvç±»ä¼¼

```python
text_loader = TextLoader(
	file_path = file_path,
    encoding="utf-8"
)

docs = text_loader.load()
```

jsonåŠ è½½å™¨

```python
json_loader = JSONLoader(
	file_path = "1.json",
    jq_schema = ".", # å–å‡ºæ‰€æœ‰æ•°æ®ï¼ŒåŠ è½½æŸä¸ªå­—æ®µ jq_schema = "./messages[].content"
    text_content = False, # å°†åŠ è½½çš„jsonå¯¹è±¡è½¬æ¢ä¸ºjsonå­—ç¬¦ä¸²
)

docs = json_loader.load()

for doc in docs:
    print(doc)
```

```python
loader = JSONLoader(
	file_path = file_path,
    jq_schema = ".data.items[]",
    content_key='.title + "\n\n" + .content',
    is_content_key_jq_parsable = True
)
```

htmlåŠ è½½å™¨ï¼Œmdç±»ä¼¼

```python
html_loader = UnStructuredHTMLLoader(
	file_path = "asset/load/05-load.html",
    mode = "elements",  # æŒ‰è¯­ä¹‰å…ƒç´ æ‹†åˆ†æˆå¤šä¸ªç‹¬ç«‹çš„å°æ–‡æ¡£ï¼Œè¿˜æœ‰paged,elements,single
    strategy = "fast" # åˆ†ä¸ºfast(è§£æå¿«ä¼šä¸¢å¤±æ•°æ®)ï¼Œhi_resï¼ˆé«˜åˆ†è¾¨ç‡é€Ÿåº¦æ…¢ï¼‰, ocr_only(å¼ºåˆ¶ä½¿ç”¨ocræ–‡æœ¬ï¼Œä»…é€‚ç”¨äºå›¾åƒ)
)
```

##### 2. TextSplitter çš„ä½¿ç”¨

TextSplitter æ˜¯ä½œä¸ºå„ç§å…·ä½“çš„æ–‡æ¡£æ‹†åˆ†å™¨çš„çˆ¶ç±»

å¸¸ç”¨å±æ€§

![image-20251019135534819](assets/image-20251019135534819.png)

å¸¸ç”¨æ–¹æ³•

> Document åŒ…æ‹¬ metadata å…ƒæ•°æ®å’Œ page_content å±æ€§

æŒ‰ç…§å­—ç¬¦ä¸²è¿›è¡Œæ‹†åˆ†

`split_text(xxx)` ï¼šä¼ å…¥çš„å‚æ•°ç±»å‹ä¸ºstrï¼Œè¿”å›ä¸º List[str]

`create_doucuments(xxx)`ï¼šä¼ å…¥List[str]ï¼Œè¿”å›List[Document]

æŒ‰ç…§Documentè¿›è¡Œæ‹†åˆ†

`split_document(xxx)` : ä¼ å…¥ List[Document]ï¼Œè¿”å› List[Document]

##### 3. CharacterTextSplitter

chunk_size é»˜è®¤å€¼ä¸º4000ï¼Œå°½é‡åœ¨è¿™é™„è¿‘

```python
text = """
	LangChain æ˜¯ä¸€ä¸ªç”¨äºå¼€å‘ç”±è¯­è¨€æ¨¡å‹é©±åŠ¨çš„åº”ç”¨ç¨‹åºçš„æ¡†æ¶ï¼Œå®ƒæä¾›äº†ä¸€å¥—å·¥å…·å’ŒæŠ½è±¡
"""

splitter = CharacterTextSplitter(
	chunk_size = 50,
    chunk_overlap = 5,
    separator = "" # è®¾ç½®ä¸ºç©ºå­—ç¬¦ä¸²è¡¨ç¤ºç¦ç”¨åˆ†éš”ç¬¦ï¼Œå¦‚æœæœ‰ç¬¦å·åˆ™åˆ†éš”ç¬¦ä¼˜å…ˆä¸”ä¼šå¿½ç•¥chunk_size
)

texts = splitter.split_text(text)

for i, chunk in enumerate(texts):
    print(chunk)
```

##### 4. RecursiveCharacterTextSplitter

æœ€å¸¸ç”¨çš„é€’å½’å­—ç¬¦æ–‡æœ¬åˆ’åˆ†å™¨ï¼Œé‡åˆ°ç‰¹å®šå­—ç¬¦æ—¶è¿›è¡Œåˆ†å‰²ã€‚é»˜è®¤æƒ…å†µï¼Œå°è¯•åˆ†å‰²çš„å­—ç¬¦åŒ…æ‹¬ `["\n\n", "\n", " ", ""]`

```python
with open("1.txt", enconding = "utf-8") as f:
    state_of_the_union = f.read()
    
text_splitter = RecursiveCharacterTextSplitter(
	chunk_size = 100,
    chunk_overlap=20,
    seperators = [] # è‡ªå·±æŒ‡å®šåˆ’åˆ†ç¬¦å·
    length_function=len     # æŒ‰å­—ç¬¦è®¡æ•°
)

texts = text_splitter.create_documents([state_of_the_union])
```

##### 5. TokenTextSplitter/CharacterTextSplitter

æ–‡æ¡£åˆ†å—æŒ‰ç…§ token è¿›è¡Œåˆ’åˆ† ï¼Œå¤šäº†ä¸€ä¸ªå±æ€§ `encoding_name` å³ç¼–ç å™¨ï¼Œchunk_size å°±æ˜¯ token æ•°äº†ï¼ˆä½†æ˜¯åˆ’åˆ†å¯èƒ½è¶…è¿‡å®ƒï¼‰

##### 6. å‘é‡åµŒå…¥

```python
from langchain_openai import OPENAIEmbeddings
import os
import dotenv

embedding_model = OpenAIEmbeddings(model ="text-embedding-ada-002")
text = "Nice to meet you!"
embedded_query = embedding_model.embed_query(text = text)
embeddings = embeddings_model.embeded_model.embed_documents(texts)
```

##### 7. å‘é‡æ•°æ®åº“

Chroma æ•°æ®åº“ï¼Œå¦‚æœæ²¡æœ‰æŒ‡æ˜ `persist_directory` åˆ™å­˜å‚¨åœ¨å†…å­˜ä¸­

åœ¨å‘é‡æ•°æ®åº“ä¸­ä¸ä»…å­˜å‚¨äº†å‘é‡è¿˜å­˜å‚¨äº†æ–‡æ¡£æœ¬èº«

```python
db = Chroma.from_documents(
	doucuments = splitter_docs,
	embedding=embedding_model
)

query = "å“ºä¹³åŠ¨ç‰©"

docs = db.similarity_search(query, k=3) # è¿”å›æœ€ç›¸å…³çš„3ä¸ªæ–‡æ¡£

for i, doc in enumerate(docs, 1):
    print(doc.page_content)
```

```python
db = Chroma.from_documents(
	doucuments = splitter_docs,
	embedding=embedding_model
)

query = "å“ºä¹³åŠ¨ç‰©"
embedding_vector = embedding.embed_query(query)

docs = db.similarity_search(query, k=3) # è¿”å›æœ€ç›¸å…³çš„3ä¸ªæ–‡æ¡£

for i, doc in enumerate(docs, 1):
    print(doc.page_content)
```

```python
docs = db.similarity_search_with_score(query)
```

```python
docs = db._similarity_search_with_relevance_scores(query)
```

##### 8. å‘é‡æ£€ç´¢Retriever

å‚æ•°ä¸»è¦æ˜¯æœ‰ `search_type` æœç´¢çš„ç­–ç•¥ï¼Œ`search_kwargs` æœç´¢çš„å‚æ•°ï¼Œæ¯”å¦‚ `k` è¡¨ç¤ºè¿”å›çš„æ–‡æ¡£ä¸ªæ•°

```python
retriever = db.as_retriever(search_kwargs={"k":4})

docs = retriever.invoke(input = "what's ai")
```

### ä¸‰ã€RAGæ£€ç´¢å¢å¼ºæŠ€æœ¯

#### 1. åŸºæœ¬æ­¥éª¤

ä¿¡æ¯æ£€ç´¢å¢å¼ºæŠ€æœ¯æ˜¯å°†ç”¨æˆ·çš„æé—®åœ¨çŸ¥è¯†åº“ä¸­æ‰¾åˆ°ç±»ä¼¼çš„æ–‡ç« ç‰‡æ®µï¼Œç„¶ååŠ å…¥åˆ°æ¨¡å‹æç¤ºè¯çš„ä¸Šä¸‹æ–‡ä¸­ï¼Œè¿™æ ·å°±ä¸è¦å¯¹æ¨¡å‹è¿›è¡Œå¤æ‚çš„å¾®è°ƒå·¥ä½œï¼Œæé«˜æ•ˆç‡ã€‚åŸºæœ¬æ­¥éª¤åŒ…æ‹¬åŠ è½½æ–‡æ¡£ï¼Œå°†æ–‡æ¡£åˆ†å—ï¼ŒåµŒå…¥å‘é‡ï¼ŒçŸ¥è¯†åº“æ£€ç´¢æ„å»ºï¼ŒæŸ¥è¯¢

ç¬¬ä¸€æ­¥ï¼ŒåŠ è½½å¿…è¦çš„åº“

```python
from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from typing import List
from langchain_core.documents import Document
```

ç¬¬äºŒæ­¥ï¼ŒåŠ è½½æ–‡æ¡£

```python
def load_documents(folder_path: str) -> List[Document]:
    documents = []
    for filename in os.listdir(folder_path):
        if filename.endswith(".pdf"):
            loader = PyPDFLoader(os.path.join(folder_path, filename))
            documents.extend(loader.load())
        elif filename.endswith(".docx"):
            loader = Docx2txtLoader(os.path.join(folder_path, filename))
            documents.extend(loader.load())
    return documents
 
folder_path = "docs"
documents = load_documents(folder_path)
print(documents)
# æ˜¯æ–‡ä»¶é¡µæ•°ä¸æ˜¯æ–‡ä»¶æ•°
print(f"Loaded {len(documents)} documents")
```

ç¬¬ä¸‰æ­¥ï¼Œåˆ†å‰²æ–‡æ¡£æˆä¸åŒçš„chunk

```python
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000, 
    chunk_overlap=200,
    length_function=len,
)

splits = text_splitter.split_documents(documents)
print(f"Split into {len(splits)} chunks")
print(splits[0])
# metadata ç”¨æ¥ç¡®è®¤æ–‡ä»¶å—æ¥æº
print(splits[0].metadata)
```

ç¬¬å››æ­¥ï¼ŒåµŒå…¥

æ–¹æ³•ä¸€ï¼Œåˆ©ç”¨api

```python
from langchain_google_genai import GoogleGenerativeAIEmbeddings

embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
document_embeddings = embeddings.embed_documents([split.page_content for split in splits])
print(f"Created embeddings for {len(document_embeddings)} document chunks.")
print(document_embeddings[0][:5])
```

æ–¹æ³•äºŒï¼Œè‡ªå·±å†™

```python
# æœ¬åœ°åŠ è½½
from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings

embedding_function = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")
document_embeddings = embedding_function.embed_documents([split.page_content for split in splits])
print(document_embeddings[0][:5])  # Printing first 5 elements of the first embedding
```

ç¬¬äº”æ­¥ï¼Œå®šä¹‰çŸ¥è¯†åº“

```python
from langchain_chroma import Chroma
 
collection_name = "my_collection"
vectorstore = Chroma.from_documents(
    collection_name=collection_name,
    documents=splits,
    embedding=embedding_function,
    persist_directory="./chroma_db"
)
print("Vector store created and persisted to './chroma_db'")
```

æŸ¥è¯¢å¦‚ä¸‹

```python
query = "What is the core of artificial intelligence?"
search_results = vectorstore.similarity_search(query, k=2)
print(f"\nTop 2 most relevant chunks for the query: '{query}'\n")
for i, result in enumerate(search_results, 1):
    print(f"Result {i}:")
    print(f"Source: {result.metadata.get('source', 'Unknown')}")
    print(f"Content: {result.page_content}")
    print()
```

ç¬¬å…­æ­¥ï¼Œå®šä¹‰æ£€ç´¢å™¨å’Œé“¾

```python
retriever = vectorstore.as_retriever(search_kwargs={"k": 2})
retriever_results = retriever.invoke(query)
print(retriever_results)

from langchain_core.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

template = """Answer the question based only on the following context:
{context}
Question: {question}
Answer: """

prompt = ChatPromptTemplate.from_template(template)

def docs2str(docs):
    return "\n\n".join(doc.page_content for doc in docs)

rag_chain = (
    {"context": retriever | docs2str, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

question = "What is the core of artificial intelligence?"
response = rag_chain.invoke(question)
print(f"Question: {question}")
print(f"Answer: {response}")
```

æ¦‚æ‹¬å†å²å¯¹è¯

```python
from langchain_core.prompts import MessagesPlaceholder
from langchain.chains import create_history_aware_retriever
from langchain.chains.combine_documents import create_stuff_documents_chain

# æ¦‚æ‹¬é—®é¢˜
contextualize_q_system_prompt = """
Given a chat history and the latest user question
which might reference context in the chat history,
formulate a standalone question which can be understood
without the chat history. Do NOT answer the question,
just reformulate it if needed and otherwise return it as is.
"""

contextualize_q_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", contextualize_q_system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ]
)

contextualize_chain = contextualize_q_prompt | llm | StrOutputParser()
print(contextualize_chain.invoke({"input": "What is the core of artificial intelligence?", "chat_history": []}))
```

#### 2. å…¸å‹åº”ç”¨ NL2SQL

NL2SQL

è¿æ¥æ•°æ®åº“

```python
from urllib.parse import quote_plus
import os

db_user = "root"
db_password = "Pandy!@#123456"
db_host = "localhost"
db_name = "classicmodels"
from langchain_community.utilities.sql_database import SQLDatabase
# db = SQLDatabase.from_uri(f"mysql+pymysql://{db_user}:{db_password}@{db_host}/{db_name}",sample_rows_in_table_info=1,include_tables=['customers','orders'],custom_table_info={'customers':"customer"})
db = SQLDatabase.from_uri(f"mysql+pymysql://{db_user}:{quote_plus(db_password)}@{db_host}/{db_name}")
print(db.dialect)
print(db.get_usable_table_names())
print(db.table_info)
```

è¿›è¡ŒåŸºç¡€æŸ¥è¯¢

```python
from langchain.chains import create_sql_query_chain
from langchain.schema.runnable import RunnableLambda
import re

def clean_sql_output(text: str) -> str:
    """æ¸…ç† SQL è¾“å‡ºï¼Œç§»é™¤ markdown æ ¼å¼"""
    cleaned = re.sub(r'```\w*\n?', '', text)
    cleaned = cleaned.replace('```', '').strip()
    return cleaned

base_chain = create_sql_query_chain(llm, db)
generate_query = base_chain | RunnableLambda(clean_sql_output)

query = generate_query.invoke({"question": "what is price of `1968 Ford Mustang`"})
print(query) 
```

æ ¹æ®è¿”å›çš„queryå¾—åˆ°ç»“æœ

```python
from langchain_community.tools.sql_database.tool import QuerySQLDataBaseTool
execute_query = QuerySQLDataBaseTool(db=db)
execute_query.invoke(query)
```

å®Œæ•´çš„æµç¨‹ï¼Œå°†è‡ªç„¶è¯­è¨€è½¬åŒ–ä¸ºsqlåœ¨è½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€ï¼Œä¸‹é¢æ˜¯å¯¹ä»£ç ä¸­chainçš„æ‹†åˆ†è§£é‡Š

1. `RunnablePassthrough.assign(query=generate_query)`

```python
# è¾“å…¥: {"question": "what is price of 1968 Ford Mustang"}
# è¾“å‡º: {
#   "question": "what is price of 1968 Ford Mustang",
#   "query": "SELECT buyPrice FROM products WHERE productName = '1968 Ford Mustang'"
# }
```

- `RunnablePassthrough` å°†åŸå§‹è¾“å…¥ä¼ é€’ä¸‹å»
- `.assign(query=generate_query)` æ·»åŠ ä¸€ä¸ªæ–°å­—æ®µ `query`ï¼Œå€¼æ˜¯ `generate_query` çš„æ‰§è¡Œç»“æœ

2. `.assign(result=itemgetter("query") | execute_query)`

```python
# è¾“å…¥: {
#   "question": "what is price of 1968 Ford Mustang", 
#   "query": "SELECT buyPrice FROM products WHERE productName = '1968 Ford Mustang'"
# }
# è¾“å‡º: {
#   "question": "what is price of 1968 Ford Mustang",
#   "query": "SELECT buyPrice FROM products WHERE productName = '1968 Ford Mustang'",
#   "result": "[(103.42,)]"  # æ•°æ®åº“æŸ¥è¯¢ç»“æœ
# }
```

- `itemgetter("query")` æå–ä¸Šä¸€æ­¥çš„ `query` å­—æ®µ
- `| execute_query` æ‰§è¡Œè¿™ä¸ª SQL æŸ¥è¯¢
- `.assign(result=...)` å°†æŸ¥è¯¢ç»“æœæ·»åŠ ä¸º `result` å­—æ®µ

3. `| rephrase_answer`

```python
# è¾“å…¥: {
#   "question": "what is price of 1968 Ford Mustang",
#   "query": "SELECT buyPrice FROM products WHERE productName = '1968 Ford Mustang'", 
#   "result": "[(103.42,)]"
# }
# è¾“å‡º: "The price of 1968 Ford Mustang is $103.42"
```

- æ¥æ”¶åŒ…å«é—®é¢˜ã€æŸ¥è¯¢å’Œç»“æœçš„å­—å…¸
- ç”¨ LLM ç”Ÿæˆè‡ªç„¶è¯­è¨€ç­”æ¡ˆ

```python
from operator import itemgetter

from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough

answer_prompt = PromptTemplate.from_template(
    """Given the following user question, corresponding SQL query, and SQL result, answer the user question.

Question: {question}
SQL Query: {query}
SQL Result: {result}
Answer: """
)

rephrase_answer = answer_prompt | llm | StrOutputParser()

chain = (
    RunnablePassthrough.assign(query=generate_query).assign(
        result=itemgetter("query") | execute_query
    )
    | rephrase_answer
)

chain.invoke({"question": "How many customers have an order count greater than 5"})
```

ä¸‹é¢æ˜¯èå…¥few-shotè¿›è¡Œè®¾è®¡

```python
examples = [
   {
       "input": "List all customers in France with a credit limit over 20,000.",
       "query": "SELECT * FROM customers WHERE country = 'France' AND creditLimit > 20000;"
   },
   {
       "input": "Get the highest payment amount made by any customer.",
       "query": "SELECT MAX(amount) FROM payments;"
   }
]
```

```python
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder,FewShotChatMessagePromptTemplate,PromptTemplate

example_prompt = ChatPromptTemplate.from_messages(
   [
       ("human", "{input}\nSQLQuery:"),
       ("ai", "{query}"),
   ]
)
few_shot_prompt = FewShotChatMessagePromptTemplate(
   example_prompt=example_prompt,
   examples=examples,
   # input_variables=["input","top_k"],
   input_variables=["input"],
)
print(few_shot_prompt.format(input1="How many products are there?"))
```

æˆ‘ä»¬ä¹Ÿå¯ä»¥é‡‡ç”¨åŠ¨æ€few-shotçš„æ–¹æ³•ï¼Œå³ä»ç»™å®šçš„ç¤ºä¾‹ä¸­é€‰å‡ºå…³è”ç¨‹åº¦æœ€é«˜çš„ç¤ºä¾‹

```python
from langchain_community.vectorstores import Chroma
from langchain_core.example_selectors import SemanticSimilarityExampleSelector
from langchain_openai import OpenAIEmbeddings

vectorstore = Chroma()
vectorstore.delete_collection()
example_selector = SemanticSimilarityExampleSelector.from_examples(
   examples,
   OpenAIEmbeddings(),
   vectorstore,
   k=2,
   input_keys=["input"],
)
example_selector.select_examples({"input": "how many employees we have?"})
few_shot_prompt = FewShotChatMessagePromptTemplate(
   example_prompt=example_prompt,
   example_selector=example_selector,
   input_variables=["input","top_k"],
)
print(few_shot_prompt.format(input="How many products are there?"))
final_prompt = ChatPromptTemplate.from_messages(
   [
       ("system", "You are a MySQL expert. Given an input question, create a syntactically correct MySQL query to run. Unless otherwise specificed.\n\nHere is the relevant table info: {table_info}\n\nBelow are a number of examples of questions and their corresponding SQL queries."),
       few_shot_prompt,
       ("human", "{input}"),
   ]
)
print(final_prompt.format(input="How many products are there?",table_info="some table info"))
generate_query = create_sql_query_chain(llm, db,final_prompt)
chain = (
RunnablePassthrough.assign(query=generate_query).assign(
   result=itemgetter("query") | execute_query
)
| rephrase_answer
)
chain.invoke({"question": "How many csutomers with credit limit more than 50000"})
```

ä¸ºäº†æé«˜æ•ˆç‡ï¼Œæˆ‘ä»¬éœ€è¦å…ˆé€‰æ‹©å¯èƒ½çš„table

```python
from operator import itemgetter
from langchain.chains.openai_tools import create_extraction_chain_pydantic
from langchain_core.pydantic_v1 import BaseModel, Field
from typing import List
import pandas as pd

def get_table_details():
  # Read the CSV file into a DataFrame
  table_description = pd.read_csv("database_table_descriptions.csv")
  table_docs = []

  # Iterate over the DataFrame rows to create Document objects
  table_details = ""
  for index, row in table_description.iterrows():
      table_details = table_details + "Table Name:" + row['Table'] + "\n" + "Table Description:" + row['Description'] + "\n\n"

  return table_details


class Table(BaseModel):
  """Table in SQL database."""

  name: str = Field(description="Name of table in SQL database.")

# table_names = "\n".join(db.get_usable_table_names())
table_details = get_table_details()
print(table_details)
table_details_prompt = f"""Return the names of ALL the SQL tables that MIGHT be relevant to the user question. \
The tables are:

{table_details}

Remember to include ALL POTENTIALLY RELEVANT tables, even if you're not sure that they're needed."""

table_chain = create_extraction_chain_pydantic(Table, llm, system_message=table_details_prompt)
tables = table_chain.invoke({"input": "give me details of customer and their order count"})
def get_tables(tables: List[Table]) -> List[str]:
    tables  = [table.name for table in tables]
    return tables

select_table = {"input": itemgetter("question")} | create_extraction_chain_pydantic(Table, llm, system_message=table_details_prompt) | get_tables
select_table.invoke({"question": "give me details of customer and their order count"})
chain = (
RunnablePassthrough.assign(table_names_to_use=select_table) |
RunnablePassthrough.assign(query=generate_query).assign(
    result=itemgetter("query") | execute_query
)
| rephrase_answer
)
chain.invoke({"question": "How many cutomers with order count more than 5"})
```

è®°å½•å†å²

```python
from langchain.memory import ChatMessageHistory
history = ChatMessageHistory()
final_prompt = ChatPromptTemplate.from_messages(
   [
       ("system", "You are a MySQL expert. Given an input question, create a syntactically correct MySQL query to run. Unless otherwise specificed.\n\nHere is the relevant table info: {table_info}\n\nBelow are a number of examples of questions and their corresponding SQL queries. Those examples are just for referecne and hsould be considered while answering follow up questions"),
       few_shot_prompt,
       MessagesPlaceholder(variable_name="messages"),
       ("human", "{input}"),
   ]
)
print(final_prompt.format(input="How many products are there?",table_info="some table info",messages=[]))
generate_query = create_sql_query_chain(llm, db,final_prompt)

chain = (
RunnablePassthrough.assign(table_names_to_use=select_table) |
RunnablePassthrough.assign(query=generate_query).assign(
   result=itemgetter("query") | execute_query
)
| rephrase_answer
)
question = "How many cutomers with order count more than 5"
response = chain.invoke({"question": question,"messages":history.messages})
There are 2 customers with an order count of more than 5.
history.add_user_message(question)
history.add_ai_message(response)
response = chain.invoke({"question": "Can you list there names?","messages":history.messages})
```

### å››ã€LangGraph

> åŸºæœ¬è®¾è®¡æ€è·¯ï¼šç¬¬ä¸€æ­¥æ˜¯è¦å…ˆæŠŠå·¥ä½œæµè½¬åŒ–ä¸ºæœ‰å‘å›¾çš„å½¢å¼ï¼Œç„¶åæ˜ç¡®æ¯ä¸ªèŠ‚ç‚¹è¦åšä»€ä¹ˆï¼Œä¹‹åè®¾è®¡çŠ¶æ€å»å®ç°ï¼ŒçŠ¶æ€å°±æ˜¯åœ¨èŠ‚ç‚¹ä¹‹é—´å¯ä»¥ä¿å­˜çš„çŸ­æœŸè®°å¿†

#### 1. åŸºæœ¬æ­¥éª¤

å®šä¹‰çŠ¶æ€ï¼Œ$state$ æ˜¯ç”¨æ¥ä¿å­˜å’Œä¼ é€’æ•´ä¸ªå·¥ä½œæµä¸­èŠ‚ç‚¹ä¹‹é—´çš„å…±äº«æ•°æ®çš„å¯¹è±¡ï¼Œå³æ•´ä¸ªæµç¨‹çš„ä¸Šä¸‹æ–‡ã€‚ç”¨ `stage["messages"].get()` æˆ–è€… `stage["messages"].append()` æ¥å®ç°è·å–å’Œæ·»åŠ åŠŸèƒ½

> `TypeDict` è¡¨ç¤ºè¿™æ˜¯ä¸€ä¸ªå­—å…¸å½¢çŠ¶ç±»å‹
>
> `Annotated` æ˜¯ Python ç±»å‹ç³»ç»Ÿé‡Œçš„ä¸€ä¸ªâ€œ**ç»™ç±»å‹åŠ å…ƒæ•°æ®**â€çš„å·¥å…·
>
> è¯­æ³•ï¼š`Annotated[åŸå§‹ç±»å‹, å…ƒæ•°æ®1, å…ƒæ•°æ®2, ...]`
>
> ```python
> from typing_extensions import Annotated
> import operator
> 
> Messages = Annotated[list[str], operator.add]  # åˆå¹¶æ—¶ç”¨â€œ+â€æ‹¼æ¥
> Counter  = Annotated[int, operator.add]        # åˆå¹¶æ—¶ç”¨â€œ+â€ç´¯åŠ 
> ```

æ ¸å¿ƒè¦ç‚¹ï¼š

- **é™æ€ç±»å‹æ£€æŸ¥å™¨**æŠŠå®ƒå½“ä½œâ€œä»ç„¶æ˜¯åŸå§‹ç±»å‹â€ï¼Œä¸ä¼šæ”¹å˜ç±»å‹åˆ¤æ–­ï¼ˆ`Annotated[int, ...]` ä»æ˜¯ `int`ï¼‰ã€‚
- **è¿è¡Œæ—¶/æ¡†æ¶**å¯ä»¥è¯»å–è¿™äº›â€œå…ƒæ•°æ®â€æ¥åšé¢å¤–çš„äº‹ï¼ˆæ ¡éªŒã€åºåˆ—åŒ–ã€åˆå¹¶ç­–ç•¥ç­‰ï¼‰ã€‚

è¿™ä¸ªçŠ¶æ€è¡¨ç¤ºå¦‚æœçŠ¶æ€åˆå¹¶ï¼Œåˆ™è‡ªåŠ¨æ·»åŠ æ–°çš„ä¿¡æ¯

`add_messages` æ˜¯è‡ªåŠ¨æ·»åŠ æ–°çš„ä¿¡æ¯

```python
from typing import Annotated, TypedDict
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langgraph.graph.message import add_messages

class State(TypedDict):
    messages: Annotated[list[BaseMessage], add_messages]
```

ä¸‹é¢æ˜¯å®šä¹‰ä¸€ä¸ªèŠ‚ç‚¹ï¼Œå¹¶è®¾è®¡ä¸€ä¸ªç®€å•çš„å›¾

```python
def chatbot_node(state: State) -> State:
    response = llm.invoke(state["messages"])
    return {"messages": [response]}

from langgraph.graph import StateGraph, START, END

graph_builder = StateGraph(State)
graph_builder.add_node("chatbot", chatbot_node)
graph_builder.add_edge(START, "chatbot")
graph_builder.add_edge("chatbot", END)
graph = graph_builder.compile()
```

ä¸‹é¢è¿›è¡Œæµ‹è¯•

```python
def test_chatbot(message: str):
    initial_state = {"messages": [HumanMessage(content=message)]}
    result = graph.invoke(initial_state)
    print("ğŸ¤– Assistant:", result["messages"][-1].content)

test_chatbot("Hello! My name is Pradip")
test_chatbot("Do you remember my name?")
```

æˆ‘ä»¬å¯ä»¥å‘ç°å®ƒæ˜¯æ²¡æœ‰è®°å¿†çš„ï¼Œæ‰€ä»¥éœ€è¦åŠ å…¥è®°å¿†ä¿¡æ¯ï¼Œç”¨configè¡¨æ˜çº¿ç¨‹id

```python
from langgraph.checkpoint.memory import MemorySaver

memory = MemorySaver()

# Compile the graph again with memory enabled
graph_with_memory = graph_builder.compile(checkpointer=memory)

def chat_with_memory(message: str, thread_id: str):
    config = {"configurable": {"thread_id": thread_id}}
    initial_state = {"messages": [HumanMessage(content=message)]}
    result = graph_with_memory.invoke(initial_state, config)
    print("ğŸ¤– Assistant:", result["messages"][-1].content)

# Start a conversation
chat_with_memory("Hi, my name is Pradip", thread_id="thread-1")
chat_with_memory("What's my name?", thread_id="thread-1")
```

åŠ å…¥ $tool$ çš„å›¾

```python
llm = ChatOpenAI(model="gpt-4.1-mini", temperature=0.7)
llm_with_tools = llm.bind_tools(tools)  # `tools` already contains `calculator` and `search_tool`
from langchain_core.messages import HumanMessage, AIMessage

def chatbot_node(state: AgentState) -> AgentState:
    """Gatekeeper: answer directly or request a tool"""
    system_message = (
        "You are a helpful assistant.\n"
        "Use the `web_search` tool for realâ€‘time facts and `calculator` for maths.\n"
        "Otherwise answer directly."
    )

    messages = [
        {"role": "system", "content": system_message},
        *state["messages"],
    ]

    response = llm_with_tools.invoke(messages)
    return {"messages": [response]}  # LangGraph merges this into the running state


from langgraph.prebuilt import ToolNode
tool_node = ToolNode(tools)  # automatically dispatches and streams results back
from typing import Literal

def should_continue(state: AgentState) -> Literal["tools", "end"]:
    last = state["messages"][-1]
    return "tools" if getattr(last, "tool_calls", None) else "end"
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver

workflow = StateGraph(AgentState)
workflow.add_node("chatbot", chatbot_node)
workflow.add_node("tools",   tool_node)

workflow.add_edge(START, "chatbot")
workflow.add_conditional_edges("chatbot", should_continue, {"tools": "tools", "end": END})
workflow.add_edge("tools", "chatbot")  # come back after tools run

app = workflow.compile(checkpointer=MemorySaver())

def chat_with_agent(msg: str, thread_id="demo"):
    cfg = {"configurable": {"thread_id": thread_id}}
    state = {"messages": [HumanMessage(content=msg)]}
    result = app.invoke(state, cfg)
    print(result["messages"][-1].content)

chat_with_agent("What's 15% of 240?")
chat_with_agent("Search for recent news about artificial intelligence")

```

#### 2. æ„å»ºRAG Agent

![image-20251002101029677](assets/image-20251002101029677.png)

åŠ è½½æ–‡æ¡£

```python
# â”€â”€ Build & persist a Chroma index â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from pathlib import Path
from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma

SOURCE_DIR   = Path("docs")          # put your files here
INDEX_DIR    = Path("chroma_db_1")   # will be created if missing
EMBED_MODEL  = "text-embedding-3-small"

# Load docs (keep only pdf/docx for brevity)
docs = []
for f in SOURCE_DIR.glob("*.*"):
    if f.suffix == ".pdf":
        docs += PyPDFLoader(str(f)).load()
    elif f.suffix == ".docx":
        docs += Docx2txtLoader(str(f)).load()

# Split & embed
chunks     = RecursiveCharacterTextSplitter(chunk_size=1_000, chunk_overlap=200).split_documents(docs)
embeddings = OpenAIEmbeddings(model=EMBED_MODEL)

vectordb = Chroma.from_documents(
    documents         = chunks,
    embedding         = embeddings,
    persist_directory = str(INDEX_DIR),
    collection_name   = "kb_collection",
)
vectordb.persist()
print("âœ… Index built â†’", INDEX_DIR.resolve())
```

æ„å»ºä¸€ä¸ªå›å½’å™¨ä½œä¸ºå·¥å…·

```python
retriever = vectordb.as_retriever(search_kwargs={"k": 2})

@tool
def rag_search_tool(query: str) -> str:
    """Search the knowledgeâ€‘base for relevant chunks"""
    results = retriever.invoke(query)
    return "".join(d.page_content for d in results)
```

$fallback$ æœºåˆ¶è¿›è¡Œå®æ—¶çš„ç½‘ç»œæœç´¢

```python
from langchain_tavily import TavilySearch

tavily = TavilySearch(max_results=3, topic="general")

@tool
def web_search_tool(query: str) -> str:
    """Upâ€‘toâ€‘date web info via Tavily"""
    return "

".join(r["content"] for r in tavily.invoke({"query": query})["results"])  # simplified
```

æ‰©å±•çŠ¶æ€

```python
class AgentState(State):          # add to previous `State`
    route:    str          # "rag", "answer", "web", "end"
    rag:      str | None   # KB result
    web:      str | None   # webâ€‘search snippets
```

å†³ç­–æ‰§è¡ŒèŠ‚ç‚¹

| Node            | What it does                                                 |
| --------------- | ------------------------------------------------------------ |
| **router_node** | Uses an LLM with structured output to decide the `route` â€“ *rag*, *answer*, or *end*. |
| **rag_node**    | Runs `rag_search_tool`, then asks a *judge* LLM if the chunks are **sufficient**. Sets `route` to *answer* or *web*. |
| **web_node**    | Calls `web_search_tool` and passes snippets along.           |
| **answer_node** | Crafts the final reply, combining any `rag` and/or `web` context. |

```python
# â”€â”€ Structured helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class RouteDecision(BaseModel):
    route: Literal["rag", "answer", "end"]
    reply: str | None = None

class RagJudge(BaseModel):
    sufficient: bool

router_llm = ChatOpenAI(model="gpt-4.1-mini", temperature=0).with_structured_output(RouteDecision)
judge_llm  = ChatOpenAI(model="gpt-4.1-mini", temperature=0).with_structured_output(RagJudge)
answer_llm = ChatOpenAI(model="gpt-4.1-mini", temperature=0.7)

# â”€â”€ Router â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def router_node(state: AgentState) -> AgentState:
    q = state["messages"][-1].content
    decision = router_llm.invoke([
        ("system", "Decide route: rag / answer / end"),
        ("user", q)
    ])
    new_state = {**state, "route": decision.route}
    if decision.route == "end":
        new_state["messages"] += [AIMessage(content=decision.reply or "Hello!")]
    return new_state

# â”€â”€ RAG lookup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def rag_node(state: AgentState) -> AgentState:
    q = state["messages"][-1].content
    chunks = rag_search_tool.invoke(q)
    verdict = judge_llm.invoke([("user", f"Question: {q}
Docs: {chunks[:300]}â€¦")])
    return {**state, "rag": chunks, "route": "answer" if verdict.sufficient else "web"}

# â”€â”€ Web search & Answer nodes omitted for brevity (same as notebook) â”€â”€
```

å®Œæˆå›¾çš„æ„å»º

```python
agent_graph = StateGraph(AgentState)
agent_graph.add_node("router",      router_node)
agent_graph.add_node("rag_lookup",  rag_node)
agent_graph.add_node("web_search",  web_node)
agent_graph.add_node("answer",      answer_node)

agent_graph.set_entry_point("router")
agent_graph.add_conditional_edges("router", from_router,
        {"rag": "rag_lookup", "answer": "answer", "end": END})
agent_graph.add_conditional_edges("rag_lookup", after_rag,
        {"answer": "answer", "web": "web_search"})
agent_graph.add_edge("web_search", "answer")
agent_graph.add_edge("answer", END)

agent = agent_graph.compile(checkpointer=MemorySaver())
```

æµ‹è¯•

```python
if __name__ == "__main__":
    config = {"configurable": {"thread_id": "threadâ€‘12"}}
    while True:
        q = input("You: ").strip()
        if q in {"quit", "exit"}: break
        result = agent.invoke({"messages": [HumanMessage(content=q)]}, config)
        print(result["messages"][-1].content)
```

#### 3. é”™è¯¯å¤„ç†

é‡è¯•æœºåˆ¶

```python
from langgraph.types import RetryPolicy

workflow.add_node(
    "search_documentation",
    search_documentation,
    retry_policy=RetryPolicy(max_attempts=3, initial_interval=1.0)
)
```

LLM è‡ªåŠ¨æ¢å¤

```pythonÂ 
def execute_tool(state: State) -> Command[Literal["agent", "execute_tool"]]:
    try:
        result = run_tool(state['tool_call'])
        return Command(update={"tool_result": result}, goto="agent")
    except ToolError as e:
        # Let the LLM see what went wrong and try again
        return Command(
            update={"tool_result": f"Tool error: {str(e)}"},
            goto="agent"
        )
```

ç”¨æˆ·ä¿®å¤

```python
def lookup_customer_history(state: State) -> Command[Literal["draft_response"]]:
    if not state.get('customer_id'):
        user_input = interrupt({
            "message": "Customer ID needed",
            "request": "Please provide the customer's account ID to look up their subscription history"
        })
        return Command(
            update={"customer_id": user_input['customer_id']},
            goto="lookup_customer_history"
        )
    # Now proceed with the lookup
    customer_data = fetch_customer_history(state['customer_id'])
    return Command(update={"customer_history": customer_data}, goto="draft_response")
```

#### 3. agent å¸¸è§æ„é€ å½¢å¼

![Agent Workflow](assets/agent_workflow.png)

### äº”ã€OpenAI

#### 1.function calling

##### 1.1 åŸºæœ¬æµç¨‹

å·¥å…·è°ƒç”¨æ˜¯é€šè¿‡ OpenAI API åœ¨åº”ç”¨ç¨‹åºå’Œæ¨¡å‹ä¹‹é—´è¿›è¡Œçš„å¤šæ­¥éª¤å¯¹è¯ã€‚å·¥å…·è°ƒç”¨æµç¨‹åŒ…å«äº”ä¸ªæ­¥éª¤ï¼š

1. ä½¿ç”¨æ¨¡å‹å¯ä»¥è°ƒç”¨çš„å·¥å…·å‘æ¨¡å‹å‘å‡ºè¯·æ±‚
2. æ¥æ”¶æ¥è‡ªæ¨¡å‹çš„å·¥å…·è°ƒç”¨
3. ä½¿ç”¨å·¥å…·è°ƒç”¨çš„è¾“å…¥åœ¨åº”ç”¨ç¨‹åºç«¯æ‰§è¡Œä»£ç 
4. ä½¿ç”¨å·¥å…·è¾“å‡ºå‘æ¨¡å‹å‘å‡ºç¬¬äºŒä¸ªè¯·æ±‚
5. æ¥æ”¶æ¥è‡ªæ¨¡å‹çš„æœ€ç»ˆå“åº”ï¼ˆæˆ–æ›´å¤šå·¥å…·è°ƒç”¨ï¼‰

![image-20250820094103703](assets/image-20250820094103703.png)

##### 1.2 ä½¿ç”¨ç¤ºä¾‹

åˆå§‹åŒ–å’Œå·¥å…·å®šä¹‰

å®šä¹‰ä¸€ä¸ªåä¸º `get_horoscope` çš„å‡½æ•°ï¼Œå³æ ¹æ®æ˜Ÿåº§é¢„æµ‹è¿åŠ¿ï¼Œåªæœ‰ä¸€ä¸ªå‚æ•°å³æ˜Ÿåº§å

```python
from openai import OpenAI
import json
client = OpenAI()

tools = [
    {
        "type": "function",
        "name": "get_horoscope",
        "description": "Get today's horoscope for an astrological sign.",
        "parameters": {
            "type": "object",
            "properties": {
                "sign": {
                    "type": "string",
                    "description": "An astrological sign like Taurus or Aquarius",
                },
            },
            "required": ["sign"],
        },
    },
]
```

é¦–æ¬¡ $API$ è°ƒç”¨

ç”¨æˆ·è¯¢é—®æ°´ç“¶åº§è¿åŠ¿ï¼Œæ¨¡å¼è¯†åˆ«éœ€è¦è°ƒç”¨å·¥å…·

```python
input_list = [
    {"role": "user", "content": "What is my horoscope? I am an Aquarius."}
]

response = client.responses.create(
    model="gpt-5",
    tools=tools,
    input=input_list,
)
```

æå–å‡½æ•°è°ƒç”¨ä¿¡æ¯ï¼Œè¿™é‡Œè·å¾—çš„å‚æ•°æ˜¯`{"sign": "Aquarius"}`

```python
function_call = None
function_call_arguments = None
input_list += response.output
for item in response.output:
    if item.type == "function_call":
        function_call = item
        function_call_arguments = json.loads(item.arguments)
```

æ‰§è¡Œå‡½æ•°é€»è¾‘

```python
def get_horoscope(sign):
    return f"{sign}: Next Tuesday you will befriend a baby otter."

result = {"horoscope": get_horoscope(function_call_arguments["sign"])}
```

å°†å›å¤ç»™æ¨¡å‹è®©å…¶å†è°ƒç”¨å¾—åˆ°æœ€ç»ˆå›å¤

```python
input_list.append({
    "type": "function_call_output",
    "call_id": function_call.call_id,
    "output": json.dumps(result),
})

response = client.responses.create(
    model="gpt-5",
    instructions="Respond only with a horoscope generated by a tool.",
    tools=tools,
    input=input_list,
)
```

##### 1.3 å®šä¹‰å¤§æ¨¡å‹è°ƒç”¨çš„å·¥å…·æ ¼å¼

å¤§æ¨¡å‹ä¼šæ ¹æ®è¿™ä¸ªjsonæ–‡ä»¶å’Œç”¨æˆ·çš„æé—®æ¥è¿›è¡Œæ„å›¾è¯†åˆ«ï¼Œå†³å®šæ˜¯å¦è¦è°ƒç”¨å·¥å…·

| Field         | Description                                                  |
| :------------ | :----------------------------------------------------------- |
| `type`        | This should always be `function`                             |
| `name`        | The function's name (e.g. `get_weather`)                     |
| `description` | Details on when and how to use the function                  |
| `parameters`  | [JSON schema](https://json-schema.org/) defining the function's input arguments |
| `strict`      | Whether to enforce strict mode for the function call         |

```json
{
    "type": "function",
    "name": "get_weather",
    "description": "Retrieves current weather for the given location.",
    "parameters": {
        "type": "object",
        "properties": {
            "location": {
                "type": "string",
                "description": "City and country e.g. BogotÃ¡, Colombia"
            },
            "units": {
                "type": "string",
                "enum": ["celsius", "fahrenheit"],
                "description": "Units the temperature will be returned in."
            }
        },
        "required": ["location", "units"],
        "additionalProperties": false
    },
    "strict": true
}
```

##### 1.4 å·¥å…·è°ƒç”¨è¿”å›æ ¼å¼

```json
{
    "id": "fc_12345xyz",
    "call_id": "call_12345xyz",
    "type": "function_call",
    "name": "get_weather",
    "arguments": "{\"location\":\"Paris, France\"}"
}
```

#### 2. è¯·æ±‚æ ¼å¼

å¿…é¡»å‚æ•°

```json
{
  "model": "gpt-4o",           // æ¨¡å‹åç§°
  "messages": [                // å¯¹è¯æ¶ˆæ¯æ•°ç»„
    {
      "role": "system",
      "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹"
    },
    {
      "role": "user", 
      "content": "ä½ å¥½"
    }
  ]
}
```

å¸¸è§å¯ç”¨å‚æ•°

```json
{
  "max_tokens": 150,           // æœ€å¤§ç”Ÿæˆtokenæ•°
  "max_completion_tokens": 150, // å®Œæˆtokenæ•°é™åˆ¶ï¼ˆæ–°ç‰ˆæœ¬ï¼‰
  "temperature": 0.7,          // æ¸©åº¦ (0-2ï¼Œæ§åˆ¶éšæœºæ€§)
  "top_p": 0.9,               // æ ¸é‡‡æ · (0-1)
  "n": 1,                     // ç”Ÿæˆå“åº”æ•°é‡
  "stop": ["\n", "ã€‚"],        // åœæ­¢è¯åˆ—è¡¨
  "stream": false             // æ˜¯å¦æµå¼è¿”å›
}
```

å¸¦å·¥å…·çš„è¯·æ±‚æ ¼å¼

```json
{
  "model": "gpt-4",
  "messages": [
    {"role": "user", "content": "è¯·å¸®æˆ‘è¯»å–æ–‡ä»¶ data.csv"}
  ],
  "tools": [
    {
      "type": "function",
      "function": {
        "name": "file_reader",
        "description": "è¯»å–æ–‡ä»¶å†…å®¹"
      }
    }
  ],
  "tool_choice": "auto"  // è‡ªåŠ¨å†³å®šæ˜¯å¦è°ƒç”¨å·¥å…·
}
```

å›¾ç‰‡ æ–‡ä»¶æ ¼å¼

```json
input=[{
    "role": "user",
    "content": [
        {"type": "input_text", "text": "what's in this image?"},
        {
            "type": "input_image",
            "image_url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",
        },
    ],
}]

input=[
    {
        "role": "user",
        "content": [
            {
                "type": "input_text",
                "text": "Analyze the letter and provide a summary of the key points.",
            },
            {
                "type": "input_file",
                "file_url": "https://www.berkshirehathaway.com/letters/2024ltr.pdf",
            },
        ],
    },
]

// ä¸Šä¼ æ–‡ä»¶
input=[
    {
        "role": "user",
        "content": [
            {
                "type": "input_file",
                "file_id": file.id,
            },
            {
                "type": "input_text",
                "text": "What is the first dragon in the book?",
            },
        ]
    }
]
```

#### 3. å“åº”æ ¼å¼

##### 3.1 éæµå¼è¯·æ±‚å“åº”æ ¼å¼

```json
{
    "id": "chatcmpl-1214900a-6cdd-4148-b663-b5e2f642b4de",
    "created": 1751494488,
    "model": "claude-sonnet-4-20250514",
    "object": "chat.completion",
    "system_fingerprint": null,
    "choices": [
        {
            "finish_reason": "stop",
            "index": 0,
            "message": {
                "content": "Hello! I'm doing well, thank you for asking. I'm here and ready to help with whatever you'd like to discuss or work on. How are you doing today?",
                "role": "assistant",
                "tool_calls": null,
                "function_call": null
            }
        }
    ],
    "usage": {
        "completion_tokens": 39,
        "prompt_tokens": 13,
        "total_tokens": 52,
        "completion_tokens_details": null,
        "prompt_tokens_details": {
            "audio_tokens": null,
            "cached_tokens": 0
        },
        "cache_creation_input_tokens": 0,
        "cache_read_input_tokens": 0
    }
}
```

##### 3.2 æµå¼è¯·æ±‚å“åº”æ ¼å¼

```json
data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1677652288,"model":"gpt-4o","choices":[{"index":0,"delta":{"content":"ä½ "},"finish_reason":null}]}

data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1677652288,"model":"gpt-4o","choices":[{"index":0,"delta":{"content":"å¥½"},"finish_reason":null}]}

data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1677652288,"model":"gpt-4o","choices":[{"index":0,"delta":{},"finish_reason":"stop"}]}

data: [DONE]
```

### å…­ã€Agent2Agent

#### 1. æ¦‚è¿°

$A2A$ åè®®æ˜¯ç”¨æ¥æ§åˆ¶å¤šä¸ª $agent$ ä¹‹é—´çš„äº¤äº’ï¼Œå› ä¸ºæœ‰å¾ˆå¤šä¸åŒçš„æ¡†æ¶æ¯”å¦‚ $langchain$ï¼Œ$crewai$ ç­‰ç­‰ï¼Œå¦‚æœä¸€ä¸ª $agent$ è¦è·å–å¦ä¸€ä¸ª $agent$ æœåŠ¡éœ€è¦ä¸€ä¸ªåè®®æ¥æ§åˆ¶ä»–ä»¬ä¹‹é—´é€šä¿¡çš„æ–¹å¼ã€‚å®ƒåˆ©ç”¨ $HTTP/HTTPS$ ä½œä¸ºä¼ è¾“å±‚ï¼Œé‡‡ç”¨ $JSON-RPC \ 2.0$ çš„æ¶ˆæ¯æ ¼å¼

![image-20250923105828919](assets/image-20250923105828919.png)

#### 2. Task ä¸ Message

- **åˆ›å»º**ï¼šTask ç”± Client å‘èµ·ï¼ˆé€šè¿‡ `tasks/send` æˆ– `tasks/sendSubscribe` æ–¹æ³•ï¼‰ã€‚  
- **çŠ¶æ€ç®¡ç†**ï¼šTask çš„çŠ¶æ€ç”± Server ç»´æŠ¤å’Œæ›´æ–°ã€‚  
- **å”¯ä¸€æ ‡è¯†**ï¼šæ¯ä¸ª Task æ‹¥æœ‰å”¯ä¸€ IDï¼Œç”¨äºæ ‡è¯†äº¤äº’ï¼ˆé”™è¯¯çŠ¶æ€ã€å‘é€è¡¥å……è¾“å…¥ã€å–æ¶ˆç­‰ï¼‰ã€‚  
- **ä¼šè¯å…³è”**ï¼šTask å¯æŒ‚è½½åˆ°ä¸€ä¸ª `sessionId` ä¸‹ï¼Œå¤šä¸ªä»»åŠ¡å±äºåŒä¸€ä¼šè¯ã€‚  
- **æ¶ˆæ¯äº¤æ¢**ï¼šClient å’Œ Server åœ¨ Task çš„ä¸Šä¸‹æ–‡ä¸­äº¤æ¢ **Message**ã€‚  
- **ç»“æœç”Ÿæˆ**ï¼šServer åœ¨ Task æ‰§è¡Œè¿‡ç¨‹ä¸­æˆ–å®Œæˆåç”Ÿæˆ **Artifact** ä½œä¸ºç»“æœã€‚  

```text
Task = { id, status, input, output, sessionId }
```

```mermaid
flowchart TD
    A[åˆ›å»ºä»»åŠ¡ tasks/send] --> B[Submitted å·²æäº¤]
    B --> C[Working è¿›è¡Œä¸­]
    C -->|æ‰§è¡ŒæˆåŠŸ| D[Completed å·²å®Œæˆ]
    C -->|æ‰§è¡Œå¤±è´¥| E[Failed å¤±è´¥]
    C -->|ç­‰å¾…ç”¨æˆ·è¾“å…¥| F[InputRequired éœ€è¦è¾“å…¥]
    C -->|ç”¨æˆ·å–æ¶ˆ| G[Canceled å·²å–æ¶ˆ]
    C -->|æœªçŸ¥é”™è¯¯| H[Unknown æœªçŸ¥]
```

**Message æŠ½è±¡æ¨¡å‹**

```
Message = { role âˆˆ {user, agent}, parts[], metadata }
```

- **role**: æ¶ˆæ¯å‘é€è€…çš„è§’è‰²ï¼ˆ`user` æˆ– `agent`ï¼‰
- **parts[]**: å†…å®¹ç‰‡æ®µæ•°ç»„ï¼ˆè‡³å°‘ä¸€ä¸ªï¼‰
- **metadata**: ä¸æ¶ˆæ¯ç›¸å…³çš„ä¸Šä¸‹æ–‡ä¿¡æ¯

**Part ç±»å‹**

```
Part âˆˆ { TextPart, FilePart, BytesPart, URIPart, DataPart }
```

- **TextPart**: åŒ…å«çº¯æ–‡æœ¬å†…å®¹
- **FilePart**: åŒ…å«æ–‡ä»¶è·¯å¾„ã€æ–‡ä»¶å†…å®¹æˆ–å¼•ç”¨
- **BytesPart**: åŒ…å«åŸå§‹äºŒè¿›åˆ¶æ•°æ®ï¼ˆé€‚ç”¨äºå›¾åƒ/éŸ³é¢‘ç­‰ï¼‰
- **URIPart**: åŒ…å«ä¸€ä¸ªå¯è®¿é—®çš„èµ„æºåœ°å€ (URL/URI)
- **DataPart**: åŒ…å«ç»“æ„åŒ– JSON æ•°æ®

**Message ç¤ºä¾‹**

```
interface Message {
  role: "user" | "agent";     
  parts: Part[];              
  metadata?: Record<string, any> | null;
}
```

**Artifacts** ä»£è¡¨ $agent$ çš„äº§ç‰©ï¼Œå³æ–‡ä»¶æˆ–è€…è¾“å‡ºç»“æœ	

**æ ¸å¿ƒæ¦‚å¿µï¼ˆå¿«é€Ÿè®°å¿†å¡ï¼‰**

- **Messageï¼ˆæ¶ˆæ¯ï¼‰**ï¼šæ— çŠ¶æ€æˆ–è½»é‡çš„äº¤äº’ï¼›é€šå¸¸ç”¨äºå³æ—¶ç­”å¤æˆ–åå•†ï¼ˆâ€œä½ èƒ½åšè¿™ä¸ªå—ï¼Ÿâ€ï¼‰ã€‚
- **Taskï¼ˆä»»åŠ¡ï¼‰**ï¼šæœ‰çŠ¶æ€çš„å·¥ä½œå•å…ƒï¼Œç”¨äºå¯è·Ÿè¸ªã€å¯èƒ½è€—æ—¶ã€å¯ä¸­æ–­/éœ€è¾“å…¥çš„æ“ä½œã€‚
- **contextId**ï¼šæŠŠä¸€ç³»åˆ— Message/Task ç»‘åœ¨ä¸€èµ·çš„ä¼šè¯/ä¸Šä¸‹æ–‡æ ‡è¯†ï¼ˆä»£è¡¨åŒä¸€å¯¹è¯/ç›®æ ‡ï¼‰ã€‚
- **taskId**ï¼šä»»åŠ¡å”¯ä¸€æ ‡è¯†ï¼ˆTask å¯¹è±¡çš„ idï¼‰ã€‚
- **artifact / artifactId**ï¼šä»»åŠ¡äº§å‡ºçš„èµ„æºï¼ˆæ–‡ä»¶/å›¾åƒ/æ–‡æ¡£ç­‰ï¼‰ä¸å…¶ç‰ˆæœ¬ IDã€‚
- **referenceTaskIds**ï¼šåœ¨æ–°äº¤äº’ä¸­å¼•ç”¨ä¹‹å‰çš„ä»»åŠ¡ï¼ŒæŒ‡æ˜è¯¥æ“ä½œæ˜¯åŸºäºå“ªä¸ªå·²å®Œæˆ/ç»ˆæ­¢ä»»åŠ¡çš„äº§ç‰©è¿›è¡Œçš„â€œåç»­/æ”¹è¿›â€ã€‚

**ä¸¤ç±»åŸºæœ¬å“åº”ï¼ˆä»£ç†çš„é€‰æ‹©ï¼‰**

- **Stateless Message**ï¼šç«‹å³å®Œæˆï¼Œä¸éœ€è¦åç»­çŠ¶æ€ç®¡ç†ï¼ˆä¾‹å¦‚ï¼šæŸ¥è¯¢ã€ç®€å•ç¡®è®¤ï¼‰ã€‚
- **Stateful Task**ï¼šåˆ›å»ºä¸€ä¸ª Task å¯¹è±¡ï¼Œåç»­é€šè¿‡ Task çŠ¶æ€æ¥è·Ÿè¸ªæ‰§è¡Œè¿›åº¦ä¸éœ€è¦çš„è¾“å…¥ã€‚

ä»£ç†æœ‰ä¸‰ç§è§’è‰²é£æ ¼ï¼š

1. **Message-only agent**ï¼šåªè¿”å› Messageï¼ˆè½»é‡ã€ç®€å•ï¼‰ã€‚
2. **Task-generating agent**ï¼šä¸€åˆ‡éƒ½ç”¨ Taskï¼ˆå³ä¾¿å¾ˆç®€å•çš„æ“ä½œä¹Ÿå»ºæˆå·²å®Œæˆçš„ Taskï¼‰ã€‚
3. **Hybrid agent**ï¼šå…ˆç”¨ Message åå•†èŒƒå›´ï¼Œå†åˆ›å»º Task æ¥æ‰§è¡Œï¼ˆæœ€å¸¸è§ã€æœ€çµæ´»ï¼‰ã€‚

**Task çš„å…¸å‹ç”Ÿå‘½å‘¨æœŸï¼ˆçŠ¶æ€æœºï¼‰**

å¸¸è§çŠ¶æ€ï¼ˆå®é™…å®ç°å¯æ‰©å±•ï¼‰ï¼š

- `created` / `queued`ï¼ˆå¯é€‰ï¼‰ï¼šä»»åŠ¡å·²æ¥æ”¶ã€ç­‰å¾…æ‰§è¡Œã€‚
- `in_progress`ï¼ˆè¿è¡Œä¸­ï¼‰ï¼šæ­£åœ¨æ‰§è¡Œã€‚
- `input-required`ï¼šä»£ç†éœ€è¦å®¢æˆ·ç«¯è¡¥å……ä¿¡æ¯ï¼ˆå¹¶è¿”å›è¯´æ˜/è¡¨å•ï¼‰ã€‚
- `auth-required`ï¼šéœ€è¦é¢å¤–æˆæƒ/å‡­è¯ï¼ˆä¾‹å¦‚ OAuth æˆæƒã€mTLSï¼‰ã€‚
- `blocked`ï¼ˆå¯é€‰ï¼‰ï¼šå¤–éƒ¨ä¾èµ–é˜»å¡ã€‚
- ç»ˆæ€ï¼ˆä¸å¯å¤åŸï¼‰ï¼š`completed`ã€`canceled`ã€`rejected`ã€`failed`ã€‚

å…¸å‹è½¬ç§»ï¼š
 `created` â†’ `in_progress` â†’ (`input-required` â†” `in_progress`) â†’ `completed`
 æˆ– `in_progress` â†’ `failed` / `canceled` / `rejected`
 å¦‚æœ `input-required`/`auth-required`ï¼Œå®¢æˆ·ç«¯å¿…é¡»åœ¨åŒä¸€ contextId ä¸­ç»§ç»­å‘é€æ¶ˆæ¯å¹¶å¡«å……æ‰€éœ€å­—æ®µ â€”â€” ä»£ç†åœ¨æ”¶åˆ°è¡¥å……åç»§ç»­æ‰§è¡Œå¹¶æ›´æ–° task çŠ¶æ€ã€‚

ä¾‹å¦‚

å®¢æˆ·ç«¯ï¼šè¯·æ±‚ç”Ÿæˆå›¾åƒï¼ˆæ—  contextId â€”â€” æ–°ä¼šè¯ï¼‰

```json
// client -> agent: message.send (req-001)
{
  "jsonrpc": "2.0",
  "id": "req-001",
  "method": "message.send",
  "params": {
    "message": {
      "role": "user",
      "parts": [{"kind": "text", "text": "Generate an image of a sailboat on the ocean."}],
      "messageId": "msg-user-001"
    }
  }
}
```

è§£é‡Šï¼š`messageId` ç”¨äºå»é‡/å¹‚ç­‰ï¼›æœªä¼  `contextId` è¡¨ç¤ºå¼€å¯æ–°ä¼šè¯ã€‚

ä»£ç†ï¼šåˆ›å»ºå¹¶ç«‹å³å®Œæˆä¸€ä¸ª Taskï¼ˆç”Ÿæˆå›¾ç‰‡ï¼‰ï¼Œè¿”å› `contextId`ã€`taskId`ã€artifactã€‚

```json
{
  "result": {
    "id": "task-boat-gen-123",
    "contextId": "ctx-conversation-abc",
    "status": {"state": "completed"},
    "artifacts": [
      {
        "artifactId": "artifact-boat-v1-xyz",
        "name": "sailboat_image.png",
        "description": "A generated image of a sailboat on the ocean.",
        "parts": [ /* file base64 bytes ... */ ]
      }
    ],
    "kind": "task"
  }
}
```

è§£é‡Šï¼š

- `contextId` è¡¨ç¤ºä¼šè¯ä¸Šä¸‹æ–‡ï¼Œåç»­æ¶ˆæ¯è¦å¸¦å®ƒä»¥è¡¨æ˜ç»§ç»­è¿™ä¸ªç›®æ ‡ã€‚
- `artifactId` å”¯ä¸€æ ‡è¯†ç”Ÿæˆçš„å›¾åƒç‰ˆæœ¬ï¼ˆv1ï¼‰ã€‚
- `task-boat-gen-123` æ˜¯ç¬¬ä¸€æ¬¡ä»»åŠ¡çš„ taskIdã€‚

å®¢æˆ·ç«¯ï¼šè¦æ±‚æŠŠèˆ¹æ”¹æˆçº¢è‰²ï¼Œå¸¦ä¸Šç›¸åŒ `contextId` å¹¶ç”¨ `referenceTaskIds` æŒ‡å‘åŸ task

```json
// client -> agent: message.send (req-002)
{
  "jsonrpc": "2.0",
  "id": "req-002",
  "method": "message.send",
  "params": {
    "message": {
      "role": "user",
      "messageId": "msg-user-002",
      "contextId": "ctx-conversation-abc",
      "referenceTaskIds": ["task-boat-gen-123"],
      "parts": [{"kind": "text", "text": "Please modify the sailboat to be red."}]
    }
  }
}
```

è§£é‡Šï¼š

- `contextId` ç»´æŒä¼šè¯ï¼Œ`referenceTaskIds` æ˜ç¡®è¯·æ±‚åŸºäºå‰ä¸€æ¬¡ task çš„äº§ç‰©è¿›è¡Œæ”¹è¿›ã€‚å®¢æˆ·ç«¯ä¹Ÿå¯ä»¥ç›´æ¥ä¼  `artifactId` æ¥æŒ‡å®šæ”¹å“ªä¸€ä¸ªç‰ˆæœ¬ã€‚

ä»£ç†ï¼šåˆ›å»ºæ–° Task å¹¶è¿”å›æ–°çš„ artifactï¼ˆä¿ç•™ nameï¼Œä½† artifactId æ–°ï¼‰

```json
{
  "result": {
    "id": "task-boat-color-456",
    "contextId": "ctx-conversation-abc",
    "status": {"state": "completed"},
    "artifacts": [
      {
        "artifactId": "artifact-boat-v2-red-pqr",
        "name": "sailboat_image.png",            // ä¿æŒ name ä¸å˜ï¼Œä¾¿äºå®¢æˆ·ç«¯è¯†åˆ«â€œåŒä¸€èµ„æºçš„æ–°ç‰ˆâ€
        "description": "A generated image of a red sailboat on the ocean.",
        "parts": [ /* new base64 bytes ... */ ]
      }
    ],
    "kind": "task"
  }
}
```

#### 3. $Agent \ Card $ 

**æ ¸å¿ƒå±æ€§**

- `name`, `description`, `version`: åŸºç¡€ä¿¡æ¯åŒ…æ‹¬æ™ºèƒ½ä½“çš„åå­—ä¸æè¿°
- `url`: æœåŠ¡ç«¯è¿è¡Œåœ¨çš„åœ°å€
- `capabilities`:  `streaming`ï¼ˆæµå¼ï¼‰ or `pushNotifications` ï¼ˆç»“æœå®æ—¶æ¨é€ï¼‰.
- `defaultInputModes` / `defaultOutputModes`: é»˜è®¤è¾“å…¥åª’ä»‹
- `skills`: æä¾›çš„èƒ½åŠ›ï¼Œä¸º $AgentSkill$ å¯¹è±¡ï¼ŒåŒ…æ‹¬`id`, `name`, `description`, `inputModes`, `outputModes`, and `examples`.

ç¤ºä¾‹

```python
# This will be the public-facing agent card
public_agent_card = AgentCard(
    name='Hello World Agent',
    description='Just a hello world agent',
    url='http://localhost:9999/',
    version='1.0.0',
    default_input_modes=['text'],
    default_output_modes=['text'],
    capabilities=AgentCapabilities(streaming=True),
    skills=[skill],  # Only the basic skill for the public card
    supports_authenticated_extended_card=True,
)
```

**$AgentCard$ å‘ç°ç­–ç•¥**

**(1) Well-Known URIï¼ˆå›ºå®šåœ°å€å‘ç°ï¼‰**

- å°±åƒç½‘ç«™çš„â€œé—¨ç‰Œå·â€ã€‚

- æœåŠ¡å™¨ä¼šæŠŠå®ƒçš„ *Agent Card* æ–‡ä»¶æ”¾åœ¨å›ºå®šè·¯å¾„ï¼š

  ```
  https://{domain}/.well-known/agent-card.json
  ```

- å®¢æˆ·ç«¯åªè¦çŸ¥é“å¯¹æ–¹çš„åŸŸåï¼Œå°±èƒ½å»è¿™ä¸ªæ ‡å‡†åœ°å€æ‹¿åˆ°å®ƒçš„ Agent Cardã€‚

ä¼˜ç‚¹ï¼šç®€å•ã€æ ‡å‡†åŒ–ï¼Œè‡ªåŠ¨åŒ–å‘ç°å¾ˆæ–¹ä¾¿ã€‚
ç¼ºç‚¹ï¼šå¦‚æœå¡ç‰‡é‡Œæœ‰æ•æ„Ÿä¿¡æ¯ï¼Œéœ€è¦åŠ è®¤è¯ã€‚

**(2) Curated Registriesï¼ˆé›†ä¸­å¼æ³¨å†Œä¸­å¿ƒå‘ç°ï¼‰**

- ç±»ä¼¼äºâ€œåº”ç”¨å•†åº—â€æˆ–â€œä¼ä¸šå†…éƒ¨æœåŠ¡ç›®å½•â€ã€‚
- æœ‰ä¸ªä¸­å¿ƒåŒ–çš„ **æ³¨å†Œè¡¨ï¼ˆRegistryï¼‰**ï¼Œç»Ÿä¸€ç®¡ç†æ‰€æœ‰ Agent çš„ *Agent Card*ã€‚
- å®¢æˆ·ç«¯å»æŸ¥æ³¨å†Œè¡¨ï¼Œæ¯”å¦‚æŒ‰ â€œæŠ€èƒ½ / æ ‡ç­¾ / æä¾›æ–¹åç§°â€ æ¥æœç´¢ã€‚

ä¼˜ç‚¹ï¼šé›†ä¸­ç®¡ç†ã€æ”¯æŒæƒé™æ§åˆ¶ï¼Œèƒ½æŒ‰åŠŸèƒ½å‘ç° Agentã€‚
ç¼ºç‚¹ï¼šè¦é¢å¤–éƒ¨ç½²å’Œç»´æŠ¤ä¸€ä¸ªæ³¨å†Œè¡¨æœåŠ¡ï¼ŒA2A æ ‡å‡†é‡Œæ²¡å®šä¹‰ç»Ÿä¸€ APIã€‚

**(3) Direct Configurationï¼ˆç›´æ¥é…ç½®å‘ç°ï¼‰**

- æœ€ç®€å•ç²—æš´ï¼š**æ‰‹å·¥é…ç½®**ã€‚
- å®¢æˆ·ç«¯ç›´æ¥å†™æ­» Agent Card çš„ä¿¡æ¯ï¼ˆç¡¬ç¼–ç ã€é…ç½®æ–‡ä»¶ã€ç¯å¢ƒå˜é‡ï¼‰ã€‚

ä¼˜ç‚¹ï¼šå®ç°å®¹æ˜“ï¼Œé€‚åˆæµ‹è¯•ã€ç§æœ‰åœºæ™¯ã€‚
ç¼ºç‚¹ï¼šç¼ºä¹çµæ´»æ€§ï¼Œä¸€æ”¹ä¿¡æ¯å°±å¾—é‡æ–°é…ç½®å®¢æˆ·ç«¯ã€‚

å®¢æˆ·ç«¯é€šè¿‡è§£æ **AgentCard**ï¼Œå¯ä»¥çŸ¥é“ï¼š  

- ç›®æ ‡ Agent èƒ½åšä»€ä¹ˆ  
- å¦‚ä½•ä¸ä¹‹é€šä¿¡ï¼ˆURLï¼‰  
- æ˜¯å¦æ”¯æŒæµå¼ä¼ è¾“æˆ–æ¨é€é€šçŸ¥  
- éœ€è¦å“ªç§è®¤è¯æ–¹å¼  
- æä¾›çš„æŠ€èƒ½åŠç»†èŠ‚  

```mermaid
flowchart TD
  subgraph æ›¿ä»£æ–¹å¼
      A1[å®¢æˆ·ç«¯] -->|æŸ¥è¯¢| A2[æ³¨å†Œä¸­å¿ƒ]
      A2 -->|è¿”å›| A3[Agent Card åˆ—è¡¨]
  end

  subgraph æ ‡å‡†æ–¹å¼
      B1[å®¢æˆ·ç«¯] -->|è¯·æ±‚ GET /.well-known/agent.json| B2[Agent æœåŠ¡]
      B2 -->|è¿”å›| B3[Agent Card JSON]
  end
```

#### 4. $Agent \ Executor$

$AgentExecutor$ æ¥å£æä¾›äº†ä¸¤ä¸ªæ–¹æ³•

- `async def execute(self, context: RequestContext, event_queue: EventQueue)`ï¼šå¤„ç†æœŸæœ›å“åº”æˆ–äº‹ä»¶æµçš„ä¼ å…¥è¯·æ±‚ã€‚å®ƒå¤„ç†ç”¨æˆ·çš„è¾“å…¥ï¼ˆå¯é€šè¿‡ `context` è·å¾—ï¼‰ï¼Œå¹¶ä½¿ç”¨`event_queue`å‘å›`Message`ã€`Task`ã€`TaskStatusUpdateEvent`æˆ–`TaskArtifactUpdateEvent`å¯¹è±¡ã€‚
- `async def cancel(self, context: RequestContext, event_queue: EventQueue)`ï¼šå¤„ç†å–æ¶ˆæ­£åœ¨è¿›è¡Œçš„ä»»åŠ¡çš„è¯·æ±‚ã€‚

`RequestContext`æœ‰å…³ä¼ å…¥è¯·æ±‚çš„ä¿¡æ¯ï¼Œä¾‹å¦‚ç”¨æˆ·æ¶ˆæ¯å’Œä»»ä½•ç°æœ‰ä»»åŠ¡çš„è¯¦ç»†ä¿¡æ¯ã€‚`EventQueue`æ‰§è¡Œå™¨ä½¿ç”¨ å‘å®¢æˆ·ç«¯å‘é€äº‹ä»¶ã€‚

**ç¤ºä¾‹**

é¦–å…ˆå®šä¹‰ä¸€ä¸ªæ™ºèƒ½ä½“

``` pythonÂ 
class HelloWorldAgent:
    """Hello World Agent."""

    async def invoke(self) -> str:
        return 'Hello World'
```

ç„¶åå®šä¹‰æ‰§è¡Œå™¨

```python
class HelloWorldAgentExecutor(AgentExecutor):
    """Test AgentProxy Implementation."""

    def __init__(self):
        self.agent = HelloWorldAgent()
        
    async def execute(
        self,
        context: RequestContext,
        event_queue: EventQueue,
    ) -> None:
        result = await self.agent.invoke()
        await event_queue.enqueue_event(new_agent_text_message(result))
      
   	async def cancel(
        self, context: RequestContext, event_queue: EventQueue
    ) -> None:
        raise Exception('cancel not supported')
```

å½“ `message/send` æˆ– `message/stream` è¯·æ±‚è¿›å…¥æ—¶ï¼ˆä¸¤è€…éƒ½ç”±è¿™ä¸ªæ‰§è¡Œå™¨`execute`å¤„ç†ï¼‰ï¼š

1. å®ƒè°ƒç”¨`self.agent.invoke()`ä»¥è·å–â€œHello Worldâ€å­—ç¬¦ä¸²ã€‚
2. `Message` å®ƒä½¿ç”¨å®ç”¨å‡½æ•°åˆ›å»ºä¸€ä¸ª $A2A$ å¯¹è±¡ `new_agent_text_message`ã€‚
3. å®ƒä¼šå°†æ­¤æ¶ˆæ¯æ”¾å…¥é˜Ÿåˆ—ä¸­`event_queue`ã€‚åº•å±‚`DefaultRequestHandler`ä¼šå¤„ç†æ­¤é˜Ÿåˆ—ï¼Œå¹¶å°†å“åº”å‘é€ç»™å®¢æˆ·ç«¯ã€‚å¯¹äºåƒè¿™æ ·çš„å•æ¡æ¶ˆæ¯ï¼Œåœ¨æµå…³é—­ä¹‹å‰ï¼Œå®ƒå°†äº§ç”Ÿä¸€ä¸ªå“åº”`message/send`æˆ–ä¸€ä¸ªäº‹ä»¶`message/stream`åœ¨æµå…³é—­ä¹‹å‰

#### 5. å¯åŠ¨æœåŠ¡

**æ ¸å¿ƒç»„ä»¶è¯´æ˜**

1. **DefaultRequestHandler**

- SDK æä¾›çš„é»˜è®¤è¯·æ±‚å¤„ç†å™¨ã€‚  
- å®ƒéœ€è¦ä¸¤ä¸ªå‚æ•°ï¼š  
  1. **AgentExecutor**ï¼šä½ è‡ªå·±å®ç°çš„ Agent é€»è¾‘ï¼ˆè¿™é‡Œæ˜¯ `HelloWorldAgentExecutor`ï¼‰ã€‚  
  2. **TaskStore**ï¼šä»»åŠ¡å­˜å‚¨å™¨ï¼ˆè¿™é‡Œç”¨ `InMemoryTaskStore` å†…å­˜ç‰ˆï¼‰ã€‚  

ä½œç”¨ï¼š  
- å°† **A2A RPC è¯·æ±‚** è·¯ç”±åˆ°å¯¹åº”çš„ executor æ–¹æ³•ï¼Œæ¯”å¦‚ `execute`ã€`cancel`ã€‚  
- ç”¨ `TaskStore` ç®¡ç†ä»»åŠ¡çš„ç”Ÿå‘½å‘¨æœŸï¼ˆçŠ¶æ€ä¿å­˜ã€æµå¼äº¤äº’ã€é‡æ–°è®¢é˜…ç­‰ï¼‰ã€‚  

2. **A2AStarletteApplication**

- ä¸€ä¸ªåŸºäº **Starlette** çš„åº”ç”¨åŒ…è£…å™¨ã€‚  
- åˆå§‹åŒ–éœ€è¦ï¼š  
  - `agent_card`: Agent çš„å…ƒä¿¡æ¯ï¼ˆä¼šåœ¨ `/.well-known/agent-card.json` å…¬å¼€ï¼‰ã€‚  
  - `http_handler`: è¯·æ±‚å¤„ç†å™¨ï¼ˆé€šå¸¸æ˜¯ `DefaultRequestHandler`ï¼‰ã€‚  
  - `extended_agent_card`: å¯é€‰ï¼Œç”¨äºè®¤è¯ç”¨æˆ·çš„æ‰©å±•ç‰ˆæœ¬ Agent Cardã€‚  

ä½œç”¨ï¼š  
- å°† Agent ä¿¡æ¯æš´éœ²å‡ºæ¥ã€‚  
- å¤„ç†æ‰€æœ‰ A2A æ–¹æ³•è°ƒç”¨ï¼Œè½¬å‘åˆ° `AgentExecutor`ã€‚  

3. **uvicorn.run(server.build())**

- `A2AStarletteApplication.build()` ä¼šç”Ÿæˆä¸€ä¸ªæ ‡å‡†çš„ **Starlette app**ã€‚  
- `uvicorn.run()` å¯åŠ¨ HTTP æœåŠ¡ï¼Œè®© Agent å¯¹å¤–å¯è®¿é—®ã€‚  

å‚æ•°ï¼š  
- `host="0.0.0.0"` â†’ æœåŠ¡å¯¹å¤–ç½‘å¯è§ã€‚  
- `port=9999` â†’ ç›‘å¬ 9999 ç«¯å£ï¼ˆä¸ AgentCard é‡Œçš„ `url` ä¿æŒä¸€è‡´ï¼‰ã€‚  

```python
import uvicorn

from a2a.server.apps import A2AStarletteApplication
from a2a.server.request_handlers import DefaultRequestHandler
from a2a.server.tasks import InMemoryTaskStore
from a2a.types import (
    AgentCapabilities,
    AgentCard,
    AgentSkill,
)
from agent_executor import HelloWorldAgentExecutor


if __name__ == '__main__':
    # å®šä¹‰åŸºç¡€æŠ€èƒ½
    skill = AgentSkill(
        id='hello_world',
        name='Returns hello world',
        description='just returns hello world',
        tags=['hello world'],
        examples=['hi', 'hello world'],
    )

    # å®šä¹‰æ‰©å±•æŠ€èƒ½ï¼ˆä»…è®¤è¯ç”¨æˆ·å¯è§ï¼‰
    extended_skill = AgentSkill(
        id='super_hello_world',
        name='Returns a SUPER Hello World',
        description='A more enthusiastic greeting, only for authenticated users.',
        tags=['hello world', 'super', 'extended'],
        examples=['super hi', 'give me a super hello'],
    )

    # å…¬å…± Agent Cardï¼ˆåŒ¿åç”¨æˆ·å¯è§ï¼‰
    public_agent_card = AgentCard(
        name='Hello World Agent',
        description='Just a hello world agent',
        url='http://localhost:9999/',
        version='1.0.0',
        default_input_modes=['text'],
        default_output_modes=['text'],
        capabilities=AgentCapabilities(streaming=True),
        skills=[skill],  # å…¬å…±å¡åªåŒ…å«åŸºç¡€ skill
        supports_authenticated_extended_card=True,
    )

    # è®¤è¯ç”¨æˆ·ä¸“å±çš„æ‰©å±• Agent Card
    specific_extended_agent_card = public_agent_card.model_copy(
        update={
            'name': 'Hello World Agent - Extended Edition',
            'description': 'The full-featured hello world agent for authenticated users.',
            'version': '1.0.1',
            'skills': [skill, extended_skill],  # åŒæ—¶åŒ…å«ä¸¤ä¸ª skill
        }
    )

    # è¯·æ±‚å¤„ç†å™¨ï¼šç»‘å®šæ‰§è¡Œå™¨å’Œä»»åŠ¡å­˜å‚¨
    request_handler = DefaultRequestHandler(
        agent_executor=HelloWorldAgentExecutor(),
        task_store=InMemoryTaskStore(),
    )

    # å¯åŠ¨ A2A åº”ç”¨
    server = A2AStarletteApplication(
        agent_card=public_agent_card,
        http_handler=request_handler,
        extended_agent_card=specific_extended_agent_card,
    )

    uvicorn.run(server.build(), host='0.0.0.0', port=9999)
```

#### 6. ä½¿ç”¨æœåŠ¡

**è·å– $Agent \ Card $ å¹¶åˆå§‹åŒ–æ™ºèƒ½ä½“**

```python
base_url = 'http://localhost:9999'

async with httpx.AsyncClient() as httpx_client:
    # Initialize A2ACardResolver
    resolver = A2ACardResolver(
        httpx_client=httpx_client,
        base_url=base_url,
        # agent_card_path uses default, extended_agent_card_path also uses default
    )
```

`A2ACardResolver` æ˜¯ä¸€ä¸ªæ–¹ä¾¿çš„å·¥å…·ã€‚å®ƒé¦–å…ˆä»æœåŠ¡å™¨çš„ `/.well-known/agent-card.json` è·¯å¾„æ‹‰å– AgentCardï¼Œç„¶åç”¨å®ƒåˆå§‹åŒ–å®¢æˆ·ç«¯ã€‚

**å‘é€ä¸€ä¸ªéæµå¼è¯·æ±‚**

```python
client = A2AClient(
    httpx_client=httpx_client, agent_card=final_agent_card_to_use
)
logger.info('A2AClient initialized.')
send_message_payload: dict[str, Any] = {
    'message': {
        'role': 'user',
        'parts': [
            {'kind': 'text', 'text': 'how much is 10 USD in INR?'}
        ],
        'messageId': uuid4().hex,
    },
}
request = SendMessageRequest(
    id=str(uuid4()), params=MessageSendParams(**send_message_payload)
)

response = await client.send_message(request)
print(response.model_dump(mode='json', exclude_none=True))
```

- `send_message_payload` æ„é€  `MessageSendParams` æ‰€éœ€çš„æ•°æ®ã€‚
- ç”¨ `SendMessageRequest` å°è£…å®ƒã€‚
- å‘é€åï¼Œä¼šå¾—åˆ°ä¸€ä¸ª `SendMessageResponse`ï¼Œå…¶ä¸­å¯èƒ½æ˜¯æˆåŠŸå“åº”ï¼ˆå¸¦ agent çš„ `Message`ï¼‰æˆ–é”™è¯¯å“åº”ï¼ˆ`JSONRPCErrorResponse`ï¼‰ã€‚

**Handling Task IDs (Illustrative Note for Helloworld):**

Helloworld å®¢æˆ·ç«¯æ²¡æœ‰ç›´æ¥è°ƒç”¨ `get_task` æˆ– `cancel_task`ï¼Œå› ä¸ºè¿™ä¸ªç®€å•çš„ agent åœ¨è°ƒç”¨ `message/send` æ—¶ï¼Œ`DefaultRequestHandler` ç›´æ¥è¿”å›ä¸€ä¸ª `Message` å“åº”ï¼Œè€Œä¸æ˜¯ `Task` å¯¹è±¡ã€‚
 æ›´å¤æ‚çš„ agentï¼ˆä¾‹å¦‚ LangGraph ç¤ºä¾‹ï¼‰å¦‚æœ `message/send` è¿”å› `Task` å¯¹è±¡ï¼Œé‚£ä¹ˆå°±å¯ä»¥ç”¨å®ƒçš„ `id` æ¥è°ƒç”¨ `get_task` æˆ– `cancel_task`ã€‚

**å‘é€ä¸€ä¸ªæµå¼ä¿¡æ¯è¯·æ±‚**

```python
streaming_request = SendStreamingMessageRequest(
    id=str(uuid4()), params=MessageSendParams(**send_message_payload)
)

stream_response = client.send_message_streaming(streaming_request)

async for chunk in stream_response:
    print(chunk.model_dump(mode='json', exclude_none=True))
```

- è¿™ä¸ªæ–¹æ³•è°ƒç”¨ agent çš„ `message/stream` æ¥å£ã€‚
- `DefaultRequestHandler` ä¼šè°ƒç”¨ `HelloWorldAgentExecutor.execute`ã€‚
- æ‰§è¡Œè¿‡ç¨‹ä¸­ä¼šäº§ç”Ÿä¸€ä¸ª "Hello World" æ¶ˆæ¯ï¼Œç„¶åå…³é—­äº‹ä»¶é˜Ÿåˆ—ã€‚
- å®¢æˆ·ç«¯å°†æ”¶åˆ°è¿™æ¡æ¶ˆæ¯ä½œä¸ºä¸€ä¸ª `SendStreamingMessageResponse` äº‹ä»¶ï¼Œç„¶åæµç»“æŸã€‚
- `stream_response` æ˜¯ä¸€ä¸ªå¼‚æ­¥ç”Ÿæˆå™¨ (AsyncGenerator)ã€‚

**æœŸæœ›è¾“å‡º**

è¿è¡Œ `test_client.py` æ—¶ï¼Œä½ ä¼šçœ‹åˆ°å¦‚ä¸‹ JSON è¾“å‡ºï¼ˆ`id` ç­‰å­—æ®µæ¯æ¬¡è¿è¡Œä¼šä¸åŒï¼‰ï¼š

```python
// Non-streaming response
{"jsonrpc":"2.0","id":"xxxxxxxx","result":{"type":"message","role":"agent","parts":[{"type":"text","text":"Hello World"}],"messageId":"yyyyyyyy"}}

// Streaming response (one chunk)
{"jsonrpc":"2.0","id":"zzzzzzzz","result":{"type":"message","role":"agent","parts":[{"type":"text","text":"Hello World"}],"messageId":"wwwwwwww","final":true}}
```

### ä¸ƒã€claude code agent

#### 1. ClaudeAgentOptions

å¯¹äº $claude \ code$ è¿›è¡Œé…ç½®ï¼ŒåŒ…æ‹¬æ¨¡å‹ï¼Œå·¥å…·ä½¿ç”¨ï¼Œ$hook$ é’©å­å‡½æ•°ç­‰

```python
@dataclass
class ClaudeAgentOptions:
    allowed_tools: list[str] = field(default_factory=list)
    system_prompt: str | SystemPromptPreset | None = None
    mcp_servers: dict[str, McpServerConfig] | str | Path = field(default_factory=dict)
    permission_mode: PermissionMode | None = None
    continue_conversation: bool = False
    resume: str | None = None
    max_turns: int | None = None
    disallowed_tools: list[str] = field(default_factory=list)
    model: str | None = None
    permission_prompt_tool_name: str | None = None
    cwd: str | Path | None = None
    settings: str | None = None
    add_dirs: list[str | Path] = field(default_factory=list)
    env: dict[str, str] = field(default_factory=dict)
    extra_args: dict[str, str | None] = field(default_factory=dict)
    max_buffer_size: int | None = None
    debug_stderr: Any = sys.stderr  # Deprecated
    stderr: Callable[[str], None] | None = None
    can_use_tool: CanUseTool | None = None
    hooks: dict[HookEvent, list[HookMatcher]] | None = None
    user: str | None = None
    include_partial_messages: bool = False
    fork_session: bool = False
    agents: dict[str, AgentDefinition] | None = None
    setting_sources: list[SettingSource] | None = None
```

ä¾‹å¦‚ä¸‹é¢çš„è¡¨è¾¾å¼

```python
CLAUDE_OPTIONS = ClaudeAgentOptions(
    model="claude-sonnet-4-5",

    # å…³é”®1: æ·»åŠ æ‰€æœ‰éœ€è¦çš„å·¥å…·åˆ°å…è®¸åˆ—è¡¨
    allowed_tools=["Bash"] + PLAYWRIGHT_TOOLS,

    mcp_servers=str(BASE_PATH.parent / ".mcp.json"),

    # å…³é”®2: ä¸ºä¸åŒå·¥å…·é…ç½®ä¸åŒçš„ Hook
    hooks={
        "PreToolUse": [
            # Hook 1: æ‰¹å‡† Bash
            HookMatcher(
                matcher="Bash",
                hooks=[auto_approve_bash]
            )
        ] + PLAYWRIGHT_HOOKS,
    }
)
```

#### 2. Hook é’©å­å‡½æ•°

`HookEvent` æ˜¯è¡¨ç¤ºæ”¯æŒçš„ hook äº‹ä»¶ç±»å‹ï¼ŒLiteralæ˜¯å¸¸é‡

```python
HookEvent = Literal[
    "PreToolUse",      # Called before tool execution
    "PostToolUse",     # Called after tool execution
    "UserPromptSubmit", # Called when user submits a prompt
    "Stop",            # Called when stopping execution
    "SubagentStop",    # Called when a subagent stops
    "PreCompact"       # Called before message compaction
]
```

`HookCallBack` æ˜¯é’©å­å‡½æ•°ï¼Œç”¨äºå®é™…å¤„ç†

è¾“å…¥ä¸º

- `input_data`: Hook-specific input data (see [hook documentation](https://docs.claude.com/en/docs/claude-code/hooks#hook-input))
- `tool_use_id`: Optional tool use identifier (for tool-related hooks)
- `context`: Hook context with additional information 

è¾“å‡ºä¸º

- `decision`: `"block"` to block the action
- `systemMessage`: System message to add to the transcript
- `hookSpecificOutput`: Hook-specific output data

`input_data` æ˜¯è¾“å…¥æ•°æ®å¦‚ä¸‹

```json
{
  // Common fields
  session_id: string
  transcript_path: string  // Path to conversation JSON
  cwd: string              // The current working directory when the hook is invoked

  // Event-specific fields
  hook_event_name: string
  ...
}
```

`HookContext`

```python
@dataclass
class HookContext:
    signal: Any | None = None  # Future: abort signal support
```

`HookMatcher`

å°†é’©å­å‡½æ•°å¯¹åº”åˆ°æŒ‡å®šçš„äº‹ä»¶æˆ–è€…å·¥å…·

```python
@dataclass
class HookMatcher:
    matcher: str | None = None        # Tool name or pattern to match (e.g., "Bash", "Write|Edit")
    hooks: list[HookCallback] = field(default_factory=list)  # List of callbacks to execute
```

ç¤ºä¾‹

```python
from claude_agent_sdk import query, ClaudeAgentOptions, HookMatcher, HookContext
from typing import Any

async def validate_bash_command(
    input_data: dict[str, Any],
    tool_use_id: str | None,
    context: HookContext
) -> dict[str, Any]:
    """Validate and potentially block dangerous bash commands."""
    if input_data['tool_name'] == 'Bash':
        command = input_data['tool_input'].get('command', '')
        if 'rm -rf /' in command:
            return {
                'hookSpecificOutput': {
                    'hookEventName': 'PreToolUse',
                    'permissionDecision': 'deny',
                    'permissionDecisionReason': 'Dangerous command blocked'
                }
            }
    return {}

async def log_tool_use(
    input_data: dict[str, Any],
    tool_use_id: str | None,
    context: HookContext
) -> dict[str, Any]:
    """Log all tool usage for auditing."""
    print(f"Tool used: {input_data.get('tool_name')}")
    return {}

options = ClaudeAgentOptions(
    hooks={
        'PreToolUse': [
            HookMatcher(matcher='Bash', hooks=[validate_bash_command]),
            HookMatcher(hooks=[log_tool_use])  # Applies to all tools
        ],
        'PostToolUse': [
            HookMatcher(hooks=[log_tool_use])
        ]
    }
)

async for message in query(
    prompt="Analyze this codebase",
    options=options
):
    print(message)
```

ä½¿ç”¨å‘½ä»¤å¯ä»¥è‡ªåŠ¨è·³è¿‡æ‰€æœ‰æƒé™ç¡®è®¤

```bash
claude --dangerously-skip-permissions
```

#### 3. query å‡½æ•°

åˆ›å»ºä¸€ä¸ªæ–°äº¤äº’ä½¿ç”¨claude codeï¼Œè¿”å›ä¸€ä¸ªå¼‚æ­¥å¤„ç†å™¨æµå¼è¿”å›ï¼Œå®ƒä¸å¸¦è®°å¿†

Returns an `AsyncIterator[Message]` that yields messages from the conversation. è¿”å›çš„æ˜¯ä¿¡æ¯åˆ—è¡¨

```python
async def query(
    *,
    prompt: str | AsyncIterable[dict[str, Any]],
    options: ClaudeAgentOptions | None = None
) -> AsyncIterator[Message]
```

```python
import asyncio
from claude_agent_sdk import query, ClaudeAgentOptions

async def main():
    options = ClaudeAgentOptions(
        system_prompt="You are an expert Python developer",
        permission_mode='acceptEdits',
        cwd="/home/user/project"
    )

    async for message in query(
        prompt="Create a Python web server",
        options=options
    ):
        print(message)


asyncio.run(main())
```

#### 4. tool å·¥å…·

é€šè¿‡è£…é¥°å™¨çš„å½¢å¼åŠ åˆ°å‡½æ•°ä¸Šï¼Œå®šä¹‰å¦‚ä¸‹

```python
def tool(
    name: str,
    description: str,
    input_schema: type | dict[str, Any]
) -> Callable[[Callable[[Any], Awaitable[dict[str, Any]]]], SdkMcpTool[Any]]
```

`input_schema` å¯ä»¥æ˜¯ç®€å•çš„æ˜ å°„ä¹Ÿå¯ä»¥æ˜¯ $json$ æ ¼å¼

```python
{"text": str, "count": int, "enabled": bool}

{
    "type": "object",
    "properties": {
        "text": {"type": "string"},
        "count": {"type": "integer", "minimum": 0}
    },
    "required": ["text"]
}
```

### å…«ã€agent è®¾è®¡æ¨¡å¼

#### 1. ReAct

ReActèŒƒå¼é€šè¿‡ä¸€ç§ç‰¹æ®Šçš„æç¤ºå·¥ç¨‹æ¥å¼•å¯¼æ¨¡å‹ï¼Œä½¿å…¶æ¯ä¸€æ­¥çš„è¾“å‡ºéƒ½éµå¾ªä¸€ä¸ªå›ºå®šçš„è½¨è¿¹ï¼š

- **Thought (æ€è€ƒ)ï¼š** è¿™æ˜¯æ™ºèƒ½ä½“çš„â€œå†…å¿ƒç‹¬ç™½â€ã€‚å®ƒä¼šåˆ†æå½“å‰æƒ…å†µã€åˆ†è§£ä»»åŠ¡ã€åˆ¶å®šä¸‹ä¸€æ­¥è®¡åˆ’ï¼Œæˆ–è€…åæ€ä¸Šä¸€æ­¥çš„ç»“æœã€‚
- **Action (è¡ŒåŠ¨)ï¼š** è¿™æ˜¯æ™ºèƒ½ä½“å†³å®šé‡‡å–çš„å…·ä½“åŠ¨ä½œï¼Œé€šå¸¸æ˜¯è°ƒç”¨ä¸€ä¸ªå¤–éƒ¨å·¥å…·ï¼Œä¾‹å¦‚ `Search['åä¸ºæœ€æ–°æ¬¾æ‰‹æœº']`ã€‚
- **Observation (è§‚å¯Ÿ)ï¼š** è¿™æ˜¯æ‰§è¡Œ`Action`åä»å¤–éƒ¨å·¥å…·è¿”å›çš„ç»“æœï¼Œä¾‹å¦‚æœç´¢ç»“æœçš„æ‘˜è¦æˆ–APIçš„è¿”å›å€¼ã€‚

![image-20251107095631820](assets/image-20251107095631820.png)

è®¾è®¡å·¥å…·

```python
from serpapi import SerpApiClient

def search(query: str) -> str:
    """
    ä¸€ä¸ªåŸºäºSerpApiçš„å®æˆ˜ç½‘é¡µæœç´¢å¼•æ“å·¥å…·ã€‚
    å®ƒä¼šæ™ºèƒ½åœ°è§£ææœç´¢ç»“æœï¼Œä¼˜å…ˆè¿”å›ç›´æ¥ç­”æ¡ˆæˆ–çŸ¥è¯†å›¾è°±ä¿¡æ¯ã€‚
    """
    print(f"ğŸ” æ­£åœ¨æ‰§è¡Œ [SerpApi] ç½‘é¡µæœç´¢: {query}")
    try:
        api_key = os.getenv("SERPAPI_API_KEY")
        if not api_key:
            return "é”™è¯¯:SERPAPI_API_KEY æœªåœ¨ .env æ–‡ä»¶ä¸­é…ç½®ã€‚"

        params = {
            "engine": "google",
            "q": query,
            "api_key": api_key,
            "gl": "cn",  # å›½å®¶ä»£ç 
            "hl": "zh-cn", # è¯­è¨€ä»£ç 
        }
        
        client = SerpApiClient(params)
        results = client.get_dict()
        
        # æ™ºèƒ½è§£æ:ä¼˜å…ˆå¯»æ‰¾æœ€ç›´æ¥çš„ç­”æ¡ˆ
        if "answer_box_list" in results:
            return "\n".join(results["answer_box_list"])
        if "answer_box" in results and "answer" in results["answer_box"]:
            return results["answer_box"]["answer"]
        if "knowledge_graph" in results and "description" in results["knowledge_graph"]:
            return results["knowledge_graph"]["description"]
        if "organic_results" in results and results["organic_results"]:
            # å¦‚æœæ²¡æœ‰ç›´æ¥ç­”æ¡ˆï¼Œåˆ™è¿”å›å‰ä¸‰ä¸ªæœ‰æœºç»“æœçš„æ‘˜è¦
            snippets = [
                f"[{i+1}] {res.get('title', '')}\n{res.get('snippet', '')}"
                for i, res in enumerate(results["organic_results"][:3])
            ]
            return "\n\n".join(snippets)
        
        return f"å¯¹ä¸èµ·ï¼Œæ²¡æœ‰æ‰¾åˆ°å…³äº '{query}' çš„ä¿¡æ¯ã€‚"

    except Exception as e:
        return f"æœç´¢æ—¶å‘ç”Ÿé”™è¯¯: {e}"
```

æ„å»ºå·¥å…·æ‰§è¡Œå™¨ 

æ³¨å†Œå·¥å…·æ—¶åŒ…æ‹¬åå­—ï¼Œæè¿°å’Œå‡½æ•°

```python
from typing import Dict, Any

class ToolExecutor:
    """
    ä¸€ä¸ªå·¥å…·æ‰§è¡Œå™¨ï¼Œè´Ÿè´£ç®¡ç†å’Œæ‰§è¡Œå·¥å…·ã€‚
    """
    def __init__(self):
        self.tools: Dict[str, Dict[str, Any]] = {}

    def registerTool(self, name: str, description: str, func: callable):
        """
        å‘å·¥å…·ç®±ä¸­æ³¨å†Œä¸€ä¸ªæ–°å·¥å…·ã€‚
        """
        if name in self.tools:
            print(f"è­¦å‘Š:å·¥å…· '{name}' å·²å­˜åœ¨ï¼Œå°†è¢«è¦†ç›–ã€‚")
        self.tools[name] = {"description": description, "func": func}
        print(f"å·¥å…· '{name}' å·²æ³¨å†Œã€‚")

    def getTool(self, name: str) -> callable:
        """
        æ ¹æ®åç§°è·å–ä¸€ä¸ªå·¥å…·çš„æ‰§è¡Œå‡½æ•°ã€‚
        """
        return self.tools.get(name, {}).get("func")

    def getAvailableTools(self) -> str:
        """
        è·å–æ‰€æœ‰å¯ç”¨å·¥å…·çš„æ ¼å¼åŒ–æè¿°å­—ç¬¦ä¸²ã€‚
        """
        return "\n".join([
            f"- {name}: {info['description']}" 
            for name, info in self.tools.items()
        ])
```

å°†æˆ‘ä»¬å†™çš„å·¥å…·æ³¨å†Œåˆ° $ToolExecutor$

```python
if __name__ == '__main__':
   	# 1. åˆå§‹åŒ–å·¥å…·æ‰§è¡Œå™¨
    toolExecutor = ToolExecutor()

    # 2. æ³¨å†Œæˆ‘ä»¬çš„å®æˆ˜æœç´¢å·¥å…·
    search_description = "ä¸€ä¸ªç½‘é¡µæœç´¢å¼•æ“ã€‚å½“ä½ éœ€è¦å›ç­”å…³äºæ—¶äº‹ã€äº‹å®ä»¥åŠåœ¨ä½ çš„çŸ¥è¯†åº“ä¸­æ‰¾ä¸åˆ°çš„ä¿¡æ¯æ—¶ï¼Œåº”ä½¿ç”¨æ­¤å·¥å…·ã€‚"
    toolExecutor.registerTool("Search", search_description, search)
    
    # 3. æ‰“å°å¯ç”¨çš„å·¥å…·
    print("\n--- å¯ç”¨çš„å·¥å…· ---")
    print(toolExecutor.getAvailableTools())

    # 4. æ™ºèƒ½ä½“çš„Actionè°ƒç”¨ï¼Œè¿™æ¬¡æˆ‘ä»¬é—®ä¸€ä¸ªå®æ—¶æ€§çš„é—®é¢˜
    print("\n--- æ‰§è¡Œ Action: Search['è‹±ä¼Ÿè¾¾æœ€æ–°çš„GPUå‹å·æ˜¯ä»€ä¹ˆ'] ---")
    tool_name = "Search"
    tool_input = "è‹±ä¼Ÿè¾¾æœ€æ–°çš„GPUå‹å·æ˜¯ä»€ä¹ˆ"

    tool_function = toolExecutor.getTool(tool_name)
    if tool_function:
        observation = tool_function(tool_input)
        print("--- è§‚å¯Ÿ (Observation) ---")
        print(observation)
    else:
        print(f"é”™è¯¯:æœªæ‰¾åˆ°åä¸º '{tool_name}' çš„å·¥å…·ã€‚")
    
```

ReAct æç¤ºè¯è®¾è®¡

```python
# ReAct æç¤ºè¯æ¨¡æ¿
REACT_PROMPT_TEMPLATE = """
è¯·æ³¨æ„ï¼Œä½ æ˜¯ä¸€ä¸ªæœ‰èƒ½åŠ›è°ƒç”¨å¤–éƒ¨å·¥å…·çš„æ™ºèƒ½åŠ©æ‰‹ã€‚

å¯ç”¨å·¥å…·å¦‚ä¸‹:
{tools}

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¿›è¡Œå›åº”:

Thought: ä½ çš„æ€è€ƒè¿‡ç¨‹ï¼Œç”¨äºåˆ†æé—®é¢˜ã€æ‹†è§£ä»»åŠ¡å’Œè§„åˆ’ä¸‹ä¸€æ­¥è¡ŒåŠ¨ã€‚
Action: ä½ å†³å®šé‡‡å–çš„è¡ŒåŠ¨ï¼Œå¿…é¡»æ˜¯ä»¥ä¸‹æ ¼å¼ä¹‹ä¸€:
- `{{tool_name}}[{{tool_input}}]`:è°ƒç”¨ä¸€ä¸ªå¯ç”¨å·¥å…·ã€‚
- `Finish[æœ€ç»ˆç­”æ¡ˆ]`:å½“ä½ è®¤ä¸ºå·²ç»è·å¾—æœ€ç»ˆç­”æ¡ˆæ—¶ã€‚
- å½“ä½ æ”¶é›†åˆ°è¶³å¤Ÿçš„ä¿¡æ¯ï¼Œèƒ½å¤Ÿå›ç­”ç”¨æˆ·çš„æœ€ç»ˆé—®é¢˜æ—¶ï¼Œä½ å¿…é¡»åœ¨Action:å­—æ®µåä½¿ç”¨ finish(answer="...") æ¥è¾“å‡ºæœ€ç»ˆç­”æ¡ˆã€‚

ç°åœ¨ï¼Œè¯·å¼€å§‹è§£å†³ä»¥ä¸‹é—®é¢˜:
Question: {question}
History: {history}
"""
```

ReActAgent è®¾è®¡

```python
class ReActAgent:
    def __init__(self, llm_client: HelloAgentsLLM, tool_executor: ToolExecutor, max_steps: int = 5):
        self.llm_client = llm_client
        self.tool_executor = tool_executor
        self.max_steps = max_steps
        self.history = []
       
    def _parse_output(self, text: str):
        """è§£æLLMçš„è¾“å‡ºï¼Œæå–Thoughtå’ŒActionã€‚"""
        thought_match = re.search(r"Thought: (.*)", text)
        action_match = re.search(r"Action: (.*)", text)
        thought = thought_match.group(1).strip() if thought_match else None
        action = action_match.group(1).strip() if action_match else None
        return thought, action

    def _parse_action(self, action_text: str):
        """è§£æActionå­—ç¬¦ä¸²ï¼Œæå–å·¥å…·åç§°å’Œè¾“å…¥ã€‚"""
        match = re.match(r"(\w+)\[(.*)\]", action_text)
        if match:
            return match.group(1), match.group(2)
        return None, None
        
    def run(self, question: str):
        """
        è¿è¡ŒReActæ™ºèƒ½ä½“æ¥å›ç­”ä¸€ä¸ªé—®é¢˜ã€‚
        """
        self.history = [] # æ¯æ¬¡è¿è¡Œæ—¶é‡ç½®å†å²è®°å½•
        current_step = 0

        while current_step < self.max_steps:
            current_step += 1
            print(f"--- ç¬¬ {current_step} æ­¥ ---")

            # 1. æ ¼å¼åŒ–æç¤ºè¯
            tools_desc = self.tool_executor.getAvailableTools()
            history_str = "\n".join(self.history)
            prompt = REACT_PROMPT_TEMPLATE.format(
                tools=tools_desc,
                question=question,
                history=history_str
            )

            # 2. è°ƒç”¨LLMè¿›è¡Œæ€è€ƒ
            messages = [{"role": "user", "content": prompt}]
            response_text = self.llm_client.think(messages=messages)
            
            if not response_text:
                print("é”™è¯¯:LLMæœªèƒ½è¿”å›æœ‰æ•ˆå“åº”ã€‚")
                break

            # 3. è§£æLLMçš„è¾“å‡º
            thought, action = self._parse_output(response_text)
            
            if thought:
                print(f"æ€è€ƒ: {thought}")

            if not action:
                print("è­¦å‘Š:æœªèƒ½è§£æå‡ºæœ‰æ•ˆçš„Actionï¼Œæµç¨‹ç»ˆæ­¢ã€‚")
                break

            # 4. æ‰§è¡ŒAction
            if action.startswith("Finish"):
                # å¦‚æœæ˜¯FinishæŒ‡ä»¤ï¼Œæå–æœ€ç»ˆç­”æ¡ˆå¹¶ç»“æŸ
                final_answer = re.match(r"Finish\[(.*)\]", action).group(1)
                print(f"ğŸ‰ æœ€ç»ˆç­”æ¡ˆ: {final_answer}")
                return final_answer
            
            tool_name, tool_input = self._parse_action(action)
            if not tool_name or not tool_input:
                # ... å¤„ç†æ— æ•ˆActionæ ¼å¼ ...
                continue

            print(f"ğŸ¬ è¡ŒåŠ¨: {tool_name}[{tool_input}]")
            
            tool_function = self.tool_executor.getTool(tool_name)
            if not tool_function:
                observation = f"é”™è¯¯:æœªæ‰¾åˆ°åä¸º '{tool_name}' çš„å·¥å…·ã€‚"
            else:
                observation = tool_function(tool_input) # è°ƒç”¨çœŸå®å·¥å…·
                print(f"ğŸ‘€ è§‚å¯Ÿ: {observation}")
            
            # å°†æœ¬è½®çš„Actionå’ŒObservationæ·»åŠ åˆ°å†å²è®°å½•ä¸­
            self.history.append(f"Action: {action}")
            self.history.append(f"Observation: {observation}")

        # å¾ªç¯ç»“æŸ
        print("å·²è¾¾åˆ°æœ€å¤§æ­¥æ•°ï¼Œæµç¨‹ç»ˆæ­¢ã€‚")
        return None
```

#### 2. Plan-and-solve

1. **è§„åˆ’é˜¶æ®µ (Planning Phase)**ï¼š é¦–å…ˆï¼Œæ™ºèƒ½ä½“ä¼šæ¥æ”¶ç”¨æˆ·çš„å®Œæ•´é—®é¢˜ã€‚å®ƒçš„ç¬¬ä¸€ä¸ªä»»åŠ¡ä¸æ˜¯ç›´æ¥å»è§£å†³é—®é¢˜æˆ–è°ƒç”¨å·¥å…·ï¼Œè€Œæ˜¯**å°†é—®é¢˜åˆ†è§£ï¼Œå¹¶åˆ¶å®šå‡ºä¸€ä¸ªæ¸…æ™°ã€åˆ†æ­¥éª¤çš„è¡ŒåŠ¨è®¡åˆ’**ã€‚è¿™ä¸ªè®¡åˆ’æœ¬èº«å°±æ˜¯ä¸€æ¬¡å¤§è¯­è¨€æ¨¡å‹çš„è°ƒç”¨äº§ç‰©ã€‚
2. **æ‰§è¡Œé˜¶æ®µ (Solving Phase)**ï¼š åœ¨è·å¾—å®Œæ•´çš„è®¡åˆ’åï¼Œæ™ºèƒ½ä½“è¿›å…¥æ‰§è¡Œé˜¶æ®µã€‚å®ƒä¼š**ä¸¥æ ¼æŒ‰ç…§è®¡åˆ’ä¸­çš„æ­¥éª¤ï¼Œé€ä¸€æ‰§è¡Œ**ã€‚æ¯ä¸€æ­¥çš„æ‰§è¡Œéƒ½å¯èƒ½æ˜¯ä¸€æ¬¡ç‹¬ç«‹çš„ LLM è°ƒç”¨ï¼Œæˆ–è€…æ˜¯å¯¹ä¸Šä¸€æ­¥ç»“æœçš„åŠ å·¥å¤„ç†ï¼Œç›´åˆ°è®¡åˆ’ä¸­çš„æ‰€æœ‰æ­¥éª¤éƒ½å®Œæˆï¼Œæœ€ç»ˆå¾—å‡ºç­”æ¡ˆã€‚

![image-20251108090259230](assets/image-20251108090259230.png)

$PlanAgent$ è®¾è®¡

````python
PLANNER_PROMPT_TEMPLATE = """
ä½ æ˜¯ä¸€ä¸ªé¡¶çº§çš„AIè§„åˆ’ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†ç”¨æˆ·æå‡ºçš„å¤æ‚é—®é¢˜åˆ†è§£æˆä¸€ä¸ªç”±å¤šä¸ªç®€å•æ­¥éª¤ç»„æˆçš„è¡ŒåŠ¨è®¡åˆ’ã€‚
è¯·ç¡®ä¿è®¡åˆ’ä¸­çš„æ¯ä¸ªæ­¥éª¤éƒ½æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„ã€å¯æ‰§è¡Œçš„å­ä»»åŠ¡ï¼Œå¹¶ä¸”ä¸¥æ ¼æŒ‰ç…§é€»è¾‘é¡ºåºæ’åˆ—ã€‚
ä½ çš„è¾“å‡ºå¿…é¡»æ˜¯ä¸€ä¸ªPythonåˆ—è¡¨ï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ éƒ½æ˜¯ä¸€ä¸ªæè¿°å­ä»»åŠ¡çš„å­—ç¬¦ä¸²ã€‚

é—®é¢˜: {question}

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºä½ çš„è®¡åˆ’,```pythonä¸```ä½œä¸ºå‰åç¼€æ˜¯å¿…è¦çš„:
```python
["æ­¥éª¤1", "æ­¥éª¤2", "æ­¥éª¤3", ...]
```
"""
````

```python
# å‡å®š llm_client.py ä¸­çš„ HelloAgentsLLM ç±»å·²ç»å®šä¹‰å¥½
# from llm_client import HelloAgentsLLM

class Planner:
    def __init__(self, llm_client):
        self.llm_client = llm_client

    def plan(self, question: str) -> list[str]:
        """
        æ ¹æ®ç”¨æˆ·é—®é¢˜ç”Ÿæˆä¸€ä¸ªè¡ŒåŠ¨è®¡åˆ’ã€‚
        """
        prompt = PLANNER_PROMPT_TEMPLATE.format(question=question)
        
        # ä¸ºäº†ç”Ÿæˆè®¡åˆ’ï¼Œæˆ‘ä»¬æ„å»ºä¸€ä¸ªç®€å•çš„æ¶ˆæ¯åˆ—è¡¨
        messages = [{"role": "user", "content": prompt}]
        
        print("--- æ­£åœ¨ç”Ÿæˆè®¡åˆ’ ---")
        # ä½¿ç”¨æµå¼è¾“å‡ºæ¥è·å–å®Œæ•´çš„è®¡åˆ’
        response_text = self.llm_client.think(messages=messages) or ""
        
        print(f"âœ… è®¡åˆ’å·²ç”Ÿæˆ:\n{response_text}")
        
        # è§£æLLMè¾“å‡ºçš„åˆ—è¡¨å­—ç¬¦ä¸²
        try:
            # æ‰¾åˆ°```pythonå’Œ```ä¹‹é—´çš„å†…å®¹
            plan_str = response_text.split("```python")[1].split("```")[0].strip()
            # ä½¿ç”¨ast.literal_evalæ¥å®‰å…¨åœ°æ‰§è¡Œå­—ç¬¦ä¸²ï¼Œå°†å…¶è½¬æ¢ä¸ºPythonåˆ—è¡¨
            plan = ast.literal_eval(plan_str)
            return plan if isinstance(plan, list) else []
        except (ValueError, SyntaxError, IndexError) as e:
            print(f"âŒ è§£æè®¡åˆ’æ—¶å‡ºé”™: {e}")
            print(f"åŸå§‹å“åº”: {response_text}")
            return []
        except Exception as e:
            print(f"âŒ è§£æè®¡åˆ’æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
            return []
```

$ExcutorAgent$ è®¾è®¡

```python
EXECUTOR_PROMPT_TEMPLATE = """
ä½ æ˜¯ä¸€ä½é¡¶çº§çš„AIæ‰§è¡Œä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä¸¥æ ¼æŒ‰ç…§ç»™å®šçš„è®¡åˆ’ï¼Œä¸€æ­¥æ­¥åœ°è§£å†³é—®é¢˜ã€‚
ä½ å°†æ”¶åˆ°åŸå§‹é—®é¢˜ã€å®Œæ•´çš„è®¡åˆ’ã€ä»¥åŠåˆ°ç›®å‰ä¸ºæ­¢å·²ç»å®Œæˆçš„æ­¥éª¤å’Œç»“æœã€‚
è¯·ä½ ä¸“æ³¨äºè§£å†³â€œå½“å‰æ­¥éª¤â€ï¼Œå¹¶ä»…è¾“å‡ºè¯¥æ­¥éª¤çš„æœ€ç»ˆç­”æ¡ˆï¼Œä¸è¦è¾“å‡ºä»»ä½•é¢å¤–çš„è§£é‡Šæˆ–å¯¹è¯ã€‚

# åŸå§‹é—®é¢˜:
{question}

# å®Œæ•´è®¡åˆ’:
{plan}

# å†å²æ­¥éª¤ä¸ç»“æœ:
{history}

# å½“å‰æ­¥éª¤:
{current_step}

è¯·ä»…è¾“å‡ºé’ˆå¯¹â€œå½“å‰æ­¥éª¤â€çš„å›ç­”:
"""
```

```python
class Executor:
    def __init__(self, llm_client):
        self.llm_client = llm_client

    def execute(self, question: str, plan: list[str]) -> str:
        """
        æ ¹æ®è®¡åˆ’ï¼Œé€æ­¥æ‰§è¡Œå¹¶è§£å†³é—®é¢˜ã€‚
        """
        history = "" # ç”¨äºå­˜å‚¨å†å²æ­¥éª¤å’Œç»“æœçš„å­—ç¬¦ä¸²
        
        print("\n--- æ­£åœ¨æ‰§è¡Œè®¡åˆ’ ---")
        
        for i, step in enumerate(plan):
            print(f"\n-> æ­£åœ¨æ‰§è¡Œæ­¥éª¤ {i+1}/{len(plan)}: {step}")
            
            prompt = EXECUTOR_PROMPT_TEMPLATE.format(
                question=question,
                plan=plan,
                history=history if history else "æ— ", # å¦‚æœæ˜¯ç¬¬ä¸€æ­¥ï¼Œåˆ™å†å²ä¸ºç©º
                current_step=step
            )
            
            messages = [{"role": "user", "content": prompt}]
            
            response_text = self.llm_client.think(messages=messages) or ""
            
            # æ›´æ–°å†å²è®°å½•ï¼Œä¸ºä¸‹ä¸€æ­¥åšå‡†å¤‡
            history += f"æ­¥éª¤ {i+1}: {step}\nç»“æœ: {response_text}\n\n"
            
            print(f"âœ… æ­¥éª¤ {i+1} å·²å®Œæˆï¼Œç»“æœ: {response_text}")

        # å¾ªç¯ç»“æŸåï¼Œæœ€åä¸€æ­¥çš„å“åº”å°±æ˜¯æœ€ç»ˆç­”æ¡ˆ
        final_answer = response_text
        return final_answer
```

æ„å»ºä¸»ç±»

```python
class PlanAndSolveAgent:
    def __init__(self, llm_client):
        """
        åˆå§‹åŒ–æ™ºèƒ½ä½“ï¼ŒåŒæ—¶åˆ›å»ºè§„åˆ’å™¨å’Œæ‰§è¡Œå™¨å®ä¾‹ã€‚
        """
        self.llm_client = llm_client
        self.planner = Planner(self.llm_client)
        self.executor = Executor(self.llm_client)

    def run(self, question: str):
        """
        è¿è¡Œæ™ºèƒ½ä½“çš„å®Œæ•´æµç¨‹:å…ˆè§„åˆ’ï¼Œåæ‰§è¡Œã€‚
        """
        print(f"\n--- å¼€å§‹å¤„ç†é—®é¢˜ ---\né—®é¢˜: {question}")
        
        # 1. è°ƒç”¨è§„åˆ’å™¨ç”Ÿæˆè®¡åˆ’
        plan = self.planner.plan(question)
        
        # æ£€æŸ¥è®¡åˆ’æ˜¯å¦æˆåŠŸç”Ÿæˆ
        if not plan:
            print("\n--- ä»»åŠ¡ç»ˆæ­¢ --- \næ— æ³•ç”Ÿæˆæœ‰æ•ˆçš„è¡ŒåŠ¨è®¡åˆ’ã€‚")
            return

        # 2. è°ƒç”¨æ‰§è¡Œå™¨æ‰§è¡Œè®¡åˆ’
        final_answer = self.executor.execute(question, plan)
        
        print(f"\n--- ä»»åŠ¡å®Œæˆ ---\næœ€ç»ˆç­”æ¡ˆ: {final_answer}")
```

#### 3. Reflection

1. **æ‰§è¡Œ (Execution)**ï¼šé¦–å…ˆï¼Œæ™ºèƒ½ä½“ä½¿ç”¨æˆ‘ä»¬ç†Ÿæ‚‰çš„æ–¹æ³•ï¼ˆå¦‚ ReAct æˆ– Plan-and-Solveï¼‰å°è¯•å®Œæˆä»»åŠ¡ï¼Œç”Ÿæˆä¸€ä¸ªåˆæ­¥çš„è§£å†³æ–¹æ¡ˆæˆ–è¡ŒåŠ¨è½¨è¿¹ã€‚è¿™å¯ä»¥çœ‹ä½œæ˜¯â€œåˆç¨¿â€ã€‚
2. **åæ€ (Reflection)**ï¼šæ¥ç€ï¼Œæ™ºèƒ½ä½“è¿›å…¥åæ€é˜¶æ®µã€‚å®ƒä¼šè°ƒç”¨ä¸€ä¸ªç‹¬ç«‹çš„ã€æˆ–è€…å¸¦æœ‰ç‰¹æ®Šæç¤ºè¯çš„å¤§è¯­è¨€æ¨¡å‹å®ä¾‹ï¼Œæ¥æ‰®æ¼”ä¸€ä¸ªâ€œè¯„å®¡å‘˜â€çš„è§’è‰²ã€‚è¿™ä¸ªâ€œè¯„å®¡å‘˜â€ä¼šå®¡è§†ç¬¬ä¸€æ­¥ç”Ÿæˆçš„â€œåˆç¨¿â€ï¼Œå¹¶ä»å¤šä¸ªç»´åº¦è¿›è¡Œè¯„ä¼°ï¼Œä¾‹å¦‚ï¼š
   - **äº‹å®æ€§é”™è¯¯**ï¼šæ˜¯å¦å­˜åœ¨ä¸å¸¸è¯†æˆ–å·²çŸ¥äº‹å®ç›¸æ‚–çš„å†…å®¹ï¼Ÿ
   - **é€»è¾‘æ¼æ´**ï¼šæ¨ç†è¿‡ç¨‹æ˜¯å¦å­˜åœ¨ä¸è¿è´¯æˆ–çŸ›ç›¾ä¹‹å¤„ï¼Ÿ
   - **æ•ˆç‡é—®é¢˜**ï¼šæ˜¯å¦æœ‰æ›´ç›´æ¥ã€æ›´ç®€æ´çš„è·¯å¾„æ¥å®Œæˆä»»åŠ¡ï¼Ÿ
   - **é—æ¼ä¿¡æ¯**ï¼šæ˜¯å¦å¿½ç•¥äº†é—®é¢˜çš„æŸäº›å…³é”®çº¦æŸæˆ–æ–¹é¢ï¼Ÿ æ ¹æ®è¯„ä¼°ï¼Œå®ƒä¼šç”Ÿæˆä¸€æ®µç»“æ„åŒ–çš„**åé¦ˆ (Feedback)**ï¼ŒæŒ‡å‡ºå…·ä½“çš„é—®é¢˜æ‰€åœ¨å’Œæ”¹è¿›å»ºè®®ã€‚
3. **ä¼˜åŒ– (Refinement)**ï¼šæœ€åï¼Œæ™ºèƒ½ä½“å°†â€œåˆç¨¿â€å’Œâ€œåé¦ˆâ€ä½œä¸ºæ–°çš„ä¸Šä¸‹æ–‡ï¼Œå†æ¬¡è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹ï¼Œè¦æ±‚å®ƒæ ¹æ®åé¦ˆå†…å®¹å¯¹åˆç¨¿è¿›è¡Œä¿®æ­£ï¼Œç”Ÿæˆä¸€ä¸ªæ›´å®Œå–„çš„â€œä¿®è®¢ç¨¿â€ã€‚

![image-20251108092725287](assets/image-20251108092725287.png)

å…ˆè®¾è®¡æç¤ºè¯ï¼Œåˆ†ä¸ºåˆå§‹æ‰§è¡Œæç¤ºè¯ï¼Œåæ€æç¤ºè¯ï¼Œä¼˜åŒ–æç¤ºè¯

```python
INITIAL_PROMPT_TEMPLATE = """
ä½ æ˜¯ä¸€ä½èµ„æ·±çš„Pythonç¨‹åºå‘˜ã€‚è¯·æ ¹æ®ä»¥ä¸‹è¦æ±‚ï¼Œç¼–å†™ä¸€ä¸ªPythonå‡½æ•°ã€‚
ä½ çš„ä»£ç å¿…é¡»åŒ…å«å®Œæ•´çš„å‡½æ•°ç­¾åã€æ–‡æ¡£å­—ç¬¦ä¸²ï¼Œå¹¶éµå¾ªPEP 8ç¼–ç è§„èŒƒã€‚

è¦æ±‚: {task}

è¯·ç›´æ¥è¾“å‡ºä»£ç ï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–çš„è§£é‡Šã€‚
"""
```

````python
REFLECT_PROMPT_TEMPLATE = """
ä½ æ˜¯ä¸€ä½æå…¶ä¸¥æ ¼çš„ä»£ç è¯„å®¡ä¸“å®¶å’Œèµ„æ·±ç®—æ³•å·¥ç¨‹å¸ˆï¼Œå¯¹ä»£ç çš„æ€§èƒ½æœ‰æè‡´çš„è¦æ±‚ã€‚
ä½ çš„ä»»åŠ¡æ˜¯å®¡æŸ¥ä»¥ä¸‹Pythonä»£ç ï¼Œå¹¶ä¸“æ³¨äºæ‰¾å‡ºå…¶åœ¨<strong>ç®—æ³•æ•ˆç‡</strong>ä¸Šçš„ä¸»è¦ç“¶é¢ˆã€‚

# åŸå§‹ä»»åŠ¡:
{task}

# å¾…å®¡æŸ¥çš„ä»£ç :
```python
{code}
```

è¯·åˆ†æè¯¥ä»£ç çš„æ—¶é—´å¤æ‚åº¦ï¼Œå¹¶æ€è€ƒæ˜¯å¦å­˜åœ¨ä¸€ç§<strong>ç®—æ³•ä¸Šæ›´ä¼˜</strong>çš„è§£å†³æ–¹æ¡ˆæ¥æ˜¾è‘—æå‡æ€§èƒ½ã€‚
å¦‚æœå­˜åœ¨ï¼Œè¯·æ¸…æ™°åœ°æŒ‡å‡ºå½“å‰ç®—æ³•çš„ä¸è¶³ï¼Œå¹¶æå‡ºå…·ä½“çš„ã€å¯è¡Œçš„æ”¹è¿›ç®—æ³•å»ºè®®ï¼ˆä¾‹å¦‚ï¼Œä½¿ç”¨ç­›æ³•æ›¿ä»£è¯•é™¤æ³•ï¼‰ã€‚
å¦‚æœä»£ç åœ¨ç®—æ³•å±‚é¢å·²ç»è¾¾åˆ°æœ€ä¼˜ï¼Œæ‰èƒ½å›ç­”â€œæ— éœ€æ”¹è¿›â€ã€‚

è¯·ç›´æ¥è¾“å‡ºä½ çš„åé¦ˆï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–çš„è§£é‡Šã€‚
"""
````

````python
REFINE_PROMPT_TEMPLATE = """
ä½ æ˜¯ä¸€ä½èµ„æ·±çš„Pythonç¨‹åºå‘˜ã€‚ä½ æ­£åœ¨æ ¹æ®ä¸€ä½ä»£ç è¯„å®¡ä¸“å®¶çš„åé¦ˆæ¥ä¼˜åŒ–ä½ çš„ä»£ç ã€‚

# åŸå§‹ä»»åŠ¡:
{task}

# ä½ ä¸Šä¸€è½®å°è¯•çš„ä»£ç :
```
{last_code_attempt}
è¯„å®¡å‘˜çš„åé¦ˆï¼š
{feedback}

è¯·æ ¹æ®è¯„å®¡å‘˜çš„åé¦ˆï¼Œç”Ÿæˆä¸€ä¸ªä¼˜åŒ–åçš„æ–°ç‰ˆæœ¬ä»£ç ã€‚
ä½ çš„ä»£ç å¿…é¡»åŒ…å«å®Œæ•´çš„å‡½æ•°ç­¾åã€æ–‡æ¡£å­—ç¬¦ä¸²ï¼Œå¹¶éµå¾ªPEP 8ç¼–ç è§„èŒƒã€‚
è¯·ç›´æ¥è¾“å‡ºä¼˜åŒ–åçš„ä»£ç ï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–çš„è§£é‡Šã€‚
"""
````

```python
# å‡è®¾ llm_client.py å’Œ memory.py å·²å®šä¹‰
# from llm_client import HelloAgentsLLM
# from memory import Memory

class ReflectionAgent:
    def __init__(self, llm_client, max_iterations=3):
        self.llm_client = llm_client
        self.memory = Memory()
        self.max_iterations = max_iterations

    def run(self, task: str):
        print(f"\n--- å¼€å§‹å¤„ç†ä»»åŠ¡ ---\nä»»åŠ¡: {task}")

        # --- 1. åˆå§‹æ‰§è¡Œ ---
        print("\n--- æ­£åœ¨è¿›è¡Œåˆå§‹å°è¯• ---")
        initial_prompt = INITIAL_PROMPT_TEMPLATE.format(task=task)
        initial_code = self._get_llm_response(initial_prompt)
        self.memory.add_record("execution", initial_code)

        # --- 2. è¿­ä»£å¾ªç¯:åæ€ä¸ä¼˜åŒ– ---
        for i in range(self.max_iterations):
            print(f"\n--- ç¬¬ {i+1}/{self.max_iterations} è½®è¿­ä»£ ---")

            # a. åæ€
            print("\n-> æ­£åœ¨è¿›è¡Œåæ€...")
            last_code = self.memory.get_last_execution()
            reflect_prompt = REFLECT_PROMPT_TEMPLATE.format(task=task, code=last_code)
            feedback = self._get_llm_response(reflect_prompt)
            self.memory.add_record("reflection", feedback)

            # b. æ£€æŸ¥æ˜¯å¦éœ€è¦åœæ­¢
            if "æ— éœ€æ”¹è¿›" in feedback:
                print("\nâœ… åæ€è®¤ä¸ºä»£ç å·²æ— éœ€æ”¹è¿›ï¼Œä»»åŠ¡å®Œæˆã€‚")
                break

            # c. ä¼˜åŒ–
            print("\n-> æ­£åœ¨è¿›è¡Œä¼˜åŒ–...")
            refine_prompt = REFINE_PROMPT_TEMPLATE.format(
                task=task,
                last_code_attempt=last_code,
                feedback=feedback
            )
            refined_code = self._get_llm_response(refine_prompt)
            self.memory.add_record("execution", refined_code)
        
        final_code = self.memory.get_last_execution()
        print(f"\n--- ä»»åŠ¡å®Œæˆ ---\næœ€ç»ˆç”Ÿæˆçš„ä»£ç :\n```python\n{final_code}\n```")
        return final_code

    def _get_llm_response(self, prompt: str) -> str:
        """ä¸€ä¸ªè¾…åŠ©æ–¹æ³•ï¼Œç”¨äºè°ƒç”¨LLMå¹¶è·å–å®Œæ•´çš„æµå¼å“åº”ã€‚"""
        messages = [{"role": "user", "content": prompt}]
        rresponse_text = self.llm_client.think(messages=messages) or ""
        return response_text
```

































































