# VERL 

> **æ–‡æ¡£ç›®æ ‡**: å®Œå…¨ç†è§£ GRPO è®­ç»ƒæµç¨‹ï¼ŒçŸ¥é“æ¯ä¸€æ­¥åœ¨å“ªä¸ªæ–‡ä»¶çš„å“ªä¸€è¡Œï¼Œæ•°æ®å¦‚ä½•æµè½¬ï¼Œä»¥åŠå¦‚ä½•ä¿®æ”¹ä»£ç ã€‚

---

## ç›®å½•

1. [å¿«é€Ÿæ¦‚è§ˆ](#1-å¿«é€Ÿæ¦‚è§ˆ)
2. [G å‚æ•°é…ç½®](#2-g-å‚æ•°é…ç½®)
3. [å®Œæ•´æ‰§è¡Œæµç¨‹](#3-å®Œæ•´æ‰§è¡Œæµç¨‹)
4. [æ ¸å¿ƒç®—æ³•è¯¦è§£](#4-æ ¸å¿ƒç®—æ³•è¯¦è§£)
5. [ä¿®æ”¹ä»£ç æŒ‡å—](#5-ä¿®æ”¹ä»£ç æŒ‡å—)
6. [å¸¸è§é—®é¢˜](#6-å¸¸è§é—®é¢˜)

---

## 1. å¿«é€Ÿæ¦‚è§ˆ

### 1.1 GRPO ç®—æ³•æ ¸å¿ƒæ€æƒ³

**ä¼ ç»Ÿ PPO**: éœ€è¦ä¸€ä¸ª Critic ç½‘ç»œä¼°è®¡ Valueï¼Œä½¿ç”¨ GAE è®¡ç®—ä¼˜åŠ¿ã€‚

**GRPO (Group Relative Policy Optimization)**:
- ä¸ºæ¯ä¸ª prompt ç”Ÿæˆ **G ä¸ªå›ç­”** (ä¾‹å¦‚ G=5)
- å¯¹æ¯ä¸ªå›ç­”æ‰“åˆ† (æ­£ç¡®=1.0, é”™è¯¯=0.0)
- è®¡ç®—**ç»„å†…ç›¸å¯¹ä¼˜åŠ¿**: `advantage = (score - mean) / std`
- å¥½å›ç­”å¾—æ­£åˆ†ï¼Œåå›ç­”å¾—è´Ÿåˆ†
- ä½¿ç”¨ PPO æ›´æ–°ç­–ç•¥ï¼Œå¢åŠ å¥½å›ç­”æ¦‚ç‡ï¼Œé™ä½åå›ç­”æ¦‚ç‡

**ä¼˜åŠ¿**:
- âœ… ä¸éœ€è¦ Critic ç½‘ç»œï¼Œç®€åŒ–è®­ç»ƒ
- âœ… ç»„å†…å¯¹æ¯”å­¦ä¹ ï¼Œæ›´ç¨³å®š
- âœ… ç‰¹åˆ«é€‚åˆæ•°å­¦ã€ä»£ç ç­‰æœ‰æ˜ç¡®æ­£è¯¯çš„ä»»åŠ¡

### 1.2 æ‰§è¡Œæµç¨‹æ¦‚è§ˆ

```
å¯åŠ¨ â†’ åŠ è½½æ•°æ® â†’ ç”Ÿæˆ G ä¸ªå›ç­” â†’ è®¡ç®—å¥–åŠ± â†’ è®¡ç®— GRPO ä¼˜åŠ¿ â†’ æ›´æ–°æ¨¡å‹ â†’ éªŒè¯ â†’ ä¿å­˜
```

### 1.3 æ ¸å¿ƒæ–‡ä»¶åœ°å›¾

| åŠŸèƒ½æ¨¡å— | æ ¸å¿ƒæ–‡ä»¶ | å…³é”®è¡Œæ•° |
|---------|---------|---------|
| **å¯åŠ¨å…¥å£** | `verl/trainer/main_ppo.py` | 35-368 |
| **è®­ç»ƒå¾ªç¯** | `verl/trainer/ppo/ray_trainer.py` | 977-1325 |
| **GRPO ç®—æ³•** | `verl/trainer/ppo/core_algos.py` | 265-328 |
| **ç”Ÿæˆå›ç­”** | `verl/workers/fsdp_workers.py` | 911-957 |
| **æ›´æ–°ç­–ç•¥** | `verl/workers/actor/dp_actor.py` | 398-600 |
| **GSM8K è¯„åˆ†** | `verl/utils/reward_score/gsm8k.py` | 52-72 |
| **é…ç½®æ–‡ä»¶** | `examples/grpo_trainer/run_qwen3-8b.sh` | å…¨éƒ¨ |

---

## 2. G å‚æ•°é…ç½®

### 2.1 G å‚æ•°åœ¨å“ªé‡Œé…ç½®ï¼Ÿ

**æ–‡ä»¶**: `examples/grpo_trainer/run_qwen3-8b.sh`

```bash
# ç¬¬ 31 è¡Œ
actor_rollout_ref.rollout.n=5
```

**å«ä¹‰**:
- `n=5` è¡¨ç¤ºä¸ºæ¯ä¸ª prompt ç”Ÿæˆ **5 ä¸ªä¸åŒçš„å›ç­”**
- è¿™å°±æ˜¯ GRPO ä¸­çš„ **G å‚æ•°**
- G è¶Šå¤§ï¼Œè®­ç»ƒè¶Šç¨³å®šï¼Œä½†è®¡ç®—æˆæœ¬è¶Šé«˜

### 2.2 G å‚æ•°å¦‚ä½•ä¼ é€’ï¼Ÿ

```
run_qwen3-8b.sh:31 (n=5)
  â†“ (é€šè¿‡ Hydra é…ç½®ç³»ç»Ÿ)
verl/trainer/main_ppo.py:36
  â†“
config.actor_rollout_ref.rollout.n
  â†“
verl/trainer/ppo/ray_trainer.py:1057
  â†“
gen_batch.repeat(repeat_times=config.actor_rollout_ref.rollout.n)
```

### 2.3 å¦‚ä½•ä¿®æ”¹ G å‚æ•°ï¼Ÿ

**æ–¹æ³• 1**: ä¿®æ”¹è„šæœ¬
```bash
# åœ¨ run_qwen3-8b.sh ç¬¬ 31 è¡Œ
actor_rollout_ref.rollout.n=10  # æ”¹ä¸º 10
```

**æ–¹æ³• 2**: å‘½ä»¤è¡Œè¦†ç›–
```bash
bash run_qwen3-8b.sh actor_rollout_ref.rollout.n=10
```

**å»ºè®®å€¼**:
- **å°ä»»åŠ¡**: n=5 (é»˜è®¤)
- **å¤æ‚ä»»åŠ¡**: n=8~10 (æ›´ç¨³å®š)
- **å¿«é€Ÿå®éªŒ**: n=2~3 (è®¡ç®—å¿«ä½†ä¸ç¨³å®š)

---

## 3. å®Œæ•´æ‰§è¡Œæµç¨‹

### æµç¨‹å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 0: å¯åŠ¨å’Œåˆå§‹åŒ–                                         â”‚
â”‚   æ–‡ä»¶: main_ppo.py:35-368                                   â”‚
â”‚   ä½œç”¨: å¯åŠ¨ Ray, åŠ è½½æ•°æ®é›†, åˆå§‹åŒ– Worker                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 1: åŠ è½½ Batch                                          â”‚
â”‚   æ–‡ä»¶: ray_trainer.py:1033-1051                            â”‚
â”‚   è¾“å…¥: 256 ä¸ª prompt (æ•°å­¦é¢˜)                               â”‚
â”‚   è¾“å‡º: batch (DataProto)                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 2: é‡å¤ Batch (n=5)                                    â”‚
â”‚   æ–‡ä»¶: ray_trainer.py:1057-1059                            â”‚
â”‚   è¾“å…¥: 256 prompts                                         â”‚
â”‚   è¾“å‡º: 1280 prompts (256 Ã— 5)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 3: ç”Ÿæˆå›ç­” (vLLM)                                     â”‚
â”‚   æ–‡ä»¶: ray_trainer.py:1066 â†’ fsdp_workers.py:911          â”‚
â”‚   è¾“å…¥: 1280 prompts                                        â”‚
â”‚   è¾“å‡º: 1280 responses + rollout_log_probs                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 4: è®¡ç®—å¥–åŠ±                                            â”‚
â”‚   æ–‡ä»¶: ray_trainer.py:1128 â†’ gsm8k.py:52                  â”‚
â”‚   è¾“å…¥: 1280 responses                                      â”‚
â”‚   è¾“å‡º: 1280 scores (1.0 æˆ– 0.0)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 5: è®¡ç®— old_log_probs                                  â”‚
â”‚   æ–‡ä»¶: ray_trainer.py:1146 â†’ dp_actor.py:180              â”‚
â”‚   è¾“å…¥: input_ids + responses                               â”‚
â”‚   è¾“å‡º: old_log_probs (ç”¨äº PPO clip)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 6: è®¡ç®— GRPO ä¼˜åŠ¿                                      â”‚
â”‚   æ–‡ä»¶: ray_trainer.py:1222 â†’ core_algos.py:265            â”‚
â”‚   è¾“å…¥: scores, uid (åˆ†ç»„æ ‡è¯†)                               â”‚
â”‚   è¾“å‡º: advantages (ç»„å†…æ ‡å‡†åŒ–ä¼˜åŠ¿)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 7: æ›´æ–° Actor (PPO)                                    â”‚
â”‚   æ–‡ä»¶: ray_trainer.py:1247 â†’ dp_actor.py:398              â”‚
â”‚   è¾“å…¥: advantages, old_log_probs                           â”‚
â”‚   è¾“å‡º: æ›´æ–°åçš„æ¨¡å‹å‚æ•°                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 8: éªŒè¯æµ‹è¯• (æ¯ 5 æ­¥)                                  â”‚
â”‚   æ–‡ä»¶: ray_trainer.py:531-630                              â”‚
â”‚   è¾“å‡º: val_accuracy, val_pass@5                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 9: ä¿å­˜æ£€æŸ¥ç‚¹ (æ¯ 20 æ­¥)                               â”‚
â”‚   æ–‡ä»¶: ray_trainer.py:1286                                 â”‚
â”‚   è¾“å‡º: checkpoint æ–‡ä»¶                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Step 0: å¯åŠ¨å’Œåˆå§‹åŒ–

#### ğŸ“ æ–‡ä»¶: `verl/trainer/main_ppo.py`

#### ğŸ¯ ä½œç”¨
å¯åŠ¨æ•´ä¸ªè®­ç»ƒç³»ç»Ÿï¼Œåˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶ã€‚

#### ğŸ“ ä»£ç è¯¦è§£

**å…¥å£å‡½æ•°** (ç¬¬ 35-42 è¡Œ):
```python
@hydra.main(config_path="config", config_name="ppo_trainer", version_base=None)
def main(config):
    """Main entry point for PPO training with Hydra configuration management."""
    run_ppo(config)
```

**è§£é‡Š**:
- `@hydra.main`: Hydra è£…é¥°å™¨ï¼Œè‡ªåŠ¨åŠ è½½é…ç½®æ–‡ä»¶
- `config_path="config"`: é…ç½®æ–‡ä»¶ç›®å½•
- `config_name="ppo_trainer"`: é»˜è®¤é…ç½®æ–‡ä»¶å
- å‘½ä»¤è¡Œå‚æ•°ä¼šè¦†ç›–é…ç½®æ–‡ä»¶ (ä¾‹å¦‚ `algorithm.adv_estimator=grpo`)

---

**å¯åŠ¨ Ray é›†ç¾¤** (ç¬¬ 55-74 è¡Œ):
```python
def run_ppo(config, task_runner_class=None) -> None:
    if not ray.is_initialized():
        # è®¾ç½®ç¯å¢ƒå˜é‡
        default_runtime_env = get_ppo_ray_runtime_env()
        ray_init_kwargs = config.ray_kwargs.get("ray_init", {})
        runtime_env_kwargs = ray_init_kwargs.get("runtime_env", {})

        runtime_env = OmegaConf.merge(default_runtime_env, runtime_env_kwargs)
        ray_init_kwargs = OmegaConf.create({**ray_init_kwargs, "runtime_env": runtime_env})

        # åˆå§‹åŒ– Ray
        ray.init(**OmegaConf.to_container(ray_init_kwargs))
```

**è§£é‡Š**:
- Ray æ˜¯åˆ†å¸ƒå¼è®¡ç®—æ¡†æ¶
- ç”¨äºç®¡ç†å¤š GPU è®­ç»ƒ
- `runtime_env`: è®¾ç½®ç¯å¢ƒå˜é‡ (CUDA, vLLM ç­‰)

---

**å¯åŠ¨ä»»åŠ¡** (ç¬¬ 76-96 è¡Œ):
```python
if task_runner_class is None:
    task_runner_class = ray.remote(num_cpus=1)(TaskRunner)

runner = task_runner_class.remote()
ray.get(runner.run.remote(config))
```

**è§£é‡Š**:
- `TaskRunner`: ä¸»è®­ç»ƒç±»
- `ray.remote()`: å°†ç±»è½¬ä¸º Ray Actor (å¯ä»¥è¿œç¨‹è°ƒç”¨)
- `runner.run.remote(config)`: è¿œç¨‹æ‰§è¡Œ `run` æ–¹æ³•
- `ray.get()`: ç­‰å¾…æ‰§è¡Œå®Œæˆ

---

**TaskRunner.run()** (ç¬¬ 262-368 è¡Œ):
```python
def run(self, config):
    # 1. æ·»åŠ  Worker
    actor_rollout_cls, ray_worker_group_cls = self.add_actor_rollout_worker(config)
    self.add_critic_worker(config)  # GRPO ä¸éœ€è¦ï¼Œä½†ä¼šæ£€æŸ¥
    self.add_reward_model_worker(config)
    self.add_ref_policy_worker(config, actor_rollout_cls)

    # 2. éªŒè¯é…ç½®
    validate_config(
        config=config,
        use_reference_policy=need_reference_policy(self.role_worker_mapping),
        use_critic=need_critic(config),
    )

    # 3. åŠ è½½æ¨¡å‹è·¯å¾„
    local_path = copy_to_local(
        config.actor_rollout_ref.model.path,  # Qwen/Qwen3-8B
        use_shm=config.actor_rollout_ref.model.get("use_shm", False)
    )

    # 4. åŠ è½½ tokenizer
    trust_remote_code = config.data.get("trust_remote_code", False)
    tokenizer = hf_tokenizer(local_path, trust_remote_code=trust_remote_code)
    processor = hf_processor(local_path, trust_remote_code=trust_remote_code)

    # 5. åŠ è½½å¥–åŠ±å‡½æ•°
    reward_fn = load_reward_manager(
        config, tokenizer, num_examine=0, **config.reward_model.get("reward_kwargs", {})
    )
    val_reward_fn = load_reward_manager(
        config, tokenizer, num_examine=1, **config.reward_model.get("reward_kwargs", {})
    )

    # 6. åˆ›å»ºæ•°æ®é›†
    train_dataset = create_rl_dataset(
        config.data.train_files,  # $HOME/data/gsm8k/train.parquet
        config.data,
        tokenizer,
        processor,
        is_train=True,
    )
    val_dataset = create_rl_dataset(
        config.data.val_files,  # $HOME/data/gsm8k/test.parquet
        config.data,
        tokenizer,
        processor,
        is_train=False,
    )

    # 7. åˆ›å»º Trainer
    trainer = RayPPOTrainer(
        config=config,
        tokenizer=tokenizer,
        processor=processor,
        role_worker_mapping=self.role_worker_mapping,
        resource_pool_manager=resource_pool_manager,
        ray_worker_group_cls=ray_worker_group_cls,
        reward_fn=reward_fn,
        val_reward_fn=val_reward_fn,
        train_dataset=train_dataset,
        val_dataset=val_dataset,
        collate_fn=collate_fn,
        train_sampler=train_sampler,
    )

    # 8. åˆå§‹åŒ– Worker å¹¶å¼€å§‹è®­ç»ƒ
    trainer.init_workers()
    trainer.fit()  # â† è¿›å…¥ä¸»è®­ç»ƒå¾ªç¯
```

**è§£é‡Š**:
- **Worker**: åˆ†å¸ƒå¼è®­ç»ƒä¸­çš„å·¥ä½œè¿›ç¨‹
  - `ActorRollout`: è´Ÿè´£ç”Ÿæˆå›ç­”å’Œè®­ç»ƒç­–ç•¥
  - `Critic`: ä¼°è®¡ Value (GRPO ä¸éœ€è¦)
  - `RewardModel`: è®¡ç®—å¥–åŠ± (å¦‚æœä½¿ç”¨æ¨¡å‹æ‰“åˆ†)
  - `RefPolicy`: å‚è€ƒç­–ç•¥ (ç”¨äº KL æ•£åº¦)

- **æ•°æ®é›†**: GSM8K æ•°å­¦é¢˜
  - `train.parquet`: è®­ç»ƒé›†
  - `test.parquet`: éªŒè¯é›†

- **å¥–åŠ±å‡½æ•°**: `load_reward_manager` ä¼šæ ¹æ®é…ç½®åŠ è½½
  - å¯¹äº GSM8K: åŠ è½½ `verl/utils/reward_score/gsm8k.py:compute_score`

#### ğŸ”§ ä¿®æ”¹ç‚¹
- **ä¿®æ”¹æ¨¡å‹**: ç¬¬ 307 è¡Œ `config.actor_rollout_ref.model.path`
- **ä¿®æ”¹æ•°æ®é›†**: ç¬¬ 331ã€340 è¡Œ `config.data.train_files`, `config.data.val_files`
- **ä¿®æ”¹å¥–åŠ±å‡½æ•°**: ç¬¬ 319 è¡Œ `load_reward_manager` (éœ€è¦æ³¨å†Œè‡ªå®šä¹‰å¥–åŠ±)

---

### Step 1: åŠ è½½ Batch

#### ğŸ“ æ–‡ä»¶: `verl/trainer/ppo/ray_trainer.py`

#### ğŸ¯ ä½œç”¨
ä»æ•°æ®é›†ä¸­åŠ è½½ä¸€ä¸ª batch çš„ promptã€‚

#### ğŸ“ ä»£ç è¯¦è§£ (ç¬¬ 1032-1051 è¡Œ)

```python
for epoch in range(current_epoch, self.config.trainer.total_epochs):  # 15 ä¸ª epoch
    for batch_dict in self.train_dataloader:  # éå†æ•°æ®é›†
        metrics = {}
        timing_raw = {}

        # 1. å°† dict è½¬ä¸º DataProto
        batch: DataProto = DataProto.from_single_dict(batch_dict)

        # 2. è®¾ç½®æ¸©åº¦å‚æ•° (æ§åˆ¶ç”Ÿæˆéšæœºæ€§)
        batch.meta_info["temperature"] = self.config.actor_rollout_ref.rollout.temperature

        # 3. æ·»åŠ  uid (ç”¨äº GRPO åˆ†ç»„)
        batch.non_tensor_batch["uid"] = np.array(
            [str(uuid.uuid4()) for _ in range(len(batch.batch))], dtype=object
        )
```

**batch_dict çš„ç»“æ„** (æ¥è‡ª DataLoader):
```python
batch_dict = {
    # Tensor æ•°æ®
    "input_ids": torch.Tensor([256, 512]),      # 256 ä¸ª prompt, æ¯ä¸ªæœ€å¤š 512 tokens
    "attention_mask": torch.Tensor([256, 512]), # æ³¨æ„åŠ› mask

    # é Tensor æ•°æ®
    "reward_model": [
        {
            "ground_truth": "8",     # æ­£ç¡®ç­”æ¡ˆ
            "style": "rule",         # ä½¿ç”¨è§„åˆ™è¯„åˆ†
            "data_source": "gsm8k"
        },
        # ... 256 ä¸ªå…ƒç´ 
    ]
}
```

**è½¬æ¢åçš„ DataProto**:
```python
batch = DataProto(
    batch={
        "input_ids": torch.Tensor([256, 512]),
        "attention_mask": torch.Tensor([256, 512]),
    },
    non_tensor_batch={
        "uid": np.array([
            "uuid-0", "uuid-1", "uuid-2", ..., "uuid-255"  # 256 ä¸ªå”¯ä¸€ ID
        ]),
        "reward_model": [...]  # 256 ä¸ªé…ç½®
    },
    meta_info={
        "temperature": 0.0  # è´ªå©ªè§£ç 
    }
)
```

**uid çš„ä½œç”¨**:
- åœ¨ GRPO ä¸­ï¼ŒåŒä¸€ä¸ª prompt çš„ 5 ä¸ªå›ç­”éœ€è¦**å…±äº«åŒä¸€ä¸ª uid**
- ç”¨äºåœ¨ Step 6 ä¸­æŒ‰ uid åˆ†ç»„è®¡ç®—ç»„å†…ä¼˜åŠ¿

#### ğŸ”§ ä¿®æ”¹ç‚¹
- **ä¿®æ”¹ batch_size**: `run_qwen3-8b.sh:10` â†’ `data.train_batch_size=1024`
- **ä¿®æ”¹ temperature**: `run_qwen3-8b.sh` ä¸­æ·»åŠ  `actor_rollout_ref.rollout.temperature=0.7` (å¢åŠ éšæœºæ€§)

---

### Step 2: é‡å¤ Batch (n=5)

#### ğŸ“ æ–‡ä»¶: `verl/trainer/ppo/ray_trainer.py`

#### ğŸ¯ ä½œç”¨
å°†æ¯ä¸ª prompt é‡å¤ n=5 æ¬¡ï¼Œä¸º GRPO å‡†å¤‡æ•°æ®ã€‚

#### ğŸ“ ä»£ç è¯¦è§£ (ç¬¬ 1053-1059 è¡Œ)

```python
# 1. æå–ç”Ÿæˆæ‰€éœ€çš„å­—æ®µ
gen_batch = self._get_gen_batch(batch)

# 2. è®¾ç½®å…¨å±€æ­¥æ•° (ç”¨äºè¿½è¸ª)
gen_batch.meta_info["global_steps"] = self.global_steps

# 3. é‡å¤ n æ¬¡ (n=5)
gen_batch_output = gen_batch.repeat(
    repeat_times=self.config.actor_rollout_ref.rollout.n,  # 5
    interleave=True  # äº¤é”™é‡å¤
)
```

**`_get_gen_batch` å‡½æ•°**:
```python
def _get_gen_batch(self, batch):
    """æå–ç”Ÿæˆæ‰€éœ€çš„å­—æ®µ"""
    return batch.select(batch_keys=["input_ids", "attention_mask", "position_ids"])
```

**`repeat` æ–¹æ³•çš„æ•ˆæœ**:

**è¾“å…¥** (256 ä¸ª prompt):
```python
gen_batch.batch = {
    "input_ids": [[prompt_0], [prompt_1], ..., [prompt_255]],  # (256, 512)
}
gen_batch.non_tensor_batch = {
    "uid": ["uuid-0", "uuid-1", ..., "uuid-255"]
}
```

**è¾“å‡º** (1280 ä¸ª prompt, 256 Ã— 5):
```python
gen_batch_output.batch = {
    "input_ids": [
        [prompt_0], [prompt_0], [prompt_0], [prompt_0], [prompt_0],  # prompt_0 é‡å¤ 5 æ¬¡
        [prompt_1], [prompt_1], [prompt_1], [prompt_1], [prompt_1],  # prompt_1 é‡å¤ 5 æ¬¡
        ...,
        [prompt_255], [prompt_255], [prompt_255], [prompt_255], [prompt_255]
    ]  # (1280, 512)
}

gen_batch_output.non_tensor_batch = {
    "uid": [
        "uuid-0", "uuid-0", "uuid-0", "uuid-0", "uuid-0",  # åŒä¸€ä¸ª prompt çš„ 5 ä¸ªå›ç­”å…±äº« uid
        "uuid-1", "uuid-1", "uuid-1", "uuid-1", "uuid-1",
        ...,
        "uuid-255", "uuid-255", "uuid-255", "uuid-255", "uuid-255"
    ]  # (1280,)
}
```

**ä¸ºä»€ä¹ˆéœ€è¦ `interleave=True`?**

- ä¿è¯åŒä¸€ä¸ª prompt çš„ 5 ä¸ªå›ç­”åœ¨ batch ä¸­æ˜¯è¿ç»­çš„
- æ–¹ä¾¿åç»­æŒ‰ uid åˆ†ç»„

#### ğŸ”§ ä¿®æ”¹ç‚¹
- **ä¿®æ”¹ç”Ÿæˆæ•°é‡**: `run_qwen3-8b.sh:31` â†’ `actor_rollout_ref.rollout.n=10` (æ”¹ä¸ºç”Ÿæˆ 10 ä¸ªå›ç­”)

---

### Step 3: ç”Ÿæˆå›ç­” (vLLM)

#### ğŸ“ æ–‡ä»¶é“¾
```
verl/trainer/ppo/ray_trainer.py:1064-1071
  â†“ (RPC è°ƒç”¨)
verl/workers/fsdp_workers.py:911-957
  â†“ (è°ƒç”¨ rollout)
verl/workers/rollout/vllm_rollout/vllm_rollout.py
```

#### ğŸ¯ ä½œç”¨
ä½¿ç”¨ vLLM å¼•æ“å¹¶è¡Œç”Ÿæˆ 1280 ä¸ªå›ç­”ã€‚

#### ğŸ“ ä»£ç è¯¦è§£

**è°ƒç”¨ç”Ÿæˆ** (ray_trainer.py:1064-1071):
```python
with marked_timer("gen", timing_raw, color="red"):
    # æ ¹æ®æ˜¯å¦å¼‚æ­¥æ¨¡å¼é€‰æ‹©ç”Ÿæˆæ–¹å¼
    if not self.async_rollout_mode:
        gen_batch_output = self.actor_rollout_wg.generate_sequences(gen_batch_output)
    else:
        gen_batch_output = self.async_rollout_manager.generate_sequences(gen_batch_output)

    # è®°å½•ç”Ÿæˆæ—¶é—´
    timing_raw.update(gen_batch_output.meta_info["timing"])
    gen_batch_output.meta_info.pop("timing", None)
```

**Worker ç”Ÿæˆæ–¹æ³•** (fsdp_workers.py:911-957):
```python
def generate_sequences(self, prompts: DataProto):
    """ç”Ÿæˆåºåˆ—çš„ä¸»å‡½æ•°"""
    # 1. å°†æ•°æ®ç§»åˆ° GPU
    prompts = prompts.to(get_device_id())

    # 2. è®¾ç½®ç”Ÿæˆå‚æ•°
    meta_info = {
        "eos_token_id": self.tokenizer.eos_token_id,
        "pad_token_id": self.tokenizer.pad_token_id,
    }
    prompts.meta_info.update(meta_info)

    timing_generate = {}

    # 3. åˆ‡æ¢åˆ° rollout æ¨¡å¼ (å¦‚æœåŒæ—¶ç”¨äºè®­ç»ƒå’Œç”Ÿæˆ)
    if self._is_actor:
        loop = get_event_loop()
        loop.run_until_complete(self.rollout_mode())
        log_gpu_memory_usage("After switch to rollout mode", logger=logger)

    # 4. è°ƒç”¨ vLLM ç”Ÿæˆ
    with simple_timer("generate_sequences", timing_generate):
        output = self.rollout.generate_sequences(prompts=prompts)

    # 5. åˆ‡æ¢å›è®­ç»ƒæ¨¡å¼
    if self._is_actor:
        loop.run_until_complete(self.trainer_mode())
        log_gpu_memory_usage("After switch to trainer mode", logger=logger)

    # 6. è®°å½• timing
    timing_generate_topk_ratio, timing_generate_min, timing_generate_max = \
        topk_reduce_ratio_min_max(timing_generate["generate_sequences"])
    timing_generate = reduce_timing(timing_generate)
    timing_generate.update({
        "generation_timing/max": timing_generate_max,
        "generation_timing/min": timing_generate_min,
        "generation_timing/topk_ratio": timing_generate_topk_ratio,
    })
    output.meta_info["timing"] = timing_generate

    # 7. ç§»å› CPU å¹¶æ¸…ç†ç¼“å­˜
    output = output.to("cpu")
    get_torch_device().empty_cache()

    return output
```

**vLLM ç”Ÿæˆæ ¸å¿ƒé€»è¾‘** (ç®€åŒ–ç‰ˆ):
```python
# å®é™…åœ¨ vllm_rollout.py ä¸­å®ç°
def generate_sequences(self, prompts: DataProto) -> DataProto:
    """ä½¿ç”¨ vLLM å¼•æ“ç”Ÿæˆ"""
    # 1. å‡†å¤‡ sampling parameters
    sampling_params = SamplingParams(
        temperature=prompts.meta_info.get("temperature", 0.0),
        top_p=self.config.top_p,
        top_k=self.config.top_k,
        max_tokens=self.config.max_response_length,
        n=1,  # æ¯ä¸ª prompt ç”Ÿæˆ 1 ä¸ªå›ç­” (å·²ç»åœ¨å¤–éƒ¨é‡å¤äº†)
    )

    # 2. è°ƒç”¨ vLLM engine
    outputs = self.llm.generate(
        prompt_token_ids=prompts.batch["input_ids"].tolist(),
        sampling_params=sampling_params,
    )

    # 3. æå–ç»“æœ
    responses = []
    rollout_log_probs = []
    for output in outputs:
        responses.append(output.outputs[0].token_ids)
        rollout_log_probs.append(output.outputs[0].cumulative_logprob)

    # 4. è½¬ä¸º DataProto
    return DataProto(
        batch={
            "responses": torch.tensor(responses),
            "rollout_log_probs": torch.tensor(rollout_log_probs),
            "response_mask": compute_response_mask(responses),
        }
    )
```

**ç”Ÿæˆç»“æœ**:
```python
output = DataProto(
    batch={
        "responses": torch.Tensor([1280, 1024]),  # ç”Ÿæˆçš„ token IDs
        # æ¯ä¸ª response çš„å½¢çŠ¶: [token_0, token_1, ..., eos_token, pad, pad, ...]

        "rollout_log_probs": torch.Tensor([1280, 1024]),  # æ¯ä¸ª token çš„ log æ¦‚ç‡
        # vLLM ç”Ÿæˆæ—¶è®¡ç®—çš„ log P(token | prefix)

        "response_mask": torch.Tensor([1280, 1024]),  # æœ‰æ•ˆ token çš„ mask
        # 1 è¡¨ç¤ºæœ‰æ•ˆ token, 0 è¡¨ç¤º padding
    }
)
```

**ç¤ºä¾‹** (ç¬¬ 0 ä¸ª prompt çš„ 5 ä¸ªå›ç­”):
```
Prompt: "Olivia has $23. She bought five bagels for $3 each. How much money does she have left?"

Response 0: "She spent 5 * $3 = $15. So she has $23 - $15 = $8 left. #### 8"
Response 1: "The bagels cost 5 Ã— 3 = 15 dollars. She has 23 - 15 = 8 dollars left. #### 8"
Response 2: "Cost is 5*3=15. Remaining is 23-15=8. #### 8"
Response 3: "She bought 5 bagels at $3 each = $15. She started with $23. 23-15=8. #### 7"  # é”™è¯¯!
Response 4: "5 bagels cost $15 total. $23 - $15 = $8. #### 8"
```

#### ğŸ”§ ä¿®æ”¹ç‚¹
- **ä¿®æ”¹ç”Ÿæˆå¼•æ“**: `run_qwen3-8b.sh:29` â†’ `actor_rollout_ref.rollout.name=sglang` (æ”¹ç”¨ SGLang)
- **ä¿®æ”¹ max_tokens**: `run_qwen3-8b.sh:12` â†’ `data.max_response_length=2048` (å…è®¸æ›´é•¿å›ç­”)
- **ä¿®æ”¹é‡‡æ ·å‚æ•°**: åœ¨é…ç½®ä¸­æ·»åŠ  `actor_rollout_ref.rollout.temperature=0.7` (å¢åŠ å¤šæ ·æ€§)

---

### Step 4: è®¡ç®—å¥–åŠ±

#### ğŸ“ æ–‡ä»¶é“¾
```
verl/trainer/ppo/ray_trainer.py:1102-1128
  â†“
verl/trainer/ppo/reward.py:200-219
  â†“
verl/utils/reward_score/gsm8k.py:52-72
```

#### ğŸ¯ ä½œç”¨
å¯¹ç”Ÿæˆçš„ 1280 ä¸ªå›ç­”è¿›è¡Œè¯„åˆ†ã€‚

#### ğŸ“ ä»£ç è¯¦è§£

**åˆå¹¶æ•°æ®** (ray_trainer.py:1102-1106):
```python
# 1. é‡å¤åŸå§‹ batch ä»¥å¯¹é½ 5 ä¸ªå›ç­”
batch = batch.repeat(repeat_times=self.config.actor_rollout_ref.rollout.n, interleave=True)

# 2. åˆå¹¶ç”Ÿæˆçš„ responses
batch = batch.union(gen_batch_output)

# 3. è®¡ç®— response_mask (å¦‚æœæ²¡æœ‰)
if "response_mask" not in batch.batch.keys():
    batch.batch["response_mask"] = compute_response_mask(batch)
```

**åˆå¹¶åçš„æ•°æ®ç»“æ„**:
```python
batch.batch = {
    "input_ids": torch.Tensor([1280, 512]),           # é‡å¤åçš„ prompt
    "attention_mask": torch.Tensor([1280, 1536]),     # prompt + response
    "responses": torch.Tensor([1280, 1024]),          # ç”Ÿæˆçš„å›ç­”
    "rollout_log_probs": torch.Tensor([1280, 1024]), # vLLM çš„ log_probs
    "response_mask": torch.Tensor([1280, 1024]),      # response çš„ mask
}

batch.non_tensor_batch = {
    "uid": np.array([...]),  # 1280 ä¸ª uid (æ¯ç»„ 5 ä¸ªç›¸åŒ)
    "reward_model": [...],   # 1280 ä¸ªé…ç½® (æ¯ç»„ 5 ä¸ªç›¸åŒ)
}
```

---

**è°ƒç”¨å¥–åŠ±è®¡ç®—** (ray_trainer.py:1117-1128):
```python
with marked_timer("reward", timing_raw, color="yellow"):
    # 1. å¦‚æœä½¿ç”¨ RM æ¨¡å‹æ‰“åˆ†
    if self.use_rm and "rm_scores" not in batch.batch.keys():
        reward_tensor = self.rm_wg.compute_rm_score(batch)
        batch = batch.union(reward_tensor)

    # 2. è°ƒç”¨å¥–åŠ±å‡½æ•° (è§„åˆ™æˆ–å…¶ä»–)
    if self.config.reward_model.launch_reward_fn_async:
        # å¼‚æ­¥è°ƒç”¨ (ä¸é˜»å¡)
        future_reward = compute_reward_async.remote(
            data=batch, config=self.config, tokenizer=self.tokenizer
        )
    else:
        # åŒæ­¥è°ƒç”¨
        reward_tensor, reward_extra_infos_dict = compute_reward(batch, self.reward_fn)
```

---

**compute_reward å‡½æ•°** (reward.py:200-219):
```python
@tqbridge(put_data=False)
def compute_reward(data: DataProto, reward_fn: AbstractRewardManager) -> tuple[torch.Tensor, dict[str, Any]]:
    """
    è®¡ç®—å¥–åŠ±çš„ç»Ÿä¸€æ¥å£

    Args:
        data: åŒ…å« input_ids, responses, reward_model é…ç½®çš„ DataProto
        reward_fn: å¥–åŠ±ç®¡ç†å™¨å®ä¾‹ (ä¾‹å¦‚ NaiveRewardManager)

    Returns:
        reward_tensor: shape (batch_size, response_length)
        reward_extra_info: é¢å¤–ä¿¡æ¯ (ä¾‹å¦‚å‡†ç¡®ç‡)
    """
    # è°ƒç”¨ reward_fn (ä¼šè‡ªåŠ¨è°ƒç”¨ compute_score)
    result = reward_fn(data, return_dict=True)

    reward_tensor = result["reward_tensor"]
    reward_extra_info = result.get("reward_extra_info", {})

    return reward_tensor, reward_extra_info
```

**NaiveRewardManager å†…éƒ¨é€»è¾‘** (ç®€åŒ–):
```python
def __call__(self, data: DataProto, return_dict=True):
    """å¯¹æ¯ä¸ª response è°ƒç”¨ compute_score"""
    rewards = []
    extra_infos = defaultdict(list)

    for i in range(len(data)):
        # æå–å•ä¸ªæ ·æœ¬
        response_text = self.tokenizer.decode(data[i].batch["responses"], skip_special_tokens=True)
        ground_truth = data[i].non_tensor_batch["reward_model"]["ground_truth"]

        # è°ƒç”¨ç”¨æˆ·å®šä¹‰çš„ compute_score
        score = self.compute_score(response_text, ground_truth)
        rewards.append(score)

        # è®°å½•é¢å¤–ä¿¡æ¯
        extra_infos["is_correct"].append(score > 0)

    # è½¬ä¸º tensor (outcome reward: åªæœ‰æœ€åä¸€ä¸ª token æœ‰å¥–åŠ±)
    reward_tensor = torch.zeros(len(data), max_response_length)
    for i, score in enumerate(rewards):
        last_valid_idx = data[i].batch["response_mask"].sum() - 1
        reward_tensor[i, last_valid_idx] = score

    return {
        "reward_tensor": reward_tensor,
        "reward_extra_info": extra_infos
    }
```

---

**GSM8K compute_score** (gsm8k.py:52-72):
```python
def compute_score(solution_str, ground_truth, method="strict", format_score=0.0, score=1.0):
    """
    GSM8K è¯„åˆ†å‡½æ•°

    Args:
        solution_str: æ¨¡å‹ç”Ÿæˆçš„å›ç­”æ–‡æœ¬
        ground_truth: æ­£ç¡®ç­”æ¡ˆ (ä¾‹å¦‚ "8")
        method: æå–æ–¹æ³• ("strict" æˆ– "flexible")
        format_score: æ ¼å¼æ­£ç¡®ä½†ç­”æ¡ˆé”™è¯¯çš„åˆ†æ•° (é»˜è®¤ 0.0)
        score: ç­”æ¡ˆæ­£ç¡®çš„åˆ†æ•° (é»˜è®¤ 1.0)

    Returns:
        float: 0.0 æˆ– 1.0
    """
    # 1. æå–ç­”æ¡ˆ
    answer = extract_solution(solution_str=solution_str, method=method)

    # 2. åˆ¤æ–­æ­£ç¡®æ€§
    if answer is None:
        return 0  # æ²¡æœ‰æ‰¾åˆ°ç­”æ¡ˆæ ¼å¼
    else:
        if answer == ground_truth:
            return score  # 1.0
        else:
            return format_score  # 0.0
```

**extract_solution å‡½æ•°** (gsm8k.py:20-49):
```python
def extract_solution(solution_str, method="strict"):
    """
    ä»å›ç­”ä¸­æå–æ•°å­—ç­”æ¡ˆ

    Args:
        solution_str: å›ç­”æ–‡æœ¬
        method: "strict" (ä¸¥æ ¼åŒ¹é… #### [æ•°å­—]) æˆ– "flexible" (ä»»æ„æ•°å­—)

    Returns:
        str or None: æå–çš„ç­”æ¡ˆ
    """
    # ä¼˜åŒ–: åªåŒ¹é…æœ€å 300 ä¸ªå­—ç¬¦ (ç­”æ¡ˆé€šå¸¸åœ¨æœ«å°¾)
    if len(solution_str) > 300:
        solution_str = solution_str[-300:]

    if method == "strict":
        # åŒ¹é… "#### [æ•°å­—]" æ ¼å¼
        solutions = re.findall(r"#### (\-?[0-9\.\,]+)", solution_str)
        if len(solutions) == 0:
            final_answer = None
        else:
            # å–æœ€åä¸€ä¸ªåŒ¹é…
            final_answer = solutions[-1].replace(",", "").replace("$", "")
    elif method == "flexible":
        # åŒ¹é…ä»»æ„æ•°å­—
        answer = re.findall(r"(\-?[0-9\.\,]+)", solution_str)
        final_answer = None
        if len(answer) > 0:
            # æ‰¾æœ€åä¸€ä¸ªéç©ºæ•°å­—
            for final_answer in reversed(answer):
                if final_answer not in ["", "."]:
                    break

    return final_answer
```

**ç¤ºä¾‹è¯„åˆ†**:
```python
# Response 0
solution_str = "She spent 5 * $3 = $15. So she has $23 - $15 = $8 left. #### 8"
ground_truth = "8"
extract_solution(solution_str) â†’ "8"
compute_score(solution_str, ground_truth) â†’ 1.0 âœ“

# Response 3 (é”™è¯¯)
solution_str = "... 23-15=8. #### 7"
ground_truth = "8"
extract_solution(solution_str) â†’ "7"
compute_score(solution_str, ground_truth) â†’ 0.0 âœ—

# Response æ— æ ¼å¼
solution_str = "I don't know."
ground_truth = "8"
extract_solution(solution_str) â†’ None
compute_score(solution_str, ground_truth) â†’ 0.0 âœ—
```

**æœ€ç»ˆå¥–åŠ±å¼ é‡**:
```python
reward_tensor = torch.Tensor([1280, 1024])

# å¯¹äºç¬¬ 0 ä¸ªæ ·æœ¬ (å›ç­”æ­£ç¡®):
reward_tensor[0] = [0, 0, 0, ..., 0, 1.0]  # åªæœ‰æœ€åä¸€ä¸ªæœ‰æ•ˆ token æ˜¯ 1.0

# å¯¹äºç¬¬ 3 ä¸ªæ ·æœ¬ (å›ç­”é”™è¯¯):
reward_tensor[3] = [0, 0, 0, ..., 0, 0.0]  # æœ€åä¸€ä¸ªæœ‰æ•ˆ token æ˜¯ 0.0
```

---

### ğŸ” å¥–åŠ±å‡½æ•°æŸ¥æ‰¾æœºåˆ¶è¯¦è§£

#### ğŸ“‹ æ•´ä½“æµç¨‹

å¥–åŠ±å‡½æ•°çš„æŸ¥æ‰¾éµå¾ªä»¥ä¸‹ä¼˜å…ˆçº§é¡ºåº:

```
1. è‡ªå®šä¹‰å¥–åŠ±å‡½æ•° (custom_reward_function)
   â†“ (å¦‚æœæ²¡æœ‰)
2. é»˜è®¤å¥–åŠ±å‡½æ•° (default_compute_score)
   â†“ (æ ¹æ® data_source è·¯ç”±)
3. å…·ä½“ä»»åŠ¡çš„ compute_score å‡½æ•°
```

---

#### ğŸ¯ æŸ¥æ‰¾æ­¥éª¤ 1: load_reward_manager

æ–‡ä»¶: `verl/trainer/ppo/reward.py:120-196`

```python
def load_reward_manager(config, tokenizer, num_examine, **reward_kwargs):
    """
    åŠ è½½å¥–åŠ±ç®¡ç†å™¨çš„ä¸»å‡½æ•°

    Args:
        config: å®Œæ•´é…ç½® (åŒ…å« reward_model, data ç­‰)
        tokenizer: åˆ†è¯å™¨
        num_examine: è°ƒè¯•æ—¶æ‰“å°çš„æ ·æœ¬æ•°
        **reward_kwargs: é¢å¤–çš„å¥–åŠ±å‡½æ•°å‚æ•°

    Returns:
        AbstractRewardManager: å¥–åŠ±ç®¡ç†å™¨å®ä¾‹
    """

    # Step 1: å°è¯•è·å–è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°
    compute_score = get_custom_reward_fn(config)

    # Step 2: å¦‚æœæ²¡æœ‰è‡ªå®šä¹‰å‡½æ•°,ä½¿ç”¨é»˜è®¤å‡½æ•°
    if compute_score is None:
        compute_score = default_compute_score

    # Step 3: å®ä¾‹åŒ–å¥–åŠ±ç®¡ç†å™¨ (é»˜è®¤æ˜¯ NaiveRewardManager)
    reward_manager_cls = get_reward_manager_cls(config.reward_manager.name)

    return reward_manager_cls(
        tokenizer=tokenizer,
        num_examine=num_examine,
        compute_score=compute_score,
        reward_fn_key=config.data.reward_fn_key,  # å…³é”®! å†³å®šä»å“ªé‡Œè¯»å– data_source
        **reward_kwargs
    )
```

**å…³é”®å‚æ•°**:
- `config.reward_manager.name`: å¥–åŠ±ç®¡ç†å™¨ç±»å‹ (é»˜è®¤ `"naive"`)
- `config.data.reward_fn_key`: ä»æ•°æ®ä¸­è¯»å–å“ªä¸ªå­—æ®µä½œä¸º data_source (é»˜è®¤ `"data_source"`)

---

#### ğŸ¯ æŸ¥æ‰¾æ­¥éª¤ 2: get_custom_reward_fn

æ–‡ä»¶: `verl/trainer/ppo/reward.py:63-118`

```python
def get_custom_reward_fn(config):
    """
    ä»å¤–éƒ¨æ–‡ä»¶åŠ è½½è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°

    Args:
        config: é…ç½®å­—å…¸,åŒ…å« custom_reward_function å­—æ®µ

    Returns:
        callable or None: è‡ªå®šä¹‰å¥–åŠ±å‡½æ•° (å¦‚æœé…ç½®äº†), å¦åˆ™è¿”å› None
    """
    reward_fn_config = config.get("custom_reward_function") or {}
    file_path = reward_fn_config.get("path")

    # å¦‚æœæ²¡æœ‰é…ç½®è‡ªå®šä¹‰å‡½æ•°,è¿”å› None
    if not file_path:
        return None

    function_name = reward_fn_config.get("name")

    # åŠ¨æ€åŠ è½½å¤–éƒ¨ Python æ–‡ä»¶
    spec = importlib.util.spec_from_file_location("custom_module", file_path)
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)

    # è·å–æŒ‡å®šçš„å‡½æ•°
    raw_fn = getattr(module, function_name)

    # åˆå¹¶ reward_kwargs (é¢å¤–çš„å‚æ•°)
    reward_kwargs = dict(reward_fn_config.get("reward_kwargs", {}))
    return partial(_call_with_kwargs, raw_fn, reward_kwargs)
```

**é…ç½®ç¤ºä¾‹**:
```yaml
custom_reward_function:
  path: "/path/to/my_reward.py"
  name: "my_compute_score"
  reward_kwargs:
    threshold: 0.8
    bonus: 0.5
```

---

#### ğŸ¯ æŸ¥æ‰¾æ­¥éª¤ 3: default_compute_score

æ–‡ä»¶: `verl/utils/reward_score/__init__.py:19-115`

å¦‚æœæ²¡æœ‰è‡ªå®šä¹‰å‡½æ•°,åˆ™ä½¿ç”¨ `default_compute_score`,å®ƒä¼šæ ¹æ® **data_source** è·¯ç”±åˆ°ä¸åŒçš„ä»»åŠ¡:

```python
def default_compute_score(data_source, solution_str, ground_truth, extra_info=None, **kwargs):
    """
    é»˜è®¤è¯„åˆ†å‡½æ•°,æ ¹æ®æ•°æ®æºè·¯ç”±åˆ°ä¸åŒçš„è¯„åˆ†é€»è¾‘

    Args:
        data_source: æ•°æ®é›†åç§° (ä¾‹å¦‚ "openai/gsm8k")
        solution_str: æ¨¡å‹ç”Ÿæˆçš„å›ç­”æ–‡æœ¬
        ground_truth: æ­£ç¡®ç­”æ¡ˆ
        extra_info: é¢å¤–ä¿¡æ¯ (å¯é€‰)

    Returns:
        float: å¥–åŠ±åˆ†æ•°
    """

    # GSM8K æ•°å­¦é¢˜
    if data_source == "openai/gsm8k":
        from . import gsm8k
        res = gsm8k.compute_score(solution_str, ground_truth)

    # MATH æ•°æ®é›†
    elif data_source in ["lighteval/MATH", "DigitalLearningGmbH/MATH-lighteval"]:
        from . import math_reward
        res = math_reward.compute_score(solution_str, ground_truth)

    # ä»£ç æ‰§è¡Œä»»åŠ¡
    elif data_source in ["codecontests", "apps", "codeforces"]:
        from . import sandbox_fusion
        res = sandbox_fusion.compute_score(
            sandbox_url, solution_str, ground_truth, continuous=True
        )

    # é—®ç­”ä»»åŠ¡
    elif data_source in ["searchR1_nq", "searchR1_triviaqa"]:
        from . import search_r1_like_qa_em
        res = search_r1_like_qa_em.compute_score(solution_str, ground_truth)

    else:
        raise NotImplementedError(f"Reward function is not implemented for {data_source=}")

    return float(res)
```

**æ”¯æŒçš„æ•°æ®é›†** (æˆªè‡³å½“å‰ç‰ˆæœ¬):

| æ•°æ®é›†åˆ†ç±» | data_source å€¼ | è¯„åˆ†å‡½æ•°æ–‡ä»¶ |
|----------|---------------|------------|
| æ•°å­¦é¢˜ | `openai/gsm8k` | `gsm8k.py` |
| æ•°å­¦é¢˜ | `lighteval/MATH`, `HuggingFaceH4/MATH-500` | `math_reward.py` |
| æ•°å­¦é¢˜ | `math_dapo`, `aime*` | `math_dapo.py` |
| å‡ ä½•é¢˜ | `hiyouga/geometry3k` | `geo3k.py` |
| ä»£ç æ‰§è¡Œ | `codecontests`, `apps`, `codeforces`, `taco` | `sandbox_fusion.py` |
| é—®ç­” | `searchR1_nq`, `searchR1_triviaqa`, `searchR1_popqa` | `search_r1_like_qa_em.py` |

---

#### ğŸ¯ æŸ¥æ‰¾æ­¥éª¤ 4: NaiveRewardManager è°ƒç”¨

æ–‡ä»¶: `verl/workers/reward_manager/naive.py:46-126`

```python
class NaiveRewardManager(AbstractRewardManager):
    def __init__(self, tokenizer, num_examine, compute_score=None, reward_fn_key="data_source"):
        self.tokenizer = tokenizer
        self.num_examine = num_examine
        self.compute_score = compute_score or default_compute_score  # å…³é”®!
        self.reward_fn_key = reward_fn_key  # å…³é”®! ä»å“ªä¸ªå­—æ®µè¯»å– data_source

    def __call__(self, data: DataProto, return_dict=False):
        """å¯¹æ¯ä¸ªæ ·æœ¬è°ƒç”¨ compute_score"""
        reward_tensor = torch.zeros_like(data.batch["responses"], dtype=torch.float32)

        for i in range(len(data)):
            # 1. è§£ç  response
            response_str = self.tokenizer.decode(
                data[i].batch["responses"], skip_special_tokens=True
            )

            # 2. è·å– ground_truth
            ground_truth = data[i].non_tensor_batch["reward_model"]["ground_truth"]

            # 3. è·å– data_source (ä» reward_fn_key æŒ‡å®šçš„å­—æ®µè¯»å–)
            data_source = data[i].non_tensor_batch[self.reward_fn_key]

            # 4. è°ƒç”¨ compute_score
            score = self.compute_score(
                data_source=data_source,
                solution_str=response_str,
                ground_truth=ground_truth,
                extra_info=data[i].non_tensor_batch.get("extra_info", {})
            )

            # 5. å¡«å……åˆ° reward_tensor (outcome reward: åªæœ‰æœ€åä¸€ä¸ª token æœ‰å¥–åŠ±)
            valid_response_length = data[i].batch["attention_mask"][prompt_length:].sum()
            reward_tensor[i, valid_response_length - 1] = score

        return reward_tensor
```

**å…³é”®é€»è¾‘**:
1. `reward_fn_key` å†³å®šä»å“ªé‡Œè¯»å– `data_source`:
   - é»˜è®¤æ˜¯ `"data_source"` å­—æ®µ
   - ä¹Ÿå¯ä»¥é…ç½®ä¸º `"dataset_name"` ç­‰å…¶ä»–å­—æ®µ
2. `data_source` çš„å€¼å†³å®šè°ƒç”¨å“ªä¸ª `compute_score` å‡½æ•°
3. å¥–åŠ±åªç»™æœ€åä¸€ä¸ªæœ‰æ•ˆ token (outcome reward)

---

#### ğŸ¯ data_source ä»å“ªé‡Œæ¥?

**ä»æ•°æ®é›†ä¸­è¯»å–**:

ä»¥ GSM8K ä¸ºä¾‹ (æ–‡ä»¶: `data/gsm8k/train.parquet`):

```python
{
    "data_source": "openai/gsm8k",  # â† è¿™ä¸ªå­—æ®µ!
    "prompt": "Olivia has $23. She bought five bagels for $3 each. How much money does she have left?",
    "reward_model": {
        "ground_truth": "8",
        "style": "step-by-step"
    }
}
```

**åœ¨æ•°æ®åŠ è½½æ—¶è®¾ç½®**:

æ–‡ä»¶: `verl/data/reward_dataset.py`

```python
def preprocess_item(item):
    """é¢„å¤„ç†å•ä¸ªæ•°æ®æ ·æœ¬"""
    return {
        "data_source": item.get("data_source", "openai/gsm8k"),  # é»˜è®¤å€¼
        "prompt": item["prompt"],
        "reward_model": item["reward_model"],
        # ...
    }
```

---

#### ğŸ¯ å®Œæ•´è°ƒç”¨é“¾æ€»ç»“

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. å¯åŠ¨è„šæœ¬: run_qwen3-8b.sh                                â”‚
â”‚     â†’ è®¾ç½® data.reward_fn_key="data_source"                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. main.py:274                                             â”‚
â”‚     â†’ reward_fn = load_reward_manager(config, tokenizer)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. reward.py:120 load_reward_manager()                     â”‚
â”‚     â†’ compute_score = get_custom_reward_fn(config)          â”‚
â”‚     â†’ å¦‚æœä¸º None, åˆ™ compute_score = default_compute_score â”‚
â”‚     â†’ return NaiveRewardManager(compute_score=...)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. ray_trainer.py:1117                                     â”‚
â”‚     â†’ reward_tensor = compute_reward(batch, self.reward_fn) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. reward.py:200 compute_reward()                          â”‚
â”‚     â†’ result = reward_fn(data)  # è°ƒç”¨ NaiveRewardManager   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  6. naive.py:89 NaiveRewardManager.__call__()               â”‚
â”‚     â†’ data_source = data[i].non_tensor_batch[reward_fn_key] â”‚
â”‚     â†’ score = self.compute_score(                           â”‚
â”‚           data_source=data_source,  # "openai/gsm8k"        â”‚
â”‚           solution_str=response_str,                        â”‚
â”‚           ground_truth=ground_truth                         â”‚
â”‚       )                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  7. __init__.py:44 default_compute_score()                  â”‚
â”‚     â†’ if data_source == "openai/gsm8k":                     â”‚
â”‚           from . import gsm8k                               â”‚
â”‚           res = gsm8k.compute_score(solution_str, ...)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  8. gsm8k.py:52 compute_score()                             â”‚
â”‚     â†’ answer = extract_solution(solution_str)               â”‚
â”‚     â†’ return 1.0 if answer == ground_truth else 0.0         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

---

### ğŸ› ï¸ å¦‚ä½•ä¿®æ”¹å¥–åŠ±å‡½æ•°

æ ¹æ®ä½ çš„éœ€æ±‚,æœ‰ **ä¸‰ç§** ä¿®æ”¹å¥–åŠ±å‡½æ•°çš„æ–¹æ³•:

---

#### æ–¹æ³• 1: ä½¿ç”¨è‡ªå®šä¹‰å¥–åŠ±å‡½æ•° (æ¨è)

é€‚ç”¨åœºæ™¯: å®Œå…¨è‡ªå®šä¹‰çš„è¯„åˆ†é€»è¾‘,ä¸æƒ³ä¿®æ”¹æ¡†æ¶ä»£ç 

**æ­¥éª¤ 1: åˆ›å»ºå¥–åŠ±å‡½æ•°æ–‡ä»¶**

åˆ›å»º `my_reward.py`:
```python
# my_reward.py
def my_compute_score(data_source, solution_str, ground_truth, extra_info=None, **kwargs):
    """
    è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°

    Args:
        data_source: æ•°æ®é›†åç§° (ä¾‹å¦‚ "my_dataset")
        solution_str: æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬
        ground_truth: æ­£ç¡®ç­”æ¡ˆ
        extra_info: é¢å¤–ä¿¡æ¯
        **kwargs: å…¶ä»–å‚æ•° (ä¾‹å¦‚ threshold, bonus ç­‰)

    Returns:
        float: å¥–åŠ±åˆ†æ•° (0.0 åˆ° 1.0)
    """
    # ç¤ºä¾‹ 1: ç²¾ç¡®åŒ¹é…
    if solution_str.strip() == ground_truth.strip():
        return 1.0
    else:
        return 0.0

    # ç¤ºä¾‹ 2: ä»£ç æ‰§è¡Œ
    # try:
    #     exec_result = execute_code(solution_str)
    #     return 1.0 if exec_result == ground_truth else 0.0
    # except:
    #     return 0.0

    # ç¤ºä¾‹ 3: ä½¿ç”¨é¢å¤–å‚æ•°
    # threshold = kwargs.get("threshold", 0.8)
    # similarity = compute_similarity(solution_str, ground_truth)
    # return 1.0 if similarity >= threshold else 0.0
```

**æ­¥éª¤ 2: ä¿®æ”¹é…ç½®**

ç¼–è¾‘ `examples/grpo_qwen3/config/grpo_qwen3.yaml` æˆ–åœ¨å¯åŠ¨è„šæœ¬ä¸­æ·»åŠ :

```yaml
custom_reward_function:
  path: "/path/to/my_reward.py"  # ç»å¯¹è·¯å¾„
  name: "my_compute_score"        # å‡½æ•°å
  reward_kwargs:                  # å¯é€‰: é¢å¤–å‚æ•°
    threshold: 0.8
    bonus: 0.5
```

æˆ–åœ¨ `run_qwen3-8b.sh` ä¸­æ·»åŠ :
```bash
python3 -m verl.trainer.main_ppo \
    algorithm.grpo=true \
    custom_reward_function.path=/path/to/my_reward.py \
    custom_reward_function.name=my_compute_score \
    custom_reward_function.reward_kwargs.threshold=0.8
```

**ä¼˜ç‚¹**:
- âœ… ä¸éœ€è¦ä¿®æ”¹æ¡†æ¶ä»£ç 
- âœ… å¯ä»¥éšæ„ä¿®æ”¹å¥–åŠ±å‡½æ•°,ä¸å½±å“å…¶ä»–ä»»åŠ¡
- âœ… æ”¯æŒä¼ é€’è‡ªå®šä¹‰å‚æ•°

**ç¼ºç‚¹**:
- âŒ éœ€è¦æä¾›ç»å¯¹è·¯å¾„

---

#### æ–¹æ³• 2: åœ¨ default_compute_score ä¸­æ·»åŠ æ–°çš„ data_source

é€‚ç”¨åœºæ™¯: ä½ æœ‰ä¸€ä¸ªæ–°ä»»åŠ¡ç±»å‹,æƒ³è¦æ°¸ä¹…æ·»åŠ åˆ°æ¡†æ¶ä¸­

**æ­¥éª¤ 1: åˆ›å»ºè¯„åˆ†å‡½æ•°æ–‡ä»¶**

åˆ›å»º `verl/utils/reward_score/my_task.py`:
```python
# verl/utils/reward_score/my_task.py

def compute_score(solution_str, ground_truth, **kwargs):
    """
    æˆ‘çš„ä»»åŠ¡çš„è¯„åˆ†å‡½æ•°

    Args:
        solution_str: æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬
        ground_truth: æ­£ç¡®ç­”æ¡ˆ

    Returns:
        float: å¥–åŠ±åˆ†æ•°
    """
    # ä½ çš„è¯„åˆ†é€»è¾‘
    if "æ­£ç¡®" in solution_str:
        return 1.0
    else:
        return 0.0
```

**æ­¥éª¤ 2: æ³¨å†Œåˆ° default_compute_score**

ç¼–è¾‘ `verl/utils/reward_score/__init__.py`:
```python
def default_compute_score(
    data_source,
    solution_str,
    ground_truth,
    extra_info=None,
    **kwargs,
):
    # ... ç°æœ‰ä»£ç  ...

    # æ·»åŠ ä½ çš„ä»»åŠ¡
    elif data_source == "my_custom_task":
        from . import my_task
        res = my_task.compute_score(solution_str, ground_truth)

    else:
        raise NotImplementedError(f"Reward function is not implemented for {data_source=}")

    return float(res)
```

**æ­¥éª¤ 3: ä¿®æ”¹æ•°æ®é›†å’Œé…ç½®**

ç¡®ä¿ä½ çš„æ•°æ®é›†ä¸­åŒ…å«æ­£ç¡®çš„ `data_source`:
```json
{
    "data_source": "my_custom_task",  # â† å¿…é¡»åŒ¹é…!
    "prompt": "ä½ çš„é—®é¢˜",
    "reward_model": {
        "ground_truth": "æ­£ç¡®ç­”æ¡ˆ"
    }
}
```

åœ¨ `run_qwen3-8b.sh` ä¸­ç¡®ä¿:
```bash
data.reward_fn_key=data_source  # ä½¿ç”¨é»˜è®¤å€¼
```

**ä¼˜ç‚¹**:
- âœ… æˆä¸ºæ¡†æ¶çš„ä¸€éƒ¨åˆ†,å¯ä»¥å¤ç”¨
- âœ… å…¶ä»–äººä¹Ÿå¯ä»¥ä½¿ç”¨ä½ çš„è¯„åˆ†å‡½æ•°

**ç¼ºç‚¹**:
- âŒ éœ€è¦ä¿®æ”¹æ¡†æ¶ä»£ç 
- âŒ éœ€è¦ç¡®ä¿ data_source å­—æ®µä¸€è‡´

---

#### æ–¹æ³• 3: ä¿®æ”¹ç°æœ‰çš„è¯„åˆ†å‡½æ•°

é€‚ç”¨åœºæ™¯: ä½ æƒ³è°ƒæ•´ GSM8Kã€MATH ç­‰ç°æœ‰ä»»åŠ¡çš„è¯„åˆ†é€»è¾‘

**æ­¥éª¤: ç›´æ¥ä¿®æ”¹å¯¹åº”çš„æ–‡ä»¶**

ä¾‹å¦‚ä¿®æ”¹ GSM8K çš„è¯„åˆ†:

ç¼–è¾‘ `verl/utils/reward_score/gsm8k.py`:
```python
def compute_score(solution_str, ground_truth, method="strict", format_score=0.0, score=1.0):
    """ä¿®æ”¹åçš„ GSM8K è¯„åˆ†å‡½æ•°"""

    # åŸå§‹é€»è¾‘
    answer = extract_solution(solution_str=solution_str, method=method)

    if answer is None:
        return 0

    # ä¿®æ”¹ 1: ç»™æ ¼å¼æ­£ç¡®ä½†ç­”æ¡ˆé”™è¯¯çš„ä¸€äº›åˆ†æ•°
    if answer == ground_truth:
        return score  # 1.0
    else:
        return format_score  # åŸæ¥æ˜¯ 0.0, å¯ä»¥æ”¹æˆ 0.3

    # ä¿®æ”¹ 2: æ·»åŠ éƒ¨åˆ†åˆ†æœºåˆ¶
    # if answer == ground_truth:
    #     return 1.0
    # elif is_close_to_answer(answer, ground_truth):
    #     return 0.5  # éƒ¨åˆ†åˆ†
    # else:
    #     return 0.0
```

ç„¶ååœ¨é…ç½®ä¸­å¯ä»¥ä¼ é€’å‚æ•°:
```bash
python3 -m verl.trainer.main_ppo \
    algorithm.grpo=true \
    reward_model.reward_kwargs.format_score=0.3 \
    reward_model.reward_kwargs.score=1.0
```

**ä¼˜ç‚¹**:
- âœ… ç®€å•ç›´æ¥
- âœ… å¯ä»¥åˆ©ç”¨ç°æœ‰çš„ extract_solution ç­‰è¾…åŠ©å‡½æ•°

**ç¼ºç‚¹**:
- âŒ ä¼šå½±å“æ‰€æœ‰ä½¿ç”¨è¯¥è¯„åˆ†å‡½æ•°çš„ä»»åŠ¡
- âŒ éœ€è¦å°å¿ƒ,é¿å…ç ´ååŸæœ‰é€»è¾‘

---

#### ğŸ“Š ä¸‰ç§æ–¹æ³•å¯¹æ¯”

| ç‰¹æ€§ | æ–¹æ³• 1: è‡ªå®šä¹‰å‡½æ•° | æ–¹æ³• 2: æ·»åŠ  data_source | æ–¹æ³• 3: ä¿®æ”¹ç°æœ‰å‡½æ•° |
|-----|------------------|----------------------|-------------------|
| ä¿®æ”¹æ¡†æ¶ä»£ç  | âŒ ä¸éœ€è¦ | âœ… éœ€è¦ | âœ… éœ€è¦ |
| é€‚ç”¨åœºæ™¯ | ä¸´æ—¶å®éªŒ | æ–°ä»»åŠ¡ç±»å‹ | è°ƒæ•´ç°æœ‰ä»»åŠ¡ |
| å¯å¤ç”¨æ€§ | âŒ ä½ | âœ… é«˜ | âš ï¸ ä¸­ |
| çµæ´»æ€§ | âœ… é«˜ | âœ… é«˜ | âš ï¸ ä¸­ |
| æ¨èåº¦ | â­â­â­â­â­ | â­â­â­â­ | â­â­â­ |

---

#### ğŸ’¡ å®é™…æ¡ˆä¾‹: æ•°å­¦ç«èµ›ä»»åŠ¡

å‡è®¾ä½ è¦åš AMC æ•°å­¦ç«èµ›,éœ€è¦:
- ç­”æ¡ˆæ ¼å¼: `\boxed{æ•°å­—}`
- è¯„åˆ†: æ­£ç¡® 1.0, é”™è¯¯ 0.0, æ ¼å¼é”™è¯¯ -0.1

**ä½¿ç”¨æ–¹æ³• 1 (æ¨è)**:

åˆ›å»º `amc_reward.py`:
```python
import re

def compute_amc_score(data_source, solution_str, ground_truth, extra_info=None, **kwargs):
    """AMC æ•°å­¦ç«èµ›è¯„åˆ†"""
    # æå– \boxed{...} ä¸­çš„ç­”æ¡ˆ
    match = re.search(r'\\boxed\{([^}]+)\}', solution_str)

    if match is None:
        return -0.1  # æ ¼å¼é”™è¯¯

    answer = match.group(1).strip()

    if answer == ground_truth:
        return 1.0
    else:
        return 0.0
```

é…ç½®:
```bash
python3 -m verl.trainer.main_ppo \
    algorithm.grpo=true \
    custom_reward_function.path=/path/to/amc_reward.py \
    custom_reward_function.name=compute_amc_score
```

---

#### ğŸ› è°ƒè¯•æŠ€å·§

**1. æ‰“å°å¥–åŠ±åˆ†æ•°**

åœ¨ `verl/workers/reward_manager/naive.py:109-118` ä¸­,è®¾ç½® `num_examine=5`:
```bash
python3 -m verl.trainer.main_ppo \
    algorithm.grpo=true \
    reward_model.reward_kwargs.num_examine=5
```

è¿™ä¼šæ‰“å°å‰ 5 ä¸ªæ ·æœ¬çš„:
- `[prompt]`: è¾“å…¥é—®é¢˜
- `[response]`: æ¨¡å‹ç”Ÿæˆçš„å›ç­”
- `[ground_truth]`: æ­£ç¡®ç­”æ¡ˆ
- `[score]`: è®¡ç®—çš„å¥–åŠ±

**2. éªŒè¯ data_source**

åœ¨æ•°æ®é›†åŠ è½½åæ‰“å°:
```python
# åœ¨ verl/data/reward_dataset.py ä¸­æ·»åŠ 
print(f"Sample data_source: {item['data_source']}")
```

**3. æµ‹è¯•å¥–åŠ±å‡½æ•°**

ç‹¬ç«‹æµ‹è¯•:
```python
from verl.utils.reward_score.gsm8k import compute_score

solution = "The answer is 8. #### 8"
ground_truth = "8"
score = compute_score(solution, ground_truth)
print(f"Score: {score}")  # åº”è¯¥æ˜¯ 1.0
```

---

### Step 5: è®¡ç®— old_log_probs

#### ğŸ“ æ–‡ä»¶é“¾
```
verl/trainer/ppo/ray_trainer.py:1145-1159
  â†“ (RPC è°ƒç”¨)
verl/workers/fsdp_workers.py:961-998
  â†“
verl/workers/actor/dp_actor.py:180-250
```

#### ğŸ¯ ä½œç”¨
è®¡ç®—å½“å‰ç­–ç•¥å¯¹å·²ç”Ÿæˆåºåˆ—çš„ log æ¦‚ç‡ï¼Œä½œä¸º PPO çš„"æ—§ç­–ç•¥"ã€‚

#### ğŸ“ ä»£ç è¯¦è§£

**ä¸ºä»€ä¹ˆéœ€è¦ old_log_probs?**

PPO ç®—æ³•éœ€è¦ä¸¤ä¸ªç­–ç•¥:
- **Ï€_old** (æ—§ç­–ç•¥): ç”¨äº PPO clip çš„å‚è€ƒç‚¹ï¼Œä¿è¯æ›´æ–°ä¸è¦å¤ªæ¿€è¿›
- **Ï€_Î¸** (æ–°ç­–ç•¥): å½“å‰æ­£åœ¨æ›´æ–°çš„ç­–ç•¥

**è®¡ç®—æµç¨‹**:
```
å·²ç”Ÿæˆçš„åºåˆ— â†’ å½“å‰ Actor æ¨¡å‹ â†’ è®¡ç®— log P(token | prefix) â†’ old_log_probs
```

---

**è°ƒç”¨ compute_log_prob** (ray_trainer.py:1145-1159):
```python
else:  # é bypass æ¨¡å¼ (æ ‡å‡†æµç¨‹)
    with marked_timer("old_log_prob", timing_raw, color="blue"):
        # 1. è°ƒç”¨ Actor Worker è®¡ç®— log_prob
        old_log_prob = self.actor_rollout_wg.compute_log_prob(batch)

        # 2. æå– entropy (ç”¨äº entropy bonus)
        entropys = old_log_prob.batch["entropys"]
        response_masks = batch.batch["response_mask"]
        actor_config = self.config.actor_rollout_ref.actor

        # 3. èšåˆ entropy (å¹³å‡å€¼)
        entropy_agg = agg_loss(
            loss_mat=entropys,
            loss_mask=response_masks,
            loss_agg_mode=actor_config.loss_agg_mode,
            loss_scale_factor=actor_config.loss_scale_factor,
        )
        old_log_prob_metrics = {"actor/entropy": entropy_agg.detach().item()}
        metrics.update(old_log_prob_metrics)

        # 4. ç§»é™¤ entropy, ä¿ç•™ old_log_probs
        old_log_prob.batch.pop("entropys")

        # 5. åˆå¹¶åˆ° batch
        batch = batch.union(old_log_prob)
```

---

**Worker çš„ compute_log_prob** (fsdp_workers.py:961-998):
```python
@register(dispatch_mode=make_nd_compute_dataproto_dispatch_fn(mesh_name="actor"))
@DistProfiler.annotate(color="blue", role="actor_compute_log_prob")
def compute_log_prob(self, data: DataProto):
    """
    è®¡ç®— log æ¦‚ç‡

    Args:
        data: åŒ…å« input_ids, responses, response_mask

    Returns:
        DataProto: åŒ…å« old_log_probs, entropys
    """
    # 1. å¦‚æœä½¿ç”¨ parameter offload, å…ˆåŠ è½½æ¨¡å‹åˆ° GPU
    if self._is_offload_param:
        load_fsdp_model_to_gpu(self.actor_module_fsdp)

    # 2. è®¾ç½® meta_info (æ§åˆ¶ micro_batch å¤§å°)
    data.meta_info["micro_batch_size"] = self.config.rollout.log_prob_micro_batch_size_per_gpu
    data.meta_info["max_token_len"] = self.config.rollout.log_prob_max_token_len_per_gpu
    data.meta_info["use_dynamic_bsz"] = self.config.rollout.log_prob_use_dynamic_bsz
    data.meta_info["temperature"] = self.config.rollout.temperature

    # 3. è°ƒç”¨ actor.compute_log_prob
    with self.ulysses_sharding_manager:
        with adapter_ctx:  # å¦‚æœæ˜¯ LoRA, å¯èƒ½éœ€è¦ç¦ç”¨ adapter
            output, entropys = self.actor.compute_log_prob(data=data, calculate_entropy=True)

        output = DataProto.from_dict(
            tensors={"old_log_probs": output, "entropys": entropys},
            meta_info={"temperature": self.config.rollout.temperature},
        )

    # 4. ç§»å› CPU
    output = output.to("cpu")

    # 5. å¦‚æœä½¿ç”¨ offload, å¸è½½æ¨¡å‹
    if self._is_offload_param:
        offload_fsdp_model_to_cpu(self.actor_module_fsdp)
        log_gpu_memory_usage("After offload actor model during compute_log_prob", logger=logger)

    return output
```

---

**Actor çš„ compute_log_prob** (dp_actor.py:180-250, ç®€åŒ–ç‰ˆ):
```python
def compute_log_prob(self, data: DataProto, calculate_entropy: bool = False):
    """
    è®¡ç®— log æ¦‚ç‡çš„æ ¸å¿ƒå‡½æ•°

    æµç¨‹:
    1. åˆå¹¶ input_ids å’Œ responses ä¸ºå®Œæ•´åºåˆ—
    2. æ¨¡å‹å‰å‘ä¼ æ’­å¾—åˆ° logits
    3. ä» logits è®¡ç®— log_probs
    4. åªä¿ç•™ response éƒ¨åˆ†çš„ log_probs
    """
    # 1. å‡†å¤‡æ•°æ®
    data = data.to(get_device_id())
    input_ids = data.batch["input_ids"]         # (bsz, prompt_len)
    responses = data.batch["responses"]          # (bsz, response_len)
    response_mask = data.batch["response_mask"]  # (bsz, response_len)
    temperature = data.meta_info["temperature"]

    # 2. åˆå¹¶ input å’Œ response
    full_input_ids = torch.cat([input_ids, responses], dim=1)
    # full_input_ids: (bsz, prompt_len + response_len)

    # 3. è®¡ç®— attention_mask
    full_attention_mask = torch.cat([
        data.batch["attention_mask"],
        response_mask
    ], dim=1)

    # 4. åˆ† micro-batch å¤„ç† (é¿å… OOM)
    micro_batch_size = data.meta_info["micro_batch_size"]  # 32
    all_log_probs = []
    all_entropys = [] if calculate_entropy else None

    for i in range(0, len(full_input_ids), micro_batch_size):
        micro_input_ids = full_input_ids[i:i+micro_batch_size]
        micro_attention_mask = full_attention_mask[i:i+micro_batch_size]

        # 5. æ¨¡å‹å‰å‘ä¼ æ’­
        with torch.no_grad():
            outputs = self.actor_module(
                input_ids=micro_input_ids[:, :-1],  # å»æ‰æœ€åä¸€ä¸ª token (teacher forcing)
                attention_mask=micro_attention_mask[:, :-1],
            )
            logits = outputs.logits  # (micro_bsz, seq_len, vocab_size)

        # 6. è®¡ç®— log_probs
        # logprobs_from_logits: ä» logits å’Œ labels è®¡ç®— log P(label | prefix)
        log_probs = logprobs_from_logits(
            logits=logits,
            labels=micro_input_ids[:, 1:],  # å»æ‰ç¬¬ä¸€ä¸ª token (shifted labels)
            temperature=temperature,
        )  # (micro_bsz, seq_len)

        # 7. åªä¿ç•™ response éƒ¨åˆ†
        prompt_len = input_ids.size(1)
        response_log_probs = log_probs[:, prompt_len:]  # (micro_bsz, response_len)
        all_log_probs.append(response_log_probs)

        # 8. è®¡ç®— entropy (å¦‚æœéœ€è¦)
        if calculate_entropy:
            entropys = self.compute_entropy_from_logits(logits, temperature)
            response_entropys = entropys[:, prompt_len:]
            all_entropys.append(response_entropys)

    # 9. åˆå¹¶æ‰€æœ‰ micro-batch
    log_probs = torch.cat(all_log_probs, dim=0)  # (bsz, response_len)
    entropys = torch.cat(all_entropys, dim=0) if calculate_entropy else None

    return log_probs, entropys
```

**logprobs_from_logits å‡½æ•°** (ç®€åŒ–):
```python
def logprobs_from_logits(logits, labels, temperature=1.0):
    """
    ä» logits è®¡ç®— log P(labels | prefix)

    Args:
        logits: (bsz, seq_len, vocab_size)
        labels: (bsz, seq_len)
        temperature: æ¸©åº¦å‚æ•°

    Returns:
        log_probs: (bsz, seq_len)
    """
    # 1. æ¸©åº¦ç¼©æ”¾
    logits = logits / temperature

    # 2. è®¡ç®— log softmax
    log_probs_all = torch.log_softmax(logits, dim=-1)  # (bsz, seq_len, vocab_size)

    # 3. é€‰æ‹© labels å¯¹åº”çš„ log_prob
    log_probs = torch.gather(
        log_probs_all,
        dim=-1,
        index=labels.unsqueeze(-1)
    ).squeeze(-1)  # (bsz, seq_len)

    return log_probs
```

**ç¤ºä¾‹**:
```python
# å‡è®¾ vocab_size = 50000

# è¾“å…¥
input_ids = [[token_1, token_2, ..., token_512]]  # prompt
responses = [[token_513, token_514, ..., token_1536]]  # response
full_ids = [[token_1, ..., token_512, token_513, ..., token_1536]]

# å‰å‘ä¼ æ’­
logits = model(full_ids[:, :-1])  # shape: (1, 1535, 50000)

# è®¡ç®— log_probs
log_probs = logprobs_from_logits(logits, full_ids[:, 1:])  # shape: (1, 1535)

# åªä¿ç•™ response éƒ¨åˆ†
response_log_probs = log_probs[:, 512:]  # shape: (1, 1024)

# ç¤ºä¾‹å€¼
response_log_probs[0] = [
    -2.5,   # log P(token_513 | token_1...token_512)
    -1.8,   # log P(token_514 | token_1...token_513)
    -3.2,   # log P(token_515 | token_1...token_514)
    ...
]
```

**æœ€ç»ˆè¾“å‡º**:
```python
old_log_prob = DataProto(
    batch={
        "old_log_probs": torch.Tensor([1280, 1024]),  # æ¯ä¸ª token çš„ log P
        "entropys": torch.Tensor([1280, 1024]),       # æ¯ä¸ª token çš„ entropy
    }
)
```

#### ğŸ”§ ä¿®æ”¹ç‚¹
- **ä¿®æ”¹ micro_batch_size**: `run_qwen3-8b.sh:27` â†’ `actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=64` (å¢å¤§ä»¥åŠ é€Ÿ)
- **ç¦ç”¨ entropy è®¡ç®—**: éœ€è¦åœ¨ä»£ç ä¸­ä¿®æ”¹ `calculate_entropy=False` (å¦‚æœä¸éœ€è¦ entropy bonus)

---

### Step 6: è®¡ç®— GRPO ä¼˜åŠ¿

#### ğŸ“ æ–‡ä»¶é“¾
```
verl/trainer/ppo/ray_trainer.py:1222-1230
  â†“
verl/trainer/ppo/ray_trainer.py:181-259 (compute_advantage)
  â†“
verl/trainer/ppo/core_algos.py:265-328 (compute_grpo_outcome_advantage)
```

#### ğŸ¯ ä½œç”¨
**GRPO çš„æ ¸å¿ƒ**: è®¡ç®—ç»„å†…ç›¸å¯¹ä¼˜åŠ¿ï¼Œè®©æ¨¡å‹å­¦ä¼šåŒºåˆ†å¥½åå›ç­”ã€‚

#### ğŸ“ ä»£ç è¯¦è§£

**è°ƒç”¨ compute_advantage** (ray_trainer.py:1222-1230):
```python
with marked_timer("adv", timing_raw, color="brown"):
    # 1. è·å– reward_tensor (å¦‚æœå¼‚æ­¥)
    if self.config.reward_model.launch_reward_fn_async:
        reward_tensor, reward_extra_infos_dict = ray.get(future_reward)
    batch.batch["token_level_scores"] = reward_tensor

    # 2. åº”ç”¨ KL penalty (å¦‚æœé…ç½®)
    if self.config.algorithm.use_kl_in_reward:
        batch, kl_metrics = apply_kl_penalty(
            batch, kl_ctrl=self.kl_ctrl_in_reward, kl_penalty=self.config.algorithm.kl_penalty
        )
        metrics.update(kl_metrics)
    else:
        # ç›´æ¥ä½¿ç”¨ scores ä½œä¸º rewards
        batch.batch["token_level_rewards"] = batch.batch["token_level_scores"]

    # 3. è®¡ç®—ä¼˜åŠ¿
    norm_adv_by_std_in_grpo = self.config.algorithm.get("norm_adv_by_std_in_grpo", True)

    batch = compute_advantage(
        batch,
        adv_estimator=self.config.algorithm.adv_estimator,  # "grpo"
        gamma=self.config.algorithm.gamma,                  # 1.0
        lam=self.config.algorithm.lam,                      # 1.0
        num_repeat=self.config.actor_rollout_ref.rollout.n,  # 5
        norm_adv_by_std_in_grpo=norm_adv_by_std_in_grpo,   # True
        config=self.config.algorithm,
    )
```

---

**compute_advantage å‡½æ•°** (ray_trainer.py:181-259):
```python
def compute_advantage(
    data: DataProto,
    adv_estimator: AdvantageEstimator,
    gamma: float = 1.0,
    lam: float = 1.0,
    num_repeat: int = 1,
    norm_adv_by_std_in_grpo: bool = True,
    config: Optional[AlgoConfig] = None,
) -> DataProto:
    """
    è®¡ç®—ä¼˜åŠ¿çš„ç»Ÿä¸€æ¥å£

    æ ¹æ® adv_estimator é€‰æ‹©ä¸åŒçš„ä¼˜åŠ¿è®¡ç®—æ–¹æ³•:
    - GAE: éœ€è¦ value å‡½æ•°
    - GRPO: ç»„å†…ç›¸å¯¹ä¼˜åŠ¿
    - REINFORCE++: æ—  baseline
    - ç­‰ç­‰
    """
    # 1. ç¡®ä¿æœ‰ response_mask
    if "response_mask" not in data.batch.keys():
        data.batch["response_mask"] = compute_response_mask(data)

    # 2. æ ¹æ® adv_estimator é€‰æ‹©æ–¹æ³•
    if adv_estimator == AdvantageEstimator.GAE:
        # GAE: éœ€è¦ Critic ç½‘ç»œ
        advantages, returns = core_algos.compute_gae_advantage_return(
            token_level_rewards=data.batch["token_level_rewards"],
            values=data.batch["values"],  # æ¥è‡ª Critic
            response_mask=data.batch["response_mask"],
            gamma=gamma,
            lam=lam,
        )

    elif adv_estimator == AdvantageEstimator.GRPO:
        # GRPO: ç»„å†…ç›¸å¯¹ä¼˜åŠ¿
        grpo_calculation_mask = data.batch["response_mask"]

        advantages, returns = core_algos.compute_grpo_outcome_advantage(
            token_level_rewards=data.batch["token_level_rewards"],
            response_mask=grpo_calculation_mask,
            index=data.non_tensor_batch["uid"],  # â† ç”¨äºåˆ†ç»„!
            norm_adv_by_std_in_grpo=norm_adv_by_std_in_grpo,
        )

    else:
        # å…¶ä»–ä¼˜åŠ¿ä¼°è®¡å™¨
        adv_estimator_fn = core_algos.get_adv_estimator_fn(adv_estimator)
        advantages, returns = adv_estimator_fn(...)

    # 3. æ·»åŠ åˆ° batch
    data.batch["advantages"] = advantages
    data.batch["returns"] = returns

    return data
```

---

**GRPO æ ¸å¿ƒç®—æ³•** (core_algos.py:265-328):
```python
def compute_grpo_outcome_advantage(
    token_level_rewards: torch.Tensor,  # (1280, 1024)
    response_mask: torch.Tensor,        # (1280, 1024)
    index: np.ndarray,                   # (1280,) - uid æ•°ç»„
    epsilon: float = 1e-6,
    norm_adv_by_std_in_grpo: bool = True,
    config: Optional[AlgoConfig] = None,
) -> tuple[torch.Tensor, torch.Tensor]:
    """
    GRPO ä¼˜åŠ¿è®¡ç®—

    æ ¸å¿ƒæ€æƒ³:
    1. è®¡ç®—æ¯ä¸ªå›ç­”çš„æ€»åˆ†: score = sum(token_level_rewards)
    2. æŒ‰ uid åˆ†ç»„: åŒä¸€ä¸ª prompt çš„ G ä¸ªå›ç­”åˆ†åˆ°ä¸€ç»„
    3. è®¡ç®—ç»„å†…å‡å€¼å’Œæ ‡å‡†å·®: mean_g, std_g
    4. æ ‡å‡†åŒ–ä¼˜åŠ¿: advantage_i = (score_i - mean_g) / std_g

    è¿™æ ·:
    - é«˜äºå¹³å‡åˆ†çš„å›ç­” â†’ æ­£ä¼˜åŠ¿ â†’ å¢åŠ æ¦‚ç‡
    - ä½äºå¹³å‡åˆ†çš„å›ç­” â†’ è´Ÿä¼˜åŠ¿ â†’ é™ä½æ¦‚ç‡
    """
    # 1. è®¡ç®—æ¯ä¸ªå›ç­”çš„æ€»åˆ†
    scores = token_level_rewards.sum(dim=-1)  # (1280,)
    # scores[i] = sum of all token rewards for response i

    # 2. åˆå§‹åŒ–åˆ†ç»„å­—å…¸
    id2score = defaultdict(list)  # {uid: [score1, score2, ...]}
    id2mean = {}                   # {uid: mean_score}
    id2std = {}                    # {uid: std_score}

    with torch.no_grad():
        bsz = scores.shape[0]  # 1280

        # 3. æŒ‰ uid åˆ†ç»„
        for i in range(bsz):
            id2score[index[i]].append(scores[i])

        # ç°åœ¨ id2score çš„ç»“æ„:
        # {
        #   "uuid-0": [score_0, score_1, score_2, score_3, score_4],  # 5 ä¸ªåˆ†æ•°
        #   "uuid-1": [score_5, score_6, score_7, score_8, score_9],
        #   ...
        #   "uuid-255": [score_1275, score_1276, score_1277, score_1278, score_1279]
        # }

        # 4. è®¡ç®—æ¯ç»„çš„å‡å€¼å’Œæ ‡å‡†å·®
        for idx in id2score:
            if len(id2score[idx]) == 1:
                # åªæœ‰ 1 ä¸ªæ ·æœ¬, æ— æ³•è®¡ç®— std
                id2mean[idx] = torch.tensor(0.0)
                id2std[idx] = torch.tensor(1.0)
            elif len(id2score[idx]) > 1:
                # æœ‰å¤šä¸ªæ ·æœ¬
                scores_tensor = torch.stack(id2score[idx])  # (5,)
                id2mean[idx] = torch.mean(scores_tensor)    # æ ‡é‡
                id2std[idx] = torch.std(scores_tensor)      # æ ‡é‡
            else:
                raise ValueError(f"no score in prompt index: {idx}")

        # 5. æ ‡å‡†åŒ–æ¯ä¸ªåˆ†æ•°
        for i in range(bsz):
            if norm_adv_by_std_in_grpo:
                # æ ‡å‡† GRPO: (score - mean) / std
                scores[i] = (scores[i] - id2mean[index[i]]) / (id2std[index[i]] + epsilon)
            else:
                # Dr.GRPO: score - mean (ä¸é™¤ä»¥æ ‡å‡†å·®)
                # å‚è€ƒè®ºæ–‡: https://arxiv.org/abs/2503.20783
                scores[i] = scores[i] - id2mean[index[i]]

        # 6. å¹¿æ’­åˆ°æ‰€æœ‰ token
        # scores ç°åœ¨æ˜¯ (1280,), éœ€è¦æ‰©å±•åˆ° (1280, 1024)
        # æ¯ä¸ª token çš„ advantage éƒ½æ˜¯åŒä¸€ä¸ªå€¼ (å› ä¸ºæ˜¯ outcome reward)
        scores = scores.unsqueeze(-1) * response_mask  # (1280, 1024)
        # unsqueeze(-1): (1280,) â†’ (1280, 1)
        # * response_mask: å¹¿æ’­ä¹˜æ³•, åªä¿ç•™æœ‰æ•ˆ token

    # 7. è¿”å› advantages å’Œ returns (å¯¹äº GRPO, ä¸¤è€…ç›¸åŒ)
    return scores, scores
```

---

**å…·ä½“è®¡ç®—ç¤ºä¾‹**:

å‡è®¾ `uuid-0` çš„ 5 ä¸ªå›ç­”çš„åˆ†æ•°:
```python
# åŸå§‹åˆ†æ•° (æ¥è‡ª Step 4 çš„å¥–åŠ±è®¡ç®—)
scores = [1.0, 1.0, 1.0, 0.0, 1.0]

# 1. è®¡ç®—ç»„å†…å‡å€¼
mean = (1.0 + 1.0 + 1.0 + 0.0 + 1.0) / 5 = 0.8

# 2. è®¡ç®—ç»„å†…æ ‡å‡†å·®
# std = sqrt(E[(x - mean)^2])
variance = ((1.0-0.8)^2 + (1.0-0.8)^2 + (1.0-0.8)^2 + (0.0-0.8)^2 + (1.0-0.8)^2) / 5
         = (0.04 + 0.04 + 0.04 + 0.64 + 0.04) / 5
         = 0.8 / 5
         = 0.16
std = sqrt(0.16) = 0.4

# 3. æ ‡å‡†åŒ–æ¯ä¸ªåˆ†æ•°
adv_0 = (1.0 - 0.8) / 0.4 = 0.2 / 0.4 = 0.5
adv_1 = (1.0 - 0.8) / 0.4 = 0.2 / 0.4 = 0.5
adv_2 = (1.0 - 0.8) / 0.4 = 0.2 / 0.4 = 0.5
adv_3 = (0.0 - 0.8) / 0.4 = -0.8 / 0.4 = -2.0  # â† è´Ÿä¼˜åŠ¿!
adv_4 = (1.0 - 0.8) / 0.4 = 0.2 / 0.4 = 0.5

# éªŒè¯é›¶å’Œæ€§è´¨
sum(advantages) = 0.5 + 0.5 + 0.5 + (-2.0) + 0.5 = 0 âœ“
```

**å…³é”®æ€§è´¨**:
1. **é›¶å’Œ**: åŒä¸€ç»„å†…çš„ advantages ä¹‹å’Œä¸º 0
2. **ç›¸å¯¹æ€§**: ä¼˜åŠ¿æ˜¯ç›¸å¯¹äºç»„å†…å…¶ä»–å›ç­”
3. **æ–¹å·®å½’ä¸€åŒ–**: é™¤ä»¥ std ä½¿å¾—ä¼˜åŠ¿åœ¨åˆç†èŒƒå›´å†… (é€šå¸¸ [-3, 3])

**ä¸ºä»€ä¹ˆ GRPO æœ‰æ•ˆ?**
- **å¥½å›ç­”** (score > mean): å¾—åˆ°**æ­£ä¼˜åŠ¿** â†’ PPO ä¼š**å¢åŠ **å…¶ç”Ÿæˆæ¦‚ç‡
- **åå›ç­”** (score < mean): å¾—åˆ°**è´Ÿä¼˜åŠ¿** â†’ PPO ä¼š**é™ä½**å…¶ç”Ÿæˆæ¦‚ç‡
- **ä¸éœ€è¦ Value å‡½æ•°**: ç›´æ¥ç”¨ç»„å†…å¯¹æ¯”æ›¿ä»£ baseline

---

**æœ€ç»ˆè¾“å‡º**:
```python
batch.batch["advantages"] = torch.Tensor([1280, 1024])
batch.batch["returns"] = torch.Tensor([1280, 1024])  # å¯¹äº GRPO, ä¸ advantages ç›¸åŒ

# ç¤ºä¾‹å€¼ (ç¬¬ 0 ä¸ª prompt çš„ 5 ä¸ªå›ç­”)
batch.batch["advantages"][0:5, -1] = [0.5, 0.5, 0.5, -2.0, 0.5]  # æœ€åä¸€ä¸ª token
# å…¶ä»– token ä½ç½®çš„ advantage ä¹Ÿæ˜¯ç›¸åŒå€¼ (å¹¿æ’­)
```

#### ğŸ”§ ä¿®æ”¹ç‚¹

**ç¦ç”¨æ ‡å‡†å·®å½’ä¸€åŒ–** (ä½¿ç”¨ Dr.GRPO):
```bash
# run_qwen3-8b.sh
algorithm.norm_adv_by_std_in_grpo=False
```

**æ•ˆæœ**:
- æ ‡å‡† GRPO: `advantage = (score - mean) / std`
- Dr.GRPO: `advantage = score - mean`
- Dr.GRPO å¯èƒ½åœ¨æŸäº›ä»»åŠ¡ä¸Šæ›´ç¨³å®š (é¿å… std è¿‡å°å¯¼è‡´çˆ†ç‚¸)

---

### Step 7: æ›´æ–° Actor (PPO)

#### ğŸ“ æ–‡ä»¶é“¾
```
verl/trainer/ppo/ray_trainer.py:1241-1249
  â†“ (RPC è°ƒç”¨)
verl/workers/fsdp_workers.py:868-907
  â†“
verl/workers/actor/dp_actor.py:398-600
  â†“
verl/trainer/ppo/core_algos.py:907-996 (compute_policy_loss_vanilla)
```

#### ğŸ¯ ä½œç”¨
ä½¿ç”¨ PPO ç®—æ³•æ›´æ–°ç­–ç•¥ï¼Œå¢åŠ å¥½å›ç­”æ¦‚ç‡ï¼Œé™ä½åå›ç­”æ¦‚ç‡ã€‚

#### ğŸ“ ä»£ç è¯¦è§£

**è°ƒç”¨ update_actor** (ray_trainer.py:1241-1249):
```python
# å®ç° critic warmup (GRPO é€šå¸¸è®¾ä¸º 0)
if self.config.trainer.critic_warmup <= self.global_steps:
    with marked_timer("update_actor", timing_raw, color="red"):
        # è®¾ç½® meta_info
        rollout_config = self.config.actor_rollout_ref.rollout
        batch.meta_info["multi_turn"] = rollout_config.multi_turn.enable
        batch.meta_info["temperature"] = rollout_config.temperature

        # è°ƒç”¨ Worker æ›´æ–°
        actor_output = self.actor_rollout_wg.update_actor(batch)

    # æ”¶é›† metrics
    actor_output_metrics = reduce_metrics(actor_output.meta_info["metrics"])
    metrics.update(actor_output_metrics)
```

---

**Worker çš„ update_actor** (fsdp_workers.py:868-907):
```python
def update_actor(self, data: DataProto):
    """
    Worker å±‚çš„ update_actor

    ä¸»è¦èŒè´£:
    1. ç®¡ç†æ¨¡å‹å’Œä¼˜åŒ–å™¨çš„åŠ è½½/å¸è½½ (å¦‚æœä½¿ç”¨ offload)
    2. è°ƒç”¨ actor.update_policy è¿›è¡Œå®é™…è®­ç»ƒ
    3. è®°å½•æ€§èƒ½æŒ‡æ ‡ (MFU, å†…å­˜ä½¿ç”¨)
    4. æ›´æ–°å­¦ä¹ ç‡
    """
    assert self._is_actor

    # 1. åŠ è½½æ¨¡å‹å’Œä¼˜åŒ–å™¨åˆ° GPU (å¦‚æœä½¿ç”¨ offload)
    if self._is_offload_param:
        load_fsdp_model_to_gpu(self.actor_module_fsdp)
    if self._is_offload_optimizer:
        load_fsdp_optimizer(optimizer=self.actor_optimizer, device_id=get_device_id())

    with self.ulysses_sharding_manager:
        # 2. æ•°æ®ç§»å› CPU (ä¼šåœ¨ micro batch æ—¶ç§»åˆ° GPU)
        data = data.to("cpu")

        # 3. è°ƒç”¨ actor.update_policy
        with Timer(name="update_policy", logger=None) as timer:
            metrics = self.actor.update_policy(data=data)
        delta_time = timer.last

        # 4. è®¡ç®— MFU (Model FLOPs Utilization)
        global_num_tokens = data.meta_info["global_token_num"]
        estimated_flops, promised_flops = self.flops_counter.estimate_flops(
            global_num_tokens, delta_time
        )
        metrics["perf/mfu/actor"] = (
            estimated_flops * self.config.actor.ppo_epochs / promised_flops / self.world_size
        )

        # 5. è®°å½•å†…å­˜ä½¿ç”¨
        metrics["perf/max_memory_allocated_gb"] = get_torch_device().max_memory_allocated() / (1024**3)
        metrics["perf/max_memory_reserved_gb"] = get_torch_device().max_memory_reserved() / (1024**3)
        metrics["perf/cpu_memory_used_gb"] = psutil.virtual_memory().used / (1024**3)

        # 6. å­¦ä¹ ç‡è°ƒåº¦
        lr = self.actor_lr_scheduler.get_last_lr()[0]
        metrics["actor/lr"] = lr.item() if torch.is_tensor(lr) else lr
        self.actor_lr_scheduler.step()

        output = DataProto(meta_info={"metrics": metrics})
        output = output.to("cpu")

    # 7. å¸è½½æ¨¡å‹å’Œä¼˜åŒ–å™¨ (å¦‚æœä½¿ç”¨ offload)
    if self._is_offload_param:
        offload_fsdp_model_to_cpu(self.actor_module_fsdp)
    if self._is_offload_optimizer:
        offload_fsdp_optimizer(optimizer=self.actor_optimizer)

    return output
```

---

**Actor çš„ update_policy** (dp_actor.py:398-600, æ ¸å¿ƒéƒ¨åˆ†):
```python
def update_policy(self, data: DataProto):
    """
    PPO ç­–ç•¥æ›´æ–°çš„æ ¸å¿ƒå‡½æ•°

    æµç¨‹:
    1. åˆ† mini-batch (256 per batch)
    2. åˆ† micro-batch (32 per batch)
    3. å¯¹æ¯ä¸ª micro-batch:
       a. å‰å‘ä¼ æ’­è®¡ç®— new_log_prob
       b. è®¡ç®— PPO Loss
       c. åå‘ä¼ æ’­
       d. ç´¯ç§¯æ¢¯åº¦
    4. æ¢¯åº¦è£å‰ª
    5. ä¼˜åŒ–å™¨æ­¥è¿›
    """
    # 1. ç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒæ¨¡å¼
    self.actor_module.train()

    temperature = data.meta_info["temperature"]

    # 2. é€‰æ‹©éœ€è¦çš„å­—æ®µ
    select_keys = [
        "responses",
        "response_mask",
        "input_ids",
        "attention_mask",
        "position_ids",
        "old_log_probs",  # â† æ¥è‡ª Step 5
        "advantages",     # â† æ¥è‡ª Step 6
    ]
    if self.config.use_kl_loss:
        select_keys.append("ref_log_prob")

    data = data.select(batch_keys=select_keys)

    # 3. åˆ† mini-batch
    # PPO è®ºæ–‡: https://arxiv.org/abs/1707.06347
    mini_batches = data.split(self.config.ppo_mini_batch_size)
    # 1280 / 256 = 5 ä¸ª mini-batch

    on_policy = len(mini_batches) == 1 and self.config.ppo_epochs == 1

    metrics = {}

    # 4. PPO epochs (é€šå¸¸ä¸º 1)
    for _ in range(self.config.ppo_epochs):
        for batch_idx, mini_batch in enumerate(mini_batches):
            # 5. åˆ† micro-batch (ç”¨äºæ¢¯åº¦ç´¯ç§¯)
            self.gradient_accumulation = (
                self.config.ppo_mini_batch_size // self.config.ppo_micro_batch_size_per_gpu
            )
            micro_batches = mini_batch.split(self.config.ppo_micro_batch_size_per_gpu)
            # 256 / 32 = 8 ä¸ª micro-batch

            # 6. æ¸…é›¶æ¢¯åº¦
            self.actor_optimizer.zero_grad()

            # 7. éå† micro-batch
            for micro_batch in micro_batches:
                micro_batch = micro_batch.to(get_device_id())
                micro_batch_metrics = {}

                model_inputs = {**micro_batch.batch, **micro_batch.non_tensor_batch}
                response_mask = model_inputs["response_mask"]   # (32, 1024)
                old_log_prob = model_inputs["old_log_probs"]    # (32, 1024)
                advantages = model_inputs["advantages"]          # (32, 1024)

                # 8. è®¡ç®— loss_scale_factor (ç”¨äºæ¢¯åº¦ç´¯ç§¯)
                loss_scale_factor = 1 / self.gradient_accumulation  # 1/8

                # 9. å‰å‘ä¼ æ’­
                entropy, log_prob = self._forward_micro_batch(
                    model_inputs, temperature=temperature, calculate_entropy=True
                )
                # log_prob: (32, 1024)
                # entropy: (32, 1024)

                # 10. å¦‚æœæ˜¯ on-policy, ç›´æ¥ç”¨å½“å‰ log_prob ä½œä¸º old
                if on_policy:
                    old_log_prob = log_prob.detach()
                else:
                    old_log_prob = model_inputs["old_log_probs"]

                # 11. è®¡ç®— policy loss
                loss_mode = self.config.policy_loss.get("loss_mode", "vanilla")
                policy_loss_fn = get_policy_loss_fn(loss_mode)

                pg_loss, pg_metrics = policy_loss_fn(
                    old_log_prob=old_log_prob,
                    log_prob=log_prob,
                    advantages=advantages,
                    response_mask=response_mask,
                    loss_agg_mode=self.config.loss_agg_mode,
                    config=self.config,
                )
                # pg_loss æ˜¯æ ‡é‡
                micro_batch_metrics.update(pg_metrics)

                policy_loss = pg_loss

                # 12. æ·»åŠ  entropy loss (å¦‚æœé…ç½®)
                if self.config.entropy_coeff != 0:
                    entropy_agg = agg_loss(
                        loss_mat=entropy,
                        loss_mask=response_mask,
                        loss_agg_mode=self.config.loss_agg_mode
                    )
                    policy_loss -= entropy_agg * self.config.entropy_coeff

                # 13. æ·»åŠ  KL loss (å¦‚æœé…ç½®)
                if self.config.use_kl_loss:
                    ref_log_prob = model_inputs["ref_log_prob"]
                    kld = kl_penalty(
                        logprob=log_prob,
                        ref_logprob=ref_log_prob,
                        kl_penalty=self.config.kl_loss_type  # "low_var_kl"
                    )
                    kl_loss = agg_loss(
                        loss_mat=kld,
                        loss_mask=response_mask,
                        loss_agg_mode=self.config.loss_agg_mode
                    )
                    policy_loss = policy_loss + kl_loss * self.config.kl_loss_coef
                    micro_batch_metrics["actor/kl_loss"] = kl_loss.detach().item() * loss_scale_factor

                # 14. ç¼©æ”¾ loss (ç”¨äºæ¢¯åº¦ç´¯ç§¯)
                loss = policy_loss * loss_scale_factor

                # 15. åå‘ä¼ æ’­
                if self.scaler is not None:  # æ··åˆç²¾åº¦è®­ç»ƒ
                    self.scaler.scale(loss).backward()
                else:
                    loss.backward()

                # 16. è®°å½• metrics
                micro_batch_metrics["actor/pg_loss"] = pg_loss.detach().item() * loss_scale_factor
                append_to_dict(metrics, micro_batch_metrics)

            # 17. æ¢¯åº¦è£å‰ª
            if self.config.max_grad_norm is not None:
                if self.scaler is not None:
                    self.scaler.unscale_(self.actor_optimizer)

                if isinstance(self.actor_module, FSDPModule) and self.actor_module.fsdp2:
                    total_norm = fsdp2_clip_grad_norm_(
                        self.actor_module, self.config.max_grad_norm
                    )
                else:
                    total_norm = torch.nn.utils.clip_grad_norm_(
                        self.actor_module.parameters(), self.config.max_grad_norm
                    )
                metrics["actor/grad_norm"] = [total_norm.item()]

            # 18. ä¼˜åŒ–å™¨æ­¥è¿›
            if self.scaler is not None:
                self.scaler.step(self.actor_optimizer)
                self.scaler.update()
            else:
                self.actor_optimizer.step()

    # 19. èšåˆ metrics
    for key, value_list in metrics.items():
        if isinstance(value_list, list):
            metrics[key] = sum(value_list) / len(value_list)

    return metrics
```

---

**PPO Loss è®¡ç®—** (core_algos.py:907-996):
```python
@register_policy_loss("vanilla")
def compute_policy_loss_vanilla(
    old_log_prob: torch.Tensor,    # (32, 1024)
    log_prob: torch.Tensor,         # (32, 1024)
    advantages: torch.Tensor,       # (32, 1024)
    response_mask: torch.Tensor,    # (32, 1024)
    loss_agg_mode: str = "token-mean",
    config: Optional[ActorConfig] = None,
    rollout_is_weights: torch.Tensor | None = None,
) -> tuple[torch.Tensor, dict[str, Any]]:
    """
    æ ‡å‡† PPO Clip Loss

    PPO è®ºæ–‡: https://arxiv.org/abs/1707.06347

    æ ¸å¿ƒæ€æƒ³:
    L_CLIP(Î¸) = -E[min(ratio * A, clip(ratio, 1-Îµ, 1+Îµ) * A)]

    å…¶ä¸­:
    - ratio = Ï€_Î¸(a|s) / Ï€_old(a|s) = exp(log_prob - old_log_prob)
    - A = advantages
    - Îµ = clip_epsilon (é»˜è®¤ 0.2)
    """
    assert config is not None

    # 1. è·å– clip å‚æ•°
    clip_ratio = config.clip_ratio  # 0.2
    clip_ratio_low = config.clip_ratio_low if config.clip_ratio_low is not None else clip_ratio
    clip_ratio_high = config.clip_ratio_high if config.clip_ratio_high is not None else clip_ratio
    clip_ratio_c = config.get("clip_ratio_c", 3.0)  # dual-clip PPO

    cliprange = clip_ratio
    cliprange_low = clip_ratio_low
    cliprange_high = clip_ratio_high

    # 2. è®¡ç®— ratio = Ï€_Î¸ / Ï€_old
    negative_approx_kl = log_prob - old_log_prob  # (32, 1024)
    # Clamp for stability
    negative_approx_kl = torch.clamp(negative_approx_kl, min=-20.0, max=20.0)
    ratio = torch.exp(negative_approx_kl)  # (32, 1024)
    # ratio[i, j] = P_new(token_j | prefix) / P_old(token_j | prefix)

    # 3. è®¡ç®— KL divergence (ç”¨äºç›‘æ§)
    ppo_kl = verl_F.masked_mean(-negative_approx_kl, response_mask)

    # 4. è®¡ç®—ä¸¤ä¸ª surrogate
    pg_losses1 = -advantages * ratio  # (32, 1024)
    # ç¬¬ä¸€ä¸ª surrogate: -A * ratio

    pg_losses2 = -advantages * torch.clamp(
        ratio, 1 - cliprange_low, 1 + cliprange_high
    )  # (32, 1024)
    # ç¬¬äºŒä¸ª surrogate: -A * clip(ratio, 1-Îµ, 1+Îµ)

    # 5. å– maximum (å› ä¸ºæœ‰è´Ÿå·, å®é™…æ˜¯å– minimum)
    clip_pg_losses1 = torch.maximum(pg_losses1, pg_losses2)
    # clip_pg_losses1 = max(-A*ratio, -A*clip(ratio))
    #                 = -min(A*ratio, A*clip(ratio))

    # 6. è®¡ç®— clip fraction (ç”¨äºç›‘æ§)
    pg_clipfrac = verl_F.masked_mean(
        torch.gt(pg_losses2, pg_losses1).float(), response_mask
    )

    # 7. Dual-clip PPO (å¤„ç†è´Ÿ advantage)
    pg_losses3 = -advantages * clip_ratio_c  # (32, 1024)
    clip_pg_losses2 = torch.min(pg_losses3, clip_pg_losses1)
    pg_clipfrac_lower = verl_F.masked_mean(
        torch.gt(clip_pg_losses1, pg_losses3) * (advantages < 0).float(), response_mask
    )

    # 8. æ ¹æ® advantage ç¬¦å·é€‰æ‹© loss
    pg_losses = torch.where(advantages < 0, clip_pg_losses2, clip_pg_losses1)

    # 9. åº”ç”¨ rollout correction weights (å¦‚æœæœ‰)
    if rollout_is_weights is not None:
        pg_losses = pg_losses * rollout_is_weights

    # 10. èšåˆ loss
    pg_loss = agg_loss(
        loss_mat=pg_losses,
        loss_mask=response_mask,
        loss_agg_mode=loss_agg_mode,  # "token-mean"
        **config.global_batch_info
    )

    # 11. è¿”å› metrics
    pg_metrics = {
        "actor/pg_clipfrac": pg_clipfrac.detach().item(),
        "actor/ppo_kl": ppo_kl.detach().item(),
        "actor/pg_clipfrac_lower": pg_clipfrac_lower.detach().item(),
    }

    return pg_loss, pg_metrics
```

---

**å…·ä½“è®¡ç®—ç¤ºä¾‹**:

å‡è®¾ä¸€ä¸ª token:
```python
# è¾“å…¥
old_log_prob = -2.5  # log P_old(token | prefix)
log_prob = -2.0      # log P_new(token | prefix)
advantage = 0.5      # æ­£ä¼˜åŠ¿ (å¥½å›ç­”)

# 1. è®¡ç®— ratio
ratio = exp(log_prob - old_log_prob)
      = exp(-2.0 - (-2.5))
      = exp(0.5)
      = 1.65
# å«ä¹‰: æ–°ç­–ç•¥ç”Ÿæˆè¯¥ token çš„æ¦‚ç‡æ˜¯æ—§ç­–ç•¥çš„ 1.65 å€

# 2. è®¡ç®—ä¸¤ä¸ª surrogate
surrogate1 = ratio * advantage = 1.65 * 0.5 = 0.825
surrogate2 = clip(ratio, 0.8, 1.2) * advantage
           = clip(1.65, 0.8, 1.2) * 0.5
           = 1.2 * 0.5
           = 0.6

# 3. å– minimum
clipped_surrogate = min(0.825, 0.6) = 0.6

# 4. åŠ è´Ÿå·å¾—åˆ° loss
pg_loss = -clipped_surrogate = -0.6

# 5. åå‘ä¼ æ’­å
# æ¢¯åº¦ä¼šè®© log_prob å¢å¤§ (å› ä¸º loss å¯¹ log_prob çš„æ¢¯åº¦æ˜¯è´Ÿçš„)
# â†’ P_new(token | prefix) å¢å¤§
# â†’ è¯¥ token æ›´å®¹æ˜“è¢«ç”Ÿæˆ
```

**ä¸ºä»€ä¹ˆ PPO Clip æœ‰æ•ˆ?**
- **å¥½å›ç­”** (advantage > 0):
  - å¦‚æœ ratio > 1.2: è¢« clip åˆ° 1.2, é˜²æ­¢æ›´æ–°è¿‡æ¿€
  - å¦‚æœ ratio < 0.8: loss å¾ˆå¤§, é¼“åŠ±å¢åŠ æ¦‚ç‡
  - ç¨³å®šåœ°å¢åŠ ç”Ÿæˆæ¦‚ç‡

- **åå›ç­”** (advantage < 0):
  - å¦‚æœ ratio > 1.2: loss å¾ˆå¤§, é¼“åŠ±é™ä½æ¦‚ç‡
  - å¦‚æœ ratio < 0.8: è¢« clip, é˜²æ­¢é™ä½è¿‡æ¿€
  - ç¨³å®šåœ°é™ä½ç”Ÿæˆæ¦‚ç‡

#### ğŸ”§ ä¿®æ”¹ç‚¹

**ä¿®æ”¹å­¦ä¹ ç‡**:
```bash
# run_qwen3-8b.sh:16
actor_rollout_ref.actor.optim.lr=5e-7  # é™ä½å­¦ä¹ ç‡ (æ›´ç¨³å®š)
```

**ä¿®æ”¹ mini_batch_size**:
```bash
# run_qwen3-8b.sh:18
actor_rollout_ref.actor.ppo_mini_batch_size=512  # å¢å¤§ (æ›´ç¨³å®šä½†æ…¢)
```

**ä¿®æ”¹ clip_ratio**:
```bash
# åœ¨é…ç½®ä¸­æ·»åŠ 
actor_rollout_ref.actor.clip_ratio=0.1  # æ›´ä¿å®ˆçš„æ›´æ–°
```

**ç¦ç”¨ KL loss**:
```bash
# run_qwen3-8b.sh:20
actor_rollout_ref.actor.use_kl_loss=False
```

---

### Step 8: éªŒè¯æµ‹è¯•

#### ğŸ“ æ–‡ä»¶: `verl/trainer/ppo/ray_trainer.py:531-630`

#### ğŸ¯ ä½œç”¨
æ¯ 5 æ­¥åœ¨éªŒè¯é›†ä¸Šæµ‹è¯•æ¨¡å‹æ€§èƒ½ã€‚

#### ğŸ“ ä»£ç è¯¦è§£ (ç®€åŒ–ç‰ˆ)

```python
def _validate(self):
    """éªŒè¯å‡½æ•°"""
    sample_inputs = []
    sample_outputs = []
    sample_scores = []

    for test_data in self.val_dataloader:
        test_batch = DataProto.from_single_dict(test_data)

        # 1. é‡å¤æµ‹è¯• batch (ç”Ÿæˆå¤šä¸ªå›ç­”)
        test_batch = test_batch.repeat(
            repeat_times=self.config.actor_rollout_ref.rollout.val_kwargs.n,  # é»˜è®¤ 5
            interleave=True
        )

        # 2. ç”Ÿæˆå›ç­”
        test_gen_batch = self._get_gen_batch(test_batch)
        test_gen_batch.meta_info = {
            "do_sample": self.config.actor_rollout_ref.rollout.val_kwargs.do_sample,
            "validate": True,
        }

        test_output = self.actor_rollout_wg.generate_sequences(test_gen_batch)

        # 3. è®¡ç®—å¥–åŠ±
        test_batch = test_batch.union(test_output)
        result = self.val_reward_fn(test_batch, return_dict=True)
        scores = result["reward_tensor"].sum(-1).cpu().tolist()

        sample_scores.extend(scores)

    # 4. è®¡ç®— metrics
    val_metrics = process_validation_metrics(...)

    return val_metrics
```

**éªŒè¯æŒ‡æ ‡** (å¯¹äº GSM8K):
```python
val_metrics = {
    "val/score": 0.75,        # å¹³å‡åˆ†æ•°
    "val/accuracy": 0.80,     # å‡†ç¡®ç‡ (è‡³å°‘ 1 ä¸ªæ­£ç¡®)
    "val/pass@5": 0.82,       # Pass@5 (5 ä¸ªä¸­è‡³å°‘ 1 ä¸ªæ­£ç¡®)
}
```

#### ğŸ”§ ä¿®æ”¹ç‚¹
- **ä¿®æ”¹éªŒè¯é¢‘ç‡**: `run_qwen3-8b.sh:42` â†’ `trainer.test_freq=10` (æ”¹ä¸ºæ¯ 10 æ­¥)
- **ä¿®æ”¹ç”Ÿæˆæ•°é‡**: åœ¨é…ç½®ä¸­æ·»åŠ  `actor_rollout_ref.rollout.val_kwargs.n=10`

---

### Step 9: ä¿å­˜æ£€æŸ¥ç‚¹

#### ğŸ“ æ–‡ä»¶: `verl/trainer/ppo/ray_trainer.py:1280-1286`

#### ğŸ“ ä»£ç 

```python
if self.config.trainer.save_freq > 0 and (
    is_last_step or
    self.global_steps % self.config.trainer.save_freq == 0 or  # æ¯ 20 æ­¥
    esi_close_to_expiration
):
    with marked_timer("save_checkpoint", timing_raw, color="green"):
        self._save_checkpoint()
```

**ä¿å­˜å†…å®¹**:
- Actor æ¨¡å‹å‚æ•°
- Optimizer çŠ¶æ€
- è®­ç»ƒæ­¥æ•°
- Dataloader çŠ¶æ€

#### ğŸ”§ ä¿®æ”¹ç‚¹
- **ä¿®æ”¹ä¿å­˜é¢‘ç‡**: `run_qwen3-8b.sh:41` â†’ `trainer.save_freq=10` (æ”¹ä¸ºæ¯ 10 æ­¥)

---

## 4. æ ¸å¿ƒç®—æ³•è¯¦è§£

### 4.1 GRPO vs PPO

| å¯¹æ¯”é¡¹ | PPO | GRPO |
|--------|-----|------|
| **Critic** | éœ€è¦ Value ç½‘ç»œ | ä¸éœ€è¦ |
| **ä¼˜åŠ¿ä¼°è®¡** | GAE (æ—¶åºå·®åˆ†) | ç»„å†…ç›¸å¯¹åˆ†æ•° |
| **æ¯ä¸ª prompt ç”Ÿæˆæ•°** | 1 | G (ä¾‹å¦‚ 5) |
| **é€‚ç”¨åœºæ™¯** | é€šç”¨ RL | æœ‰æ˜ç¡®æ­£è¯¯çš„ä»»åŠ¡ |
| **ä¼˜ç‚¹** | é€‚ç”¨æ€§å¹¿ | ç®€å•ã€ç¨³å®š |
| **ç¼ºç‚¹** | éœ€è¦é¢å¤–ç½‘ç»œ | åªé€‚åˆç‰¹å®šä»»åŠ¡ |

### 4.2 GRPO æ•°å­¦åŸç†

**å®šä¹‰**:
- ç»™å®š prompt $p$, ç”Ÿæˆ $G$ ä¸ªå›ç­” $\{r_1, r_2, ..., r_G\}$
- æ¯ä¸ªå›ç­”çš„åˆ†æ•°: $s_i = R(p, r_i)$

**ä¼˜åŠ¿è®¡ç®—**:
$$
A_i = \frac{s_i - \bar{s}}{\sigma_s}
$$

å…¶ä¸­:
- $\bar{s} = \frac{1}{G} \sum_{j=1}^{G} s_j$ (ç»„å†…å‡å€¼)
- $\sigma_s = \sqrt{\frac{1}{G} \sum_{j=1}^{G} (s_j - \bar{s})^2}$ (ç»„å†…æ ‡å‡†å·®)

**æ€§è´¨**:
- $\sum_{i=1}^{G} A_i = 0$ (é›¶å’Œ)
- $A_i > 0 \Leftrightarrow s_i > \bar{s}$ (é«˜äºå¹³å‡)
- $A_i < 0 \Leftrightarrow s_i < \bar{s}$ (ä½äºå¹³å‡)

### 4.3 PPO Clip æ•°å­¦åŸç†

**ç›®æ ‡å‡½æ•°**:
$$
L^{CLIP}(\theta) = -\mathbb{E}_t \left[ \min(r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t) \right]
$$

å…¶ä¸­:
- $r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$ (æ¦‚ç‡æ¯”)
- $\hat{A}_t$ (ä¼˜åŠ¿ä¼°è®¡)
- $\epsilon = 0.2$ (clip å‚æ•°)

**æ•ˆæœ**:
- å½“ $\hat{A}_t > 0$ (å¥½åŠ¨ä½œ):
  - å¦‚æœ $r_t > 1+\epsilon$: è¢« clip, é˜²æ­¢è¿‡åº¦å¢åŠ æ¦‚ç‡
  - å¦åˆ™: æ­£å¸¸å¢åŠ æ¦‚ç‡
- å½“ $\hat{A}_t < 0$ (ååŠ¨ä½œ):
  - å¦‚æœ $r_t < 1-\epsilon$: è¢« clip, é˜²æ­¢è¿‡åº¦é™ä½æ¦‚ç‡
  - å¦åˆ™: æ­£å¸¸é™ä½æ¦‚ç‡

---

## 5. ä¿®æ”¹ä»£ç æŒ‡å—

### 5.1 ä¿®æ”¹ç”Ÿæˆæ•°é‡ (G å‚æ•°)

**ä½ç½®**: `run_qwen3-8b.sh:31`

```bash
# å½“å‰: ç”Ÿæˆ 5 ä¸ª
actor_rollout_ref.rollout.n=5

# ä¿®æ”¹: ç”Ÿæˆ 10 ä¸ª
actor_rollout_ref.rollout.n=10
```

**å½±å“**:
- è®¡ç®—é‡: çº¿æ€§å¢åŠ  (10 vs 5 = 2å€)
- ç¨³å®šæ€§: å¢åŠ  (æ›´å¤šæ ·æœ¬ â†’ æ›´å‡†ç¡®çš„ mean/std)
- å†…å­˜: çº¿æ€§å¢åŠ 

---

### 5.2 ä¿®æ”¹è¯„åˆ†å‡½æ•°

**Step 1**: åˆ›å»ºè‡ªå®šä¹‰è¯„åˆ†å‡½æ•°

æ–‡ä»¶: `verl/utils/reward_score/my_math.py`
```python
def compute_score(solution_str, ground_truth, **kwargs):
    """
    è‡ªå®šä¹‰æ•°å­¦é¢˜è¯„åˆ†

    Args:
        solution_str: æ¨¡å‹ç”Ÿæˆçš„å›ç­”
        ground_truth: æ­£ç¡®ç­”æ¡ˆ

    Returns:
        float: åˆ†æ•° (0.0 åˆ° 1.0)
    """
    # 1. æå–ç­”æ¡ˆ (ä½ çš„é€»è¾‘)
    answer = extract_answer(solution_str)

    # 2. åˆ¤æ–­æ­£ç¡®æ€§
    if answer == ground_truth:
        return 1.0
    elif is_close(answer, ground_truth, tolerance=0.01):
        return 0.5  # æ¥è¿‘æ­£ç¡®ç­”æ¡ˆ
    else:
        return 0.0

def extract_answer(text):
    """æå–ç­”æ¡ˆçš„é€»è¾‘"""
    # ä¾‹å¦‚: ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼
    import re
    matches = re.findall(r"answer is (\d+\.?\d*)", text.lower())
    if matches:
        return matches[-1]
    return None
```

**Step 2**: æ³¨å†Œåˆ° reward manager

æ–‡ä»¶: `verl/trainer/ppo/reward.py:120-196`

åœ¨ `load_reward_manager` å‡½æ•°ä¸­æ·»åŠ :
```python
def get_custom_reward_fn(config):
    """è·å–è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°"""
    reward_fn_key = config.data.get("reward_fn_key", None)

    # æ·»åŠ ä½ çš„è¯„åˆ†å‡½æ•°
    if reward_fn_key == "my_math":
        from verl.utils.reward_score.my_math import compute_score
        return compute_score

    # ... å…¶ä»–è¯„åˆ†å‡½æ•°
    return None
```

**Step 3**: ä¿®æ”¹é…ç½®

```bash
# run_my_task.sh
data.reward_fn_key=my_math
```

---

### 5.3 ä¿®æ”¹ä¼˜åŠ¿è®¡ç®— (åˆ‡æ¢åˆ°å…¶ä»–ç®—æ³•)

**ç¦ç”¨æ ‡å‡†å·®å½’ä¸€åŒ–** (Dr.GRPO):
```bash
algorithm.norm_adv_by_std_in_grpo=False
```

**åˆ‡æ¢åˆ° GAE** (éœ€è¦ Critic):
```bash
algorithm.adv_estimator=gae
algorithm.gamma=0.99
algorithm.lam=0.95
trainer.critic_warmup=100  # Critic é¢„çƒ­æ­¥æ•°
```

---

### 5.4 ä¿®æ”¹å­¦ä¹ ç‡å’Œè®­ç»ƒå‚æ•°

```bash
# å­¦ä¹ ç‡
actor_rollout_ref.actor.optim.lr=5e-7  # é™ä½å­¦ä¹ ç‡

# Batch size
data.train_batch_size=512              # å‡å° batch size
actor_rollout_ref.actor.ppo_mini_batch_size=128
actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=16

# Clip ratio
actor_rollout_ref.actor.clip_ratio=0.1  # æ›´ä¿å®ˆ

# KL loss
actor_rollout_ref.actor.use_kl_loss=True
actor_rollout_ref.actor.kl_loss_coef=0.01  # å¢å¤§ KL æƒ©ç½š

# Epoch
trainer.total_epochs=20  # å¢åŠ è®­ç»ƒè½®æ•°
```

---

### 5.5 ä¿®æ”¹ç”Ÿæˆå‚æ•°

```bash
# æ¸©åº¦ (å¢åŠ å¤šæ ·æ€§)
actor_rollout_ref.rollout.temperature=0.7

# Top-p
actor_rollout_ref.rollout.top_p=0.9

# Max tokens
data.max_response_length=2048
```

---

### 5.6 ä¿®æ”¹æ¨¡å‹å’Œæ•°æ®

```bash
# æ¨¡å‹
actor_rollout_ref.model.path=meta-llama/Llama-3.1-8B

# æ•°æ®é›†
data.train_files=/path/to/your/train.parquet
data.val_files=/path/to/your/val.parquet
```

---

## 6. å¸¸è§é—®é¢˜

### Q1: è®­ç»ƒä¸æ”¶æ•›æ€ä¹ˆåŠ?

**æ£€æŸ¥æ¸…å•**:
1. **å¥–åŠ±å‡½æ•°**: æ‰“å°å‡ ä¸ªæ ·æœ¬çš„ reward, ç¡®ä¿æ­£ç¡®
   ```python
   # åœ¨ ray_trainer.py:1128 åæ·»åŠ 
   print("Sample rewards:", reward_tensor[:5].sum(-1))
   ```

2. **ä¼˜åŠ¿åˆ†å¸ƒ**: æ£€æŸ¥ advantages çš„å‡å€¼å’Œæ–¹å·®
   ```python
   # åœ¨ ray_trainer.py:1230 åæ·»åŠ 
   print("Advantage mean:", batch.batch["advantages"].mean())
   print("Advantage std:", batch.batch["advantages"].std())
   ```

3. **å­¦ä¹ ç‡**: å°è¯•é™ä½å­¦ä¹ ç‡
   ```bash
   actor_rollout_ref.actor.optim.lr=1e-7
   ```

4. **G å¤ªå°**: å¢åŠ ç”Ÿæˆæ•°é‡
   ```bash
   actor_rollout_ref.rollout.n=8
   ```

5. **KL æƒ©ç½š**: å¢åŠ  KL loss é˜²æ­¢æ¨¡å‹é€€åŒ–
   ```bash
   actor_rollout_ref.actor.kl_loss_coef=0.01
   ```

---

### Q2: æ˜¾å­˜ä¸è¶³æ€ä¹ˆåŠ?

**è§£å†³æ–¹æ¡ˆ**:

1. **å‡å° batch size**:
   ```bash
   data.train_batch_size=512
   actor_rollout_ref.actor.ppo_mini_batch_size=128
   actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=16
   ```

2. **å¯ç”¨ gradient checkpointing**:
   ```bash
   actor_rollout_ref.model.enable_gradient_checkpointing=True
   ```

3. **å¯ç”¨ parameter offload**:
   ```bash
   actor_rollout_ref.actor.fsdp_config.param_offload=True
   actor_rollout_ref.actor.fsdp_config.optimizer_offload=True
   ```

4. **å‡å°‘ç”Ÿæˆæ•°é‡**:
   ```bash
   actor_rollout_ref.rollout.n=3
   ```

5. **é™ä½ max_tokens**:
   ```bash
   data.max_response_length=512
   ```

---

### Q3: å¦‚ä½•åŠ é€Ÿè®­ç»ƒ?

1. **å¢å¤§ batch size** (å¦‚æœæ˜¾å­˜å¤Ÿ):
   ```bash
   data.train_batch_size=2048
   ```

2. **å‡å°‘éªŒè¯é¢‘ç‡**:
   ```bash
   trainer.test_freq=10
   trainer.save_freq=50
   ```

3. **ä½¿ç”¨æ›´å¿«çš„ç”Ÿæˆå¼•æ“**:
   ```bash
   actor_rollout_ref.rollout.name=sglang  # æˆ– vllm
   ```

4. **å¢å¤§ tensor parallel**:
   ```bash
   actor_rollout_ref.rollout.tensor_model_parallel_size=4
   ```

---

### Q4: å¦‚ä½•è°ƒè¯•ç”Ÿæˆè´¨é‡?

**å¯ç”¨ rollout æ—¥å¿—**:
```bash
trainer.rollout_data_dir=/path/to/save/rollout_logs
```

ç”Ÿæˆçš„æ—¥å¿—åŒ…å«:
- è¾“å…¥ prompt
- ç”Ÿæˆçš„ response
- å¥–åŠ±åˆ†æ•°
- ä¼˜åŠ¿å€¼

**æŸ¥çœ‹æ—¥å¿—**:
```python
import pandas as pd

# è¯»å–æ—¥å¿—
df = pd.read_parquet("/path/to/save/rollout_logs/step_100.parquet")

# æŸ¥çœ‹ä¸€ä¸ª prompt çš„æ‰€æœ‰å›ç­”
prompt_0 = df[df["uid"] == df["uid"].iloc[0]]
print(prompt_0[["response", "reward", "advantage"]])
```

---

### Q5: å¦‚ä½•ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ?

**é…ç½®ä¸­æ·»åŠ **:
```bash
trainer.load_checkpoint=/path/to/checkpoint
```

æˆ–åœ¨ä»£ç ä¸­:
```python
# ray_trainer.py:998
def _load_checkpoint(self):
    if self.config.trainer.get("load_checkpoint", None):
        # åŠ è½½æ£€æŸ¥ç‚¹é€»è¾‘
        ...
```

---

## æ€»ç»“

### æ ¸å¿ƒæ–‡ä»¶æ¸…å•

| æ–‡ä»¶ | ä½œç”¨ | ä¿®æ”¹å»ºè®® |
|------|------|----------|
| `run_qwen3-8b.sh` | å¯åŠ¨è„šæœ¬ | ä¿®æ”¹è¶…å‚æ•° |
| `main_ppo.py` | å¯åŠ¨å…¥å£ | ä¸€èˆ¬ä¸ä¿®æ”¹ |
| `ray_trainer.py` | è®­ç»ƒå¾ªç¯ | æ·»åŠ  debug æ—¥å¿— |
| `core_algos.py` | GRPO ç®—æ³• | ç†è§£åŸç† |
| `dp_actor.py` | PPO æ›´æ–° | ç†è§£åŸç† |
| `gsm8k.py` | è¯„åˆ†å‡½æ•° | æ›¿æ¢ä¸ºè‡ªå·±çš„ |

### ä¿®æ”¹ä»£ç çš„ä¸€èˆ¬æµç¨‹

1. **æ˜ç¡®ç›®æ ‡**: æƒ³ä¿®æ”¹ä»€ä¹ˆ? (è¯„åˆ†å‡½æ•°? ä¼˜åŠ¿è®¡ç®—? å­¦ä¹ ç‡?)
2. **æ‰¾åˆ°ä½ç½®**: æ ¹æ®æœ¬æ–‡æ¡£çš„"ä¿®æ”¹ç‚¹"ç« èŠ‚
3. **ä¿®æ”¹ä»£ç **: æœ€å¥½å…ˆåœ¨é…ç½®æ–‡ä»¶ä¿®æ”¹,å†æ”¹ä»£ç 
4. **å°è§„æ¨¡æµ‹è¯•**: ç”¨å°æ•°æ®é›†æµ‹è¯• (data.train_batch_size=64)
5. **å…¨é‡è®­ç»ƒ**: ç¡®è®¤æ— è¯¯åå…¨é‡è®­ç»ƒ

### å…³é”®æ•°æ®æµ

```
Prompt â†’ (Ã—5) â†’ 1280 prompts â†’ vLLM â†’ 1280 responses
  â†“
Reward (GSM8K) â†’ 1280 scores
  â†“
GRPO â†’ æŒ‰ uid åˆ†ç»„ â†’ è®¡ç®—ç»„å†… mean/std â†’ advantages
  â†“
PPO â†’ åˆ† mini-batch â†’ åˆ† micro-batch â†’ è®¡ç®— loss â†’ backward â†’ update
```

---

**ç¥ä½ ä½¿ç”¨æ„‰å¿«! å¦‚æœ‰é—®é¢˜,è¯·å‚è€ƒæœ¬æ–‡æ¡£ç›¸åº”ç« èŠ‚ã€‚**
