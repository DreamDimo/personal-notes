# 高质量奖励模型生成

## 一、问题描述

### 1. 目标

整个项目目标是在工程师维护基站或者其他业务场景中可能遇到的问题，希望能够通过询问大模型来得到一个较好的处理办法，该 agent 需要具备 tool calling 的能力，也就是奖励模型的设计还需要考虑 tool calling 的合理性

### 2. 数据集

telemath、TeleQA、τ2Telecom

telemath 是数学问题，可以考虑用它进行微调来提高模型的推理能力

TeleQnA 是选择题，可以考虑转换为简单题，或者可以利用选择题形成的偏好对进行 RLHF 训练

τ2Telecom 是工具调用相关，这个数据集在本项目设计中可以重点考虑

### 3. 评价指标

奖励模型评判准确率，设计思路目前还未想好，因为如果是输出 0  或者 1，那么可以考虑评测，但是如果输出的是一个范围，那我觉得有些难

或者是考虑能不能直接根据奖励模型用强化学习训练大模型，根据测试集去测，但是这样不确定算力是否足够

### 二、调研

后训练（post training）主要有几种

* SFT 监督微调，利用监督学习来对模型进行微调，fine-tuning 的方法主要就是全参和lora，目前资源足够可考虑全参微调
* zero-rl，跳过冷启动阶段，直接进行强化学习训练，这种方法可能的好处是提高大模型的熵，利用熵增定律不去限制大模型的思路，但是劣势就是可能没法掌握领域的基础知识导致思考效果也不佳
* sft+rl，先进行监督学习，然后再进行强化学习，根据 deepseek-r1 的设计思路，SFT 大概数千条数据，RL 大概数十万条数据
* 蒸馏，利用大模型的输出来训练小模型，有论文表面蒸馏方法会让小模型学到推理能力

在我调研的强化学习中，奖励模型的设计主要有两种

* RLHF 基于人类反馈的强化学习，reward model 是利用偏好对来进行奖励模型的训练，这个过程通过 <x, y1, y2> 输入到奖励模型，并说明 y1 > y2 进行训练，这里面我觉得应该是隐含了逆强化学习的思路，不直接设计奖励信号，通过偏好对进行模型的微调
* RLVR 可验证奖励的强化学习，目前主流思路应该是这个，在数学，代码等有确定答案的领域发展比较好，这里不再是奖励模型而是使用奖励信号，在数学问题中奖励信号就是答案是否正确

## 三、思路

### 1. 基于 RLVR 的奖励模型设计

基于 RLVR 的方法，在 open-ended problem 中解决思路主要是通过设计 rubrics ，然后将问题，大模型的输出，评判标准一起发给 llm-as-judge，进行评判得到对应的奖励分数，在医疗大模型领域有几个工作就是利用这种思路进行的，比如 Baichuan-M2，InfiMed-ORBIT 两个工作都采用了该思路，结合这个我想的一个思路。

如果对于每个问题都去单独设计 rubrics 来进行训练，那么成本太高并且训练数据量有限，所以需要思考如何利用有限的 rubrics 进行训练，主要思路是通过RAG 检索增强技术，将问题和对应的 rubrics 放入向量数据库 A，将所有的 rubrics 放入数据库 B，然后对于一个 problem 去两个向量数据库进行相似性搜索，找到后，把问题和两个数据库中检索到的数据输入给一个 生成模型 让它生成一些 rubrics，之后在进行合理性判断去除被命中次数太低或者太高的 rubrics

得到 rubrics 后，发给 llm-as-judge，进行评分

这个方法主要有三点设计需要仔细思考

1. 向量知识库的设计，可以考虑直接用开源的一些比如 milvus，但是检索，重排之类的设计是否需要还未确定
2. rubrics 生成模型，需要设计合适的提示词，考虑先用 telemath 进行微调让模型掌握些领域的知识，然后在这基础上生成 rubrics，数据集用 TeleQnA 形成的
3. 裁判模型设计，我认为需要监督微调，将问题，llm 的回答，rubrics，还有人类根据 rubrics 对答案的评分一起给大模型，进行训练后再去打分

这个方法需要很多专业领域的专家，比如 一些基础的 rubrics 的设计，已经裁判模型的标签，目前提供的开源数据集暂时未具备这些能力，目前考虑放弃

### 2. 基于 RLHF 的奖励模型设计

因为 TeleQnA 是选择题，所以就有偏好对，可以直接用来训练奖励模型，这种思路可以作为对比模型去比较，或者作为其中一个方法

#### 3. 逆强化学习

##### 3.1 对抗逆强化学习

> 这里有个问题，奖励模型在对抗学习中应该是作为评判器存在的，这个应该要和生成器一起训练，不能单独训练鉴别器吧

##### 3.2 字节论文 REER	

> 也不是直接训练奖励模型

转换为一个搜索问题

### 四、实验

#### 1. 基座性能评估

使用 Qwen-3-base-32B ，利用几个数据集看一下准确率

### 2. 数据蒸馏

> 这个可以做吗？因为 gemini-3-pro 的 api key 应该需要有不少的开销才能形成数据集

使用一些 sota 模型（还未调研，考虑使用 Gemini-3）的api 获得思考路径，然后去蒸馏 Qwen 模型看看效果 （不是传统软标签蒸馏，基本就是 SFT 的思路）

#### 3. 使用 RLHF 训练奖励模型

用 TeleQnA 形成的偏好对去训练



后续待定

### 五、问题




## 五、问题





# 开放问题领域中 Rubrics 奖励模型失效的场景调研

## 背景

在数学推理和代码生成等可验证任务中，强化学习领域常用 **RL with verifiable rewards (RLVR)** 等明确的奖励信号进行训练。在开放问题领域，由于答案开放性强、有效答案不唯一，研究者尝试借助由专家撰写的 **rubrics**（评判标准）对模型回答进行评分：先由专家为每个问题编写包含若干评分维度的 rubrics，构成训练数据集，然后利用检索生成模型 (例如 RAG) 为新的问题生成 rubrics，再通过大型语言模型担任裁判（LLM‑as‑judge），将回答与 rubrics 一起输入评分模型，用于训练奖励模型或评价系统。此类方法提高了开放任务的可评估性，但也存在局限。本文调研 rubrics 方法失效或效果较差的典型场景，列出相关数据集并说明原因。

## 1. 创意写作与情感智力任务

### 场景与数据集

| 场景/数据集                                                  | 主要任务                                                     | 限制与失效表现                                               |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **LitBench 创意写作基准**[cs191.stanford.edu](https://cs191.stanford.edu/projects/Spring2025/Sebastian___Russo_.pdf#:~:text=creative writing is inherently divergent%3A,But%2C they exhibit)[cs191.stanford.edu](https://cs191.stanford.edu/projects/Spring2025/Sebastian___Russo_.pdf#:~:text=Lastly%2C our human evaluation reveals,7) | 比较两篇创意小说的质量，来自 Reddit r/WritingPrompts 的 2.5k 人类写作对；训练集 4.3 万对人类偏好。 | *开放性高、缺乏客观标准*：文章中指出创意写作缺少统一的正确答案，评判依赖主观偏好，LLM‑judge 可能偏向风格而忽视内容[cs191.stanford.edu](https://cs191.stanford.edu/projects/Spring2025/Sebastian___Russo_.pdf#:~:text=creative writing is inherently divergent%3A,But%2C they exhibit)；即使采用先进的 reward model，仍有约 40% 与人类偏好不一致[cs191.stanford.edu](https://cs191.stanford.edu/projects/Spring2025/Sebastian___Russo_.pdf#:~:text=Lastly%2C our human evaluation reveals,7)。 Rubrics 难以覆盖创意风格或隐喻，模型容易受到“长度偏好”等偏见影响，导致评分不稳定。 |
| **WritingBench 生成写作基准**[arxiv.org](https://arxiv.org/html/2503.05244v1#:~:text=As illustrated in Figure 4 %2C,for the respective evaluation dimensions)[arxiv.org](https://arxiv.org/html/2503.05244v1#:~:text=Report issue for preceding element) | 涵盖 6 大类 100 个子任务的写作，包括文艺创作、商业报告、技术写作等，针对每个实例自动生成五个评分维度和细化 rubrics。 | *多维评分复杂度高、难以控制长度及顺序要求*：文中承认在复杂多维度的长度约束和章节顺序等需求上评估精度较低[arxiv.org](https://arxiv.org/html/2503.05244v1#:~:text=Report issue for preceding element)；对写作质量的评判仍带有主观性，尤其是叙事偏好和情境解释的差异[arxiv.org](https://arxiv.org/html/2503.05244v1#:~:text=Third%2C inherent challenges persist in,user preferences remains theoretically unattainable)。 |
| **EQ‑Bench 创意/情感智力评测**[eqbench.com](https://eqbench.com/about.html#:~:text=) | 利用角色扮演和分析任务评估模型的同理心、情商与社会技能。Rubric 评分和 Elo 排名同时使用。 | *主观性强且测试集小*：网站指出 EQ‑Bench 结果只是粗略指标，因情感智能没有公认标准[eqbench.com](https://eqbench.com/about.html#:~:text=)；Rubric 评分缺乏区分度，需使用模型间对比 (Elo) 来弥补[eqbench.com](https://eqbench.com/about.html#:~:text=We score the model two,Rubric score)。LLM‑judge 可能无法真实理解复杂情感，容易受到生成长度及叙述风格的偏差影响[eqbench.com](https://eqbench.com/about.html#:~:text=Judging creative writing reliably ,that LLM judges can exhibit)。 |
| **ComplexEval 高级创意写作与角色扮演**[arxiv.org](https://arxiv.org/html/2509.03419v2#:~:text=Report issue for preceding element)[arxiv.org](https://arxiv.org/html/2509.03419v2#:~:text=rates across evaluation scenarios,the bias experiments across datasets) | ComplexEval‑Advanced 数据集含创意写作、数学推理和角色扮演三类复杂任务。Rubrics 用于多维评分。 | *Rubrics 引入“标准漏洞偏差”和“维度纠缠偏差”*：研究指出 rubrics 在创意写作中易出现 **criteria loophole bias**（未覆盖的特征被忽视）和 **criteria entanglement bias**（对多个维度打分倾向相同，缺乏区分）[arxiv.org](https://arxiv.org/html/2509.03419v2#:~:text=)。实验发现小模型在创意写作场景中出现27%的标准漏洞攻击成功率，并且多个维度评分互相干扰[arxiv.org](https://arxiv.org/html/2509.03419v2#:~:text=rates across evaluation scenarios,the bias experiments across datasets)。 |

### 失效原因分析

1. **开放性与主观性强**：创意写作和情感智力任务本质上没有唯一正确答案，rubrics 无法穷尽风格、情感和作者意图，导致评分标准与人类审美不完全一致[cs191.stanford.edu](https://cs191.stanford.edu/projects/Spring2025/Sebastian___Russo_.pdf#:~:text=creative writing is inherently divergent%3A,But%2C they exhibit)。
2. **维度设计不完善**：多维 rubrics 易出现**标准漏洞**（未定义的方面被忽略）和**维度纠缠**（多个评分项被一起加减），降低评价颗粒度[arxiv.org](https://arxiv.org/html/2509.03419v2#:~:text=)。
3. **LLM 评分偏见**：LLM‑judge 在创意任务中会偏好较长或结构化的文本[cs191.stanford.edu](https://cs191.stanford.edu/projects/Spring2025/Sebastian___Russo_.pdf#:~:text=creative writing is inherently divergent%3A,But%2C they exhibit)、[eqbench.com](https://eqbench.com/about.html#:~:text=We score the model two,Rubric score)；ComplexEval 提出在使用 rubrics 时出现“注意力上限”现象，模型无法同时处理过多评分维度，从而漏掉问题[arxiv.org](https://arxiv.org/html/2509.03419v2#:~:text=3,issues even when none exist)。
4. **人工标注成本高**：LitBench、WritingBench 等数据集强调收集人类偏好或编写细化 rubrics 的成本高昂，导致规模有限，难以覆盖多样风格[cs191.stanford.edu](https://cs191.stanford.edu/projects/Spring2025/Sebastian___Russo_.pdf#:~:text=Lastly%2C our human evaluation reveals,7)、[arxiv.org](https://arxiv.org/html/2503.05244v1#:~:text=Report issue for preceding element)。

## 2. 深度研究与跨文档综合任务

### 数据集与场景

| 数据集                                                       | 描述                                                         | 限制与失效表现                                               |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **ResearchRubrics**[openreview.net](https://openreview.net/pdf/6eb990db51e74515545b04019c0722803daad2a2.pdf#:~:text=important limitations%3A for example%2C some,2025%3B Wan)[openreview.net](https://openreview.net/pdf/6eb990db51e74515545b04019c0722803daad2a2.pdf#:~:text=breadth of tasks,researchers have begun exploring AI) | 评估深度研究代理（复杂信息检索和长文答案生成），含 101 个单轮查询及 2,593 条人工撰写 rubrics，覆盖科学、法律、健康、消费等领域。 | 研究指出已有一些基准使用 LLM 自动生成 rubrics 或参考报告，会造成评估循环和监督缺失[openreview.net](https://openreview.net/pdf/6eb990db51e74515545b04019c0722803daad2a2.pdf#:~:text=important limitations%3A for example%2C some,2025%3B Wan)。此外，自动生成的 rubrics 可能错过领域细节或偏向特定解题路径[openreview.net](https://openreview.net/pdf/6eb990db51e74515545b04019c0722803daad2a2.pdf#:~:text=breadth of tasks,researchers have begun exploring AI)。在评估中，即便使用专家撰写的 rubrics，领先系统的平均符合率也低于 59%[openreview.net](https://openreview.net/pdf/6eb990db51e74515545b04019c0722803daad2a2.pdf#:~:text=for each criterion and the,document synthesis and rigorous justification)，主要是因为模型错过隐含上下文或无法对检索信息进行充分推理[arxiv.org](https://arxiv.org/html/2511.07685v1#:~:text=Deep Research ,including)。 |
| **DeepResearch Bench / ReportBench 等基准**[openreview.net](https://openreview.net/pdf/6eb990db51e74515545b04019c0722803daad2a2.pdf#:~:text=important limitations%3A for example%2C some,2025%3B Wan)[openreview.net](https://openreview.net/pdf/6eb990db51e74515545b04019c0722803daad2a2.pdf#:~:text=scores,checklists) to handle the) | 从学术论文或研讨会中生成研究类问题，并采用自生成 rubrics 或参考文献重叠度评估。 | 文中指出这些基准依赖自动生成 rubrics 和 LLM 生成的参考报告，引发评价循环和难以监督[openreview.net](https://openreview.net/pdf/6eb990db51e74515545b04019c0722803daad2a2.pdf#:~:text=important limitations%3A for example%2C some,2025%3B Wan)[openreview.net](https://openreview.net/pdf/6eb990db51e74515545b04019c0722803daad2a2.pdf#:~:text=scores,researchers have begun exploring AI)；某些基准仅覆盖单一研究环节，无法衡量广泛领域[openreview.net](https://openreview.net/pdf/6eb990db51e74515545b04019c0722803daad2a2.pdf#:~:text=scores,defined%2C exploratory problems. Their)。 |

### 失效原因分析

1. **任务复杂度高**：深度研究涉及跨文档信息整合、多步推理和事实验证，rubrics 难以全面列出所有关键点；评分模型容易遗漏隐性背景或推理链条，导致符合率低[arxiv.org](https://arxiv.org/html/2511.07685v1#:~:text=Deep Research ,including)。
2. **自动生成 rubrics 有局限**：部分基准采用 LLM 自动生成 rubrics 或参考报告，存在“自评自改”的循环问题，缺乏外部监督，并且自动 rubrics 难以捕捉领域细节，可能偏向某种解题路径[openreview.net](https://openreview.net/pdf/6eb990db51e74515545b04019c0722803daad2a2.pdf#:~:text=important limitations%3A for example%2C some,2025%3B Wan)[openreview.net](https://openreview.net/pdf/6eb990db51e74515545b04019c0722803daad2a2.pdf#:~:text=breadth of tasks,researchers have begun exploring AI)。
3. **领域广泛且要求多元**：深度研究任务覆盖健康、法律、金融、科普等，专家手工设计 rubrics 的成本高；相同任务的 rubrics 难以复用，限制了方法的泛化能力[arxiv.org](https://arxiv.org/html/2511.07685v1#:~:text=ResearchRubrics consists of 101 single,42 provide an overview of)。

## 3. 复杂角色扮演和多轮对话

### 数据集与场景

| 数据集                                                       | 描述                                                         | 限制与失效表现                                               |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **CoSER/ComplexEval 角色扮演任务**[arxiv.org](https://arxiv.org/html/2509.03419v2#:~:text=Report issue for preceding element)[arxiv.org](https://arxiv.org/html/2509.03419v2#:~:text=rates across evaluation scenarios,the bias experiments across datasets) | CoSER 数据集包含带有角色背景的对话，要求模型在戏剧角色内保持一致；ComplexEval‑Advanced 使用精细 rubrics 对每条回复进行扣分式评分。 | 研究发现 rubrics 会导致注意力过载和 **注意力上限现象**：模型每次只能识别有限数量的问题，随着评分维度增多会出现漏检[arxiv.org](https://arxiv.org/html/2509.03419v2#:~:text=3,issues even when none exist)。角色扮演 rubrics 着重戏剧一致性，在复杂情节下易出现维度纠缠，模型难以同时兼顾人物背景和情节发展[arxiv.org](https://arxiv.org/html/2509.03419v2#:~:text=rates across evaluation scenarios,the bias experiments across datasets)。 |
| **EQ‑Bench 多轮角色扮演**[eqbench.com](https://eqbench.com/about.html#:~:text=skills%2C despite these being extremely,oriented traits and abilities)[eqbench.com](https://eqbench.com/about.html#:~:text=) | 需要模型在关系纠纷、心理咨询等场景中表露感受、响应并复盘。   | 制定 rubrics 来评估情绪理解和同理心非常主观；LLM‑judge 的偏好和任务设计会影响评分，一些模型可能因为“安全”或“礼貌”倾向而得分较高但不代表真实能力[eqbench.com](https://eqbench.com/about.html#:~:text=)。 |

### 失效原因分析

1. **多轮交互信息量大**：对话或角色扮演需同时考虑上下文、角色设定、情感变化等多重因素。rubrics 对每个维度评分会导致模型注意力被分散，无法全面评估所有要点[arxiv.org](https://arxiv.org/html/2509.03419v2#:~:text=3,issues even when none exist)。
2. **难以捕捉细微的表演差异**：戏剧一致性、情绪层次等指标依赖人类的敏感度，rubrics 很难量化，导致模型根据模板评分而忽视真实表现[arxiv.org](https://arxiv.org/html/2509.03419v2#:~:text=rates across evaluation scenarios,the bias experiments across datasets)。
3. **评分主观性与偏差**：角色扮演题目常带有文化及价值观差异，不同评委对何为“共情”“恰当回应”理解不一。LLM‑judge 可能偏向符合其训练数据中的价值观，降低泛化性[eqbench.com](https://eqbench.com/about.html#:~:text=)。

## 4. 多语言和跨领域代码任务 



### 场景与数据集

| 数据集                                                       | 描述                                                         | 限制与失效表现                                               |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **Rubric Is All You Need (RIAYN)**[arxiv.org](https://arxiv.org/html/2503.23989v1#:~:text=Although this study provides valuable,another future avenue for research) | 该工作针对学生代码作业采用题目特定的 rubrics 用 GPT‑4o 评分，并用于奖励学习。 | 论文指出实验仅在 Java 作业和一门课程上测试，并且仅对 GPT‑4o 进行了评估，因此 rubrics 能否推广至其他编程语言、课程或模型仍未知[arxiv.org](https://arxiv.org/html/2503.23989v1#:~:text=Although this study provides valuable,another future avenue for research)。 |
| **ComplexEval – Code、数学推理等任务**[arxiv.org](https://arxiv.org/html/2509.03419v2#:~:text=Report issue for preceding element)[arxiv.org](https://arxiv.org/html/2509.03419v2#:~:text=rates across evaluation scenarios,the bias experiments across datasets) | 数据集包含代码生成、数学证明等多种任务，并为每种任务构建 rubrics 和参考答案。 | 实验表明引用 rubrics 在原始样本上提高了模型准确率，但在针对 rubrics 发起的对抗攻击样本上反而降低准确率[arxiv.org](https://arxiv.org/html/2509.03419v2#:~:text=Model original samples General model,Report issue for preceding element)；说明 rubrics 可能鼓励模型迎合评分点而忽视真实正确性，出现“解题模式固化”(solution fixation bias)[arxiv.org](https://arxiv.org/html/2509.03419v2#:~:text=rates across evaluation scenarios,the bias experiments across datasets)。 |

### 失效原因分析

1. **语言和领域依赖强**：RIAYN 的 rubrics 专为特定编程题目编写，难以直接迁移到其它语言或新题，且仅使用 GPT‑4o 作为评分器，缺乏跨模型验证[arxiv.org](https://arxiv.org/html/2503.23989v1#:~:text=Although this study provides valuable,another future avenue for research)。
2. **对抗攻击易致性能下降**：ComplexEval 显示在数学推理和代码任务上，使用 rubrics + 参考答案可以提升原始样本评分，但在对抗样本下反而降低 0.1–2.16% 的准确率[arxiv.org](https://arxiv.org/html/2509.03419v2#:~:text=Model original samples General model,Report issue for preceding element)，说明模型对 rubrics 过度依赖，易被恶意引导误判。
3. **解题方式固化**：rubrics 提示常强调参考答案的结构，导致模型偏爱与参考答案形式相似的输出，即使内容不完全正确也可能得高分，出现“解题模式固化”或“格式偏差”[arxiv.org](https://arxiv.org/html/2509.03419v2#:~:text=rates across evaluation scenarios,the bias experiments across datasets)。

## 5. 现实世界医疗对话与安全场景

### 数据集与场景

| 数据集                                                       | 描述                                                         | 限制与失效表现                                               |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **HealthBench**[cdn.openai.com](https://cdn.openai.com/pdf/bd7a39d5-9e9f-47b3-903c-8b847ca650c7/healthbench_paper.pdf#:~:text=the field%2C yet existing health,evals. 1) | 医疗对话基准，专家为 5k 条医疗对话生成 48k 多条会话特定 rubrics，用于评估医学助手。 | 虽然 rubrics 弥补了多选或问答评测的不足，但研究指出 rubrics 仍依赖专业人员手工制定，开放域医疗场景的安全性和信任度难以完全量化[cdn.openai.com](https://cdn.openai.com/pdf/bd7a39d5-9e9f-47b3-903c-8b847ca650c7/healthbench_paper.pdf#:~:text=the field%2C yet existing health,evals. 1)。 |
| **WildBench**[allenai.github.io](https://allenai.github.io/WildBench/WildBench_paper.pdf#:~:text=Limitations Language scope,Consequently%2C any) | 收集真实用户查询并生成每个任务的检查列表 (rubrics)。用于对齐模型回复。 | 限制包括：数据仅有英语；由 GPT‑4 生成的对话上下文可能引入偏差；rubrics 受数据源偏见影响[allenai.github.io](https://allenai.github.io/WildBench/WildBench_paper.pdf#:~:text=Limitations Language scope,Consequently%2C any)。 |

### 失效原因分析

1. **专业知识要求高**：医疗领域涉及安全和伦理，rubrics 只能覆盖有限情境，难以涵盖罕见病症或复杂医疗决策[cdn.openai.com](https://cdn.openai.com/pdf/bd7a39d5-9e9f-47b3-903c-8b847ca650c7/healthbench_paper.pdf#:~:text=the field%2C yet existing health,evals. 1)。
2. **数据和语境偏差**：WildBench 指出数据主要来自英语社区，存在文化/语言偏差，rubrics 有可能加强这些偏差，导致模型对其他语言或人群的回应不准确[allenai.github.io](https://allenai.github.io/WildBench/WildBench_paper.pdf#:~:text=Limitations Language scope,Consequently%2C any)。
3. **生成上下文存在噪声**：使用大型模型生成的对话作为上下文，rubrics 评测可能受到模型自身的偏好或错误影响，评价不一定可靠[allenai.github.io](https://allenai.github.io/WildBench/WildBench_paper.pdf#:~:text=Limitations Language scope,Consequently%2C any)。

## 6. 数据规模和人工成本限制

### 案例与现象

- **PaperBench（再现 AI 研究的基准）**：论文指出该基准只有 20 个任务，原因在于每篇论文需要细致的层次化 rubrics，人工消耗巨大；小样本数据限制了评测泛化性[arxiv.org](https://arxiv.org/pdf/2504.01848)。
- **RubricEval**：Stanford 的 RubricEval 框架提出使用任务特定 rubrics 评估开放指令，但文档指出 LLM‑as‑judge 偏好较长输出，rubrics 设计与评委偏好互相影响[web.stanford.edu](https://web.stanford.edu/class/cs224n/final-reports/256846781.pdf#:~:text=RubricEval%3A A Scalable Human,ended tasks)。此外，不同任务的 rubrics 不易复用，导致构建成本高。

### 失效原因分析

1. **人工编写 rubrics 成本高且耗时**：开放领域任务多样，手工设计 rubrics 需要专家大量时间，导致数据集规模小、覆盖面有限[arxiv.org](https://arxiv.org/pdf/2504.01848)。
2. **LLM 评价者存在偏差**：RubricEval 中观察到 LLM‑judge 与人类评价相关性仅中等，且更偏向长答案[web.stanford.edu](https://web.stanford.edu/class/cs224n/final-reports/256846781.pdf#:~:text=RubricEval%3A A Scalable Human,ended tasks)。若 rubrics 不考虑这些偏好，评价信号会被噪声污染。
3. **跨任务复用性差**：rubrics 通常针对特定任务设计，难以跨领域迁移，限制了其作为通用奖励模型的能力。

## 总结与建议

| 场景类别                        | rubrics 失效/局限性                                          | 主要原因                                                     | 代表数据集                                                   |
| ------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **创意写作与情感智力**          | 无客观标准，rubrics 难以涵盖风格；多维评分易出现标准漏洞和维度纠缠；LLM‑judge 偏向长文本或特定风格；收集人工偏好成本高，样本小。 | 主观性强、评价维度复杂、模型偏见显著。                       | LitBench[cs191.stanford.edu](https://cs191.stanford.edu/projects/Spring2025/Sebastian___Russo_.pdf#:~:text=creative writing is inherently divergent%3A,But%2C they exhibit)、WritingBench[arxiv.org](https://arxiv.org/html/2503.05244v1#:~:text=Report issue for preceding element)、EQ‑Bench[eqbench.com](https://eqbench.com/about.html#:~:text=)、ComplexEval (创意写作)[arxiv.org](https://arxiv.org/html/2509.03419v2#:~:text=)。 |
| **深度研究与跨文档综合**        | Rubrics 难以覆盖多步推理与跨文档信息，自动生成 rubrics 易缺乏领域细节；LLM‑judge 符合率低；评估循环与缺乏监督使评分不可靠。 | 任务复杂度高，信息隐含性强，自动 rubrics 不足。              | ResearchRubrics[openreview.net](https://openreview.net/pdf/6eb990db51e74515545b04019c0722803daad2a2.pdf#:~:text=important limitations%3A for example%2C some,2025%3B Wan)[arxiv.org](https://arxiv.org/html/2511.07685v1#:~:text=Deep Research ,including)、DeepResearch Bench[openreview.net](https://openreview.net/pdf/6eb990db51e74515545b04019c0722803daad2a2.pdf#:~:text=scores,researchers have begun exploring AI)。 |
| **角色扮演与多轮对话**          | 多轮对话信息量大，rubrics 维度多导致注意力上限现象；戏剧一致性难量化；评分主观性高。 | 上下文复杂度和情感判断难以标准化。                           | ComplexEval (CoSER)[arxiv.org](https://arxiv.org/html/2509.03419v2#:~:text=3,issues even when none exist)、EQ‑Bench (角色扮演)[eqbench.com](https://eqbench.com/about.html#:~:text=)。 |
| **跨语言/跨领域代码与数学推理** | Rubrics 常针对特定语言或题目设计，难以泛化；对抗攻击会降低模型性能；鼓励模型模仿格式而非真正理解。 | 语言和问题多样性高，rubrics 约束导致格式偏见和解决方案固化。 | RIAYN[arxiv.org](https://arxiv.org/html/2503.23989v1#:~:text=Although this study provides valuable,another future avenue for research)、ComplexEval (代码/数学)[arxiv.org](https://arxiv.org/html/2509.03419v2#:~:text=Model original samples General model,Report issue for preceding element)。 |
| **医疗/安全及现实对话**         | 专业知识复杂且涉及安全伦理；rubrics 难覆盖罕见情况；数据偏差和生成上下文噪声影响评估。 | 专业性、信任度和数据偏见导致 rubrics 评估效果有限。          | HealthBench[cdn.openai.com](https://cdn.openai.com/pdf/bd7a39d5-9e9f-47b3-903c-8b847ca650c7/healthbench_paper.pdf#:~:text=the field%2C yet existing health,evals. 1)、WildBench[allenai.github.io](https://allenai.github.io/WildBench/WildBench_paper.pdf#:~:text=Limitations Language scope,Consequently%2C any)。 |
| **数据规模和成本限制**          | 手工编写 rubrics 费时费力，导致数据集规模小；LLM 评价者偏见明显；rubrics 难以跨任务复用。 | 人工成本高、评价噪声大、缺乏可扩展性。                       | PaperBench[arxiv.org](https://arxiv.org/pdf/2504.01848)、RubricEval[web.stanford.edu](https://web.stanford.edu/class/cs224n/final-reports/256846781.pdf#:~:text=RubricEval%3A A Scalable Human,ended tasks)等。 |

## 结论

Rubrics 是开放问题领域奖励和评估的重要工具，但在多种场景下存在失效或表现不佳的情况：

1. **主观性强的创意和情感任务**无法用固定 rubrics 准确衡量，不同风格与个体偏好使评分结果不稳定。
2. **复杂、跨文档的深度研究任务**难以穷尽所有关键信息，自动或简单 rubrics 难以捕捉推理过程，导致低符合率。
3. **多轮对话与角色扮演任务**信息维度多且情感细腻，rubrics 会引入注意力极限和维度纠缠等问题，LLM 评判偏差明显。
4. **跨语言代码/数学推理**任务中 rubrics 容易固化解题方式，缺乏泛化性并受到对抗攻击影响。
5. **专业领域如医疗对话**需要高可信度和伦理考量，rubrics 难以覆盖全部场景；数据偏差和生成噪声会影响评估。
6. **手工设计 rubrics 的成本**限制了数据集规模和跨任务复用，且 LLM‑judge 自身偏见会干扰评分。

因此，在设计开放问题奖励模型时，应结合 rubrics 与其他评估手段（例如可验证的子任务、动态生成细化标准、多人投票偏好或排名），并关注减少模型偏见和提高评估泛化性。







# 客服大模型评测中 rubrics 失效场景调查

## 背景

在代码和数学推理领域，**RLVR**（可验证的强化学习）利用程序可验证的奖励信号进行训练和评估。例如在编程和算术题中，可以根据答案是否正确直接给出奖励。然而在开放式问题或客服场景中，输出并不是单一的正确答案，而是涉及理解、同理心、政策遵循等多方面要求。近期很多研究尝试通过 **rubrics**（评分细则）来构建奖励模型：专家先为常见问题设计评分要点，形成 rubrics 数据库，然后检索相关 rubrics，并使用 LLM 生成评分标准，再由另一个 LLM 作为裁判对回答进行评分。这种方法在医疗问答等开放问题中取得了一定效果，但在客服领域存在明显局限。

下面调研了 **tau²‑bench** 等客服评测框架，以及文献中关于 rubrics 和 LLM‑as‑judge 的局限，归纳出 rubrics 失效的典型场景，并给出相应数据集和解决思路。

## tau²‑bench 评测方法的启示

tau²‑bench 是 Sierra 团队提出的双控制客服模拟评测套件，主要面向电信、零售、航空等客服场景。每个案例包括一名用户（模拟客户）和一名代理（AI 客服），双方需通过对话协同完成任务，如修改航班或排查网络故障。论文和介绍指出，tau²‑bench 的评估不采用人工 rubric，而是使用一系列 **可验证断言**[arxiv.org](https://arxiv.org/pdf/2506.07982)、[sierra.ai](https://sierra.ai/blog/benchmarking-agents-in-collaborative-real-world-scenarios#:~:text=user understands how to navigate,adapt if something goes wrong)：

| 评估维度                        | 内容                                                         | 评测方式                         | 适用性                       |
| ------------------------------- | ------------------------------------------------------------ | -------------------------------- | ---------------------------- |
| **DB Check / Action Checks**    | 检查代理是否正确调用工具（查库、修改订单等），以及数据库状态是否符合预期。 | 程序化判断，无需 rubrics。       | 适用于工具调用任务。         |
| **Communicate Info Checks**     | 验证对话中是否包含特定信息，如代理说明赔偿政策或通知客户完成操作。 | 程序查找字符串，不依赖 rubrics。 | 适用于明确的说明要求。       |
| **Natural‑Language Assertions** | 对某些模糊或开放式要求，如“代理应解释为什么用户有资格获得补偿”，通过 LLM‑judge 检查其表达是否符合业务规则[sierra.ai](https://sierra.ai/blog/benchmarking-agents-in-collaborative-real-world-scenarios#:~:text=user understands how to navigate,adapt if something goes wrong)。 | 需要 LLM 作为裁判。              | 处理主观或难以程序化的问题。 |

tau²‑bench 的设计强调 **对于可以程序验证的环节使用程序断言，不使用 rubric 评分**；仅在无法量化的主观表达上使用 LLM 评审。该框架的成功表明，rubrics 并非客服评测的必要条件，并揭示了 rubrics 在以下场景下的失效风险。

## rubrics 在客服场景中的失效场景与原因

### 1. 多种合理表达的场景：rubrics 难以覆盖所有正确答案

客服任务常常有多个合理的回答。例如航班改签时，代理既可以简洁确认，也可以详细解释规则并提供建议。采用固定 rubrics 会强制模型采用单一表达方式，导致**创造性和灵活性受限**。

- 在 **No Free Labels: Limitations of LLM‑as‑a‑Judge** 研究中，作者发现 LLM‑judge 在开放式任务时受文本风格、顺序和长度影响[arxiv.org](https://arxiv.org/html/2503.05061v1#:~:text=have biases that can impact,2024)。rubrics 如果对答案长度、措辞或顺序有预设，可能无意中鼓励模型输出冗长或固定结构的回答，而不是根据用户意图灵活调整。
- BFF‑Bench 开发者指出，为避免 rubrics 覆盖不了的情况，他们在金融问题评测中人工撰写 **参考答案**，确保答案是唯一且无歧义[arxiv.org](https://arxiv.org/html/2503.05061v1#:~:text=Dataset ,Total 100 200 1200 3600)。客服场景往往无法做到唯一答案，因此 rubrics 难以适用。

### 2. 需要同理心和情感理解的场景：rubrics 难以量化主观体验

高质量客服不仅要解决问题，还要表现同理心、礼貌和合适的语调，这些难以用简单的 rubrics 量化。

- TELUS Digital 文章指出，对 LLM 在多轮对话中的同理心进行评估 **远比计算困惑度或 BLEU 分数复杂**，一些研究建议使用心理学测验来评估模型的情感能力[telusdigital.com](https://www.telusdigital.com/insights/data-and-ai/article/building-empathy-into-llms#:~:text=Evaluating the performance of the,LLM is the next step)。rubrics 难以反映复杂情感体验。
- **EmotionBench** 数据集（428 个情境描述和 1,266 名人类受试者情感评分）用于测试 LLM 的情绪反应[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2308.03656#:~:text=After a careful and comprehensive,emotional behaviors of human beings)。研究发现 LLM 无法与人类的情绪反应保持一致，尤其是无法关联相似情境[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2308.03656#:~:text=conclusions are as follows%3A)。此外，作者指出采用用于人类的心理量表评测 LLM 存在不稳定性，需多次测试并灵活调整[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2308.03656#:~:text=presented with positive situations)。这显示出单一 rubrics 无法稳定评估同理心。
- **Twitter Customer Care** 数据集包含 280 万条用户和品牌客服对话[arxiv.org](https://arxiv.org/pdf/2101.01334)。在利用该数据训练和评估同理心模型时，研究者发现 **生成模型的困惑度指标无法反映同理心改善**[arxiv.org](https://arxiv.org/pdf/2101.01334)。只靠 rubrics 难以评估情感表达效果，需要结合人类评审或其他指标。

### 3. 多轮协作与动态策略场景：rubrics 难以捕捉过程质量

在 tau²‑bench 的双控制任务中，代理必须指导用户完成硬件操作并适应对方反馈。这样的多轮协作要求 **动态策略和时序控制**，而 rubrics 往往针对最终输出进行评分，忽略过程。

- tau²‑bench 的设计者指出，代理不仅要调用工具，还要**考虑用户的操作能力和反馈**，在错误或意外时调整策略[sierra.ai](https://sierra.ai/blog/benchmarking-agents-in-collaborative-real-world-scenarios#:~:text=now required to explicitly model,is both timely and accurate)。rubrics 只能评价结果，无法评价指导过程是否清晰、步骤是否顺序合理、等待反馈的时机是否得当。
- 当评估需要检查每一步操作是否正确时（例如指导用户打开手机设置、重启路由器），程序断言比 rubrics 更适用：可以检查是否调用了正确工具或是否发送了正确的提示[arxiv.org](https://arxiv.org/pdf/2506.07982)。

### 4. 复杂政策和规则遵循场景：rubrics 难以涵盖大量业务规则

电信、航空等行业的客服需要遵循大量政策和合规要求。rubrics 难以全面列出所有规则，容易造成遗漏或误判。

- tau²‑bench 任务对代理提供了 **详细的政策文档和流程规则**，如退款条件、会员等级优惠等，代理需要根据不同状态做出决策[quesma.com](https://quesma.com/blog/tau2-from-llm-benchmark-to-blueprint-for-testing-ai-agents/#:~:text=,)。固定 rubrics 难以涵盖所有规则组合；如果 rubrics 未列出的规则被违反，系统也无法发现。
- WildBench 和 HealthBench 等开放领域基准指出，rubrics 设计工作量巨大且易受偏见影响，例如 **WildBench 仅提供英文 rubrics，存在潜在数据泄漏和偏见**[allenai.github.io](https://allenai.github.io/WildBench/WildBench_paper.pdf#:~:text=Limitations Language scope,Consequently%2C any)；**PaperBench** 的 rubrics 需要手工分解论文任务，成本高且易出错[arxiv.org](https://arxiv.org/pdf/2504.01848)。在业务规则更复杂的客服领域，rubrics 维护成本会更高。

### 5. 多语言或跨文化场景：rubrics 的语言和文化局限

面向全球客户的客服系统需要支持多语言交流。许多 rubrics 只在英文设计，无法直接应用于其他语言，会导致评估失真。

- WildBench 作者在局限性中承认，数据集 **只包含英文对话**，且可能带有源数据的偏见[allenai.github.io](https://allenai.github.io/WildBench/WildBench_paper.pdf#:~:text=Limitations Language scope,Consequently%2C any)。在多语言环境中，标准化 rubrics 需要针对不同语言重写；文化背景差异也会影响礼貌与表达习惯，rubrics 难以统一衡量。
- HealthBench 的 rubrics 特定于医疗场景[cdn.openai.com](https://cdn.openai.com/pdf/bd7a39d5-9e9f-47b3-903c-8b847ca650c7/healthbench_paper.pdf#:~:text=the field%2C yet existing health,evals. 1)；Rubric Is All You Need 等工作也指出 rubrics 在不同编程语言或课程设置间难以泛化[arxiv.org](https://arxiv.org/html/2503.23989v1#:~:text=Although this study provides valuable,another future avenue for research)。客服场景的多样性更大，rubrics 很难一套适用。

### 6. LLM‑as‑judge 自身偏差：rubrics 依赖的裁判会偏离人类

许多 rubrics 方案依赖 LLM 评分（LLM‑as‑judge），而研究表明 LLM 裁判存在显著偏差。

- 《No Free Labels: Limitations of LLM‑as‑a‑Judge》一文指出，LLM 裁判对回答的**长度、顺序、文风等因素过于敏感**，难以客观评价[arxiv.org](https://arxiv.org/html/2503.05061v1#:~:text=have biases that can impact,2024)。Feuer 等人发现 LLM 裁判 **过度关注风格而忽略安全性和正确性**[arxiv.org](https://arxiv.org/html/2503.05061v1#:~:text=have biases that can impact,2024)。
- 该研究构建了 (C)MT‑Bench 和 BFF‑Bench 数据集，包含数学和金融推理问题及人类标注的正确答案，用于分析裁判偏差。作者指出，即使总体上裁判与人类一致度高，其在无法回答的问题上表现很差；提供高质量的人类参考答案能显著改善评分[arxiv.org](https://arxiv.org/html/2503.05061v1#:~:text=have biases that can impact,2024)。这说明如果 rubrics 生成的“参考答案”质量不高或缺乏人类校验，裁判评分将失真。
- RubricEval 等项目也报告了 LLM 裁判对输出长度的偏好、对某些模型的偏爱等问题[web.stanford.edu](https://web.stanford.edu/class/cs224n/final-reports/256846781.pdf#:~:text=RubricEval%3A A Scalable Human,ended tasks)。因此，单纯依赖 rubrics+LLM‑judge 无法确保公平可靠的评估。

## 代表性数据集与问题

| 数据集/基准                                                  | 主要任务                                                     | 与客服场景的关联                           | 反映的 rubrics 局限                                          | 解决思路                                                     |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **Tau²‑bench**[sierra.ai](https://sierra.ai/blog/benchmarking-agents-in-collaborative-real-world-scenarios#:~:text=user understands how to navigate,adapt if something goes wrong) | 电信/零售/航空客服任务；需要用户与代理协作完成特定操作。     | 模拟真实客服流程，强调工具调用与指导用户。 | 不采用 rubrics；使用 DB 和 Action 检查、自然语言断言等自动化评估。说明 rubrics 在过程评估上不适用。 | 对可验证步骤采用程序断言；对含糊要求使用 LLM‑judge 并加入参考答案。 |
| **Twitter Customer Care dataset**[arxiv.org](https://arxiv.org/pdf/2101.01334) | 280万条真实客服对话；用于训练和评估客服模型。                | 适用于研究同理心、礼貌、情绪处理。         | 评估使用困惑度和 hits@1 等指标，发现生成模型难以仅靠这些指标衡量同理心[arxiv.org](https://arxiv.org/pdf/2101.01334)；rubrics 难以覆盖多样表达。 | 结合多轮人类评审或自动情感分析来补充指标；采用开放式问卷让用户评分服务体验。 |
| **Empathetic Dialogues** (ED) & **EmotionBench**[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2308.03656#:~:text=After a careful and comprehensive,emotional behaviors of human beings) | ED：2.4万对基于情感情境的对话；EmotionBench：428 个情境及人类情感评分。 | 研究模型识别和表达情绪的能力。             | ED 训练使检索模型同理心提高，但生成模型困惑度无提升[arxiv.org](https://arxiv.org/pdf/2101.01334)；EmotionBench 强调情感评估需心理学量表和多次测试[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2308.03656#:~:text=presented with positive situations)。rubrics 难以量化情感互动。 | 设计多维情感指标，例如情绪识别准确率、情感适当度；在模型训练和评估中结合人类评价。 |
| **BFF‑Bench** & **(C)MT‑Bench**[arxiv.org](https://arxiv.org/html/2503.05061v1#:~:text=Dataset ,Total 100 200 1200 3600) | 针对商业和数学推理的问答数据集，每个问题有唯一正确答案和链式推理。 | 虽不属客服，但用于研究 LLM‑judge 的偏差。  | 发现裁判能力取决于是否提供高质量参考；自行生成的参考易导致自偏[arxiv.org](https://arxiv.org/html/2503.05061v1#:~:text=have biases that can impact,2024)。 | 在 rubrics 评估时提供经过人类审核的参考答案；采用弱裁判+高质量参考组合减少偏差。 |
| **WildBench**[allenai.github.io](https://allenai.github.io/WildBench/WildBench_paper.pdf#:~:text=Limitations Language scope,Consequently%2C any) | 收集真实用户多轮问答进行评价；使用任务检查表（rubrics）。    | 可类比客服咨询。                           | 数据仅包含英文，存在潜在数据泄漏和偏见；使用 GPT‑4 生成对话引入模型偏向。 | 构建多语言、多文化的真实客服对话数据；采取匿名和去偏策略。   |
| **PaperBench**[arxiv.org](https://arxiv.org/pdf/2504.01848)  | 评估 AI 复制论文的方法；采用分层 rubrics。                   | 用于研究复杂任务的 rubrics 设计。          | 数据集规模小（20篇论文），rubrics 制作耗时且易出错；LLM 评判表现有限。 | 对复杂任务采用分阶段程序检查或专家审阅；提高 rubrics 复用性。 |

## 对策与建议

1. **分层评估策略**：对于客服任务中可程序验证的环节（如查询数据库、调用工具、填写表单），使用自动断言和状态检查替代 rubrics，保证评估客观准确；对于不可量化的环节（同理心、礼貌等），使用多维指标和人类评审。
2. **动态 rubrics 与参考答案结合**：对于需要参考答案的主观问题（例如解释某项赔偿政策或道歉语气是否合适），可以先检索相关业务文档和历史示例生成候选 rubrics，然后由人工审核生成高质量参考答案提供给 LLM‑judge，这样能减少裁判偏差[arxiv.org](https://arxiv.org/html/2503.05061v1#:~:text=have biases that can impact,2024)。
3. **结合情感分析和用户反馈**：引入情感分类模型或用户满意度调查，对模型的礼貌、同理心进行打分，从而补充 rubrics 难以覆盖的情感维度。利用 EmotionBench、Empathetic Dialogues 等数据集微调模型并建立客观指标，如情绪识别准确率、共情语句占比等。
4. **多语言与文化适应**：构建涵盖多语言和文化背景的客服对话数据集，避免 WildBench 等单语数据集带来的偏见[allenai.github.io](https://allenai.github.io/WildBench/WildBench_paper.pdf#:~:text=Limitations Language scope,Consequently%2C any)。rubrics 设计应针对不同语言调整礼貌表达和业务流程。
5. **避免过度依赖 LLM‑judge**：在使用 LLM‑judge 评分时需注意其偏好文风和长度等问题[arxiv.org](https://arxiv.org/html/2503.05061v1#:~:text=have biases that can impact,2024)。可以采取：
   - **随机化答案顺序和长度去偏**；
   - **使用多个裁判模型投票**；
   - **在判分提示中明确要求忽略长度及位置偏好**[arxiv.org](https://arxiv.org/html/2503.05061v1#:~:text=have biases that can impact,2024)；
   - **引入人类审核和打分**以校正裁判偏差。
6. **构建可验证奖励模型替代 rubrics**：对于特定业务任务，可将问题拆解为一系列原子操作，每个操作能通过环境状态验证是否正确。这种方法类似 RLVR，既能提供明确奖励信号，又避免 rubrics 设计难题。例如 tau²‑bench 通过组合“打开移动数据”“检查流量限制”等原子步骤生成任务，并自动验证每一步[sierra.ai](https://sierra.ai/blog/benchmarking-agents-in-collaborative-real-world-scenarios#:~:text=Tasks are built from a,precise experiments on model performance)。

## 结论

在客服大模型评测领域，rubrics 作为奖励模型或评估标准具有一定价值，但存在诸多局限：面对多样化表达、同理心和动态协作等场景难以覆盖全部正确答案；复杂政策与跨语言环境使 rubrics 设计和维护成本高；依赖 LLM‑judge 存在偏差风险。Tau²‑bench 的成功经验表明，应优先使用可程序验证的断言和状态检查作为评估基础；对于难以量化的主观维度，通过高质量参考答案、情感分析、多裁判投票和人类评审相结合的方式提升评估可靠性。建立多语言、多文化的客服对话数据集、引入用户满意度指标以及动态生成 rubrics，可帮助客服大模型更好地适应现实应用需求。















