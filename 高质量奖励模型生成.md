# 高质量奖励模型生成周报一

## 一、问题描述

### 1. 目标

整个项目目标是在工程师维护基站或者其他业务场景中可能遇到的问题，希望能够通过询问大模型来得到一个较好的处理办法，该 agent 需要具备 tool calling 的能力，也就是奖励模型的设计还需要考虑 tool calling 的合理性

### 2. 数据集

telemath、TeleQA、τ2Telecom

telemath 是数学问题，可以考虑用它进行微调来提高模型的推理能力

TeleQnA 是选择题，可以考虑转换为简单题，或者可以利用选择题形成的偏好对进行 RLHF 训练

τ2Telecom 是工具调用相关，这个数据集在本项目设计中可以重点考虑

### 3. 评价指标

奖励模型评判准确率，设计思路目前还未想好，因为如果是输出 0  或者 1，那么可以考虑评测，但是如果输出的是一个范围，那我觉得有些难

或者是考虑能不能直接根据奖励模型用强化学习训练大模型，根据测试集去测，但是这样不确定算力是否足够

### 二、调研

后训练（post training）主要有几种

* SFT 监督微调，利用监督学习来对模型进行微调，fine-tuning 的方法主要就是全参和lora，目前资源足够可考虑全参微调
* zero-rl，跳过冷启动阶段，直接进行强化学习训练，这种方法可能的好处是提高大模型的熵，利用熵增定律不去限制大模型的思路，但是劣势就是可能没法掌握领域的基础知识导致思考效果也不佳
* sft+rl，先进行监督学习，然后再进行强化学习，根据 deepseek-r1 的设计思路，SFT 大概数千条数据，RL 大概数十万条数据
* 蒸馏，利用大模型的输出来训练小模型，有论文表面蒸馏方法会让小模型学到推理能力

在我调研的强化学习中，奖励模型的设计主要有两种

* RLHF 基于人类反馈的强化学习，reward model 是利用偏好对来进行奖励模型的训练，这个过程通过 <x, y1, y2> 输入到奖励模型，并说明 y1 > y2 进行训练，这里面我觉得应该是隐含了逆强化学习的思路，不直接设计奖励信号，通过偏好对进行模型的微调
* RLVR 可验证奖励的强化学习，目前主流思路应该是这个，在数学，代码等有确定答案的领域发展比较好，这里不再是奖励模型而是使用奖励信号，在数学问题中奖励信号就是答案是否正确

## 三、思路

### 1. 基于 RLVR 的奖励模型设计

> 两个痛点，一个是高质量的rubrics生成与获取难，根据问题生成rubrics质量肯定不如人类专家，这里就有损失；再用rubrics和回答给裁判模型评判，裁判模型答准率也不确定，这两个问题限制了RLVR的发展

基于 RLVR 的方法，在 open-ended problem 中解决思路主要是通过设计 rubrics ，然后将问题，大模型的输出，评判标准一起发给 llm-as-judge，进行评判得到对应的奖励分数，在医疗大模型领域有几个工作就是利用这种思路进行的，比如 Baichuan-M2，InfiMed-ORBIT 两个工作都采用了该思路，结合这个我想的一个思路。

如果对于每个问题都去单独设计 rubrics 来进行训练，那么成本太高并且训练数据量有限，所以需要思考如何利用有限的 rubrics 进行训练，主要思路是通过RAG 检索增强技术，将问题和对应的 rubrics 放入向量数据库 A，将所有的 rubrics 放入数据库 B，然后对于一个 problem 去两个向量数据库进行相似性搜索，找到后，把问题和两个数据库中检索到的数据输入给一个 生成模型 让它生成一些 rubrics，之后在进行合理性判断去除被命中次数太低或者太高的 rubrics

得到 rubrics 后，发给 llm-as-judge，进行评分

这个方法主要有三点设计需要仔细思考

1. 向量知识库的设计，可以考虑直接用开源的一些比如 milvus，但是检索，重排之类的设计是否需要还未确定
2. rubrics 生成模型，需要设计合适的提示词，考虑先用 telemath 进行微调让模型掌握些领域的知识，然后在这基础上生成 rubrics，数据集用 TeleQnA 形成的
3. 裁判模型设计，我认为需要监督微调，将问题，llm 的回答，rubrics，还有人类根据 rubrics 对答案的评分一起给大模型，进行训练后再去打分

这个方法需要很多专业领域的专家，比如 一些基础的 rubrics 的设计，已经裁判模型的标签，目前提供的开源数据集暂时未具备这些能力，目前考虑放弃

### 2. 基于 RLHF 的奖励模型设计

> 暂时放弃

因为 TeleQnA 是选择题，所以就有偏好对，可以直接用来训练奖励模型，这种思路可以作为对比模型去比较，或者作为其中一个方法

### 3. 逆强化学习

#### 3.1 对抗逆强化学习

> 这里有个问题，奖励模型在对抗学习中应该是作为评判器存在的，这个应该要和生成器一起训练，不能单独训练鉴别器吧

#### 3.2 字节论文 REER	

> 也不是直接训练奖励模型

转换为一个搜索问题

### 4. unsupervised rl

> 无监督强化学习，利用模型自身的性能

可以考虑只给问题不给标注，来进行训练，主要是模型自身的能力，从一致性，自信度等几个方面进行评估，奖励信号用这几个评判标准 

## 四、实验

### 1. 基座性能评估

使用 Qwen-3-base-32B ，利用几个数据集看一下准确率

### 2. 数据蒸馏

> 这个可以做吗？因为 gemini-3-pro 的 api key 应该需要有不少的开销才能形成数据集

使用一些 sota 模型（还未调研，考虑使用 Gemini-3）的api 获得思考路径，然后去蒸馏 Qwen 模型看看效果 （不是传统软标签蒸馏，基本就是 SFT 的思路）

### 3. 使用 RLHF 训练奖励模型

用 TeleQnA 形成的偏好对去训练

## 五、问题

### 1. 开放领域问题的准确性如何评估？评价标准

> 需要调研

调研发现，现在大多benchmark为确定答案或者选择题，可以直接评测

对于无法直接评测的文本生成题，评测标准多为找一个模型作为裁判模型评测

  

# 高质量奖励模型生成周报二







# 高质量奖励模型生成周报四

## 一、论文

### 1. DeepSeek-GRM

### 2. On-Policy Distillation

https://thinkingmachines.ai/blog/on-policy-distillation/

### 3. RM-R1

## 二、实验

### 1. 测试DeepSeek-GRM-27B效果

以下面的题目为例

```
What is the typical coverage range of LPWANs (Low-Power Wide Area Networks) in urban areas?
	

[
"A few meters",
"2-5 km",
"10-15 km",
"50-100 km"
]

ground_truth:2-5km
```

下面是四个回答一起送进去时的答案

```
Specific standards: IEEE 802.11ah, LoRaWAN, and Sigfox  
    Analysis: IEEE 802.11ah is not relevant to LPWANs. LoRaWAN and Sigfox are two widely used LPWAN technologies, with typical urban coverage ranges of 2-5 km for LoRaWAN and 3-10 km for Sigfox. Response 1 is incorrect as it describes a range far below realistic urban coverage for any LPWAN. Response 2 accurately reflects the coverage range for LoRaWAN in urban areas. Response 3 slightly overestimates the range but is still within the ballpark for Sigfox. Response 4 significantly overestimates the coverage range, which is not typical for LPWANs in urban settings.  
    Score: \boxed{2, 9, 7, 4}
[2.0, 9.0, 7.0, 4.0]
```

如果分开打分，可以看出效果很差，它偏向于对内容完整性等打分，而不是真的准确性

1. 当输入下面时

```
What is the typical coverage range of LPWANs (Low-Power Wide Area Networks) in urban areas?
	

[
"2-5 km"
]

```

```
Analysis: Response 1 provides a specific numeric range for the typical coverage of LPWANs in urban areas, which aligns with 3GPP standards for LPWANs. The range of 2-5 km is consistent with the understanding that LPWANs are designed for long-range communication, often extending beyond typical cellular coverage. This response is highly relevant and practical, as it directly answers the question without unnecessary details.
Score: \boxed{9, 9}
[9.0, 9.0]
```

2. 

```
What is the typical coverage range of LPWANs (Low-Power Wide Area Networks) in urban areas?
	

[
"10-15 km"
]
```

```
Specific standards: 
    - ITU-T Recommendation I.580-1995 for LPWAN characteristics
    - IEEE 802.11 standards for wireless communication
    - 3GPP standards for cellular networks

    Analysis: 
    - Adherence to Instructions: Response 1 provides a specific numerical range (6-10 km), which is a direct answer to the question about the typical coverage range of LPWANs in urban areas. This response fully adheres to the instructions.
    - Practicality: The response is highly practical as it directly addresses the query with a precise and accurate range, which is useful for planning and deployment purposes in urban settings.
    - Relevance: The response is highly relevant as it focuses on the specific topic of LPWAN coverage in urban areas, without any off-topic information.

    Score: \boxed{10, 10}
[10.0, 10.0]
```

## 三、思路

![img](assets/v2-6e7e78165b1af96e810c84cf82fc7a94_r.jpg)

现在 GRPO 算法对一个 prompt 会生成多个回复，一般是5，15附近，我这里准备采用4-5

* RLHF 算法是通过偏好对来训练奖励模型，是利用监督微调的策略，但是 SFT 好像更侧重于学到表面信息，即完整性，回答的风格是否友好，而不是正确性分析，即不会主动思考为什么错误，为什么正确

* RLVR 算法对规则要求很高，需要制定规则，这个阻碍了泛化能力
* PRM 方法对数据标注要求太高，但是其对思考轨迹的研究可以借鉴

结合 RLHF 和 RLVR 算法，将 rollout 生成的多个回答一起输入给奖励模型，然后我们用 RLVR 的方法去作为奖励信号进行强化学习，这里的核心思路是考虑模型生成多条轨迹之间的关联性信息，而且我认为选择会比单独的分析效果好

我们需要的数据集是 问题，模型生成的回复（包括推理轨迹），以及正确的回复编号（但是考虑在实际过程中，可能多个回复都是正确的，因此 TeleQnA 数据集要考虑如何扩充）组成的

训练策略就是通过设计提示词 （参考论文RM-R1），然后让大模型选择正确的回复，并且要给出错误回复的问题，目前思考的奖励RM的策略是选择的回复是正确的就奖励1分，能够给出错误回复的问题每个奖励0.1分（如何审查目前尚未确定），我们希望奖励模型不仅能够确定正确的回复，并且能够给出错误回复的理由

最终在基座模型训练的时候，我们可以利用 GRPO 天然的多 rollout 能力去产生多个回复，然后输入给奖励模型，奖励模型会给出正确答案的编号，奖励1分，剩余奖励0分，因为其优势计算就是归一化过程，所以我认为可以采用

对于奖励模型的评估，我们采用测试集的方法，对于输入序列，能够选择正确答案那么就认为奖励模型是正确的

```mermaid
graph TD
	
	A["调研包括思考过程和答案的数据集(prompt采样生成多个答案，包括思考推理轨迹)"] --> B["训练Qwen3-8B 模型"]
	C["考虑将teleqna数据集补充思考轨迹数据，即输入Q和A让模型输出思考轨迹"] --> A
	S["训练具有推理能力的奖励模型，结合RLHF和RLVR的思路，因为GRPR是输出多个响应，考虑同时将所有回答一起输给奖励模型，奖励模型选择最合适的一个答案给与奖励1，其余的给奖励0"] --> A
	B --> E["让模型选择最佳答案，并且要给出其他答案推理错误的地方，训练采用强化学习策略，若模型最后输出的是ground truth那么奖励1，并且最后有推理错误标签奖励0.1"]
	F["考虑可能reward hacking，所以需要对错误标签信息进行判断"] --> E
	G["可能考虑类似于deepseek思路，只对里面长度进行检查，也可以采用llm-as-judge，但是有些复杂"] --> F
	E --> H["最终模型的准确率以teleqna的测试集，如果能够正确选择答案则视为正确"]
```

## 四、问题

### 1. 数据集构建

目前用 Qwen3-8B，但是数据中存在推理轨迹有问题，即如果答案错误，那么思考轨迹生成的质量不高，不太符合实际模型生成情况，但是思考轨迹质量不高能不能作为训练的一部分

考虑选择 Qwen3-8B 回答错的问题，然后让它生成这个问题的思考轨迹，然后用教师模型比如 gemini 来生成正确的思考轨迹，需要 gemini api，这样就有正确的思考轨迹和错误的思考轨迹

```
选项 1：指示 QoS 要求

<think>在电信中，尤其是处理 5G 新无线（NR）侧链增强时，了解特定组件的作用至关重要。问题询问的是 IUC 在这个背景下的作用。为了解决这个问题，我首先需要回顾一下 IUC 代表什么，并了解它在 NR 侧链中的相关性。

IUC 很可能指的是与质量服务（QoS）相关的术语，在侧链通信中非常重要，侧链通信用于设备间的直接通信（设备到设备 D2D），而不通过基站。在 Release 17 中，侧链的增强集中在提高可靠性、降低延迟以及支持更多的用例，例如车联网（V2X）和工业应用。

考虑到在这些场景中的 QoS 作用，确保不同类型的流量（如安全关键消息或多媒体流）得到适当的处理非常重要。这让我想到了如何在侧链通信中传达和管理 QoS 要求。

在 3GPP 标准中，QoS 通常通过诸如 QoS 流标识符（QFI）和 QoS 类别标识符（QCI）等参数进行定义。然而，问题中专门提到了 IUC。IUC 可能是一个框架的一部分，允许设备向网络中的其他设备传达其 QoS 需求。这对于管理资源并确保通信达到所需的性能水平至关重要。

考虑到 Release 17 中的增强，侧链支持更复杂的 QoS 机制，包括基于 QoS 的调度和资源分配。IUC 在这种情况下，可能是一个机制或参数，用于传递设备的 QoS 要求，使得其他设备能够更高效和可靠地进行通信。

通过考虑侧链中对 QoS 的需求以及 Release 17 引入的增强，显然 IUC 的作用是指示设备的 QoS 要求。这将允许侧链网络中的其他设备根据需要优先处理和管理资源，从而确保通信达到必要的性能标准。这一逻辑推理导致的结论是，IUC 在 NR Sidelink 增强中的作用是指示 QoS 要求。</think>
<answer>指示 QoS 要求</answer>

选项 2：启用设备间协调

<think>好吧，我需要弄清楚 IUC 在 NR Sidelink 增强中的作用，特别是根据 3GPP Release 17。让我首先回忆一下我对 NR Sidelink 的了解。侧链是 NR 中的一项功能，允许设备直接相互通信，而不需要通过基站，对吧？这对于车联网（V2X）或物联网（IoT）等需要直接通信的应用非常有用。

现在，问题是关于 IUC 的。我记得在 3GPP 标准中，涉及侧链通信的有各种组件和协议。IUC 可能是与用户设备（UE）之间的协调相关的某个术语。让我想一想侧链中的挑战。由于设备之间直接通信，如果多个 UE 同时传输，可能会有干扰问题。这可能就是为什么协调很重要的原因。

在 Release 16 中，侧链有一些增强，但问题是关于 Release 17。所以我应该检查一下 Release 17 中与侧链相关的新功能。也许 IUC 是这些增强的一部分。IUC 可能是设备间协调（Inter-UE Coordination）的意思。这很有道理，因为在 UE 之间进行协调有助于管理干扰并提高侧链通信的效率。

等等，协调在这个背景下是怎么工作的？如果多个 UE 使用相同的频段，它们需要协调它们的传输时间或功率水平，以避免冲突。这种协调可以通过一种协议来完成，允许 UE 交换它们的传输计划。IUC 可能是启用这种协调的机制。

另外，在某些场景下，比如车联网（V2X）通信，避免冲突对安全至关重要。因此，UE 可以协调其传输的系统是必不可少的。这与 IUC 是设备间协调的想法一致。

我还应该考虑侧链通信的结构。侧链有不同的模式，如点对点、点对多和多对多。在多对多场景中，协调变得更加复杂，因为多个 UE 同时在传输。IUC 会帮助通过允许 UE 协商传输参数来管理这一点。

另一个角度是控制通道的使用。在侧链中，有控制通道用于管理 UE 之间的通信。IUC 可能是这些控制机制的一部分，使 UE 能够交换它们的传输时间表或资源分配。这有助于减少干扰并确保</think>
<answer>启用设备间协调</answer>

选项 3：支持节能

<think>好吧，我需要弄清楚 IUC 在 NR Sidelink 增强中的作用，特别是根据 3GPP Release 17。让我首先回忆一下我对 NR Sidelink 的了解。侧链是 NR 中的一项功能，允许设备直接相互通信，而不需要通过基站，对吧？这对于车联网（V2X）或机器对机器（M2M）通信等应用非常有用。

现在，问题是关于 IUC 的。我UC 在电信中的含义可能是与通信协议或网络功能相关的某个缩写。既然问题提到的是 3GPP Release 17，我应该考虑 Release 17 中引入的与侧链相关的新功能或增强。

我记得侧链通信有各种组件，如发现、资源分配和安全性。但是问题特别提到 IUC。让我想一想 IUC 可能的含义。在电信中，术语如“空闲”或“上行链路”可能与之相关。等等，IUC 可能代表“空闲上行通信”或类似的意思。但我不确定。也许它与电源管理有关？因为在无线通信中，电池寿命有限的设备需要高效运行，节能是一个常见的问题。

回顾 3GPP Release 17，我记得侧链有针对提高效率和性能的增强。侧链中的一个关键领域是管理电源消耗，尤其是对于处于空闲模式或电池寿命有限的设备。如果 IUC 与空闲模式或上行链路通信相关，那么它可能是在这些状态下优化电源使用。

另一个角度是，在侧链中，设备需要管理它们的传输功率，以避免干扰并节省能量。如果 IUC 是一个有助于这一点的功能，那么它可能是关于减少设备在未积极传输或接收数据时的功耗。也许它允许设备在不需要时进入低功耗状态，从而节省能量。

我还应该考虑侧链通信的结构。侧链有不同的阶段，如发现、建立和数据传输。节能在空闲阶段或设备不活跃时尤为重要。如果 IUC 是管理这些状态的协议的一部分，那么它的作用将与节能相一致。

此外，在 3GPP 规格中，节能机制通常与某些特性相关</think>
<answer>支持节能</answer>

选项 4：提供中继选择

<think>好吧，我需要弄清楚 IUC 在 NR Sidelink 增强中的作用，特别是根据 3GPP Release 17。让我首先回忆一下我对 NR Sidelink 的了解。侧链是 5G 中的一项功能，允许设备直接相互通信，而不需要通过基站，对吧？这对于车联网（V2X）通信或物联网（IoT）等需要设备到设备（D2D）连接的场景非常有用。

现在，问题是关于 IUC 的。我记得在 3GPP 标准中，缩写通常代表某些特定的术语。IUC 可能是与某个过程或功能相关的缩写。让我想想 NR Sidelink 的组成部分。侧链有不同的方面，如发现、资源分配和中继功能。

等等，在 Release 17 中，侧链有增强，特别是针对 URLLC（超可靠低延迟通信）和增强的 MTC（机器类型通信）用例。一个关键的改进领域是中继功能。中继可以通过转发数据来帮助扩展覆盖范围并提高可靠性。所以，也许 IUC 与如何选择或管理中继有关。

让我检查一下 IUC 代表什么。IUC 可能是“用户间通信”或“用户间协调”的意思。但这似乎太宽泛了。另一种可能性是“用户间通道”或“用户间控制”。等等，在侧链的背景下，有一个叫做“中继选择”的概念，即选择最佳的中继节点来转发数据。如果 IUC 是该过程的一部分，那么它的作用可能是促进选择最佳的中继。

看一下 Release 17 中的增强，侧链通信的效率和可靠性得到了提高。中继选择是这一改进的关键，因为它确保数据通过最有效的路径传输。如果 IUC 参与其中，那么它的作用就是提供中继选择。

我还应该考虑侧链通信的结构。侧链有不同的模式，如直接通信和中继通信。在中继模式下，设备可以充当中继，转发数据到另一个设备。选择哪个设备充当中继对性能至关重要。如果 IUC 是帮助选择最佳中继的机制，那么这与答案中提到的作用一致。</think>
<answer>提供中继选择</answer>

选项 5：处理网络授权

<think>在电信中，特别是在 3GPP Release 17 中定义的 5G 新无线（NR）侧链增强中，理解特定组件的作用对于确保安全高效的通信至关重要。问题询问的是 IUC 在此背景下的作用。

首先，我需要回忆一下 IUC 代表什么。在 5G 和 NR 的领域中，IUC 很可能是与某个特定功能或协议相关的缩写。鉴于提到侧链增强，我应该专注于不经过中心网络节点而发生的直接设备到设备（D2D）通信方面。侧链通信对于车联网（V2X）、物联网（IoT）等需要设备直接交互的场景非常重要。

接下来，我考虑 NR 侧链中涉及的各种组件和协议。这些包括与发现、资源分配、安全性和授权相关的功能。由于问题专门提到了“网络授权”，我应该考虑网络如何与或授权侧链通信。在传统的蜂窝网络中，设备需要网络授权才能访问服务，但在侧链中，设备间的直接通信是常态。然而，在某些场景中，网络的参与可能仍然是必要的，特别是出于安全或监管的原因。

现在，我考虑 IUC 这个特定术语。在 3GPP Release 17 中，侧链增强的引入包括允许更安全高效的直接通信的功能。一个关键方面是确保只有授权的设备可以参与侧链通信。这让我想到了网络授权的概念。如果 IUC 与此相关，它很可能代表像“用户间通信”或类似的术语，涉及以需要网络监督的方式管理设备之间的通信。

我还考虑侧链中的安全性更广泛的背景。尽管侧链通信是直接的，但仍然存在对未经授权的访问、干扰和确保只有合法设备进行通信的担忧。这让我想到网络实体在授权这些通信中的作用。如果 IUC 是这一过程的一部分，它将负责在设备可以参与侧链通信之前验证设备的合法性。

此外，我记得在 3GPP Release 17 中，侧链的增强包括使用身份验证和授权机制。这些机制对于保持侧链通信的完整性和机密性至关重要。如果 IUC 参与其中</think>
<answer>处理网络授权</answer>
```



### 2. 训练方法选择

是选择监督微调 SFT ，还是离线强化学习

如果选择监督微调，就是把问题和几个答案一起输入给奖励模型，标签就是正确答案是1，其他是0

如果选择离线强化学习，输入相同，但是训练奖励模型的奖励选择RLVR方法，即给与正确答案奖励为1的时候奖励这个奖励模型1分，并且可能会加一些其他的额外奖励，比如发现其他推理轨迹的问题，但是这个暂时不知道怎么验证

选择这两个数据量分别是多少

### 3. 模型选择

模型是选 Base 还是 Instruct 模型，Instruct一般不生成 `<think>` 标签，能不能有思考轨迹

因为我觉得对于这种领域内的知识问答，答案不是特别能训练，需要看下中间的思考轨迹

或者直接选择一些已经训练好的 RM

# 高质量奖励模型生成周报五

## 一、论文

## 二、实验

### 1. qwen3-8b 基础测试

```
query = "What are the limitations of energy harvesting from natural sources?"
```

no-thinking 模式

```
Energy harvesting from natural sources, such as solar, wind, thermal, and kinetic energy, is a promising approach for sustainable power generation. However, it comes with several limitations. Here are the key limitations:

### 1. **Intermittency and Unpredictability**
- **Solar and wind energy** are dependent on weather conditions and time of day.
- Energy production can vary significantly, making it difficult to provide a consistent and reliable power supply.
- This variability can lead to challenges in grid stability and energy storage requirements.

### 2. **Geographical and Environmental Constraints**
- **Location-dependent** – Solar panels work best in sunny regions, and wind turbines require consistent wind speeds.
- Some areas may not be suitable for certain types of energy harvesting due to lack of natural resources or environmental conditions.

### 3. **High Initial Costs and Maintenance**
- The initial investment for installing energy harvesting systems (e.g., solar panels, wind turbines) can be high.
- These systems require regular maintenance, which adds to long-term costs and reduces overall efficiency.

### 4. **Energy Conversion Efficiency**
- The efficiency of converting natural energy into usable electricity is typically lower than 100%.
- For example, solar panels have an average efficiency of 15–22%, and wind turbines typically range from 30–50% efficiency.
- These losses reduce the overall effectiveness of energy harvesting.

### 5. **Space Requirements**
- Energy harvesting systems often require significant physical space.
- For example, large wind farms or solar farms need vast areas, which may not be available in densely populated regions.

### 6. **Weather and Climate Impact**
- Extreme weather conditions such as storms, heavy rain, or snow can damage or reduce the efficiency of energy harvesting systems.
- Climate change can also affect the availability and reliability of natural energy sources.

### 7. **Energy Storage Challenges**
- Energy harvested from natural sources is often not available on demand.
- Efficient and cost-effective energy storage solutions (like batteries) are needed to store excess energy, but they are still expensive and have their own environmental impacts.

### 8. **Environmental and Ecological Impact**
- While energy harvesting is generally considered cleaner than fossil fuels, it can still have environmental impacts.
- For example, wind turbines can affect bird and bat populations, and large solar farms can impact local ecosystems.

### 9. **Technological Limitations**
- Some natural energy sources (e.g., thermal or kinetic) are not yet efficiently harnessed due to technological constraints.
- Research and development are ongoing to improve the efficiency and scalability of these technologies.

### 10. **Regulatory and Policy Barriers**
- In some regions, there may be regulatory hurdles or lack of supportive policies that hinder the widespread adoption of natural energy harvesting.

---

### Conclusion:
While energy harvesting from natural sources is a vital part of the transition to sustainable energy, it faces several limitations that must be addressed through technological innovation, policy support, and efficient energy management systems.
```

thinkink 模式

```
------------------------------ Input ------------------------------
What are the limitations of energy harvesting from natural sources?
------------------------------ Qwen3 Output ------------------------------
<think>
Okay, so I need to figure out the limitations of energy harvesting from natural sources. Let me start by recalling what I know about energy harvesting. Energy harvesting, or scavenging, is the process of capturing and converting ambient energy from the environment into usable electrical energy. Common natural sources include solar, wind, thermal, kinetic, and even radio waves.

First, I should think about each of these sources and their specific limitations. Let's take solar energy first. The main issues with solar are things like weather dependence—cloudy days or nighttime mean no energy. Also, the efficiency of solar panels is limited; even the best ones might only convert around 20% of sunlight into electricity. Then there's the issue of space required. Large solar farms need a lot of land, which might not be feasible in urban areas. Plus, manufacturing solar panels has environmental impacts, like the use of toxic materials and energy-intensive processes.

Next, wind energy. Wind turbines depend on consistent wind speeds. If the wind is too low or too high, they can't generate much power. Also, wind is intermittent, so energy storage is needed, which adds cost and complexity. There are also environmental concerns like noise, visual impact, and effects on wildlife, especially birds and bats. Location is another factor—wind farms need to be in areas with consistent wind, which might be remote or require long transmission lines.

Thermal energy harvesting, like geothermal, depends on the availability of heat sources. Not all regions have accessible geothermal resources, and drilling can be expensive and risky. Also, geothermal plants might have emissions if not properly managed, and there's the risk of subsurface instability.

Kinetic energy harvesting, such as from vibrations or human movement, usually produces very low power. It's not practical for large-scale use and might require a lot of sensors or devices to collect enough energy. Plus, the efficiency here is low, and it's hard to maintain consistent energy flow.

Radio frequency (RF) energy harvesting is another area. It captures energy from wireless signals, but the power levels are very low, so it's not suitable for high-power applications. There's also the issue of interference with existing communication systems and the need for specialized receivers.

Other limitations might include technological challenges, like the efficiency of the conversion processes, which are still not perfect. The cost of materials and maintenance can be high. Additionally, energy harvesting systems might not be reliable in all environments, and they might require backup power sources. There's also the issue of scalability—some methods are good for small devices but not for large-scale power generation.

Wait, are there any other sources? Maybe hydroelectric, but that's more of a traditional method. Maybe also piezoelectric or thermoelectric generators. Those might have their own limitations, like low efficiency or material degradation over time.

I should also consider environmental and social factors. For example, solar panels and wind turbines can have land use conflicts, and there might be opposition from local communities. Also, the intermittent nature of some sources requires energy storage solutions, which have their own limitations in terms of cost, lifespan, and environmental impact.

Another point is that energy harvesting from natural sources might not be sufficient on its own to meet all energy demands, especially for large-scale applications. It's often used in conjunction with other energy sources or as a supplement to reduce reliance on fossil fuels.

I need to make sure I cover the main points without getting too detailed. Maybe list the limitations under different categories: environmental factors, technological constraints, economic factors, and reliability issues. Also, mention specific examples for each limitation to make it clearer.
</think>

Energy harvesting from natural sources, while promising for sustainable energy, faces several limitations across environmental, technological, economic, and reliability dimensions. Here's a structured overview of the key challenges:

### 1. **Environmental and Natural Constraints**
   - **Intermittency and Unpredictability**: 
     - Solar and wind energy depend on weather conditions (e.g., cloud cover, wind speed), leading to inconsistent power generation.
     - Geothermal and hydro sources are location-specific and may not be viable in all regions.
   - **Low Energy Density**: 
     - Sources like kinetic or RF energy harvesters produce minimal power, requiring extensive infrastructure to collect usable energy.
     - Thermal gradients (e.g., thermoelectric generators) often yield low efficiency due to small temperature differences.
   - **Resource Availability**: 
     - Geothermal and hydroelectric resources are limited to specific geographic areas, necessitating costly infrastructure (e.g., drilling, dams).

### 2. **Technological Limitations**
   - **Conversion Efficiency**: 
     - Solar panels typically convert only 15–22% of sunlight to electricity, while wind turbines achieve 30–50% efficiency, leaving significant energy unutilized.
     - Kinetic and RF harvesting suffer from low efficiency due to material and design constraints.
   - **Material and Design Challenges**: 
     - Degradation of materials (e.g., solar panel degradation over time) reduces long-term performance.
     - Piezoelectric and thermoelectric devices face limitations in scalability and durability.

### 3. **Economic and Practical Factors**
   - **High Initial Costs**: 
     - Manufacturing and installing energy harvesting systems (e.g., solar panels, wind turbines) require significant investment.
     - Maintenance and replacement costs add to long-term expenses.
   - **Storage and Grid Integration**: 
     - Intermittent sources require costly energy storage solutions (e.g., batteries) or grid upgrades to balance supply and demand.
     - Retrofitting existing infrastructure for decentralized harvesting systems can be complex and expensive.

### 4. **Environmental and Social Impact**
   - **Ecological Disruption**: 
     - Wind turbines and solar farms may disrupt local ecosystems, while geothermal drilling risks subsurface instability.
     - Large-scale projects can lead to habitat loss or pollution (e.g., toxic materials in solar panel production).
   - **Community Opposition**: 
     - Aesthetic concerns (e.g., wind turbines) or noise pollution may lead to public resistance.
     - Land use conflicts arise between energy projects and agricultural or residential areas.

### 5. **Scalability and Reliability**
   - **Limited Scalability**: 
     - Small-scale applications (e.g., IoT devices) are more feasible, but large-scale adoption remains challenging due to technical and economic barriers.
   - **Dependence on Backup Systems**: 
     - Energy harvesting alone cannot replace traditional grids, necessitating hybrid systems with backup power sources.

### 6. **Regulatory and Interference Issues**
   - **RF Energy Harvesting**: 
     - Potential interference with communication systems and regulatory hurdles for deploying RF harvesting devices.
   - **Policy and Standards**: 
     - Lack of standardized regulations for energy harvesting technologies can hinder widespread adoption.

### Conclusion
While energy harvesting from natural sources offers a sustainable alternative, its limitations highlight the need for complementary technologies (e.g., storage, smart grids) and careful planning to address environmental, economic, and technical challenges. Innovations in materials, efficiency, and integration strategies are critical to overcoming these barriers.
```

### 2. qwen3-8b 奖励模型测试

多个答案

输入

```
question_content = "What does a correlator do in direct-sequence transmission?"

candidate_answers = [
    "Applies spreading to the RF carrier",
    "Flattens the amplitude of the signal",
    "Recovers the original signal from the spread RF carrier",
    "Spreads out noise across the frequency band",
    "Monitors the wide frequency band for changes"
]
```

输出正确

````python
Evaluating answers (Judge Mode)...
======================================== Model Evaluation ========================================
<think>
Okay, let's tackle this question about what a correlator does in direct-sequence transmission. First, I need to recall what I know about direct-sequence spread spectrum (DSSS) systems. 

In DSSS, the data signal is modulated with a pseudo-random noise (PN) code, which spreads the signal over a wider bandwidth. This process is called spreading. The correlator's role is crucial here. When the receiver gets the spread signal, it needs to reverse the spreading process to recover the original data. 

So, the correlator essentially correlates the received signal with the same PN code used during transmission. This correlation process helps in despreading the signal, which means it narrows the bandwidth back to the original data rate. 

Now, looking at the options:

Option 1: Applies spreading to the RF carrier. Wait, that sounds like the spreading process, which is done by the transmitter, not the correlator. The correlator is part of the receiver, so this might be incorrect. 

Option 2: Flattens the amplitude of the signal. I don't recall anything about amplitude flattening in correlators. Maybe this is a distractor. 

Option 3: Recovers the original signal from the spread RF carrier. This aligns with what I thought earlier. The correlator's main job is to despread the signal and recover the original data. So this seems correct.

Option 4: Spreads out noise across the frequency band. Hmm, spreading noise might be a result of the spreading process, but the correlator's role is more about despeading. However, I'm not sure if this is a direct function of the correlator. Maybe this is a trick option. 

Option 5: Monitors the wide frequency band for changes. That sounds more like a function of a spectrum analyzer or a detector, not specifically the correlator. 

Now, Phase 2: Relational Cross-Examination. If Option 3 is correct (Ground Truth), then the others need to be checked against it. 

Option 1 is about spreading, which is the opposite of what the correlator does. So if the correlator is for despreading, then Option 1 is wrong. 

Option 2's amplitude flattening doesn't seem related. 

Option 4: Spreading noise might be a side effect, but the correlator's main role isn't to spread noise. The spreading is done by the transmitter. So Option 4 is incorrect. 

Option 5's monitoring function isn't the correlator's primary role. 

So, the only correct answer is Option 3. The others are incorrect based on the Ground Truth of Option 3. Therefore, the scores should be 1 for Option 3 and 0 for the rest.
</think>

```json
{
  "scores": {
    "Option 1": 0,
    "Option 2": 0,
    "Option 3": 1,
    "Option 4": 0,
    "Option 5": 0
  }
}
```
````



## 三、思路

### 1. 提示词设计

让模型先对单个回复进行打分，然后再去用他们之间的关系优化评分

> 多个不同的回复（即使它们基于同一问题生成）提供了不同的思考角度和推理过程。奖励模型可以通过对比这些回复，得到更多的信息，从而减少误判的概率。例如，如果一个回复明显不符合事实，而其他几个回复接近正确，奖励模型就能根据对比分析做出更合理的判断。
>
> 奖励模型可能会过于依赖于比较，而忽视了每个回答的独立质量。如果奖励模型主要基于多个回答之间的相对表现来做出判断，而没有足够的能力去评估每个回复的内在质量和正确性，它可能会导致过拟合，特别是在生成的回复质量参差不齐的情况下。例如，如果某些回答包含了非常模糊的内容，奖励模型可能就会错误地认为它们是“正确”的，因为它们符合其他更强的回答的模式。

这是一个非常具体且关键的数据构建问题。你现在的 JSON 数据结构是标准的“评测集格式”（Inputs + Ground Truth Label），但它还不是“SFT 训练数据格式”。

要在 SFT 中训练一个 **Generative Reward Model (GenRM)**，你需要将这个 JSON 转换为 **Prompt (输入)** + **Response (输出标签)** 的对子。

鉴于你的目标是让 RM **“判别多个正确答案”**（即 Listwise 打分，输出可能是一个类似 [1, 0, 0, 1, 0] 的判别结果），并且要有理由，我建议按照以下步骤进行数据构建。

------

### 2. 数据集构建

#### 第一步：理解数据缺口（Critical Gap）

目前只有：

- **输入材料**：问题 + 5个带思维链的选项。
- **最终裁判**："answer": "option 1"（Ground Truth）。

你缺少的是：

- **中间的分析过程（Rationale）**：即为什么 Option 1 是对的，为什么 Option 2-5 是错的。

**关键批判**：如果直接把 "option 1" 作为标签训练，RM 学不会推理。你需要用教师模型（Teacher Model）基于你的 Ground Truth，把中间的分析“补全”。

------



#### 第二步：自动化构建 SFT 数据流程

你需要写一个脚本，遍历你的 JSON 数据，调用教师模型（如 GPT-4o 或 DeepSeek-V3）来生成训练标签。

#### 1. 构造发给教师模型（Teacher）的 Prompt

我们需要让教师模型充当“数据标注员”。

**Teacher Prompt 模板：**

 code Text

downloadcontent_copy

expand_less



```
    你是一个通信领域的专家判题人。
我会给你一个问题、5个候选回复（包含思维链）以及标准答案的编号。
请你完成以下任务：
1. 仔细阅读问题和每一个候选回复。
2. 对照标准答案，分析每个回复的正确性。注意：即使某个回复不是标准答案选定的那个选项，如果它在逻辑和事实层面也是完全正确的，也应被判定为正确（这种情况较少，但需注意）。
3. 生成一段详细的点评，指出每个回复的优缺点。
4. 最后输出一个判别列表，格式为 JSON。

输入数据：
问题：{question}
标准答案指向：{answer_key} (内容是: {answer_text})

候选回复：
[回复1]: {option_1_content}
[回复2]: {option_2_content}
[回复3]: {option_3_content}
[回复4]: {option_4_content}
[回复5]: {option_5_content}

请输出分析（Analysis）和最终判定（Verdict）。
  
```

#### 2. 教师模型生成的输出（这就是你的 SFT Target）

教师模型会吐出类似这样的文本（这也是你未来 RM 应该学会输出的样子）：

 code Text

downloadcontent_copy

expand_less



```
    <analysis>
问题询问自然源能量收集的局限性。
- 回复1：指出了自然源的间歇性和不可控性（intermittent and uncontrollable），这与通信系统中对稳定电源的需求冲突，符合标准答案的逻辑。推理过程清晰。
- 回复2：讨论了电磁干扰问题。虽然这可能是个问题，但不是能量收集本身的“主要技术局限性”，属于答非所问。
- 回复3：讨论了成本和实现难度。这也是局限性之一，但在TeleQnA的标准语境下，间歇性是更本质的物理限制。判定为错误。
- 回复4：讨论了功率不足的问题。这在逻辑上是正确的，且与标准答案非常接近（能量不够支持设备）。如果有多个正确选项，此项可能也被视为正确。但在单选逻辑下，回复1的表述更贴切。
- 回复5：提到视距通信（Line-of-sight）。这是针对特定RF收集的限制，不具有普适性。
</analysis>

<verdict>
{"1": 1, "2": 0, "3": 0, "4": 0, "5": 0}
</verdict>
  
```

*(注意：在这一步，你可以人工或者用规则去检查 Teacher 的输出。比如你的 Ground Truth 说是 Option 1，如果 Teacher 说 1 和 4 都对，你可以决定是否保留这个多义性，这正是你想要的“判别多个正确答案”的能力。)*

------



#### 第三步：组装最终的 RM 训练数据

现在你有了 Input 和 Target，可以构建最终的 JSONL 数据用于 SFT 训练了。

**训练样本格式建议：**

**Input (Prompt):**

 code Text

downloadcontent_copy

expand_less



```
    <system>You are a helpful assistant acting as a Reward Model.</system>
<user>
Question: What are the limitations of energy harvesting from natural sources?

Option 1:
<think>Energy harvesting from natural sources relies on ambient inputs...</think>
<answer>Natural energy sources are intermittent and uncontrollable.</answer>

Option 2:
... (此处填入 Option 2 内容)

...

Option 5:
... (此处填入 Option 5 内容)

Please analyze the above options and identify which ones are correct.
</user>
  
```

**Output (Completion / Label):**

 code Text

downloadcontent_copy

expand_less



```
    <assistant>
Analysis:
1. Option 1 identifies the intermittent nature of natural sources... [Correct]
2. Option 2 focuses on interference... [Incorrect]
...

Verdict:
Option 1: Correct
Option 2: Incorrect
Option 3: Incorrect
Option 4: Incorrect
Option 5: Incorrect
</assistant>
  
```

------



#### 第四步：关键的数据增强策略（Data Augmentation）

为了防止模型过拟合位置（例如总认为 Option 1 是对的），你必须在构建数据时做 **Shuffle（打乱顺序）**。

在生成每一条训练数据时：

1. 
2. 读取原始 JSON 的 option 1 到 option 5。
3. **随机打乱它们的顺序**。例如，原来的 Option 1 变成了现在的位置 3。
4. 同时更新 Ground Truth。如果原来的答案是 Option 1，现在正确答案就是“位置 3”。
5. 将打乱后的文本填入 Input Prompt。
6. 让教师模型基于打乱后的顺序生成 Analysis 和 Verdict。

------



#### 针对你“判别多个正确答案”需求的特别建议

在你的原始数据中，"answer": "option 1" 暗示了这是单选题。但是在 GRPO 的训练过程中（Rollout 阶段），模型生成的 5 个回复可能：

- 
- 有两个回复完全一样（都对）。
- 有两个回复措辞不同，但意思都对。
- 全都错了。

**因此，你的 RM 训练集必须包含这些情况！** 不能只用原始的 TeleQnA 数据（它太完美了，总是1对4错）。

**如何构造“非完美”数据？**
利用你的 Teacher 模型进行 **Data Synthesis（数据合成）**：

1. 
2. **全对情况**：把 Option 1 （正确答案）复制一份，稍微改写几个词，替换掉 Option 2。现在 Option 1 和 Option 2 都是对的。训练你的 RM 识别出 {1: 1, 2: 1, 3: 0...}。
3. **全错情况**：把 Option 1 删掉，用一个错误的 Option 6 填补进来。训练你的 RM 输出 {1: 0, 2: 0, 3: 0...}。

#### 总结

你的数据构建流程应该是：

1. 
2. **Raw JSON** (Q, 5 Options, GT)
3. **Augmentation** (随机打乱顺序，随机替换选项制造多对/全错样本)
4. **Teacher Annotation** (将处理后的 Inputs + GT 发给 GPT-4，让它写 Analysis 和 Verdict)
5. **Final Formatting** (转为 SFT 的 User/Assistant 对话格式)
6. **Training** (使用 Cross-Entropy Loss 训练 RM)

这样训练出来的 RM，在推理时输入 5 个 GRPO 采样结果，就能输出一段分析，并给出一个类似 [1, 0, 0, 1, 0] 的打分向量，完美符合你的需求。



# 高质量奖励模型生成周报五

## 一、论文

### 1.SetRank: Learning a Permutation-Invariant Ranking Model for Information Retrieval

### 2. RRHF: Rank Responses to Align Language Models with Human Feedback without tears

## 二、实验

思路如图所示

![image-20251218180359102](assets/image-20251218180359102.png)

对比实验结果

| 输入模式 (Input Mode)     | 评估正确率 (Accuracy) |
| ------------------------- | --------------------- |
| 单回答输入 (Pointwise)    | 61.56%                |
| 多回答联合输入 (Listwise) | 82.29%                |

我们发现微调后的模型提升有限，需要考虑原因

| 评分设置 (Setting)                              | 评估正确率 (Accuracy) |
| ----------------------------------------------- | --------------------- |
| 基座 Qwen3-8B + 单回答输入评分 (Pointwise)      | 56.81%                |
| 基座 Qwen3-8B + 多回答联合输入评分 (Listwise)   | 77.70%                |
| 微调后 Qwen3-8B + 多回答联合输入评分 (Listwise) | 77.85%                |

微调情况

![image-20251218183720053](assets/image-20251218183720053.png)

# 高质量奖励模型生成周报六

## 一、论文

RRHF: Rank Responses to Align Language Models with Human Feedback without tears



## 二、实验

### 1. qwen/gemini 打分准确率评价

| Model                  | Accuracy |
| ---------------------- | -------- |
| Qwen3-8B               | 56.82%   |
| Qwen3-80B              | 62.86%   |
| Gemini-2.5-flash       | 68%      |
| Gemini-3-flash-preview | 70%      |

打分提示词如下

系统提示词

```python
SYSTEM_PROMPT = """You are an uncompromising Technical Fact-Checker for Telecommunications.

**Objective:**
Evaluate a SINGLE answer to a question based ONLY on **Factual Accuracy**.

**Strict Constraints:**
1. **Accuracy is the ONLY metric.** Do not deduct points for brevity, tone, or formatting. If a statement is factually true, it is Correct (1). If it contains ANY technical falsehood, physics violation, or logical contradiction, it is Incorrect (0).
2. **Binary Judgment.** Output either 1 (Correct) or 0 (Incorrect). There is no middle ground.

**Scoring Standard:**
- **1 (Correct):** Scientifically and technically accurate.
- **0 (Incorrect):** Contains errors, misconceptions, or is factually wrong.
"""
```

用户提示词

````python
#### Question
{question_content}

#### Answer to Evaluate
{option_text}

#### Evaluation Task
Please evaluate whether this answer is factually correct for the given question.

**Your Analysis:**
- Is this answer scientifically and technically accurate?
- Does it contain any factual errors, misconceptions, or logical contradictions?

**Output Format:**
Please provide your verdict in the following format:
```json
{{
  "score": 0 or 1
}}
```

Where:
- **1** means the answer is Correct (factually accurate)
- **0** means the answer is Incorrect (contains errors or is factually wrong)
"""
````

### 2. 使用gemini 蒸馏结果

使用2000条问题，共8000左右回答打分进行蒸馏

| Model                       | Accuracy |
| --------------------------- | -------- |
| gemini3-distillded-qwen3-8B | 69.10%   |
| gemini3-distilled-qwen3-32B | 70.45%   |

### 3. 之前尝试的问题

之前尝试的 listwise 方法，经过数据清洗验证后，发现其仅在单选题能力增强

分析它做对的题目，发现它经常会出现某某回答正确，所以另一个回答不正确的情况，没有给出为什么不对的理由。所以如果一组输入，模型可能偏向于对最确定的答案给正确，所以其单选得分率搞

进一步数据增强验证发现，正确率仅有4%的提升，所以猜测效果并没有特别理想

这个方法目前暂时停止

## 三、华为提供的可以尝试的方向

对于更通用的 QA 对，怎么去生成多个干扰项，去生成多个相似的答案，生成一套类似的多选题或者单选题去蒸馏模型

能否根据上面多组输入

可以本地化部署 deepseek 或者 qwen 去做推理 阿里云跳板？

我们只输入答案，不输入中间思考轨迹

工作流

根据QA对生成多个答案（主动学习），主要是错误答案，也生成正确答案

把生成的答案和打分给教师模型生成打分轨迹，用Oracle视角

将输出和打分轨迹蒸馏给奖励模型

将奖励模型接入到verl进行训练

# 高质量奖励模型生成周报七

## 一、论文

**D-GEN: Automatic Distractor Generation and Evaluation for Reliable Assessment of Generative Models**

关于高质量错误回答生成的论文

## 二、思路设计

目前的设计大概由三个部分组成

1. 数据集的构建：由于 TeleQnA 为选择题数据集，所以由多个答案构成，可以直接用来训练奖励模型。但是对于一般数据集，由Q&A对构成，只有正确答案，没法非常好的训练奖励模型，所以**需要构造一些负样本**，以及少量的正样本。这部分需要研究创新，目前在读论文的阶段
2. 奖励模型的训练：我们将上述构造出来的类似多选题的数据集，用来训练模型。我们的策略是让 $DeepSeek-V3.2$ 作为教师模型，然后训练学生模型 $Qwen3-8B$ 来提升效果。其中为了提升训练的质量，我们的策略是让教师模型先根据问题和答案进行评分，如果评分不正确，那么我们会让它进行**二轮评分，并告诉它上轮评分错误**。为了减少偏差，**我们的评分仅设置0和1**，其中0表示回答错误，1表示回答正确。我们把评分的思考轨迹用来蒸馏小模型，以提升学生模型的打分能力。
3. 强化学习的训练：对于训练出来的奖励模型，我们进行强化学习训练。我们尝试两种方案：第一种是对于一个问题生成的几个答案，分别送给奖励模型进行打分，即 pointwise 方法；第二种是**对于一个问题生成的几个答案，我们一起送入奖励模型，即 listwise 方法**，这个方法在之前验证集是被验证有效，但是因为验证集不够强，无法说明其效果到底多好，所以需要接入强化学习训练。

总结，我们的思路主要可以概括为  **Group is better than one** ，主要想法就是通过丰富模型的输入来提高模型的效果。

其中，中间阶段**奖励模型的评价准确率**我们的思路如下：
我们对于奖励模型的评价准确率，采用的方法是利用测试集部分的数据，一个问题和多个答案，然后正确答案我们就认为打分应该是1，错误答案打分应该是0。我们的准确率设置为 $Acc=\frac{打分正确的选项个数}{全部的选项个数}$ 。

我们的数据集划分策略如下：

对于整体数据集，我们采用八二分的方法，将其分为训练集和测试集

对于训练集，我们再次采用八二分的方法，分为奖励模型训练数据集和强化学习训练数据集。其中奖励模型训练的数据集同时用作基座模型 $Qwen3-8B$ 的 $SFT$ 数据集。我们将经过微调的基座模型接入强化学习中作为 actor 和 refer，将训练好的奖励模型接入强化学习种作为 reward。

对于测试集，我们可以同时用来中间阶段测试奖励模型的效果，以及最后强化学习后训练的基座模型的效果

**核心思路流程图**

![image-20251231190240541](assets/image-20251231190240541.png)

## 三、实验进展

### 1. DeepSeek-v3.2 蒸馏后奖励模型的评价准确率

$DeepSeek$ 评价准确率未测试（之前测过 $Gemini-3-flash-preveiw$ 的准确率为 70%）、多组答案一起输入奖励模型后得到的训练模型评价准确率未测试（测试集不够强，可能导致该模型准确率很高）。

目前准确率已接近 80%，而且是最后一轮的 checkpoint，前面的checkpoint 有 val_loss 更低的时候

| Model                          | Accuracy |
| ------------------------------ | -------- |
| qwen3-8B                       | 56%      |
| deepseek-V3.2-distill-qwen3-8b | 78%      |

### 2. 训练过程的主要曲线图

#### 2.1 当单点输入训练时，奖励模型曲线图

对于该模型的训练，我们采用的核心参数如下：
```yaml
per_device_train_batch_size: 2
learning_rate: 1.0e-4
num_train_epochs: 3.0
lr_scheduler_type: cosine
```

![image-20251231182240347](assets/image-20251231182240347.png)

#### 2.2 当多点训练时奖励模型的曲线图

对于该模型的训练，我们采用的核心参数如下：

```yaml
per_device_train_batch_size: 2
learning_rate: 1.0e-4
num_train_epochs: 3.0
lr_scheduler_type: cosine
```

![image-20251231182419666](assets/image-20251231182419666.png)

#### 2.3 经过监督微调的基座模型的曲线图

对于该模型的训练，我们采用的核心参数如下：

```yaml
per_device_train_batch_size: 2
learning_rate: 1.0e-4
num_train_epochs: 3.0
lr_scheduler_type: cosine
```

![image-20251231182356226](assets/image-20251231182356226.png)

#### 2.4 进行强化学习的奖励变化图（还未跑完全部）

我们设置的参数如下：由于受到服务器资源限制，batch_size 设为1，采样次数设置为 4
```bash
data.max_prompt_length=2048 \
data.max_response_length=512 \
data.train_batch_size=1 \
actor_rollout_ref.rollout.n=4 \
trainer.total_epochs=10 \
```

由于我们的 batch_size 设置为1，所以我在返回的模型中多加了一个每八步取一个平均奖励的操作

> 中间断是因为服务器资源问题

![image-20251231182501619](assets/image-20251231182501619.png)

![image-20251231182512099](assets/image-20251231182512099.png)

## 四、问题

### 1. 对于监督微调，我们 epoch 设置为 3 的话，发现 val-loss 变大了，那是不是我们需要采用的时 epoch 为 2附近的 checkpoint？

### 2. 对于强化学习训练，batch_size 设置为 1 是否可行？因为设置为 2 就有时候会遇到 OOM 现象

### 3. 对于整体流程的数据集划分是否合理？我们对于训练集也进行八二分，分别用于奖励模型的训练（包括基座模型的训练）和强化学习的训练

### 4. 对于强化学习和监督微调，epoch 和 train_batch_size 有什么最佳实践的参数设置吗？ 

 

# 高质量奖励模型生成周报八

## 一、论文

## 二、实验

### 1.实验结果

我们在 TeleQnA 测试集上的评分

> 我们的评测方法是 使用 lm-eval-harness 框架，然后选择题评分，即生成每个选项的概率

qwen3-8b-multi-choice 是指全部使用选择题微调，格式如下

```json
{
    "instruction": "You are an expert telecommunications engineer. Answer the following question accurately and concisely.",
    "input": "What does GERAN stand for?\n\nOptions:\noption 1: Geographical Routing and Network\noption 2: Group Call Service and Coverage\noption 3: GSM/EDGE Radio Access Network\noption 4: GPRS MS and Session\noption 5: Group Controller and Session",
    "output": "The correct answer is option 3: GSM/EDGE Radio Access Network"
}
```

qwen3-8b-QA 是指全部用问答题微调，格式如下

```json
{
    "instruction": "You are an expert in 3GPP standards and telecommunications. Answer the following technical question accurately and concisely based on 3GPP specifications.",
    "input": "Which scenario is characterized by high requirements on the communications system regarding communication service availability? [3GPP Release 17]",
    "output": "Automation for electricity distribution and smart grid"
}
```

qwen3-8b-multi-choice30%+QA70% 是指采用 30% 的选择题和 70% 的问答题进行微调

| Model                          | accuracy |
| ------------------------------ | -------- |
| qwen3-8b                       | 71.4%    |
| qwen3-8b-multi-choice          | 49.9%    |
| qwen3-8b-multi-choice30%+QA70% | 50.1%    |
| qwen3-8b-QA                    | 25.8%    |
| qwen3-8b-QA-rl                 | 25.1%    |

评测的配置文件如下

**gpt4-evaluation**

```yaml
task: teleqna
dataset_path: json
dataset_kwargs:
  data_files:
    test: /mnt/public/wwj/zhaozq/exp1/lm-evaluation-harness/lm_eval/tasks/teleqna/teleqna_test.jsonl

output_type: multiple_choice
test_split: test

doc_to_text: >
  You are an expert telecommunications engineer. Answer the following question accurately and concisely.
  
  Question: {{question}}

  Options:
  {% for c in choices %}
  option {{ loop.index }}: {{ c }}
  {% endfor %}

  The correct answer is option 

# --- 核心修复开始 ---
# 旧写法（报错）："{{ range(1, choices|length + 1) | list }}" -> 生成 [1, 2, 3] (int)
# 新写法：手动构造字符串列表字符串 -> 生成 ["1", "2", "3"] (string)
doc_to_choice: >-
  [{% for i in range(1, choices|length + 1) %}"{{i}}"{% if not loop.last %}, {% endif %}{% endfor %}]
# --- 核心修复结束 ---

# 这里保持不变，只要 target 是整数索引（如 1），框架会自动去上面生成的列表中取第 1 个元素（即 "2"）
doc_to_target: "{{target}}"

metric_list:
  - metric: acc
    aggregation: mean
    higher_is_better: true

metadata:
  version: 3.0
```

## 三、问题

**核心问题：如何进行评测？**

对于任务要求而言，主要是奖励模型的**设计和评测**。设计部分我们的奖励模型设计思想就是**根据QA对制造多选题**，然后进行**模型蒸馏**，**干扰项**这个我目前还在研究论文。我们的评价策略，之前周会讲过，就是我们**只打0分或者1分**，然后根据这个**选项实际是正确还是错误**判断评分准不准确，周会上和各位老师讨论过应该认为这个评价方法没什么明显问题。

TeleQnA本身就是选择题直接跳过了第一个步骤，这个方法应该是有效的，**对于 TeleQnA 打分准确率已经基本接近80%**。所以奖励模型的设计和评价我们大概是这样进行的，设计部分还差数据集处理这个前置步骤正在研究。

然后在各位老师周会的建议下。我们还在进行的另外一个工作是把奖励模型接入**强化学习训练**，来验证一下真实的效果。我们的实验思路是先对 qwen3-8b 进行全参微调，然后进行强化学习训练。微调和强化学习比例大概是5：1。就是因为我们第一步就是将数据集变成multi-choice，所以我们评价自然也准备采用选择题答对的比例来评判，这个标准首先应该是比较客观的。因为对于QA对而言，我们给的正确答案就是answer，其他选项是我们制造的干扰项。

但是我们发现**微调后的模型不会做选择题**了，我们用lm-eval-harness框架写TeleQnA任务并评测时发现，qwen3-8b准确率为71%。但是qwen3-8b-sft只有25%。qwen3-8b-rl只有28%。然后我觉得要混入一些选择题，我把训练集部分30%还原为选择题微调后重新测试，**准确率只有50%**。而且它**对格式比较敏感**，比如我训练集输出的是The correct anwser is option X，那么我在写测试脚本的时候必须也这样写，否则准确率就会降低。

这个就是涉及一个关键问题，开放问题的准确率应该如何评估。我看了下对于数学等可验证问题，评估方法依然比较简单，只要提取\<answer> 标签就行。对于其他任务的 benchmark，我看了下评价方式主要就是使用llm-as-judge和选择题两种，但是如果使用llm-as-judge可能还需要根据问题设计裁判模型



# 高质量奖励模型生成周报九

## 一、论文与思路

### 1. 数据集扩展与生成

对于数据集扩展部分，即干扰项生成与正确项生成，目前打算采用提示词设计的方法推进。

### 2. 奖励模型的训练

根据第一步获得的结果，用 deepseek 生成打分的 cot 轨迹，然后蒸馏到 Qwen3-8b 模型上。

### 3. 奖励模型的对比

* 对比仅一个正确答案和一个错误答案的数据集去蒸馏训练生成式奖励模型，证明多个负样本有效
* 对比 RLHF 方法，证明生成式奖励模型优于判别式奖励模型

### 4. 奖励模型的评价1

我们让模型只输出0分或者1分，1代表正确，0代表错误，看看其输出的正确率

> 但是对于 RLHF 如何和我们这些奖励模型对比，还是需要思考的一个问题，因为其分数是连续的，如果我强制改成离散的会影响损失函数的设计和模型的训练效果。我认为不同训练机制的奖励模型， 难以有一个统一的标准评价，一个主要原因是分数分布规律不一致。

### 5. 奖励模型的评价2

接入强化学习训练看下效果

> 这里我想问下提升多少算好的？

### 6. 评价2 的评价方法

把问题，模型生成的答案和 gt 一起送给裁判模型，让它评价准确率，提示词设计为：

```markdown
Score Meaning:
- Score 1: The answer is CORRECT and fully addresses the question
- Score 0: The answer is INCORRECT or does not properly address the question

#### Question
{question}

#### Answer to Evaluate
{model_answer}

#### Reference Answer (Ground Truth)
{ground_truth}

**Important Evaluation Guidelines:**
1. **If you are CERTAIN** that the answer to evaluate is correct or incorrect based on your expertise, evaluate accordingly.
2. **If you are UNCERTAIN** about the correctness of the answer, you MUST refer to the reference answer (ground truth) above to make your judgment.
3. A question may have multiple correct answers, and the answer to evaluate does not need to match the reference answer exactly in wording. However, if you are uncertain, use the reference answer as the authoritative source.
4. When in doubt, prioritize the reference answer for evaluation.
```

### 7. 之前尝试过的一个创新点

#### 我们认为多个答案一起输入给奖励模型可能会提高它打分的准确率，让模型学到更多，但是目前数据集只有一个正确答案，所以奖励模型会倾向于只给其中一个答案打高分，造成了 reward hacking 现象，后续强化这个数据集后重新进行评测。目前效果都比较一般。并且奖励函数明显训不上去。

## 二、实验

### 1. 实验结果对比1

分别采用 deepseek 和我们自己的 reward model 为奖励模型，进行评测。

评测的方法为：让模型根据测试集的问题生成答案，然后把答案送给 **deepseek 直接评判**是否正确。

> 让 deepseek 直接评判的原因是，我们先测试了把答案和 gt 一起送给 deepseek 让它进行评价，发现准确率比较低，均在50%左右，经过研究打分数据集发现，一个问题可能有多个答案，直接用 gt 可能会导致一些原本正确但是和 gt 不一致的答案被判为错误。

**强化学习，监督微调的 epoch 分别设置为1，2**

| Model                | llm-as-judge     | accuracy   |
| -------------------- | ---------------- | ---------- |
| Qwen3-8b             | deepseek         | 78.81%     |
| Qwen3-8b             | reward model     | 95.43%     |
| Qwen3-8b-sft         | deepseek         | 29.99%     |
| **Qwen3-8b-rl**      | **deepseek**     | **73.95%** |
| **Qwen3-8b-rl**      | **reward model** | **88.65%** |
| Qwen3-8b-rl-listwise | deepseek         | 75.52%     |
| Qwen3-8b-sft         | deepseek         | 29.99%     |

我们发现经过**强化学习**训练的模型效果**全面下降**，我们使用 cursor 里面的 gpt5.2 对结果进行分析发现主要原因是强化学习后的模型出现幻觉。个人认为这个解释不合理。

于是进一步增加了利用我们自己奖励模型打分的方法，发现正确率**不如 Base 模型高**，这个显然不合理，因为强化学习优化的目标就是去拟合奖励模型的打分倾向。

我们进一步思考发现，主要原因是我们训练奖励模型的回答**长度都很短**，因为原来的数据集长度很短，大概每个回答都不超过 20 个token；而我们不管是用 Base 模型还是强化学习训练后的模型，都有个显著的特征就是回答很长，远远超过原来的数据集。

所以**我们认为是奖励模型训练数据和评测数据不匹配**，强化学习会倾向于输出短的答案，因为奖励模型可能内部有**对短答案打高分**的倾向。而 Base 模型会输出很多信息，然后 deepseek 只要看到和 gt 有关的信息就认为是正确的，所以 Base 模型在裁判模型的评价下分数很高。

我们发现的主要依据是：Base 模型在我们自己训练的奖励模型上表现效果超乎想象的好，因此我们认为我们的奖励模型对于这种很长的回答没有很好的鲁棒性。

因此，我们做出的调整主要是：强制要求模型**输出不超过 30 个 token 的答案**，这样才能更好的适应奖励模型的评价，训练才有意义。

> 由于资源限制，我们这部分还正处在实验当中，在资源允许的情况下，我们尽快得出结果，看一下是否有效。

同时，对于**评测方法的改进上**，我们总结出一套较为完整的评测方法。因为 deepseek 的打分准确率本身也不是很高，**直接让它评价答案可能会出现问题**。 而如果把 gt 送给模型，那么也可能会导致模型只会给和 gt 相似的答案正确，而**不会给正确但是与 gt 表述不同的答案正确**。

因此我们目前的评测方法是，把 gt 送给模型，当模型确认答案是否正确的时候，让它直接根据自己想法判断，如果**不确认的话我们给出 gt 作为参考**，让模型先看一下**这题其中一个正确答案是什么，再去打分**，我们认为这样会提高模型打分的准确率。

### 2. 实验结果对比2

下面评价标准均为客观标准，即只让模型输出 0 分或者 1 分，然后和真值对比。

RLHF 模型比较特殊，目前没想到一个评价它准确率的方法。同时由于训练出来的模型会带一个 value head，目前也没想好应该怎么使用。

| Model                                          | accuracy   |
| ---------------------------------------------- | ---------- |
| Qwen3-8b                                       | 56.82%     |
| **Qwen3-8b_distilled-deepseek_boxed**          | **79.18%** |
| Qwen3-8b_distilled-deepseek_json-score         | 77.89%     |
| Qwen3-80b                                      | 70.69%     |
| deepseek-v3.2                                  | 82.63%     |
| qwen3-235b-a22b                                | 82.08%     |
| Qwen3-8b_distilled-deepseek_boxed_tuple-anwser | 69.37%     |
| Qwen3-8b-RLHF                                  | 暂无       |

### 3. 实验结果图

#### 1. 奖励模型训练损失

第一阶段有一些下降趋势，刚开始上升我经过多轮实验发现是和 warmup 即预热步数有关系，我们刚开始设置的 warmup ratio 为0.1，模型损失上升明显，然后我修改为0.02后，模型损失有一些下降趋势。**我们认为上升主要是预热阶段模型探索导致的，在学习率下降后 loss 也随之下降** ，因此需要将 预热比例 设置低一些，在网上了解到经验值为 **0.01-0.03** 之间，即 epoch * 0.01 得到。

![image-20260115114609388](assets/image-20260115114609388.png)

![image-20260115114820125](assets/image-20260115114820125.png)

#### 2. 强化学习奖励变化曲线

虽然奖励持续上升，但是实际评测集效果不好，我们认为就是奖励模型的训练和实际使用方法不一致导致的

![image-20260115115045945](assets/image-20260115115045945.png)

![image-20260115115057892](assets/image-20260115115057892.png)

## 三、问题

在汇报过程提，主要是评测方法是否合理；生成干扰项方法是否合理；结项的具体要求。
