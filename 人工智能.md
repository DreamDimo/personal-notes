爬山伪代码

```
class Node:
    def __init__(self, state, value):
        self.state = state
        self.value = value
        self.successors = []

    def add_successor(self, successor):
        self.successors.append(successor)

def hill_climbing(problem):
    # 初始化当前节点
    current = Node(problem.initial_state(), problem.value(problem.initial_state()))

    while True:
        # 获取当前节点的邻居，选择值最高的邻居
        neighbor = max(current.successors, key=lambda x: x.value, default=None)
        
        # 如果邻居的值小于等于当前节点的值，返回当前节点的状态
        if neighbor is None or neighbor.value <= current.value:
            return current.state
        
        # 否则，更新当前节点为邻居节点
        current = neighbor

# 示例问题类
class Problem:
    def __init__(self):
        pass

    def initial_state(self):
        # 返回问题的初始状态
        return "Initial State"

    def value(self, state):
        # 返回状态的评估值
        # 例如，这里简单地返回一个整数值，实际问题会根据具体情况来计算
        return len(state)

# 测试代码
problem = Problem()
result = hill_climbing(problem)
print("Local maximum state:", result)

```

模拟退火伪代码

```
import math
import random

class Node:
    def __init__(self, state, value):
        self.state = state
        self.value = value
        self.successors = []

    def add_successor(self, successor):
        self.successors.append(successor)

def simulated_annealing(problem, schedule):
    # 初始化当前节点
    current = Node(problem.initial_state(), problem.value(problem.initial_state()))
    
    # 迭代直到温度T为0
    t = 0
    while True:
        T = schedule(t)  # 获取当前温度
        if T == 0:
            return current.state  # 当温度为0时，返回当前状态

        # 从当前节点随机选择一个邻居节点
        next_node = random.choice(current.successors)

        # 计算能量差
        delta_E = next_node.value - current.value
        
        # 如果能量差大于0，直接更新当前节点为邻居节点
        if delta_E > 0:
            current = next_node
        else:
            # 否则，根据概率接受邻居节点
            probability = math.exp(delta_E / T)
            if random.random() < probability:
                current = next_node

        t += 1  # 增加时间步

# 示例问题类
class Problem:
    def __init__(self):
        pass

    def initial_state(self):
        # 返回问题的初始状态
        return "Initial State"

    def value(self, state):
        # 返回状态的评估值
        # 例如，这里简单地返回一个整数值，实际问题会根据具体情况来计算
        return len(state)

# 示例温度调度函数
def schedule(t):
    # 温度随时间下降，假设一个简单的指数衰减调度
    initial_temp = 1000  # 初始温度
    cooling_rate = 0.99  # 冷却速率
    return initial_temp * (cooling_rate ** t)

# 测试代码
problem = Problem()
result = simulated_annealing(problem, schedule)
print("Final solution state:", result)

```

A* 算法

```
import heapq

class Node:
    def __init__(self, state, parent=None, g=0, h=0):
        self.state = state  # 当前状态
        self.parent = parent  # 父节点
        self.g = g  # 到起点的代价
        self.h = h  # 启发式估计的代价
        self.f = g + h  # f值 = g + h

    def __lt__(self, other):
        return self.f < other.f  # 用于堆排序，优先队列基于f值排序

def a_star(problem):
    start_node = Node(problem.initial_state(), None, 0, problem.heuristic(problem.initial_state()))
    
    open_list = []  # 待扩展节点
    closed_list = set()  # 已扩展节点的集合
    
    # 将起始节点添加到开放列表
    heapq.heappush(open_list, start_node)
    
    while open_list:
        # 从开放列表中取出f值最小的节点
        current_node = heapq.heappop(open_list)
        
        # 如果当前节点是目标状态，返回路径
        if problem.goal_test(current_node.state):
            return reconstruct_path(current_node)
        
        # 将当前节点添加到已扩展节点集合
        closed_list.add(current_node.state)
        
        # 获取当前节点的所有子节点
        successors = problem.expand(current_node.state)
        
        for successor in successors:
            if successor.state in closed_list:
                continue  # 如果子节点已经被扩展过，跳过
                
            g_cost = current_node.g + problem.step_cost(current_node.state, successor.state)
            h_cost = problem.heuristic(successor.state)
            successor_node = Node(successor.state, current_node, g_cost, h_cost)
            
            # 如果子节点已经在开放列表中且发现更优路径，则更新它
            if not any(node.state == successor.state and node.f <= successor_node.f for node in open_list):
                heapq.heappush(open_list, successor_node)
    
    return None  # 如果开放列表为空，说明没有找到解

def reconstruct_path(node):
    path = []
    while node:
        path.append(node.state)
        node = node.parent
    return path[::-1]  # 返回从起点到目标的路径

# 示例问题类
class Problem:
    def __init__(self):
        pass
    
    def initial_state(self):
        # 返回问题的初始状态
        return "Start State"
    
    def goal_test(self, state):
        # 判断当前状态是否为目标状态
        return state == "Goal State"
    
    def heuristic(self, state):
        # 返回当前状态到目标状态的启发式估计值
        return len(state)  # 示例：假设启发式为状态字符串的长度
    
    def step_cost(self, state1, state2):
        # 返回从状态1到状态2的实际代价
        return 1  # 示例：假设每步的代价为1
    
    def expand(self, state):
        # 返回从当前状态出发的所有后继状态
        successors = []
        if state == "Start State":
            successors = [Node("State A"), Node("State B")]
        elif state == "State A":
            successors = [Node("Goal State")]
        return successors

# 测试代码
problem = Problem()
result = a_star(problem)

if result:
    print("Path found:", result)
else:
    print("No path found.")

```

MCTS

```
# MCTS伪代码

def MCTS(root):
    while time_budget or simulation_count:
        # 选择阶段：从根节点开始选择直到遇到一个未完全扩展的叶节点
        node = select(root)
        
        # 扩展阶段：扩展一个子节点
        if node未完全扩展():
            expand(node)
        
        # 模拟阶段：从扩展的节点开始进行一次模拟
        result = simulate(node)
        
        # 回溯阶段：将模拟结果回传至路径上的所有节点
        backpropagate(node, result)
    
    return best_action(root)

def select(node):
    while node is not a leaf and node is fully expanded:
        node = best_child(node)  # 选择最优子节点
    return node

def expand(node):
    # 扩展一个新子节点
    new_node = generate_new_node(node)
    node.children.append(new_node)
    return new_node

def simulate(node):
    # 随机模拟从当前节点到终局的过程
    while not is_terminal(node):
        node = random_child(node)
    return evaluate(node)

def backpropagate(node, result):
    # 更新路径上所有节点的统计信息
    while node is not None:
        node.visits += 1
        node.wins += result
        node = node.parent

```

### 1. **模糊集的补运算（Negation / Complement）**

模糊集的补运算是基于隶属度函数 $\mu_A(x)$ 计算的。假设模糊集 $A$ 的隶属度函数为 $\mu_A(x)$，则其补集 $A'$ 的隶属度函数 $\mu_{A'}(x)$ 由下式给出：
$$
\mu_{A'}(x) = 1 - \mu_A(x)
$$

#### 计算法则：

- 若 $\mu_A(x) = 0$，则 $\mu_{A'}(x) = 1$。
- 若 $\mu_A(x) = 1$，则 $\mu_{A'}(x) = 0$。
- 介于 0 和 1 之间的值，根据上述公式线性变化。

### 2. **模糊集的并运算（Union）**

模糊集的并运算是基于两个模糊集的隶属度函数来计算的。对于两个模糊集 $A$ 和 $B$，其并集 $A \cup B$ 的隶属度函数 $\mu_{A \cup B}(x)$ 为：
$$
\mu_{A \cup B}(x) = \max(\mu_A(x), \mu_B(x))
$$

#### 计算法则：

- 并集运算选择 $x$ 在 $A$ 或 $B$ 中的隶属度最大值。
- 如果 $x$ 对应的 $\mu_A(x) = 0$ 且 $\mu_B(x) = 0$，则 $\mu_{A \cup B}(x) = 0$。
- 如果 $x$ 对应的 $\mu_A(x) = 1$ 且 $\mu_B(x) = 1$，则 $\mu_{A \cup B}(x) = 1$。

### 3. **模糊集的交运算（Intersection）**

模糊集的交运算是基于两个模糊集的隶属度函数来计算的。对于两个模糊集 $A$ 和 $B$，其交集 $A \cap B$ 的隶属度函数 $\mu_{A \cap B}(x)$ 为：
$$
\mu_{A \cap B}(x) = \min(\mu_A(x), \mu_B(x))
$$

#### 计算法则：

- 交集运算选择 $x$ 在 $A$ 和 $B$ 中的隶属度最小值。
- 如果 $x$ 对应的 $\mu_A(x) = 1$ 且 $\mu_B(x) = 1$，则 $\mu_{A \cap B}(x) = 1$。
- 如果 $x$ 对应的 $\mu_A(x) = 0$ 且 $\mu_B(x) = 0$，则 $\mu_{A \cap B}(x) = 0$。

### 4. **模糊集的乘法运算（Product）**

模糊集的乘法运算是基于两个模糊集的隶属度函数来计算的。对于两个模糊集 $A$ 和 $B$，其乘积 $A \times B$ 的隶属度函数 $\mu_{A \times B}(x)$ 为：
$$
\mu_{A \times B}(x) = \mu_A(x) \times \mu_B(x)
$$

#### 计算法则：

- 乘法运算将 $x$ 在 $A$ 和 $B$ 中的隶属度值相乘。
- 如果 $x$ 对应的 $\mu_A(x) = 0$ 或 $\mu_B(x) = 0$，则 $\mu_{A \times B}(x) = 0$。
- 如果 $x$ 对应的 $\mu_A(x) = 1$ 且 $\mu_B(x) = 1$，则 $\mu_{A \times B}(x) = 1$。

### 5. **模糊集的加法运算（Sum）**

模糊集的加法运算是基于两个模糊集的隶属度函数来计算的。对于两个模糊集 $A$ 和 $B$，其加法 $A + B$ 的隶属度函数 $\mu_{A + B}(x)$ 为：
$$
\mu_{A + B}(x) = \mu_A(x) + \mu_B(x) - \mu_A(x) \times \mu_B(x)
$$

#### 计算法则：

- 加法运算的作用是计算 $A$ 和 $B$ 在 $x$ 处的联合隶属度。
- 当 $x$ 对应的 $\mu_A(x) = 0$ 或 $\mu_B(x) = 0$ 时，结果为 $\mu_{A + B}(x) = \mu_A(x) + \mu_B(x)$。
- 当 $x$ 对应的 $\mu_A(x) = 1$ 且 $\mu_B(x) = 1$，则 $\mu_{A + B}(x) = 1$。

### 6. **模糊逻辑运算的切换法则（Implication）**

模糊逻辑中的“蕴涵”运算通常使用以下方法进行计算。对于模糊集合 $A$ 和 $B$，其蕴涵 $A \Rightarrow B$ 的隶属度函数为：
$$
\mu_{A \Rightarrow B}(x) = \min(1, 1 - \mu_A(x) + \mu_B(x))
$$
或者，使用以下形式：
$$
\mu_{A \Rightarrow B}(x) = 1 - \mu_A(x) + \min(\mu_A(x), \mu_B(x))
$$

#### 计算法则：

- 如果 $\mu_A(x) = 1$，则蕴涵结果为 $\mu_{A \Rightarrow B}(x) = \mu_B(x)$。
- 如果 $\mu_A(x) = 0$，则蕴涵结果为 $\mu_{A \Rightarrow B}(x) = 1$。

![image-20251211085947784](C:\Users\PC\AppData\Roaming\Typora\typora-user-images\image-20251211085947784.png)

![image-20251211090024291](C:\Users\PC\AppData\Roaming\Typora\typora-user-images\image-20251211090024291.png)

![image-20251211090137790](C:\Users\PC\AppData\Roaming\Typora\typora-user-images\image-20251211090137790.png)

**Q-learning算法**

就是用另一个行为策略先探索，找到一些 $max Q(s_{t+1}, a')$ 用来后面的训练更新

概述：**Q-learning** 是一种基于价值的 **强化学习** 算法，用于解决 **离策略（off-policy）** 的问题。它的目标是通过智能体与环境的交互来学习最优策略，即找到一个策略，使得在任何状态下，智能体采取某个动作时能最大化长期回报。

基本思想：Q-learning 通过不断更新每个 **状态-动作对 (s, a)** 的 **Q值**，来估计智能体在当前状态下采取某个动作后，能获得的最大长期回报。Q-learning 使用的是 **Bellman 方程** 来更新 Q 值，更新公式如下：
$$
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left( r_t + \gamma \max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t) \right)
$$

- $Q(s_t, a_t)$ 是状态 $s_t$ 下，采取动作 $a_t$ 的 Q 值。
- $\alpha$ 是学习率，控制新信息对 Q 值的影响。
- $r_t$ 是在状态 $s_t$ 采取动作 $a_t$ 后得到的即时奖励。
- $\gamma$ 是折扣因子，控制未来奖励的价值。
- $\max_{a'} Q(s_{t+1}, a')$ 是从新状态 $s_{t+1}$ 出发，选择能够最大化长期回报的动作的 Q 值。

Q-learning 的关键思想是 **最大化长期回报**，并通过不断迭代更新 Q 值，使得 Q 值趋于最优。

**步骤**

1. **初始化 Q 表**：
   初始化 Q 表，其中 Q 值通常被初始化为任意值（例如 0），或者一个较小的随机数。
2. **选择动作**：
   在每个时间步 $t$，智能体根据当前的状态 $s_t$ 选择一个动作 $a_t$。选择动作时，可以使用 **ε-贪心策略**（ε-greedy strategy）：
   - 以概率 $1 - \epsilon$ 选择具有最大 Q 值的动作（即贪心选择）。
   - 以概率 $\epsilon$ 随机选择一个动作（即探索）。
3. **执行动作并更新 Q 值**：
   执行选择的动作 $a_t$，然后根据获得的奖励 $r_t$ 和下一状态 $s_{t+1}$，使用 Q-learning 更新公式更新 Q 值。
4. **重复直到收敛**：
   重复步骤 2 和 3，直到达到预定的停止条件（例如，达到最大回合数，或者 Q 值的变化小于某个阈值）。

```python
# 初始化 Q 表为零，或小的随机值
Q = np.zeros((num_states, num_actions))

# 循环直到收敛
for episode in range(num_episodes):
    # 重置环境，获取初始状态
    state = env.reset()
    
    # 每个回合的时间步骤
    done = False
    while not done:
        # 以 ε-贪心策略选择动作
        if np.random.uniform(0, 1) < epsilon:
            action = np.random.choice(num_actions)  # 探索
        else:
            action = np.argmax(Q[state])  # 利用当前 Q 值选择最大动作

        # 执行动作并获得奖励和下一个状态
        next_state, reward, done, _ = env.step(action)
        
        # 更新 Q 值
        Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])
        
        # 更新当前状态
        state = next_state
```

##### 4.5 DQN

我们发现上面利用表格维护 $[s_t, a_t] \rightarrow q_t$ ，因为在二维地图中状态和动作的维度都比较小，所以可以，但是当维度很大的时候用表格就不合适了，所以考虑利用神经网络来拟合，即对于状态-动作对 $(s, a)$ 作为输入，神经网络的输出为 $Q_w(s, a)$ ，我们使用 MSE 作为损失函数训练，其中我们把 $r + \gamma max_{a'}Q_w(s', a')$ 作为目标值
$$
L(w) = E_{(s, a, r, s')} [(r + \gamma max_{a'}Q_w(s', a') - Q_w(s, a))^2]
$$
我们如果给上面那个损失函数求导，会遇到一个问题，「估计值」和「目标值」都是神经网络计算出来的，这个和「监督学习」是有差别的：在监督学习中，目标值是固定的标签

这会带来训练的不稳定的偏差。为了解决这个问题，我们可以「假装」目标值是固定的，也就是说，在给损失函数求梯度时，我们认为 $r + max_{a'}Q_w(s', a')$ 是个常数，而只给 $Q_w(s, a)$  求导，然后像标准梯度下降一样对当前估计值计算梯度，然后更新参数。

因为我们只计算了函数的一部分梯度，而不是整个函数的梯度，所以这种方法称为半梯度法（semi-gradient）。当然这种妥协不是没有代价的，由于忽略了目标值对参数的依赖性，这种更新方式可能导致收敛性问题。

**数据相关性**

第一个问题就是「数据相关性」。在传统的监督学习中，我们通常假设数据之间是「独立同分布」的，样本之间没有依赖性。但是，在Q-Learning这种TD学习中，数据是智能体与环境进行交互，产生一系列的「经验样本」，这些经验样本之间存在很强的「相关性」。例如，智能体连续向左移动几次，则这些经验样本在状态、动作和奖励上都非常相似。

高度相关的样本会使得模型在短时间内接触到相似的输入模式，导致模型参数更新的方向单一且不稳定。如果连续的样本都指向同一个方向的梯度，模型很容易陷入局部最优解。另外，这可能还会让算法的泛化性降低。

**经验回放**

为了解决数据相关性问题，实际中使用的DQN（Deep Q-Network）通常会使用**经验回放机制**（Experience Replay）：将智能体与环境交互的经验 $(s, a', r, s')$ 存储到一个缓冲区中，然后从缓冲区中随机采样一批经验来更新 Q 网络。

这样有什么好处呢？随机采样打破了样本之间的时间相关性，使得模型在训练时接触到的样本不再是连续的序列，而是来自不同时间点的样本，从而降低了样本之间的**时间相关性**。同时，回放缓存中存储了过去多个时间步的经验，这使得每次训练使用的样本具有更高的**多样性**，有助于模型学习到更稳健的特征。

但是 DQN 存在致命三元组问题，因此我们训练两个网络

1. **主网络（Online Network）** ：用于估计当前状态动作的Q值，并根据梯度下降进行参数更新。
2. **目标网络（Target Network）** ：用于计算TD目标，它的参数会滞后于主网络，从而提供更稳定的目标值。

```python
def train_dqn(model_name):
    env = MazeEnv()
    # 定义超参数
    num_episodes = 500
    batch_size = 32
    gamma = 0.99
    lr = 1e-3

    # epsilon 贪心相关参数
    epsilon_start = 1.0
    epsilon_end = 0.01
    epsilon_decay = 300  # 调整衰减速度

    target_update_interval = 50  # 每隔多少个 episode 同步一次目标网络
    replay_buffer_capacity = 10000

    # 创建网络
    policy_net = DQN()
    target_net = DQN()
    target_net.load_state_dict(policy_net.state_dict())
    target_net.eval()

    # 优化器
    optimizer = optim.Adam(policy_net.parameters(), lr=lr)

    # 经验回放缓冲区
    replay_buffer = ReplayBuffer(replay_buffer_capacity)

    # 记录奖励信息
    all_rewards = []

    # 训练过程
    for episode in range(num_episodes):
        state = env.reset()
        state = torch.FloatTensor(state).unsqueeze(0)
        episode_reward = 0
        done = False

        # 计算当前 epsilon
        epsilon = epsilon_end + (epsilon_start - epsilon_end) * \
            np.exp(-1. * episode / epsilon_decay)

        while not done:
            # 根据 epsilon 贪心选择动作
            if random.random() < epsilon:
                action = env.action_space.sample()
            else:
                with torch.no_grad():
                    q_values = policy_net(state)
                    action = q_values.argmax(dim=1).item()

            # 与环境进行一步交互
            next_state, reward, done, _ = env.step(action)
            next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)

            # 将 transition 存到经验回放中
            replay_buffer.push(
                state.squeeze(0).numpy(),
                action,
                reward,
                next_state_tensor.squeeze(0).numpy(),
                done
            )

            episode_reward += reward
            state = next_state_tensor

            # 每步都尝试训练（如果缓冲区够大）
            if len(replay_buffer) >= batch_size:
                # 从回放缓冲区采样
                states_b, actions_b, rewards_b, next_states_b, dones_b = replay_buffer.sample(batch_size)
                states_b = torch.FloatTensor(states_b)
                actions_b = torch.LongTensor(actions_b)
                rewards_b = torch.FloatTensor(rewards_b)
                next_states_b = torch.FloatTensor(next_states_b)
                dones_b = torch.FloatTensor(dones_b)

                # 计算 Q(s, a)
                q_values = policy_net(states_b)
                # 选出与动作对应的 Q-value
                q_values = q_values.gather(1, actions_b.unsqueeze(1)).squeeze(1)

                # 计算 Q'(s', a') 来 更新目标
                with torch.no_grad():
                    # 使用target_net来计算 max Q'(s', a')
                    next_q_values = target_net(next_states_b)
                    max_next_q_values = next_q_values.max(dim=1)[0]
                    # 如果结束，那么目标是 reward；否则是 reward + gamma * max Q'(s', a')
                    target_q_values = rewards_b + gamma * (1 - dones_b) * max_next_q_values

                # 计算损失
                loss = nn.MSELoss()(q_values, target_q_values)

                # 反向传播和更新
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

        all_rewards.append(episode_reward)

        # 每隔一段时间更新目标网络
        if (episode + 1) % target_update_interval == 0:
            target_net.load_state_dict(policy_net.state_dict())

        # 打印训练信息
        print(f"Episode {episode+1}, Epsilon: {epsilon:.3f}, Reward: {episode_reward}")

    # 保存训练好的网络
    torch.save(policy_net.state_dict(), model_name)
    print(f"model saved: {model_name}")

    return all_rewards
```

### **上采样（Upsampling）**

上采样是将数据的采样率或分辨率增加的过程。通常，这意味着将更多的样本插入到原始数据之间，从而提高数据的细节度或分辨率。

#### **常见应用**：

1. **图像处理**：在图像的分辨率较低时，使用上采样技术来提高图像的分辨率。常用的上采样方法包括双线性插值、双三次插值等。
2. **信号处理**：在某些情况下，我们希望将信号的采样率提高，以便进行更精细的分析。
3. **机器学习中的数据扩充**：在处理不平衡数据集时，通常使用上采样技术增加某些类别的样本数目。

#### **上采样方法**：

1. **插值法（Interpolation）**：最常见的上采样方法是插值。插值的目标是在现有的数据点之间插入新的数据点，使数据的采样率增加。常见的插值方法有：
   - **线性插值**：通过两点之间的直线关系来推算中间点的值。
   - **多项式插值**：使用多项式公式来拟合数据点之间的关系。
   - **样条插值**：利用分段多项式来进行平滑的插值，常用于高阶插值。
2. **零插值（Zero Padding）**：在采样点之间插入零值，这种方法通常在音频处理和信号处理时使用。

#### **例子（图像上采样）**：

假设我们有一张小尺寸的图像，我们想将其上采样到更大的尺寸。可以使用线性插值来估算新增的像素值。

### **下采样（Downsampling）**

下采样是将数据的采样率或分辨率减少的过程。通常，下采样意味着丢弃一部分数据，以减少数据的尺寸或简化数据的结构。

#### **常见应用**：

1. **图像处理**：将高分辨率图像转换为低分辨率图像。常用于减少图像的存储空间，或在图像分析中减少计算量。
2. **音频处理**：在音频压缩或格式转换时，降低采样率，以减少音频文件的大小。
3. **机器学习中的数据预处理**：当数据集过大时，可以通过下采样减少数据量，加速训练过程。

#### **下采样方法**：

1. **平均值下采样**：通过对一定窗口内的数据求平均值来减少数据点。例如，在图像下采样中，可以通过将邻近的像素值平均来替代原来的像素。
2. **随机选择**：从原始数据中随机选择部分样本来作为下采样后的数据点。这种方法常用于在处理不平衡数据集时，减少数据量。
3. **最大值/最小值下采样**：在某些应用中，我们可能选择窗口中的最大值或最小值作为下采样后的数据。

