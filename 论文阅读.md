# è®ºæ–‡é˜…è¯»

## ä¸€ã€RLPR: EXTRAPOLATING RLVR TO GENERAL DOMAINS WITHOUT VERIFIERS

### 1. æ ¸å¿ƒæ–¹æ³•

#### 1.1 æ¦‚ç‡å¥–åŠ±

ç”±äº LLM ç”Ÿæˆæ­£ç¡®ç­”æ¡ˆçš„å†…åœ¨æ¦‚ç‡ç›´æ¥è¡¨æ˜å…¶å¯¹æ¨ç†è´¨é‡çš„å†…éƒ¨è¯„ä¼°ï¼Œæˆ‘ä»¬ä½¿ç”¨å‚è€ƒç­”æ¡ˆçš„æ¯ä¸ª token çš„è§£ç æ¦‚ç‡ä½œä¸ºå¥–åŠ±ä¿¡å·ã€‚

å¯¹äºä¸€ä¸ªé—®é¢˜ Qï¼Œæˆ‘ä»¬å…ˆè®©ç­–ç•¥æ¨¡å‹ç”Ÿæˆä¸€ä¸ªç­”æ¡ˆï¼ŒæŠŠå®ƒçš„å›ç­”è§†ä¸º $o = (o_0, o_1, â€¦â€¦,0_N)$ ï¼Œ$o_i$ æ˜¯å›ç­”ä¸­ç‹¬ç«‹çš„tokenã€‚ä¸ºäº†è·å–æ¦‚ç‡ï¼Œæˆ‘ä»¬å…ˆæŠŠç”Ÿæˆç­”æ¡ˆä¸­çš„ $y$ ä»å®Œæ•´å›ç­”ä¸­æ‹¿å‡ºæ¥ï¼Œç„¶ååªä¿ç•™æ¨ç† $z$ã€‚æˆ‘ä»¬ä¹‹åæ„å»ºä¸€ä¸ªä¿®æ”¹çš„å›ç­” $o'=(o_0',o_1',â€¦â€¦,o_N')$ ï¼Œè¿™ä¸ªå›ç­”æ˜¯é€šè¿‡æŠŠç”Ÿæˆçš„ç­”æ¡ˆæ¢æˆè®­ç»ƒé›†ä¸­çš„ç­”æ¡ˆï¼Œå³ground truthã€‚è¿™ä¸ªåºåˆ—è¢«å–‚åˆ°ç­–ç•¥æ¨¡å‹æ¥ç”Ÿæˆæ¦‚ç‡ $(p_0, â€¦â€¦,p_{N'})$ ï¼Œæ¦‚ç‡å¥–åŠ±å¯ä»¥è¢«å†™ä¸º
$$
r = f_{seq}(\{p_i|o_i' \in y^*\})
$$
$f_{seq}$ æ˜¯å°†æ¯ä¸ª token çš„æ¦‚ç‡èšåˆä¸ºä¸€ä¸ªå¥–åŠ±æ ‡é‡ã€‚å¦‚æœæˆ‘ä»¬ä½¿ç”¨ $f_{\text{seq}} = \sqrt[n]{\prod_{i=1}^n p_i}$ ï¼Œå®ƒå¯¹å°æ¦‚ç‡ç‰¹åˆ«æ•æ„Ÿï¼Œæ¯”å¦‚åºåˆ—(0.01, 0.7, 0.9) å’Œ (0.05, 0.7, 0.9)ï¼Œå¥–åŠ±å·®è·å°±å¾ˆå¤§ï¼Œå³ä½¿åªæœ‰ç¬¬ä¸€ä¸ªä¸åŒã€‚å› æ­¤æˆ‘ä»¬é‡‡ç”¨ $f_{seq} = \frac{1}{|y^*|} \sum p_i$ æ¥ä½œä¸ºèšåˆæ–¹æ³•

#### 1.2 å¥–åŠ±åå·®

å¥–åŠ±å¯èƒ½ç”±ä¸¤éƒ¨åˆ†ç»„æˆ $U_r = U_z + U_{others}$ ï¼Œ$U_z$ ä»£è¡¨äº†æ¨ç†å†…å®¹çš„æ•ˆæœï¼Œ$U_{others}$ æ˜¯å…¶ä»–çš„ä¿¡æ¯ï¼Œæ‰€ä»¥è¿™ä¸ªæ½œåœ¨çš„ä¿¡æ¯å¯èƒ½ä¼šå½±å“æ¨¡å‹çš„æ­£ç¡®è¾“å‡ºï¼Œä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå¯ä»¥ç”¨åˆ°ä¸‹é¢çš„æ–¹æ³•

æˆ‘ä»¬å°†é—®é¢˜ $Q$ è¾“å…¥åˆ°å¤§æ¨¡å‹ä¸­ï¼Œè®©å®ƒç›´æ¥ç»™å‡ºç­”æ¡ˆï¼Œæˆ‘ä»¬è®¡ç®—ç›´æ¥ç»™å‡ºç­”æ¡ˆçš„æ¦‚ç‡ï¼Œç„¶åå¾—åˆ°å¥–åŠ± $r'$ï¼Œæœ€ç»ˆæˆ‘ä»¬çš„å¥–åŠ±å°±æ˜¯ 
$$
\hat r = clip(0,1, r-r')
$$
æœ€ç»ˆæˆ‘ä»¬å¯ä»¥å¾—åˆ°æ¢¯åº¦ä¸‹é™å…¬å¼ï¼ˆGRPOï¼‰
$$
\begin{align}
\nabla J_{\mathrm{RLPR}}(\theta)
    &= \nabla \mathbb{E}_{o \sim \pi_\theta(\cdot \mid x)}[\hat r] \\
    &= \sum_{o} \hat r \,\pi_\theta(o \mid x)\, \nabla \log \pi_\theta(o \mid x) \\
    &= \mathbb{E}_{o \sim \pi_\theta(\cdot \mid x)}
       \big[ \hat r \,\nabla \log \pi_\theta(o \mid x) \big] \,,
\end{align}
$$

#### 1.3 æ ‡å‡†å·®è¿‡æ»¤

å› ä¸ºåœ¨å®é™…æ“ä½œä¸­ä¸€äº›promptè¿‡äºç®€å•æˆ–è€…è¿‡éš¾ï¼Œæ¨¡å‹èƒ½å­¦åˆ°çš„ä¿¡æ¯é‡å¾ˆå°‘ï¼Œæ‰€ä»¥éœ€è¦è¿‡æ»¤è¿™éƒ¨åˆ†promptï¼Œæ¥æé«˜è®­ç»ƒæ•ˆç‡ï¼ŒRLPR é‡‡ç”¨å¥–åŠ±æ ‡å‡†å·®ä½œä¸ºè¿‡æ»¤çš„ç­–ç•¥ã€‚

1. **ä¼ ç»Ÿ RLVR é‡Œçš„ accuracy filtering åœ¨å¹²å˜›ï¼Ÿ**

> Existing RLVR methods employ accuracy filtering â€¦ by excluding too difficult and too easy prompts. Typically, this involves filtering entirely correct or incorrect prompts.

åœ¨ **RLVRï¼ˆ0/1 å¥–åŠ±ï¼‰** é‡Œï¼š

- å¯¹åŒä¸€ä¸ª promptï¼ˆé¢˜ç›®ï¼‰ï¼Œä½ ä¼šé‡‡æ ·å¤šä¸ªå›ç­”ï¼›
- å¦‚æœè¿™ä¸ª prompt å¯¹æ¨¡å‹æ¥è¯´ï¼š
  - **å¤ªå®¹æ˜“**ï¼šå‡ ä¹æ‰€æœ‰å›ç­”éƒ½æ˜¯å¯¹çš„ï¼ˆaccuracy â‰ˆ 1ï¼‰
  - **å¤ªéš¾**ï¼šå‡ ä¹æ‰€æœ‰å›ç­”éƒ½æ˜¯é”™çš„ï¼ˆaccuracy â‰ˆ 0ï¼‰

é‚£è¿™ç§ prompt å¯¹è®­ç»ƒå¸®åŠ©å¾ˆå°ï¼š

- å…¨å¯¹ â†’ æ¢¯åº¦æ–¹å‘å‡ ä¹ä¸€è‡´ã€ä¿¡æ¯å¾ˆå°‘ï¼ˆåˆå®¹æ˜“å¯¼è‡´ overfit easy caseï¼‰
- å…¨é”™ â†’ æ¨¡å‹ä¸çŸ¥é“è¯¥å¾€å“ªæ”¹ï¼ŒåªçŸ¥é“â€œä½ ç°åœ¨å¾ˆçƒ‚â€ï¼Œä½†æ²¡æœ‰åŒºåˆ†åº¦

æ‰€ä»¥ **accuracy filtering** å°±æ˜¯ï¼š

> æŠŠâ€œå®Œå…¨å…¨å¯¹â€æˆ–è€…â€œå®Œå…¨å…¨é”™â€çš„ prompt ä»è®­ç»ƒé‡Œæ’é™¤æ‰ï¼Œ
>  åªç•™ä¸‹é‚£äº›æœ‰äº›å›ç­”å¯¹ã€æœ‰äº›å›ç­”é”™çš„ prompt â€”â€”
>  è¿™äº›æ ·æœ¬çš„æ¢¯åº¦ä¿¡æ¯æ›´ä¸°å¯Œã€æ›´æœ‰æŒ‡å¯¼æ„ä¹‰ã€‚

2. **ä¸ºä»€ä¹ˆ RLPR é‡Œä¸èƒ½ç›´æ¥ç”¨è¿™ä¸ª trickï¼Ÿ**

> However, the continuous nature of PR makes it challenging to directly apply accuracy filtering since it is hard to set a universal threshold for response correctness.

RLPR ç”¨çš„æ˜¯ **probabilistic reward / reward rate**ï¼Œå¥–åŠ±æ˜¯è¿ç»­çš„ï¼Œæ¯”å¦‚ï¼š

- å¯¹ä¸€ä¸ªå›ç­”çš„å¥–åŠ±æ˜¯ï¼šé€šè¿‡çš„æµ‹è¯•æ¯”ä¾‹ 0.73
- æˆ–è€…æŸç§æ¦‚ç‡å‹ scoreï¼Œåœ¨ $[0,1]$ é‡Œè¿ç»­å˜åŒ–

è¿™æ—¶å¾ˆéš¾å®šä¹‰â€œ**å®Œå…¨å¯¹** / **å®Œå…¨é”™**â€ï¼š

- å‡ ä¹ä¸å¯èƒ½åˆšå¥½éƒ½æ˜¯ 0 æˆ– 1ï¼›
- è¯¥æ‹¿ 0.9 å½“â€œå‡ ä¹å…¨å¯¹â€å—ï¼Ÿ0.95ï¼Ÿ0.99ï¼Ÿ
- å¯¹ä¸åŒé˜¶æ®µã€ä¸åŒä»»åŠ¡ï¼Œâ€œ0.8 æ˜¯å¤ªå¥½è¿˜æ˜¯ä¸€èˆ¬â€ä¹Ÿä¸ä¸€æ ·ã€‚

æ‰€ä»¥ï¼š**æ²¡æ³•åƒ RLVR é‚£æ ·ç»™ accuracy è®¾ä¸€ä¸ªç®€å•çš„é˜ˆå€¼æ¥åšè¿‡æ»¤**ã€‚

3. **ä»–ä»¬çš„è§‚å¯Ÿï¼šç”¨â€œreward çš„æ ‡å‡†å·®â€ä»£æ›¿ accuracy**

> Through the analysis of accuracy filtering, we observe that filtering prompts with low standard deviation in reward values can effectively achieve a similar effect.

å…³é”®æ´å¯Ÿï¼š

- å¯¹åŒä¸€ä¸ª promptï¼Œä½ é‡‡æ · K ä¸ªå›ç­”ï¼Œå¾—åˆ° K ä¸ª reward $\hat r_1, \dots, \hat r_K$ï¼›
- **å¦‚æœè¿™äº› reward éƒ½å·®ä¸å¤š**ï¼Œé‚£è¿™ä¸ª prompt è¦ä¹ˆï¼š
  - æ€»æ˜¯ç»™å¾ˆé«˜åˆ†ï¼ˆæ¨¡å‹å‡ ä¹æ€»åšå¾—å¾ˆå¥½ï¼‰ï¼›æˆ–è€…
  - æ€»æ˜¯ç»™å¾ˆä½åˆ†ï¼ˆæ¨¡å‹å‡ ä¹æ€»åšå¾—å¾ˆå·®ï¼‰ï¼›æˆ–è€…
  - æ€»æ˜¯åœ¨æŸä¸ªä¸­ç­‰æ°´å¹³é™„è¿‘æŠ–åŠ¨ä½†å˜åŒ–æå°

æ€»ä¹‹ï¼Œå¯¹è®­ç»ƒæ¥è¯´ï¼Œè¿™ä¸ª prompt **å‡ ä¹ä¸åŒºåˆ†å›ç­”çš„å¥½å** â†’ ä¿¡æ¯é‡å¾ˆä½ã€‚

â€œå·®ä¸å¤šâ€åœ¨ç»Ÿè®¡å­¦é‡Œå°±æ˜¯ï¼š**æ ‡å‡†å·®ï¼ˆstdï¼‰å¾ˆå°**ã€‚

> Specifically, prompts that consistently yield all high or all low scores exhibit low standard deviation due to the boundedness of PR (i.e., all reward values lie within [0, 1]).

å› ä¸º reward è¢«é™åˆ¶åœ¨ $[0,1]$ é‡Œï¼š

- å¦‚æœéƒ½å¾ˆé«˜ï¼Œé‚£ä¸€å †å€¼éƒ½æŒ¤åœ¨ 0.9~1 é™„è¿‘ï¼›
- å¦‚æœéƒ½å¾ˆä½ï¼Œå°±æŒ¤åœ¨ 0~0.1 é™„è¿‘ï¼›

è¿™ä¸¤ç§æƒ…å†µçš„ **std éƒ½ä¼šå¾ˆå°**ã€‚
 æ‰€ä»¥ï¼š

> **â€œåˆ æ‰ std å¾ˆä½çš„ promptâ€ â‰ˆ â€œåˆ æ‰é‚£äº›æ€»æ˜¯å…¨å¥½æˆ–å…¨åçš„ promptâ€**
>  åœ¨æ•ˆæœä¸Šå’Œ RLVR çš„ accuracy filtering å¾ˆåƒã€‚

4. **ä½†ç›´æ¥ç»™ std è®¾ä¸€ä¸ªå›ºå®šé˜ˆå€¼ï¼Œä¹Ÿæœ‰é—®é¢˜**

> Meanwhile, the overall standard deviation distribution continuously shifts during training, and a fixed threshold may cause either too strict or loose filtering at different training stages.

éšç€è®­ç»ƒè¿›è¡Œï¼š

- æ¨¡å‹ä¸€å¼€å§‹å¾ˆèœï¼šå¾ˆå¤š prompt çš„ reward åˆ†å¸ƒå¯èƒ½æ¯”è¾ƒä¹± â†’ std è¾ƒå¤§ï¼›
- æ¨¡å‹è¶Šæ¥è¶Šå¼ºåï¼š
  - æœ‰äº›ç®€å•çš„ prompt å˜æˆå‡ ä¹æ€»æ˜¯é«˜åˆ† â†’ std å˜å°ï¼›
  - ä¸€äº›æéš¾ prompt ä¹Ÿå¯èƒ½ä¸€ç›´ä½åˆ† â†’ std ä¹Ÿå¾ˆå°ï¼›

æ‰€ä»¥ **â€œå…¨ä½“ prompt çš„ std åˆ†å¸ƒä¼šéšç€è®­ç»ƒä¸æ–­å˜åŒ–â€**ã€‚

å¦‚æœä½ è®¾ç½®ä¸€ä¸ªå›ºå®šé˜ˆå€¼ï¼ˆæ¯”å¦‚ std < 0.05 å°±è¿‡æ»¤ï¼‰ï¼š

- åœ¨å‰æœŸï¼š
  - å¤§å¤šæ•° prompt std éƒ½æŒºå¤§ â†’ å‡ ä¹æ²¡è¿‡æ»¤ï¼›
- åœ¨åæœŸï¼š
  - å¯èƒ½å¾ˆå¤š prompt std éƒ½å˜å° â†’ ä¸€ä¸‹å­è¿‡æ»¤æ‰å¤ªå¤šï¼Œè®­ç»ƒæ•°æ®ä¸å¤Ÿ / å¤ªåã€‚

æ‰€ä»¥ï¼š**å›ºå®šé˜ˆå€¼ä¸é€‚åº”è®­ç»ƒè¿‡ç¨‹ä¸­çš„åŠ¨æ€å˜åŒ–**ã€‚

5. **ä»–ä»¬çš„åŠæ³•ï¼šç”¨ EMA åŠ¨æ€æ›´æ–°é˜ˆå€¼ Î²**

> To address this, we adopt an exponential moving average to dynamically update the filtering threshold Î² using the average standard deviation of each training step.

åšæ³•æ˜¯ï¼š

1. åœ¨æ¯ä¸ª training stepï¼ˆæ¯”å¦‚æ¯ä¸€æ‰¹ batchï¼‰ä¸­ï¼š

   - å¯¹è¿™ä¸€æ­¥é‡Œçš„æ¯ä¸ª prompt è®¡ç®— reward çš„æ ‡å‡†å·® $\sigma_i$ï¼›
   - æ±‚å®ƒä»¬çš„å¹³å‡å€¼ $\bar\sigma_t$ï¼ˆå½“å‰ step çš„å¹³å‡ stdï¼‰ã€‚

2. ç”¨ **æŒ‡æ•°æ»‘åŠ¨å¹³å‡ (Exponential Moving Average, EMA)** æ›´æ–°é˜ˆå€¼ Î²ï¼š
   $$
   \beta_t = (1 - \alpha)\,\beta_{t-1} + \alpha\,\bar\sigma_t
   $$

   - å…¶ä¸­ $\alpha$ æ˜¯ä¸€ä¸ªå°å¸¸æ•°ï¼ˆæ¯”å¦‚ 0.01ã€0.1ï¼‰ï¼Œæ§åˆ¶æ›´æ–°é€Ÿåº¦ï¼›
   - è¿™æ · Î² ä¼šè·Ÿç€â€œå½“å‰éš¾åº¦åˆ†å¸ƒâ€æ…¢æ…¢ç§»åŠ¨ï¼Œè€Œä¸æ˜¯çªç„¶è·³å˜ã€‚

> By filtering the prompts whose reward standard deviation is less than Î²â€¦

ç„¶åï¼Œåœ¨ step tï¼š

- å¯¹äº std $\sigma_i < \beta_t$ çš„ promptï¼Œç›´æ¥è¿‡æ»¤æ‰ï¼›
- åªä¿ç•™ $\sigma_i \ge \beta_t$ çš„ prompt æ¥ç®— RL çš„æŸå¤± /æ¢¯åº¦ã€‚

ç›´è§‚ç†è§£ï¼š

> å§‹ç»ˆåªç”¨é‚£äº›â€œreward æ³¢åŠ¨æ¯”è¾ƒå¤§â€çš„ prompt è®­ç»ƒ â€”â€”
>  ä¹Ÿå°±æ˜¯æ¨¡å‹åœ¨è¿™äº›é¢˜ä¸Šæ—¶å¥½æ—¶åï¼ŒåŒºåˆ†åº¦é«˜ï¼Œ**æ›´æœ‰å­¦ä¹ ä»·å€¼**ã€‚

6. **ä¸ºä»€ä¹ˆè¯´è¿™æ˜¯â€œè‡ªé€‚åº” curriculum learningâ€ï¼Ÿ**

> â€¦ we introduce an adaptive curriculum learning mechanism to improve both the training stability and final performance.

**Curriculum learning** çš„æ ¸å¿ƒæ€æƒ³ï¼š

> ä¸€å¼€å§‹å¤šç»ƒç®€å• / ä¿¡æ¯é‡é€‚ä¸­ / ä¸å¤ªæç«¯çš„æ ·æœ¬ï¼Œ
>  ç„¶åéšç€æ¨¡å‹å˜å¼ºï¼Œè‡ªåŠ¨æŠŠæ³¨æ„åŠ›è½¬ç§»åˆ°æ›´æœ‰æŒ‘æˆ˜æ€§çš„æ ·æœ¬ä¸Šã€‚

åœ¨è¿™é‡Œï¼š

- é€šè¿‡ std è¿‡æ»¤ï¼Œæ¨¡å‹åªåœ¨â€œåŒºåˆ†åº¦é«˜â€çš„ prompt ä¸Šå­¦ä¹ ï¼›

- éšç€è®­ç»ƒè¿›å±•ï¼ŒÎ² é€šè¿‡ EMA åŠ¨æ€å˜åŒ–ï¼š

  - å½“æ•´ä½“å˜å¼ºæ—¶ï¼š
    - ç®€å• prompt çš„ std ä¼šå˜å¾—ç‰¹åˆ«å° â†’ ä¼šè¢«è¿‡æ»¤æ‰
    - è®­ç»ƒè‡ªåŠ¨é›†ä¸­åœ¨é‚£äº›è¿˜æ²¡å®Œå…¨æŒæ¡ã€è¡¨ç°ä¸Šä¸‹æ³¢åŠ¨çš„ prompt ä¸Š

- è¿™å°±ç›¸å½“äºä¸€ä¸ª **è‡ªé€‚åº”çš„â€œå‡ºé¢˜è€å¸ˆâ€**ï¼š

  > ä¸ä¼šæ°¸è¿œè®©ä½ åˆ·æœ€ç®€å•çš„é¢˜ï¼Œ
  >  ä¹Ÿä¸ä¼šä¸€ç›´ç»™ä½ åˆ·ä½ å®Œå…¨åšä¸æ¥çš„é¢˜ï¼Œ
  >  ä¼šæ ¹æ®ä½ å½“å‰æ°´å¹³ï¼Œè‡ªåŠ¨æŒ‘â€œåˆšå¥½æœ‰ä¿¡æ¯é‡çš„é¢˜â€ç»™ä½ ç»ƒã€‚

è¿™å°±æ˜¯â€œadaptive curriculum learning mechanismâ€çš„å«ä¹‰ã€‚

åŒæ—¶ï¼Œè¿™æ ·åšè¿˜æœ‰ä¸¤ä¸ªå¥½å¤„ï¼š

1. **è®­ç»ƒæ›´ç¨³å®š**ï¼š
   - é¿å…å¤§é‡â€œå…¨ 1 / å…¨ 0 ç”šè‡³ reward å‡ ä¹ä¸å˜â€çš„ prompt è´¡çŒ®è¿‘ä¹å†—ä½™æˆ–é«˜æ–¹å·®çš„æ¢¯åº¦ï¼›
2. **æœ€ç»ˆæ€§èƒ½æ›´å¥½**ï¼š
   - æ¨¡å‹èµ„æºé›†ä¸­ç”¨åœ¨â€œè¾¹ç•ŒåŒºåŸŸâ€ï¼ˆä¼šé”™ä¼šå¯¹çš„æ ·æœ¬ï¼‰ä¸Šï¼Œ
   - æœ‰åŠ©äºçœŸæ­£æé«˜å†³ç­–è¾¹ç•Œé™„è¿‘çš„è¡¨ç°ï¼ˆç±»ä¼¼æå‡ decision boundary çš„ç²¾åº¦ï¼‰ã€‚

### 2. å®éªŒ

#### 2.1 æ¨¡å‹

ä½œè€…ä½¿ç”¨ Qwen2.5 Gemma2 Llamma3.1 ä½œä¸ºåŸºåº§æ¨¡å‹ï¼Œä¸»è¦æ˜¯é‡‡ç”¨äº† Qwen2.5-7B-Base

#### 2.2 æ•°æ®é›†

è®­ç»ƒæ•°æ®é›†ï¼šåŒ…å«å¤šä¸ªé¢†åŸŸçš„é«˜è´¨é‡æ¨ç†é—®é¢˜ï¼Œä¸ºäº†ä¸“æ³¨äºé€šç”¨é¢†åŸŸï¼Œä½œè€…å»é™¤äº†æ•°æ®é›†ä¸­çš„æ•°å­¦æ¨ç†éƒ¨åˆ†ï¼Œå¹¶ä¸”ä½¿ç”¨ GPT-4.1 å»é™¤äº†æ•°æ®ä¸­è¿‡äºç®€å•çš„éƒ¨åˆ†ï¼Œæœ€ç»ˆè·å¾—äº† 77k ä¸ªprompts ï¼Œæ•°æ®é›†é“¾æ¥ï¼šhttps://huggingface.co/datasets/openbmb/RLPR-Train-Datasetï¼Œå®ƒæ˜¯åŸºäºhttps://huggingface.co/datasets/TIGER-Lab/WebInstruct-verified

è¯„æµ‹æ•°æ®é›†ï¼šè¯„æµ‹äº†æ•°å­¦æ•°æ®é›† Math-500ï¼ŒAIME24ï¼›é€šç”¨è¯„æµ‹æ•°æ®é›† MMLU-Proï¼ˆå¤šé€‰é¢˜æ„æˆï¼Œç”¨äºè¯„æµ‹æ¨¡å‹çš„å¤šé¢†åŸŸnegligenceï¼‰ï¼›GPQAç­‰

#### 2.3 å®éªŒç»†èŠ‚

1. **ç”¨ä»€ä¹ˆæ¡†æ¶è®­ç»ƒï¼Ÿ**

> We adopt the verl (Sheng et al., 2024) framework for efficient training.

- **verl**ï¼šä¸€ä¸ªä¸“é—¨åš RL-for-LLM çš„è®­ç»ƒæ¡†æ¶ï¼ˆç±»ä¼¼â€œç»™å¤§æ¨¡å‹åš PPO / GRPO / RLHF çš„å·¥ç¨‹è„šæ‰‹æ¶â€ï¼‰ã€‚

2. **rollout è¿‡ç¨‹ï¼šä¸€è½®â€œé‡‡æ · + æ›´æ–°â€æ˜¯æ€ä¹ˆåšçš„ï¼Ÿ**

> In each rollout step, we sample eight responses per prompt for a batch of 768 prompts using a temperature of 1, and subsequently perform 4 policy updates on the collected responses.

- **batch of 768 prompts**ï¼š
   æ¯ä¸€è½® rolloutï¼Œä¼šæ‹¿ **768 ä¸ªä¸åŒçš„ prompt** æ¥è·Ÿæ¨¡å‹äº¤äº’ã€‚
- **eight responses per prompt**ï¼š
   å¯¹æ¯ä¸ª promptï¼Œæ¨¡å‹ä¼šç”Ÿæˆ **8 ä¸ªå›ç­”**ï¼ˆè¿™å°±æ˜¯å‰é¢ä½ çœ‹åˆ°çš„ pass@kã€avg@kã€æ ‡å‡†å·®è¿‡æ»¤ç­‰çš„åŸºç¡€ï¼‰ã€‚
- **temperature = 1**ï¼š
   é‡‡æ ·æ—¶çš„æ¸©åº¦è®¾ä¸º 1ï¼Œæ„æ€æ˜¯ï¼š
  - ä¸ç”¨ greedy
  - ä¿æŒä¸€å®šéšæœºæ€§ï¼Œèƒ½æ¢ç´¢ä¸åŒçš„ç­”æ¡ˆï¼Œè€Œä¸æ˜¯åªå‡ºæœ€å¯èƒ½çš„é‚£ä¸€ä¸ªã€‚
- **éšåè¿›è¡Œ 4 æ¬¡ policy updates**ï¼š
  - å…ˆæŠŠè¿™ 768Ã—8 æ¡â€œ(prompt, response, reward)â€æ”¶é›†å¥½ï¼›
  - ç„¶ååœ¨è¿™äº›æ•°æ®ä¸Šè·‘ **4 ä¸ªæ¢¯åº¦æ›´æ–° step**ï¼ˆæ¯”å¦‚ 4 ä¸ª PPO update iterationï¼‰ã€‚
  - è¿™æ ·åšå« â€œå¤šæ¬¡åˆ©ç”¨åŒä¸€æ‰¹æ ·æœ¬ï¼ˆmultiple epochs over same batchï¼‰â€ï¼Œå¯ä»¥æé«˜ sample efficiencyã€‚

ç›´è§‚ç†è§£ï¼š

> æ¯è½®ï¼šå‡º 768 é“é¢˜ï¼Œæ¯é¢˜æƒ³ 8 ä¸ªç­”æ¡ˆï¼Œ
>  æ ¹æ®è¿™ 8 ä¸ªç­”æ¡ˆçš„ reward ç®—æ¢¯åº¦ï¼Œ
>  ç”¨è¿™ä¸€æ‰¹æ•°æ®æ›´æ–°ç­–ç•¥ 4 æ¬¡ã€‚

3. **è¿‡æ»¤çš„ Î² å‚æ•°**

> The scale Î² used for filtering is set to 0.5.

- è¿™é‡Œçš„ **Î²** æ˜¯ä»–ä»¬åœ¨å‰é¢æåˆ°çš„â€œè¿‡æ»¤æ ‡å‡†å·® / PRâ€æ—¶ç”¨åˆ°çš„ä¸€ä¸ª**å°ºåº¦å‚æ•°**ï¼ˆä½ åˆšæ‰é‚£æ®µ standard deviation filtering é‡Œæåˆ°ï¼‰ã€‚
- ç²—æš´ç†è§£ï¼š
   ğŸ‘‰ â€œè¿‡æ»¤çš„æ—¶å€™ï¼Œé˜ˆå€¼æ˜¯åŸºäºå¹³å‡ std ä¹˜ä¸Šä¸€ä¸ªæ¯”ä¾‹ Î²=0.5â€ï¼Œæ§åˆ¶è¿‡æ»¤çš„ä¸¥ä¸ä¸¥æ ¼ï¼ˆ0.5 æ˜¯ç»éªŒè®¾å®šï¼‰ã€‚

ï¼ˆå…·ä½“å…¬å¼åœ¨åŸæ–‡ä¼šæ›´æ¸…æ¥šï¼Œä½†æ„æ€æ˜¯ï¼šÎ² è°ƒèŠ‚è¿‡æ»¤å¼ºåº¦ã€‚ï¼‰

4. **PPO çš„ clip èŒƒå›´ï¼Œé˜²æ­¢ entropy collapse**

> The clip threshold in PPO loss is set to (0.8, 1.27) to prevent entropy collapse (Yu et al., 2025; Cui et al., 2025b).

- PPO é‡Œæœ‰ä¸ªé‡è¦çš„ä¸œè¥¿ï¼š**ratio clipping**ï¼š
  $$
  r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_\text{old}}(a_t|s_t)}
  $$
  è®­ç»ƒæ—¶ä¼šé™åˆ¶è¿™ä¸ª ratio ä¸è¦å˜åŒ–å¤ªå¤§ï¼Œé˜²æ­¢ç­–ç•¥ä¸€æ­¥æ›´æ–°å¾—å¤ªç‹ ã€‚

- è¿™é‡Œè®¾ç½® clip èŒƒå›´ä¸º **(0.8, 1.27)**ï¼š

  - æ„å‘³ç€ï¼šæ–°ç­–ç•¥åœ¨æŸä¸ªåŠ¨ä½œä¸Šçš„æ¦‚ç‡ï¼Œä¸èƒ½è¶…è¿‡æ—§ç­–ç•¥çš„ 1.27 å€ï¼Œä¹Ÿä¸èƒ½ä½äº 0.8 å€ï¼ˆåœ¨æŸå¤±é‡Œè¢«æˆªæ–­ï¼‰ã€‚

- **é˜²æ­¢ entropy collapse**ï¼š

  - å¦‚æœæ›´æ–°è¿‡çŒ›ï¼Œç­–ç•¥ä¼šæŠŠæ¦‚ç‡ mass å…¨å‹åˆ°å°‘æ•°åŠ¨ä½œä¸Šï¼Œå˜å¾—â€œéå¸¸ç¡®å®š / éå¸¸è´ªå¿ƒâ€ï¼Œä¹Ÿå°±æ˜¯ç†µå˜å¾—å¾ˆä½ï¼ˆcollapseï¼‰ï¼›
  - è¿™ä¼šå¯¼è‡´æ¢ç´¢ä¸è¶³ï¼Œè®­ç»ƒä¸ç¨³å®šã€‚

- æ‰€ä»¥åˆç†çš„ clip èŒƒå›´ï¼Œæ˜¯è®©ç­–ç•¥â€œæ…¢æ…¢å˜â€ï¼Œé¿å…ä¸€ä¸‹å­å˜å¾—æç«¯ç¡®å®šã€‚

5. **è¯„æµ‹æ—¶çš„æ¸©åº¦ & å¤šæ¬¡è¯„æµ‹å–å¹³å‡**

> During evaluation, we set the rollout temperature to 1. To reduce the evaluation variance, we evaluate the model on each benchmark multiple times and report the final Avg@k results.

- **è¯„æµ‹æ¸©åº¦è®¾ä¸º 1**ï¼š
   å’Œè®­ç»ƒé‡‡æ ·ä¸€æ ·ï¼Œè¯„æµ‹æ—¶ä¹Ÿç”¨ T=1ï¼Œè®©è¾“å‡ºæœ‰ä¸€å®šéšæœºæ€§ï¼ˆå°¤å…¶æ˜¯ä¸ºäº†ç®— pass@k / avg@k è¿™ç§å¤šå€™é€‰æŒ‡æ ‡ï¼‰ã€‚
- **å¤šæ¬¡è¯„æµ‹ï¼Œå‡å°‘æ–¹å·®**ï¼š
  - å› ä¸ºç”¨ temperature é‡‡æ ·ï¼Œç»“æœä¼šæœ‰éšæœºæ€§ï¼›
  - æ‰€ä»¥ä»–ä»¬å¯¹æ¯ä¸ª benchmark è¯„æµ‹å¤šæ¬¡ï¼Œæœ€åå¯¹æŒ‡æ ‡ï¼ˆæ¯”å¦‚ avg@kï¼‰å–å¹³å‡ï¼Œå‡å°‘ä¸€æ¬¡æ€§è¯„æµ‹çš„éšæœºæ³¢åŠ¨ã€‚

6. **æœ€å¤§ç”Ÿæˆé•¿åº¦ï¼š3072**

> The max generation length for training and evaluation is 3072, with minimal truncation observed.

- **max generation length = 3072 tokens**ï¼š
  - æ— è®ºè®­ç»ƒè¿˜æ˜¯è¯„æµ‹æ—¶ï¼Œå›ç­”æœ€é•¿ä¸è¶…è¿‡ 3072 tokenï¼›
  - è¶…äº†å°±æˆªæ–­ï¼ˆtruncationï¼‰ã€‚
- **â€œwith minimal truncationâ€**ï¼š
  - è¯´æ˜å¤§éƒ¨åˆ†ç­”æ¡ˆåœ¨è¿™ä¸ªé•¿åº¦ä¹‹å‰å·²ç»ç»“æŸï¼›
  - æˆªæ–­æƒ…å†µå¾ˆå°‘ï¼Œè¯´æ˜è¿™ä¸ªé•¿åº¦è¶³å¤Ÿè¦†ç›–ä»–ä»¬æ‰€éœ€çš„æ¨ç† + ç­”æ¡ˆé•¿åº¦ã€‚

7. **baseline çš„è¯„ä¼°è®¾ç½®**

> For baseline evaluation, we adopt the default generation temperature from the original papers. For baseline evaluation, we follow the corresponding papers to select generation parameters and use our setup if the original paper uses greedy decoding.

è¿™ä¸¤å¥å…¶å®æ˜¯ä¸€å›äº‹ï¼Œæ„æ€æ˜¯ï¼š

- å¯¹äºè¦å¯¹æ¯”çš„ **baseline æ¨¡å‹**ï¼ˆåˆ«äººçš„æ–¹æ³•ï¼‰ï¼Œè¯„ä¼°æ—¶å°½é‡ï¼š
  - **æ²¿ç”¨åŸè®ºæ–‡é‡Œçš„ç”Ÿæˆå‚æ•°**ï¼ˆtemperatureã€top-p ç­‰ï¼‰ï¼›
  - å¦‚æœåŸè®ºæ–‡ç”¨çš„æ˜¯ greedy decodingï¼Œé‚£ä»–ä»¬å°±ç”¨è‡ªå·±çš„ç»Ÿä¸€è®¾å®šæ¥è·‘ï¼Œä»¥ä¿æŒå¯¹æ¯”å…¬å¹³å’Œå¯å®ç°ã€‚
- è¿™æ ·åšæ˜¯ï¼š
  - å°½é‡ä¿æŒâ€œbaseline ç”¨å®ƒä½œè€…è®¤ä¸º best çš„è®¾ç½®â€ï¼›
  - ä½†å½“è®¾ç½®ä¸é€‚åˆ / æ²¡æ³•é‡ç°æ—¶ï¼Œç”¨ä¸€ä¸ªç»Ÿä¸€æ–¹æ¡ˆã€‚

8. **`<think></think><answer></answer> `æ¨¡æ¿ï¼šæ€ä¹ˆæŠ½å–æœ€ç»ˆç­”æ¡ˆ**

> For reliable answer extraction, we adopt the â€œ<think></think><answer></answer>â€ template of R1 (Liu et al., 2025b) during training and use the striped content inside answer tags as the generated answer.

- åœ¨ R1 ç³»åˆ—é‡Œï¼Œè¾“å‡ºé€šå¸¸ä¼šé•¿è¿™æ ·ï¼š

```
<think>
  è¿™é‡Œæ˜¯æ€ç»´é“¾ / è‰ç¨¿ / æ¨ç†è¿‡ç¨‹â€¦â€¦
</think>
<answer>
  è¿™é‡Œæ˜¯æœ€ç»ˆç®€æ´ç­”æ¡ˆ
</answer>
```

- ä»–ä»¬è®­ç»ƒæ—¶ä¹Ÿç”¨è¿™ä¸ªæ¨¡æ¿ï¼š
  - è®©æ¨¡å‹æŠŠæ¨ç†è¿‡ç¨‹æ”¾åœ¨ `<think>` é‡Œï¼›
  - æŠŠçœŸæ­£çš„ä»»åŠ¡ç­”æ¡ˆæ”¾åœ¨ `<answer>` é‡Œã€‚
- **è¯„åˆ† /è¯„æµ‹æ—¶**ï¼š
  - **åªå– `<answer>...</answer>` ä¸­é—´çš„å†…å®¹**ï¼Œä½œä¸ºâ€œæ¨¡å‹å›ç­”â€ï¼›
  - `<think>` åªæ˜¯â€œæ¨ç†è¿‡ç¨‹â€ï¼Œä¸å‚ä¸è‡ªåŠ¨åˆ¤åˆ†ï¼ˆé¿å…å½±å“è§£æï¼‰ã€‚

9. **å¯¹ Gemma / Llama çš„ç‰¹æ®Šå¤„ç†**

> For experiments on Gemma and Llama, we change the training and evaluation temperature to 0.6 and remove the <think> part in templates to prevent generation degradation.

ä»–ä»¬å‘ç°ï¼š

- åœ¨ **Gemma / Llama** è¿™äº›æ¨¡å‹ä¸Šï¼Œå¦‚æœï¼š
  - æ¸©åº¦å¤ªé«˜ï¼ˆT=1ï¼‰ï¼Œ
  - æˆ–å¼ºè¡Œè®©å®ƒä»¬è¾“å‡º `<think>` æ€ç»´é“¾ï¼Œ
- ä¼šå¯¼è‡´ç”Ÿæˆè´¨é‡**ä¸‹é™ï¼ˆdegradationï¼‰**ï¼šæ¯”å¦‚å‘æ•£ã€èƒ¡è¨€ä¹±è¯­ã€æ ¼å¼ä¹±ç­‰ã€‚

æ‰€ä»¥é’ˆå¯¹è¿™ä¸¤ä¸ªæ¨¡å‹ä»–ä»¬è°ƒæ•´ç­–ç•¥ï¼š

1. **æŠŠæ¸©åº¦é™åˆ° 0.6**ï¼š
   - å‡å°‘éšæœºæ€§ï¼Œè®©è¾“å‡ºæ›´ç¨³å®šã€‚
2. **æŠŠ `<think>` å»æ‰**ï¼š
   - åªè®©æ¨¡å‹ç›´æ¥ç»™ç­”æ¡ˆï¼Œä¸å¼ºåˆ¶è¾“å‡ºæ˜¾å¼æ€ç»´é“¾ï¼›
   - è¿™æ ·æ›´ç¬¦åˆè¿™äº› base æ¨¡å‹çš„ä¹ æƒ¯ / è®­ç»ƒæ–¹å¼ï¼Œé¿å…ä¹±å¥—ã€‚

10.  **è‡ªåŠ¨åˆ¤åˆ†ï¼šä¸å†åªé â€œè§„åˆ™è„šæœ¬â€**

> We observe that rule-based scoring scripts introduce errors in benchmarks containing question formats beyond multiple-choice.

ä»–ä»¬å‘ç°ï¼š

- æœ€å¼€å§‹ç”¨çš„æ˜¯ **rule-based scoring scripts**ï¼Œæ¯”å¦‚ï¼š
  - æ­£åˆ™è¡¨è¾¾å¼æŠ½ç­”æ¡ˆï¼›
  - ç®€å•å¯¹æ¯”å­—ç¬¦ä¸² / é€‰é¡¹ï¼›
  - é’ˆå¯¹ choice ç±»é¢˜ç›®å†™çš„ if-else åˆ¤åˆ†ã€‚
- ä½†å¯¹äº **éé€‰æ‹©é¢˜** æˆ–å¤æ‚é¢˜ï¼ˆæ¯”å¦‚å¼€æ”¾é—®ç­”ã€è¯æ˜é¢˜ã€å¤æ‚æ•°å­¦ï¼‰ï¼Œè¿™ç§â€œè§„åˆ™è„šæœ¬â€ç»å¸¸ä¼šï¼š
  - æŠ½é”™ç­”æ¡ˆï¼›
  - ç†è§£ä¸äº†æ ¼å¼å˜åŒ–ï¼›
  - å¯¼è‡´è¯„åˆ†é”™è¯¯ã€‚

> To address this, we deploy a Qwen2.5-7B-Inst model server for evaluation, and additionally leverage GPT-4.1 for more complex benchmarks, such as TheoremQA and Minerva.

ä»–ä»¬çš„è§£å†³æ–¹æ¡ˆï¼š

1. **éƒ¨ç½²ä¸€ä¸ª Qwen2.5-7B-Inst åšè‡ªåŠ¨åˆ¤å·ï¼ˆauto-graderï¼‰**ï¼š
   - æŠŠé¢˜ç›® + å‚è€ƒç­”æ¡ˆ + æ¨¡å‹è¾“å‡ºéƒ½å–‚ç»™å®ƒï¼›
   - è®© Qwen åˆ¤æ–­â€œå¯¹/é”™â€æˆ–ç»™è¯„åˆ†ï¼›
   - å¯¹äºå¤šæ•° benchmarkï¼Œè¿™ä¸ª 7B æ¨¡å‹å°±è¶³å¤Ÿèƒœä»»æ‰“åˆ†ä»»åŠ¡ã€‚
2. **å¯¹ç‰¹åˆ«å¤æ‚çš„ benchmarkï¼ˆTheoremQAã€Minerva è¿™ç±»é«˜éš¾æ•°å­¦ /ç†è®ºé¢˜ï¼‰**ï¼š
   - å†ç”¨æ›´å¼ºçš„ **GPT-4.1** æ¥åˆ¤åˆ†ï¼›
   - å› ä¸ºè¿™äº›é¢˜éœ€è¦æ›´å¼ºçš„ç†è§£å’Œæ•°å­¦èƒ½åŠ›ï¼Œ7B æ¨¡å‹å¯èƒ½å®¹æ˜“è¯¯åˆ¤ã€‚

è¿™å°±æ˜¯å…¸å‹çš„ **â€œç”¨ LLM å½“è‡ªåŠ¨é˜…å·è€å¸ˆâ€**ï¼Œæ›¿ä»£è„†å¼±çš„è§„åˆ™è„šæœ¬ã€‚

### 3. å¤ç°ç»†èŠ‚























